<html><head></head><body>
        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Finalizing Java Knowledge to a Professional Level</h1>
            

            <article>
                
<p class="calibre2">By now, you have learned the most important areas and topics needed for a professional Java developer. What we still have ahead of us in this book is to discuss some topics that will lead you from being a junior developer to a senior developer. Reading this chapter will not make anyone a senior developer, though. The previous chapters were the roads that we walked through. This chapter is only the map. If each of the previous chapters covered a short walk of a few miles in the journey of coding to reach the harbor, then this chapter is the nautical map to discover a new continent.</p>
<p class="calibre2">We will briefly bite into some very deep and high-level professional areas, such as creating a Java agent, compile-time annotation processing, polyglot programming, a bit of architecture design and tools, and techniques to work in teams. We'll do it just for the taste. Now, you have enough knowledge to understand the importance of these topics, and getting a taste will create an appetite for the coming years of self-development, or, at least, that is my intention to make you, the reader, addicted.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Java deep technologies</h1>
            

            <article>
                
<p class="calibre2">In this section, we will list three technologies:</p>
<ul class="calibre14">
<li class="calibre15">Java agent</li>
<li class="calibre15">Polyglot programming</li>
<li class="calibre15">Annotation processing</li>
</ul>
<p class="calibre2">Knowing them is not a must for a Java professional. Knowing about them is. Java agents are used mainly in development environments and in operation. They are complex runtime technologies that interact with the already running <em class="calibre12">JVM</em>. Annotation processing is the other end. Annotation processors are plugged into the Java compiler. Polyglot programming is in the middle. It is JVM programming, just like programming in Java, but by using some different language or, perhaps, some different language and Java together. Or even many languages, such as Jython, Groovy, Clojure, and Java together.</p>
<p class="calibre2">We will discuss these technologies so that we will get some idea about what they are and where to look for further information in case we want to learn more about them.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Java agent</h1>
            

            <article>
                
<p class="calibre2">A Java agent is a Java program that is loaded by the Java runtime in a special way and can be used to interfere with the byte code of the loaded classes, altering them. They can be used to:</p>
<ul class="calibre14">
<li class="calibre15">List or log, and report the loaded classes during runtime, as they are loaded</li>
<li class="calibre15">Modify the classes so that the methods will contain extra code to report runtime behavior</li>
<li class="calibre15">Support debuggers to alter the content of a class as the developer modifies the source code</li>
</ul>
<p class="calibre2">This technology is used in, for example, the products <strong class="calibre1">JRebel</strong> and <strong class="calibre1">XRebel</strong> from <a href="https://zeroturnaround.com/" class="calibre6"><span>https://zeroturnaround.com/</span></a>.</p>
<p class="calibre2">Although Java agents work in the deep details of Java, they are not magic. They are a bit complex and you need a deep understanding of Java, but anyone who can program in Java can write a Java agent. All that is required is that the class, which is the agent, has some predefined methods packaged into a <em class="calibre12">JAR</em> file along with the other classes of the agent and has a <a class="calibre6"><kbd class="calibre28">META-INF/MANIFEST.MF</kbd> </a>file that defines the names of the classes implementing the <kbd class="calibre11">premain</kbd> and/or <kbd class="calibre11">agentmain</kbd> methods, and some other fields.</p>
<p class="calibre2">The detailed and precise reference documentation is part of the <em class="calibre12">JDK JavaDoc</em> available at <a href="http://download.java.net/java/jdk9/docs/api/" class="calibre6"><span>http://download.java.net/java/jdk9/docs/api/</span></a> in the documentation of the <kbd class="calibre11">java.lang.instrument</kbd> package.</p>
<p class="calibre2">When a Java application is started with a Java agent, the command line has to contain the following option:</p>
<pre class="calibre20">
    <strong class="calibre1">-javaagent:jarpath[=options]</strong>
</pre>
<p class="calibre2">Here, <kbd class="calibre11">jarpath</kbd> points to the JAR file that contains the agent class and the manifest file. The class must have a method named <kbd class="calibre11">premain</kbd> or <kbd class="calibre11">agentmain</kbd>. It may have one or two arguments. The JVM tries to call the two-argument version first after the JVM is initialized:</p>
<pre class="calibre20">
public static void premain(String agentArgs, Instrumentation inst);
</pre>
<p class="calibre2">If a two-argument version does not exist, then the one-argument version is used, which is essentially the same as the two-argument version but misses the instrumentation argument, which, in my opinion, does not make too much sense since a Java agent cannot do much without the <kbd class="calibre11">Instrumentation</kbd> object:</p>
<pre class="calibre20">
public static void premain(String agentArgs);
</pre>
<p class="calibre2">The <kbd class="calibre11">agentArgs</kbd> parameter is the string passed as an option on the command line. The second argument, <kbd class="calibre11">Instrumentation</kbd>, provides methods to register class transformers that can modify class byte codes and also methods that can ask the JVM to perform redefinition or retransformation of classes during runtime.</p>
<p class="calibre2">Java applications can also load an agent after the program has already started. In such a case, the agent cannot be invoked before the main method of the Java application, since it has already started by that time. To separate the two cases, JVM calls <kbd class="calibre11">agentmain</kbd> in such a scenario. Note that either <kbd class="calibre11">premain</kbd> or <kbd class="calibre11">agentmain</kbd> is invoked for an agent and never both. A single agent can implement both so that it is capable of performing its task loaded at the startup, specified on the command line or after the JVM started.</p>
<p class="calibre2">If <kbd class="calibre11">agentmain</kbd> is used, it has the same arguments as <kbd class="calibre11">premain</kbd>.</p>
<p class="calibre2">There is one major and important difference between the invocation of <kbd class="calibre11">premain</kbd> and <kbd class="calibre11">agentmain</kbd>. If an agent cannot be loaded during startup, for example, if it cannot be found, if the JAR file does not exist, if the class does not have the <kbd class="calibre11">premain</kbd> method, or if it throws an exception, the JVM will abort. If the agent is loaded after the <em class="calibre12">JVM</em> is started (in this case, <kbd class="calibre11">agentmain</kbd> is to be used), the JVM will not abort if there is some error in the agent.</p>
<div class="packttip">This approach is fairly reasonable. Imagine that there is a server application that runs on the Tomcat servlet container. When a new version is started, the system is down for a maintenance period. If the new version cannot be started because the agent is not behaving well, then it is better not started. The damage to debug the situation and fix it, or roll back the application to the old version and call for a longer fixing session may be less than starting up the application and not having the proper agent functionality. If the application starts up only without the agent, then the suboptimal operation may not immediately be recognized.<br class="calibre23"/>
On the other hand, when an agent is attached later, the application is already running. An agent is attached to an already running application to get information from an already running instance. To stop the already running instance and fail it, especially in an operational environment, is more damaging than just not attaching the agent. It may not go unnoticed anyway because the agent that is most probably attached is used by operational personnel.</div>
<p class="calibre2">A <kbd class="calibre11">premain</kbd> or <kbd class="calibre11">agentmain</kbd> agent gets an <kbd class="calibre11">Instrumentation</kbd> object as the second argument. This object implements several methods. One of them is:</p>
<pre class="calibre20">
void addTransformer(ClassFileTransformer transformer)
</pre>
<p class="calibre2">The agent implements the transformer, and it has the <kbd class="calibre11">transform</kbd> method signature:</p>
<pre class="calibre20">
byte[] transform(Module module, ClassLoader loader, <br class="title-page-name"/>                 String className, <br class="title-page-name"/>                 Class&lt;?&gt; classBeingRedefined, <br class="title-page-name"/>                 ProtectionDomain protectionDomain, <br class="title-page-name"/>                 byte[] classfileBuffer) <br class="title-page-name"/>throws IllegalClassFormatException
</pre>
<p class="calibre2">This method is called by the JVM when a class is loaded or when it is to be transformed. The method gets the class object itself, but, more importantly, it gets the byte array containing the byte code of the class. The method is expected to return the byte code of the transformed class. Modifying the byte code needs some knowledge of how the byte code is built and what the structure of a class file is. There are libraries that help to do that, such as Javassist (<a href="http://www.javassist.org/" class="calibre6"><span>http://www.javassist.org/</span></a> ) or ASM (<a href="http://asm.ow2.org/" class="calibre6"><span>http://asm.ow2.org/</span></a>). Nevertheless, I will not start coding before getting acquainted with the structure of the byte code.</p>
<p class="calibre2">Agents, running in a separate thread and presumably interacting with the user or the filesystem and based upon some external observation at any time, may call the following method to perform the retransformation of the classes using the registered transformers:</p>
<pre class="calibre20">
void retransformClasses(Class&lt;?&gt;... classes)
</pre>
<p class="calibre2">The agent can also call the following method, which will redefine the classes given as arguments:</p>
<pre class="calibre20">
void redefineClasses(ClassDefinition... definitions)
</pre>
<p class="calibre2">The <kbd class="calibre11">ClassDefinition</kbd> class is simply a <kbd class="calibre11">Class</kbd> and a <kbd class="calibre11">byte[]</kbd> pair. This will redefine the classes through the class maintaining mechanism of the JVM.</p>
<p class="calibre2">Note that these methods and Java agents interact with the deep, low-level part of the JVM. This also bears the consequence that it is very easy to destroy the whole JVM. The byte code is not checked, unlike during the loading of the class, and thus, if there is some error in it, the consequence may not only be an exception but also the crashing of the JVM. Also, the redefinition and the transformations should not alter the structure of the classes. They should not change their inheritance footprint, add, rename, or remove methods, or change the signature of the methods, and this is also true for fields.</p>
<p class="calibre2">Also note that the already created objects will not be affected by the changes; they will still use the old definition of the class and only new instances will be affected.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Polyglot programming</h1>
            

            <article>
                
<p class="calibre2">Polyglot programming is the technique when there are different programming languages used in the same application. Such an approach is not only appropriate when a different part of the application runs on a different environment. For example, the client executes in the browser using JavaScript, CSS, and HTML while the server is programmed to run in a Tomcat environment in Java. This is a different story, and, usually, this is not the typical use when someone is speaking about polyglot programming.</p>
<p class="calibre2">When the application that runs on the server partially runs in Java and also in some other language, then we can speak about polyglot programming. For example, we create the order handling application in Java and some of the code that checks the correctness of the order based on the product-specific codes that the order contains is written in JavaScript. Does it ring a bell? We have already done that in this book to demonstrate the scripting API of the JDK. That was real polyglot programing even if we did not mention it that way.</p>
<p class="calibre2">The JVM that runs the compiled Java code is a very good target for different language compilers, and thus, there are many languages that compile for it. When the JVM runs the byte code of a class, it does not know what the source language was, and it does not really care; some compiler created the byte code and it just executes that.</p>
<p class="calibre2">We can use different languages, such as Jython, Groovy, and Scala, to name a few popular ones that compile for the JVM. We can write one class using one language and the other one using another. When they are put together into a JAR, WAR, or an EAR file, the runtime system will just run them.</p>
<p class="calibre2">When do we use polyglot programming?</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Polyglot configuration</h1>
            

            <article>
                
<p class="calibre2">Usually, we turn towards polyglot programming when we want to create an application that is more flexible and more configurable. Applications that get installed in many instances, usually, at different customer sites have some configurations. These configurations can be XML files, properties files, and INI files (those come from Windows). As the programs develop sooner or later, these static configuration possibilities reach their limits. Application developers soon see that they need to configure some functionality that is cumbersome to describe using these technologies. Configuration files start being larger and, also, the code that reads and interprets the configuration files grow large. Good developers have to realize that this is the situation, and before the configuration files and the code handling them become unmanageable, some scripting configuration, polyglot programming has to be implemented.</p>
<div class="packtfigure"><img class="image-border47" src="../images/00064.gif"/></div>
<p class="calibre2">Decent developer teams may reach a point when they develop their configuration language and the interpreter of that language. It can be based on XML, or it can just be any other language. After all, writing a language is fun; I have done it a few times myself. Most of these were, however, hobbies and not professional projects. Usually, there is no customer value in crafting another language. We can better use an existing one.</p>
<p class="calibre2">In the case of configuration, Groovy is a very handy language that supports complex closure and meta-class syntax and implementation. This way, the language is extremely suitable to create a domain-specific language. Since Groovy is compiled to JVM, Groovy classes can be invoked directly from Java, and in the other way round, reading the configuration is essentially invoking the class compiled from the configuration file. The compilation can be during application build time, but in the case of configuration, it makes more sense to do it during application startup. We have already seen that the Groovy implementation of the scripting API or the special API that Groovy provides is absolutely capable of doing that.</p>
<p class="calibre2">Have we seen examples of this in our book? It may be a surprise to you, but we have in fact used Groovy to describe some configuration many times. <em class="calibre12">Gradle</em> build files are nothing more than Groovy DSL developed mainly in Groovy to support project build configuration.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Polyglot scripting</h1>
            

            <article>
                
<p class="calibre2">Configuration is not the only application of polyglot programming. Configuration is executed at the program startup and the configuration data is used as static data afterwards. We can execute scripts during the application's execution any time and not only during its startup. This can be used to provide extra functionality to the program's user with installations that use the same application but are furnished with different scripts.</p>
<div class="packtinfobox">One of the first applications that provided such scripting capability was the emacs editor. The core of the application was written in C language and it contained a Lisp interpreter that let the user to write scripts, which were executed in the editor environment. The engineering program, AutoCAD, also used a Lisp interpreter for similar purposes. Why was Lisp used for this purpose?<br class="calibre23"/>
Lisp has very simple syntax, and therefore, it is easy to parse Lisp code. At the same time, the language is powerful, and last but not least, there were open source Lisp interpreters (at least one) available by the time.</div>
<p class="calibre2">To get this kind of flexibility, applications, many times, provide plugin APIs, which a developer can use to extend the application. This, however, requires that the developer sets up coding tools, including IDE, build tool, continuous integration, and so on, that is, a professional programming environment. When the task to be solved by the plugin is simple, the overhead is simply too large. In such a case, a scripting solution is handier.</p>
<p class="calibre2">Scripting is not a solution for everything. When the scripts extending the application tend to become too complex, it means that the scripting possibility is just too much. It is difficult, however, to take back a toy from a child. If users get used to the scripting possibility, then they will not take it easy if the next version of the application we release does not provide that possibility. Thus, it is extremely important to assess the possible use of the scripting capability in our application. Scripting and, more generally, any feature of our program will not be used for what we intended them for. They will be used for whatever it is possible to use them for. Users can go beyond all imagination when it comes to abusing some feature. It may be a good idea to think about limiting the scripting possibility beforehand, limiting the running time of the scripts or the size of the script our program agrees to work with. If these limitations are set reasonably, and the users understand and accept these, a plugin structure in addition to the scripting capability has to be considered.</p>
<p class="calibre2">The security of an application, including plugin or scripting extension, is also very important. The scripts or plugins run on the same JVM as the core application. Some scripting languages provide some fence around the scripts that limits the access to the core application's objects and classes, but this is an exception. Usually, scripts run with the same privilege as the core application and that way they can do just anything. Thus, scripts should be trusted the same way as the core application. Script installation or modification should never be possible for an unprivileged user of the application. Such an action is almost always reserved for the system administrator.</p>
<p class="calibre2">If an unprivileged user can upload a script to the server and then have it executed, we just opened a security hole in our application. Since access restrictions are enforced by the application, it is easy to override these limitations using an uncontrolled script. The hacker can just access other users' data easily, which he is not entitled to, and read and modify our database.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Business DSL</h1>
            

            <article>
                
<p class="calibre2">Polyglot programming may also come into the picture when the application's code can be separated into business code and technology code. The business code contains the top-level business logic that we actually write the application for, and this is the code that contains the logic that the customer pays for. The technology code is to support the algorithms coded in the business DSL.</p>
<p class="calibre2">Most of the enterprise applications contain these two types of code but many do not separate them. This leads to a monolithic application that contains repetitive code. When you feel that you are writing the same type of code when you need persistence or networking, and again the same type of code while coding some business rules, then this is the code smell that suggests that the two code types are not separated. DSL and scripting are not a magic wand and do not solve all the problems that stem from a wrong application structure. In such a situation, the code has to be refactored first to separate the business logic and the infrastructure code, and it is only the second step to implement a DSL and a business API supporting it and to rewrite the business code into the DSL. Every step of such a project delivers value for the application and even if it never gets to DSL and scripting, the effort invested is not wasted.</p>
<p class="calibre2">The business DSL scripting is very similar to pluggable scripts, except that this time it is not the application that calls the scripts from time to time to execute some special extension functionality. Instead, the DSL code calls the application through the business API that it provides. The advantage of providing the API and using a DSL is that the code that implements the business logic gets rid of the technical details, can be very abstract, and, this way, be much closer to a business-level description of the problem rather than just program code. Even some businessperson can understand a business DSL, and though it is not a goal in real-life examples, they could even write code.</p>
<div class="packtinfobox">At TU Vienna, we also used a similar approach to make semiconductor simulation more usable for the semiconductor design engineer. The core calculating code was written in Fortran. A C language framework that handled the massive simulation data input and output and that embedded the XLISP interpreter executed these programs. The Lisp code contained the simulation configuration data and could also contain simple loops when the simulation was to be executed for many configuration points.<br class="calibre23"/>
It was polyglot programming, except that we did not know that this is going to be the name years after this application coding style.</div>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Problems with polyglot</h1>
            

            <article>
                
<p class="calibre2">Polyglot programming is not only all about advantages. Before jumping into this direction, developers making the decision have to consider a lot of things.</p>
<p class="calibre2">Using another language for the application needs knowledge. Finding people who can code in the languages that are used is eventually more difficult than finding developers who only know Java. (This is also true if the kernel application language is not Java.) Different languages require different mindsets and, many times, different people. The team should also have some members who are proficient in both languages, and it is also an advantage if most of the people know at least a bit about the other language.</p>
<p class="calibre2">The toolset supporting Java is outstanding. The build tools, integrated development environment, libraries, debugging possibilities, and logging frameworks, to name a few, are all extremely good compared with other languages. Polyglot development needs support for the other language as well, which may not be as advanced as the support for Java. Many times, it is really an issue to debug DSL solutions and IDE support may also be lagging.</p>
<p class="calibre2">When we program in Java, many times, we take for granted that the IDE reads the meta-data of the libraries and whenever we need to call a method, or reference a class, the IDE suggests the best possibility. XML and properties files may also be supported and the IDE may know some of the most used frameworks, such as <em class="calibre12">Spring</em>, and understand the XML configuration handling the names of the classes as hyperlinks, even when the class names are inside some attribute strings.</p>
<p class="calibre2">This is far from being this easy in the case of other languages. For the languages that have a wide user base, the tooling support may be good, but if you pick some exotic language, you are on your own. The more exotic the language the less support you may have.</p>
<p class="calibre2">You can create some tool to support your DSL that you develop. It is not hard to do so using tools such as <a href="http://www.eclipse.org/Xtext/" class="calibre6"><span>http://www.eclipse.org/Xtext/</span></a>. In such a case, you are tied to <em class="calibre12">Eclipse</em>, which may or may not be a problem. You can pick a special language, for example, <em class="calibre12">Kotlin</em>, which is extensively supported by <em class="calibre12">IntelliJ</em>, because the same company supports the language and the IDE, but again, you buy into a special technology that can be expensive to replace in case you have to. It is generally true not only for languages but also for any technology you include into your development. When you select one, you should consider the support and the cost of getting off the horse if or when it starts dying.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Annotation processing</h1>
            

            <article>
                
<p class="calibre2">We have already discussed annotations in great detail. You may recall that we defined our annotation interfaces using the following annotation:</p>
<pre class="calibre20">
@Retention(RetentionPolicy.RUNTIME)
</pre>
<p class="calibre2">This told the Java compiler to keep the annotation and put it into the JVM code so that the code can access it during runtime using reflection. The default value is <kbd class="calibre11">RetentionPolicy.CLASS</kbd>, which means that the annotation gets into the byte code, but the JVM does not make it available for the runtime system. If we use <kbd class="calibre11">RetentionPolicy.SOURCE</kbd>, the annotation does not even get into the class file. In this case, there is only one possibility to do anything with the annotation: compile time.</p>
<p class="calibre2">How can we write code that runs during compile time? Java supports the notion of annotation processors. If there is a class on the classpath of the compiler that implements the <kbd class="calibre11">javax.annotation.processing.Processor</kbd> interface, then the compiler will invoke the implemented methods one or more times, passing information about the source file that the compiler is actually processing. The methods will be able to access the compiled methods, classes, or whatever is annotated, and also the annotation that triggered the processor invocation. It is important, however, that this access is not the same as in runtime. What the annotation processor accesses is neither a compiled nor a loaded class, that is, it is available when the code uses reflection. The source file at this time is under compilation; thus, the data structures that describe the code are actually structures of the compiler, as we will see in our next example.</p>
<p class="calibre2">The annotation processor is called one or more times. The reason it is invoked many times is that the compiler makes it possible for the annotation processors to generate source code based on what it sees in the partially compiled source code. If the annotation processor generates any Java source file, the compiler has to compile the new source code and perhaps compile some of the already compiled files again. This new compilation phase needs annotation processor support until there are no more rounds to execute.</p>
<p class="calibre2">Annotation processors are executed one after the other, and they work on the same set of source files. There is no way to specify the order of the annotation processor executions; thus, two processors working together should perform their tasks, no matter in what order they are invoked. Also, note that these codes run inside the compiler. If an annotation processor throws an exception, then the compilation process will most probably fail. Thus, throwing an exception out of the annotation processor should only be done if there is an error that cannot be recovered and the annotation processor decides that the compilation after that error cannot be complete.</p>
<p class="calibre2">When the compiler gets to the phase to execute the annotation processors, it looks at the classes that implement the <kbd class="calibre11">javax.annotation.processing.Processor</kbd> interface and creates instances of these classes. These classes have to have a public no-argument constructor. To streamline the execution of the processors and to invoke a processor only for the annotations that it can handle, the interface contains two methods:</p>
<ul class="calibre14">
<li class="calibre15"><kbd class="calibre11">getSupportedSourceVersion</kbd> to return the latest version the annotation processor can support</li>
<li class="calibre15"><kbd class="calibre11">getSupportedAnnotationTypes</kbd> to return a set of <kbd class="calibre11">String</kbd> objects containing the fully qualified class name of the annotations that this processor can handle</li>
</ul>
<p class="calibre2">If an annotation processor was created for Java 1.8, it may work with Java 9, but it may also not work. If it declares that the latest supported version is 1.8, then the compiler in a Java 9 environment will not invoke it. It is better not to invoke an annotation processor than calling it and messing up the compilation process, which may even create compiled but erroneous code.</p>
<p class="calibre2">The values returned by these methods are fairly constant for an annotation processor. An annotation processor will return the same source version it can handle and will return the same set of annotations. Therefore, it would be clever to have some way to define these values in the source code in a declarative manner.</p>
<p class="calibre2">It can be done when we extend the <kbd class="calibre11">javax.annotation.processing.AbstractProcessor</kbd> class instead of directly implementing the <kbd class="calibre11">Processor</kbd> interface. This abstract class implements these methods. Both of them get the information from the annotation so that we can decorate the class that extends the abstract class. For example, the <kbd class="calibre11">getSupportedAnnotationTypes</kbd> method looks at the <a class="calibre6"><kbd class="calibre28">SupportedAnnotationTypes</kbd></a> annotation and returns an array of annotation type strings that are listed in the annotation.</p>
<p class="calibre2">Now, this is a bit brain twisting and can also be confusing at first. We are executing our annotation processor during compile time. But the compiler itself is a Java application, and in this way, the time is runtime for the code that runs inside the compiler. The code of <kbd class="calibre11">AbstractProcessor</kbd> accesses the <kbd class="calibre11">SupportedAnnotationTypes</kbd> annotation as a runtime annotation using reflection methods. There is no magic in it. The method in the JDK 9 is as follows:</p>
<pre class="calibre20">
public Set&lt;String&gt; getSupportedAnnotationTypes() { <br class="title-page-name"/>    SupportedAnnotationTypes sat = this.getClass().getAnnotation <br class="title-page-name"/>    (SupportedAnnotationTypes.class); <br class="title-page-name"/>    if  (sat == null) { <br class="title-page-name"/>        ... error message is sent to compiler output ... <br class="title-page-name"/>        return Collections.emptySet(); <br class="title-page-name"/>    } <br class="title-page-name"/>    else <br class="title-page-name"/>        return arrayToSet(sat.value()); <br class="title-page-name"/>}
</pre>
<p class="calibre2">(The code has been edited for brevity.)</p>
<p class="calibre2">To have an example, we will sort of look at the code of a polyglot annotation processor. Our very simple annotation processor will process one simple annotation: <kbd class="calibre11">com.javax0.scriapt.CompileScript</kbd>, which can specify a script file. The annotation processor will load the script file and execute it using the scripting interface of Java 9.</p>
<div class="packtinfobox">This code was developed as a demonstration code by the author of this book a few years ago and is available with the Apache license from GitHub. Thus, the package of the classes is retained.</div>
<p class="calibre2">The annotation processor contains two code files. One of the annotation itself that the processor will work on:</p>
<pre class="calibre20">
@Retention(RetentionPolicy.SOURCE) <br class="title-page-name"/>@Target(ElementType.TYPE) <br class="title-page-name"/>public @interface <a class="calibre26">CompileScript </a>{ <br class="title-page-name"/>    String value(); <br class="title-page-name"/>    String engine() default ""; <br class="title-page-name"/>}
</pre>
<p class="calibre2">As you can see, this annotation will not get into the class file after compilation; thus, there will be no trace during runtime so that any class source may occasionally use this annotation. <kbd class="calibre11">Target</kbd> of the annotation is <kbd class="calibre11">ElementType.TYPE</kbd>, meaning that this annotation can only be applied to those Java 9 language constructs that are some kind of types: <kbd class="calibre11">class</kbd>, <kbd class="calibre11">interface</kbd>, and <kbd class="calibre11">enum</kbd>.</p>
<p class="calibre2">The annotation has two parameters. The value should specify the name of the script file, and the engine may optionally define the type of the script that is in that file. The implementation we'll create will try to identify the type of the script from the filename extension, but if somebody would like to bury some Groovy code into a file that has the <kbd class="calibre11">.jy</kbd> extension (which is usually for Jython), so be it.</p>
<p class="calibre2">The processor extends <kbd class="calibre11">AbstractProcessor</kbd> and, in this way, some of the methods are inherited at the expense of some annotations used in the class:</p>
<pre class="calibre20">
package com.javax0.scriapt; <br class="title-page-name"/>import ... <br class="title-page-name"/>@SupportedAnnotationTypes("com.javax0.scriapt.CompileScript") <br class="title-page-name"/>@SupportedSourceVersion(SourceVersion.RELEASE_9) <br class="title-page-name"/>public class Processor extends AbstractProcessor {
</pre>
<p class="calibre2">There is no need to implement the <kbd class="calibre11">getSupportedAnnotationTypes</kbd> and <kbd class="calibre11">getSupportedSourceVersion</kbd> methods. These are replaced by the use of the annotations on the class. We support only one annotation in this processor, the one that we defined in the previously listed source file, and we are prepared to manage the source code up to Java version 9. The only method we have to override is <kbd class="calibre11">process</kbd>:</p>
<pre class="calibre20">
@Override <br class="title-page-name"/>public boolean process( <br class="title-page-name"/>    final Set&lt;? extends TypeElement&gt; annotations, <br class="title-page-name"/>    final RoundEnvironment roundEnv) { <br class="title-page-name"/>        for (final Element rootElement : <br class="title-page-name"/>            roundEnv.getRootElements()) { <br class="title-page-name"/>                try { <br class="title-page-name"/>                    processClass(rootElement); <br class="title-page-name"/>                }  <br class="title-page-name"/>                catch (Exception e) { <br class="title-page-name"/>                    throw new RuntimeException(e); <br class="title-page-name"/>                } <br class="title-page-name"/>            } <br class="title-page-name"/>        return false; <br class="title-page-name"/>    }
</pre>
<p class="calibre2">This method gets two arguments. The first is the set of annotations that it was invoked for. The second is the round environment. Because the processor can be invoked many times, the different invocations may have different environments. Each invocation is in a round and the <kbd class="calibre11">RoundEnvironment</kbd> argument is an object that can be used to get information about the given round. It can be used to get the root elements of the round for which this annotation is invoked. In our case, this will be a set of class elements that have the <kbd class="calibre11">CompileScript</kbd> annotation. We iterate over this set, and for each class, we invoke the <kbd class="calibre11">processClass</kbd> method (see the next code snippet). The method may throw some checked exception and the method process cannot because it should match the same method of the interface. Thus, we catch any exception that may be thrown and we re-throw these encapsulated in <kbd class="calibre11">RunTimeException</kbd>. If any of these exceptions are thrown by the called method, then the compilation could not run the scripts and it should be treated as failed. The compilation should not succeed in such a case:</p>
<pre class="calibre20">
private void processClass(final Element element) <br class="title-page-name"/>    throws ScriptException, FileNotFoundException { <br class="title-page-name"/>        for (final AnnotationMirror annotationMirror : <br class="title-page-name"/>            element.getAnnotationMirrors()) { <br class="title-page-name"/>                processAnnotation(annotationMirror); <br class="title-page-name"/>        } <br class="title-page-name"/>    }
</pre>
<p class="calibre2">The actual annotation is not available during compile time as we already mentioned. Hence, what we have available is only a compile time mirror image of the annotation. It has the <kbd class="calibre11">AnnotationMirror</kbd> type, which can be used to get the actual type of the annotation and, also, the values of the annotation. The type of the annotation is available during compile time. The compiler needs it; otherwise, it could not compile the annotation. The values are available from the annotation itself. Our <kbd class="calibre11">processAnnotation</kbd> method handles each annotation it gets as an argument:</p>
<pre class="calibre20">
private void processAnnotation( <br class="title-page-name"/>    final AnnotationMirror annotationMirror) <br class="title-page-name"/>    throws ScriptException, FileNotFoundException { <br class="title-page-name"/>        final String script = <br class="title-page-name"/>            FromThe.annotation(annotationMirror). <br class="title-page-name"/>            getStringValue(); <br class="title-page-name"/>        final String engine = <br class="title-page-name"/>            FromThe.annotation(annotationMirror). <br class="title-page-name"/>            getStringValue("engine"); <br class="title-page-name"/>        execute(script, engine); <br class="title-page-name"/>    }
</pre>
<p class="calibre2">Our <kbd class="calibre11">@CompileScript</kbd> annotation defines two parameters. The first value is the script filename and the second one is the scripting engine name. If the second one is not specified, then an empty string is set as the default value. The <kbd class="calibre11">execute</kbd> method is called for each and every occasion of the annotation:</p>
<pre class="calibre20">
private void execute(final String scriptFileName, <br class="title-page-name"/>                    final String engineName) <br class="title-page-name"/>    throws ScriptException, FileNotFoundException { <br class="title-page-name"/>        final ScriptEngineManager factory = <br class="title-page-name"/>        new ScriptEngineManager(); <br class="title-page-name"/>        final ScriptEngine engine; <br class="title-page-name"/>        if (engineName != null &amp;&amp; engineName.length() &gt; 0) { <br class="title-page-name"/>            engine = factory.getEngineByName(engineName); <br class="title-page-name"/>        }  <br class="title-page-name"/>        else { <br class="title-page-name"/>            engine = <br class="title-page-name"/>            factory.getEngineByExtension <br class="title-page-name"/>            (getExtensionFrom(scriptFileName)); <br class="title-page-name"/>        } <br class="title-page-name"/>        Reader scriptFileReader = new FileReader <br class="title-page-name"/>        (new File(scriptFileName)); <br class="title-page-name"/>        engine.eval(scriptFileReader); <br class="title-page-name"/>    }
</pre>
<p class="calibre2">The method tries to load the script, based on the filename, and tries to instantiate the script engine, based on the given name. If there is no name given, then the filename extension is used to identify the scripting engine. By default, the JavaScript engine is on the classpath as it is part of the JDK. If any other JVM-based scripting engine is in use, then it has to be made available on the classpath or on the module path.</p>
<p class="calibre2">The last method of the class is a simple script manipulation method, nothing special. It just chops off the filename extension so that the engine can be identified based on the extension string:</p>
<pre class="calibre20">
private String getExtensionFrom(final String scriptFileName) { <br class="title-page-name"/>    final int indexOfExtension = scriptFileName.lastIndexOf('.'); <br class="title-page-name"/>    if (indexOfExtension == -1) { <br class="title-page-name"/>        return ""; <br class="title-page-name"/>    }  <br class="title-page-name"/>    else { <br class="title-page-name"/>        return scriptFileName.substring(indexOfExtension + 1); <br class="title-page-name"/>    } <br class="title-page-name"/>}
</pre>
<p class="calibre2">And just for the sake of completeness, we have the closing brace of the class:</p>
<pre class="calibre20">
}
</pre>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Programming in the enterprise</h1>
            

            <article>
                
<p class="calibre2">When a professional works for an enterprise, she does not work alone. There are a lot of people, developers as well as other coworkers, we have to cooperate with. The older the IT department of the enterprise is, and the larger the enterprise is, the more specialized roles people are in. You will certainly meet business analysts, project managers, test engineers, build engineers, subject-matter experts, testers, architects, scrum masters, and automation engineers, to name a few roles. Some of these roles may overlap, no person may have more than one responsibility, and while in other cases, some roles could even be more specialized. Some of the roles are very technical and require less business-related knowledge; others are more business oriented.</p>
<p class="calibre2">Working together as a team with so many people and with so many different roles is not simple. The complexity of the task may be overwhelming for a novice developer and cannot be done without definite policies that all members of the operation follow, more or less. Perhaps your experience will show that it is more times less than more, but that is a different story.</p>
<p class="calibre2">For the way developers work together, there are well-established industry practices. These support the <strong class="calibre1">Software Development Lifecycle</strong> (<strong class="calibre1">SDLC</strong>) using waterfall, agile, or a mix of the two models in some way. In the following sections, we will look at tools and techniques that are, or at least should have been, used in every software development organization. These are:</p>
<ul class="calibre14">
<li class="calibre15">Static code analysis tools that control the quality of the code examining the source code</li>
<li class="calibre15">Source code version control that stores all the versions of the source code and help get the source code for any old version of the development</li>
<li class="calibre15">Software versioning to keep some order of how we identify the different versions and do not get lost among the different versions</li>
<li class="calibre15">Code review and tools that help in pin-pointing bugs that are not revealed by tests and aid knowledge sharing</li>
<li class="calibre15">Knowledge base tools to record and document the findings</li>
<li class="calibre15">Issue tracking tools that record bugs, customer issues, and other tasks that somebody has to attend to</li>
<li class="calibre15">Selection process and considerations for external products and libraries</li>
<li class="calibre15">Continuous integration that keeps the software in a consistent state and reports immediately if there is some error in it before the error propagates to other versions or other code, depending on how the erroneous code gets developed</li>
<li class="calibre15">Release management, which keeps track of the different release versions of the software</li>
<li class="calibre15">Code repository, which stores the compiled and packed artifacts</li>
</ul>
<p class="calibre2">The following diagram shows the most widely used tools for these tasks:</p>
<div class="packtfigure"><img class="image-border48" src="../images/00065.gif"/></div>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Static code analysis</h1>
            

            <article>
                
<p class="calibre2">Static code analysis tools read the code just like the compiler and analyze it, but instead of compilation, they try to find errors or mistakes in it. Not the syntax errors. For that, we already have the Java compiler. Mistakes, such as using a loop variable outside a loop, which may be absolutely valid but is usually bad style and, many times, such usage comes from some simple mistakes. They also check that the code follows the styling rules that we set.</p>
<p class="calibre2">Static code analyzers help identify many small and obvious errors in the code. Sometimes, they are annoying, warning about something that may not be really a problem. In such a case, it is better to code the program a bit differently, not because we want the static code analysis to run without warning. We should never modify the code because of a tool. If we code something in such a way that it passes some quality check tool and not because it is better that way, then we are serving the tools instead of the tools serving us.</p>
<p class="calibre2">The reason to change the code to pass the code analysis is that it is very probable that the code is more readable to an average programmer if it does not violate the coding style. You or the other team members can be excellent programmers who understand the code very easily even if it uses some special construct. However, you cannot say that about all the programmers who will maintain your code in the future. The code lives a long life. I work with some programs that have been written 50 years ago. They are still running and maintained by young professionals around the age of 30. It means that they were not even born when the code was developed. It can easily happen that the person maintaining your code is not even born by the time you write the code. You cannot tell anything about their cleverness and coding practices. The best we can do is to prepare for the average and that is exactly what static code analysis tools are set for.</p>
<p class="calibre2">The checks that these tools perform are not hardwired into the tools. Some special language inside the tools describes the rules and they can be deleted, other rules can be added, and rules can be modified. This way, you can accommodate the coding standards of the enterprise you work for. The different rules can be categorized as cosmetic, minor, major, and critical. Cosmetic things are mainly warnings and we do not really care about them, even though it is nice to fix even these issues. Sometimes, these small things may signal some really big issue. We can set limits for the number of minor and major bugs before the check is declared as failing and also for the critical errors. In the last case, this limit is usually zero. If a coding error seems to be critical, then better not have any in the code.</p>
<p class="calibre2">The most frequently used tools are <strong class="calibre1">Checkstyle</strong>, <strong class="calibre1">FindBugs</strong>, and <strong class="calibre1">PMD</strong>. The execution of these tools is usually automated, and though they can be executed from the IDE or from the developer's command line, their main use is on the <strong class="calibre1">continuous integration</strong> (<strong class="calibre1">CI</strong>) server. During the build, these tools are configured on the CI server to run, and it can be configured such that the build should be broken if the static code analysis fails with some limit. Executing the static code analysis is usually the next step after compilation and unit test execution, and before the actual packaging.</p>
<p class="calibre2">The <strong class="calibre1">SonarQube</strong> tool (<a href="https://www.sonarqube.org/" class="calibre6"><span>https://www.sonarqube.org/</span></a>) is a special tool in addition to being a static code analysis tool. SonarQube maintains the history of the previous checks as well as supports unit test code coverage and can report the change of the quality over time. This way, you can see how the quality, coverage percentage, and number of different qualifications of code style errors have changed. Many times, you can see that when approaching the release date, the code quality decreases as people are in a rush. This is very bad because this is the time when most of the bugs should be eliminated. Having a statistic about the quality may help change the practice by seeing the trends before the quality, and thus the maintainability of the code gets out of hand.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Source code version control</h1>
            

            <article>
                
<p class="calibre2">Source code version control systems store different versions of the source code. These days, we cannot imagine professional software development without it. This was not always the case, but the availability of free online repositories encouraged hobby developers to use some version control, and when these developers worked for enterprises later, it was evident that the use of these systems is kind of a must.</p>
<p class="calibre2">There are many different revision control systems. The most widely used one is Git. The version control that was previously widely used was <strong class="calibre1">SVN</strong> and, even before that, <strong class="calibre1">CVS</strong>. These are less and less used these days. We can see <strong class="calibre1">SVN</strong> as a successor of <strong class="calibre1">CVS</strong> and Git as a successor of <strong class="calibre1">SVN</strong>. In addition to these, there are other version control systems such as <strong class="calibre1">Mercurial</strong>, <strong class="calibre1">Bazaar</strong>, or <strong class="calibre1">Visual Studio Team Services</strong>. For a comprehensive list of the available tools, visit the Wikipedia page at <a href="https://en.wikipedia.org/wiki/List_of_version_control_software" class="calibre6"><span>https://en.wikipedia.org/wiki/List_of_version_control_software</span></a>.</p>
<p class="calibre2">My bet is that you will meet Git in the first place and there is a high probability of you coming across SVN when programming for an enterprise. Mercury may appear in your practice but any of the others that currently exist are very rare, are used for a specific area, or are simply extinct.</p>
<p class="calibre2">Version control systems allow the development team to store the different versions of the software in an organized manner on a storage that is maintained (backed up regularly in a reliable manner). This is important for different purposes.</p>
<p class="calibre2">The first thing is that different versions of the software may be deployed to different instances. If we develop software for clients and we have many clients with whom we hope to have to make a terrific business, then different clients may have different versions. This is not only because some clients are reluctant to pay for the upgrade, and we just do not want to give the new version for free. Many times, the costs that rise on the side of the customer prevent the upgrade for a long time. Software products do not work on their own in an isolated environment. Different clients have different integrated environments; the software communicates with different other applications. When a new version is to be introduced in an enterprise environment, it has to be tested for whether it works with all the systems it has to cooperate with. This testing takes a lot of effort and money. If the new features or other values that the new version delivers over the old one do not justify the cost, then it would be waste of money to deploy the new version. The fact that there is a new version of our software does not mean that the old versions are not usable.</p>
<p class="calibre2">If there is some bug at the customer's end, then it is vital that we fix the bug in that version. To do so, the bug has to be reproduced in the development environment, which eventually means that the source code for that version has to be available for the developers.</p>
<div class="packttip">This does require the customer database to contain references to the different versions of our software products that are installed at the customer site. To make it more complicated, a customer may have more than one version at a time in different systems and may also have different licenses, so the issue is more complex than it first seems. If we do not know which version the client has, then we are in trouble.<br class="calibre23"/>
Since the database registering the versions for the customers and real life may get unsynchronized, software products log their version at startup. We have a separate section about versioning in this chapter.</div>
<p class="calibre2">If the bug is fixed in the version that the client has, the incident at the customer's end may be solved after deployment. The problem, though, still remains if the version is not the previous version of the software. The bug fix introduced to an old version of the software may still be lurking around in the later or, for that matter, earlier versions. The development team has to identify which versions are relevant to clients. For example, an old version that is not installed any more at any of the clients' sites does not deserve the investigation. After that, the relevant versions have to be investigated to check whether they exhibit the bug. This can only be done if we have the source version. Some old versions may not have the bug if the code causing the bug is introduced in later versions. Some new versions may also be immune to the bug because the bug was already fixed in the previous version, or simply because the piece of code that caused the bug was refactored even before the bug manifested. Some bugs may even affect a specific version instead of a range of products. Big fixing may be applied to different versions and they may need slightly different fixes. All this needs a maintained source version repository.</p>
<p class="calibre2">Even when we do not have different customers with different versions, it is more than likely that we have more than one version of our software in development. The development of a major release is coming to an end, and therefore, one part of the team responsible for testing and bug fixing focuses on those activities. At the same time, the development of features for the next version still goes on. The code implementing the functionalities for the next version should not get into the version that is about to be released. The new code may be very fresh, untested, and may introduce new bugs. It is very common to introduce freeze times during the release process. For example, it may be forbidden to implement any new feature of the upcoming release. This is called feature freeze.</p>
<p class="calibre2">Revision control systems deal with these freeze periods, maintaining different branches of the code. The release will be maintained in one branch and the version for later releases in a different one. When the release goes out, the bug fixes that were applied to it should also be propagated to the newer version; otherwise, it might so happen that the next version will contain bugs that were already fixed in the previous version. To do so, the release branch is merged with the ongoing one. Thus, version control systems maintain a graph of the versions, where each version of the code is a node in the graph and the changes are vertices.</p>
<p class="calibre2">Git goes very far in this direction. It supports branch creation and merging so well that developers create separate branches for each change that they create and then they merge it back with the master branch when the feature development is done. This also makes for a good opportunity for code review. The developer making the feature development or bug fix creates a pull request in the GitHub application, and another developer is requested to review the change and perform the pull. This is a kind of four-eyed principle applied to code development.</p>
<p class="calibre2">Some of the revision control systems keep the repository on a server and any change gets to the server. The advantage of this is that any change committed gets to a server disk that is regularly backed up and is thus safe. Since the server-side access is controlled, any code sent to the server cannot be rolled back without trace. All versions, even the wrong versions, are stored on the server. This may be required by some legal control. On the other hand, if commit requires network access and server interaction, it may be slow and this will, in the long run, motivate developers not to commit their changes frequently. The longer a change remains on the local machine, the more risk we have of losing some of the code, and merging becomes more and more difficult with time. To heal this situation, Git distributes the repository and the commit happens to the local repository, which is exactly the same as the remote one on some server. The repositories are synchronized when one repository pushes the changes to another one. This encourages the developers to make frequent commits to the repository, giving short commit messages, which helps in tracking the change made to the code.</p>
<p class="calibre2">Some older version control systems support file locking. This way, when a developer checks out a code file, others cannot work on the same piece of code. This essentially avoids the collisions during code merging. Over the years, this approach did not seem to fit the development methodologies. Merge issues are less of a problem than files that are checked out and forgotten. SVN supports file locking but this is not really serious and does not prevent one developer to commit changes to a file that somebody else locked. It is more of only a suggestion than real locking.</p>
<p class="calibre2">Source code repositories are very important but should not be confused with release repositories, which store the compiled released version of the code in binary. Source and release repositories work together.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Software versioning</h1>
            

            <article>
                
<p class="calibre2">Software versioning is magic. Think about the different versions of Windows or Star Wars movies. Well, the latter is not really software versioning but it shows that the issue is very general. In the case of Java, versioning is not that complex. First of all, the version of Java we use now is 9. The previous version was 1.8, before that 1.7, and so on, down to 1.0. Earlier versions of Java were called Oak but that is history. After all, that is, who can tell what Java 2 was?</p>
<p class="calibre2">Fortunately, when we create a Java application, the situation is simpler. There has been a suggestion from Oracle, from the time of Java 1.3, about how to version JARs:</p>
<p class="calibre2"><a href="http://docs.oracle.com/javase/7/docs/technotes/guides/extensions/versioning.html" class="calibre6"><span>http://docs.oracle.com/javase/7/docs/technotes/guides/extensions/versioning.html</span></a></p>
<p class="calibre2">This document distinguishes between specification version and implementation version. If the specification of a JAR content changes, the code has to behave differently from how it was behaving till then; the specification version should change. If the specification is not changed but the implementation does--for example, when we fix a bug--then the implementation version changes.</p>
<p class="calibre2">In practice, nobody has used this scheme, although it is a brilliant idea to separate the implementation and specification versions, at least, in theory. I even bet that most of your colleagues have not even ever heard about this versioning. What we use in practice is semantic versioning.</p>
<p class="calibre2">Semantic versioning (<a href="http://semver.org/" class="calibre6"><span>http://semver.org/</span></a>) mixes the specification and implementation versions into one single version number triplet. This triplet has the format of <strong class="calibre1">mmp,</strong> that is:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="calibre1">m</strong>: major version number</li>
<li class="calibre15"><strong class="calibre1">m</strong>: minor version number</li>
<li class="calibre15"><strong class="calibre1">p</strong>: patch number</li>
</ul>
<p class="calibre2">The specification says that these numbers start with zero and increase by one. If the major number is zero, it means that the software is still in development. In this state, the API is not stable and may change without a new major version number. The major version number gets to 1 when the software is released. Later it has to be increased when the API of the application (library) has changed from the previous version and the application is not backward compatible with the previous version. The minor version number is increased when the change effects only the implementation but the change is significant, perhaps, even the API is also changing but in a backward-compatible manner. The patch version is increased when some bug is fixed, but the change is not major and the API does not change. The minor and the patch levels have to be reset to zero if any version number in the triplet in front of any of them is increased: major version number increase resets both minor and patch version; minor version number increase resets patch number.</p>
<p class="calibre2">This way, semantic versioning keeps the first element of the triplet for the specification version. The minor is a mix of the specification and implementation versions. A patch version change is clearly an implementation version change.</p>
<p class="calibre2">In addition to these, semantic versioning allows appending a pre-release string, such as <kbd class="calibre11">-RC1</kbd> and <kbd class="calibre11">-RC2</kbd>. It also allows the appending of metadata, such as a date after a plus sign, for example, <kbd class="calibre11">+20160120</kbd> as a date.</p>
<p class="calibre2">The use of semantic versioning helps those that use the software to easily spot compatible versions and to see which version is older and which is newer.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Code review</h1>
            

            <article>
                
<p class="calibre2">When we create programs in a professional way, it is done in teams. There is no one-man show programming other than in a hobby or going along with the tutorials. It is not only because it is more effective to work in teams but also because one person is vulnerable. If you work alone and get hit by the bus or you hit the lottery and lose your ability or motivation to work on the project, your customer is in trouble. That is not professional. Professional projects should be resilient to any member falling off.</p>
<p class="calibre2">Teamwork needs cooperation and one form of cooperation is code review. This is the process when a developer or a group of developers reads a part of the code that some other team members have written. There are direct gains from this activity;</p>
<ul class="calibre14">
<li class="calibre15">The developers reading the code get more knowledge about the code; they learn the code. This way, if the developer creating the code gets out of the process for any reason, the others can continue the work with minimal bump.</li>
<li class="calibre15">Coding styles can be aligned. Developers, even seniors, paying careful attention make coding mistakes. This may be a bug or it may be a coding style violation. Coding style is important because the more readable the code is, the less possibility of it having unnoticed bugs. (Also see the next bullet point.) It is also important that the coding style is the same for the team. All team members should use the same style. Looking at a code that has a different style from the one I wrote is a bit harder to follow and understand. The differences may distract the reader and the team members have to be able to read the code. The code belongs to the team and not a single developer. Any team member should know the code and be able to modify it.</li>
<li class="calibre15">During code review, a lot of bugs can be discovered. The parties looking at the code and trying to understand the working of it may occasionally discover bugs from the structure of the code, which are otherwise hard to discover using tests. If you want, code review is the whitest white box test. People think differently and different mindsets catch different bugs.</li>
</ul>
<p class="calibre2">Code review can be done online and offline. It can be done in teams or peer-to-peer.</p>
<p class="calibre2">Most teams follow the code review process that GitHub supports, which is the simplest. Changes to the code are committed to a branch and are not merged with the code directly but, rather, a pull request is created on the web interface. The local policy may require that a different developer perform the pull. The web interface will highlight the changes and we can add comments to the changed code. If the comments are significant, then the original developer requesting the pull should modify the code to answer the comments and request the pull again. This ensures that at least two developers see any change; the knowledge is shared.</p>
<p class="calibre2">Feedback is peer-to-peer. It is not a senior teaching a junior. That needs a different channel. Comments in GitHub are not good for this purpose; at least, there are better channels. Perhaps talking face to face. Comments may come from a senior to a junior or from a junior to a senior. In this work, giving feedback on the quality of the code, seniors and juniors, are equal.</p>
<div class="packttip">The simplest and perhaps the most frequent comment is the following:<br class="calibre23"/>
<em class="calibre27">I can see that Xyz.java was changed in the modification but I see no change made to </em><em class="calibre27">XyzTest.java.<br class="calibre23"/></em>This is almost an instant refusal for the merge. If a new feature is developed, unit tests have to be created to test that feature. If a bug is fixed, then unit tests have to be created to prevent the bug from coming back. I personally got this comment many times, even from juniors. One of them told me, "We know that you were testing us if we dared to give feedback."<br class="calibre23"/>
God knows I was not. They did not believe.</div>
<p class="calibre2">While change review and GitHub is a good tool during development, it may not be appropriate when a larger chunk of code has to be reviewed. In such a case, other tools, such as <strong class="calibre1">FishEye</strong>, have to be used. In this tool, we can select the source files for review even if they were not recently changed. We can also select reviewers and deadlines. Commenting is similar to GitHub. Finally, this type of code review finishes with a code review session, where the developers gather and discuss the code in person.</p>
<p class="calibre2">While organizing such a session, it is important that a person who has experience managing other people mediates these sessions. Code and discussion on styles can get very personal. At the same time, when attending such a meeting, you should also pay attention so as not to get personal. There will be enough participants who may not know this or are less disciplined.</p>
<p class="calibre2">Never attend a review session without reviewing the code first using the online tools. When you make comments, the language should be very polite for the reason I have already mentioned. Finally, the mediator of the meeting should be able to separate important and not so important issues and to stop debate on bagatelle things. Somehow, the less important issues are more sensitive. I personally do not care about formatting the tab size if it is two or four spaces and if the file should contain only spaces or if tab characters are allowed, but people tend to like to waste time on such issues.</p>
<p class="calibre2">The most important issue during code review sessions is that we are professional and it may happen that I review and comment your code today, but tomorrow, it will be just the opposite, and we work together and we have to work together as a team.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Knowledge base</h1>
            

            <article>
                
<p class="calibre2">Knowledge base was a buzzword a few years ago. Few companies were evangelizing the idea of wiki technology and nobody was using it. Today, the landscape of knowledge base is totally different. All enterprises use some kind of wiki implementation that is there to share knowledge. They mostly use Confluence, but there are also other wiki solutions available, commercial and free as well.</p>
<p class="calibre2">Knowledge bases store information that you, as a developer, would write down in a paper notebook for your later reference, for example, the IP address of the development server, directories where to install the JAR files, what commands to use, what libraries you have collected, and why you use them. The major difference is that you write it in a formatted way into a wiki and it is available immediately for other developers. It is a bit of a burden on the developer to write these pages, and it needs some self-discipline first. Sticking to the example of the IP address of the development server and the install directories, you have to write not only the IP address of the server but also some text explaining what the information is, because the others may not understand it otherwise. It is also a bit of work to place the page with the information in the wiki system with a good name, linking it to other pages, or finding the appropriate position of the page in the tree of pages. If you were using the paper notebook, you could just write down the IP address and the directories on the first free page of the book and you would just remember all others.</p>
<p class="calibre2">The wiki approach will pay back when coworkers do not need to find the information themselves; you can find the information in an easier way because other coworkers have also recorded their findings in the knowledge base and, last but not least, a few months later, you find the information you recorded yourself. In the case of a paper notebook, you would turn the pages to find the IP address and you may or may not remember which one is the primary and which is the secondary server. You may even forget by then that there are two servers (or was it a double cluster?).</p>
<p class="calibre2">To have a long list of available wiki software, visit <a href="https://en.wikipedia.org/wiki/Comparison_of_wiki_software" class="calibre6"><span>https://en.wikipedia.org/wiki/Comparison_of_wiki_software</span></a>.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Issue tracking</h1>
            

            <article>
                
<p class="calibre2">Issue tracking systems keep track of issues, bugs, and other tasks. The first issue tracking systems were created to maintain the list of bugs and also the state of the bug fixing process to ensure that a bug, identified and recorded, will not get forgotten. Later, these software solutions developed and became full-fledged issue trackers and are unavoidable project management tools in every enterprise.</p>
<p class="calibre2">The most widely used issue tracking application used in many enterprises is Jira, but on the <a href="https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems" class="calibre6"><span>https://en.wikipedia.org/wiki/Comparison_of_issue-tracking_systems</span></a> page, you can find many other applications listed.</p>
<p class="calibre2">The most important feature of an issue tracker application is that it has to record an issue in detail in an editable manner. It has to record the person who recorded the issue in case more information is needed during issue handling. The source of the issue is important. Similarly, issues have to be assigned to some responsible person, who is accountable for the progress of issue handling.</p>
<p class="calibre2">Modern issue tracking systems provide complex access control, workflow management, relation management, and integration with other systems.</p>
<p class="calibre2">Access control will only allow the person who has something to do with an issue access to it, so others cannot alter the state of an issue or even read the information attached to it.</p>
<p class="calibre2">An issue may go through different workflow steps depending on the type of issue: a bug may be reported or reproduced, a root cause analyzed, a fix developed or tested, a patch created, a fix merged with the next release version or published in the release. This is a simple workflow with a few states.</p>
<p class="calibre2">Relation management allows setting different relations between issues and allowing the user to navigate from issue to issue along these relations. For example, a client reports a bug, and the bug is identified as being the same as another already fixed. In such a case, it would be insane to go through the original workflow and creating a new patch for the same bug. Instead, the issue gets a relation pointing to the original issue and sets the state to be closed.</p>
<p class="calibre2">Integration with other systems is also useful to keep a consistent development state. Version control may require that, for every commit, the commit message contains a reference to the issue that describes the requirement, bug, or change that the code modification supports. Issues may be linked to knowledge base articles or agile project management software tools using web links.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Testing</h1>
            

            <article>
                
<p class="calibre2">We have already discussed testing when we talked about unit testing. Unit testing is extremely important in agile development and it helps keep the code clean and reduce the number of errors. But this is not the only type of testing that you will see in enterprise development.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Types of tests</h1>
            

            <article>
                
<p class="calibre2">Testing is performed for many reasons but there are at least two reasons that we have to mention. One is to hunt the bugs and create error-free code as much as possible. The other is to prove that the application is usable and can be utilized for the purpose it was meant for. It is important from the enterprise point of view and considers a lot of aspects that unit test does not. While unit test focuses on one unit and, thus, is an extremely good tool to point out where the error is, it is totally unusable when it comes to discovering bugs that come from erroneous interfaces between modules. The unit tests mock external modules and, thus, test that the unit works as expected. However, if there is an error in this expectation and the other modules do not behave in the same way as the unit test mock, the error will not be discovered.</p>
<p class="calibre2">To discover the errors on this level, which is the next level above unit test, we have to use integration tests. During integration tests, we test how individual units can work together. When we program in Java, the units are usually classes; thus, the integration test will test how the different classes work together. While there is a consensus (more or less) about what a unit test is in Java programming, this is less so in the case of integration tests.</p>
<p class="calibre2">In this regard, the external dependencies, such as other modules reachable via the network or database layers may be mocked, or may be set up using some test instance during integration testing. The argument is not about whether these parts should be mocked or not, only the terminology. Mocking some components such as the database has advantages as well as drawbacks. As in the case of any mock, the drawback is the cost of setting up the mock as well as the fact that the mock behaves differently from the real system. Such a difference may result in some bugs still remaining in the system and lurking there until a later case of testing or, God forgive, production is used.</p>
<p class="calibre2">Integration tests are usually automated in a way similar to unit tests. However, they usually require more time to execute. This is the reason why these tests are not executed at each source code change. Usually, a separate maven or Gradle project is created that has a dependency on the application JAR and contains only integration test code. This project is usually compiled and executed daily.</p>
<p class="calibre2">It may happen that daily execution is not frequent enough to discover the integration issues in a timely manner, but a more frequent execution of the integration tests is still not feasible. In such a case, a subset of the integration test cases is executed more frequently, for example, every hour. This type of testing is called smoke testing.<br class="title-page-name"/>
The following diagram shows the position of the different testing types:</p>
<div class="packtfigure"><img class="image-border49" src="../images/00066.gif"/></div>
<p class="calibre2">When the application is tested in a fully set up environment, the testing is called system testing. Such testing should discover all the integration bugs that may have been lurking and covered during the previous testing phases. The different type of system tests can also discover non-functional issues. Both functional testing and performance testing are done on this level.</p>
<p class="calibre2">Functional testing checks the functions of the application. It ensures that the application functions as expected or at least has functions that are worth installing in the production environment and can lead to cost saving or profit increase. In real life, programs almost never deliver all the functions that were envisioned in any requirement documentation, but if the program is usable in a sane manner, it is worth installing it, assuming that there are no security issues or other issues.</p>
<p class="calibre2">In case there are a lot of functions in the application, functional testing may cost a lot. In such a case, some companies perform a sanity test. This test does not check the full functionality of the application, only a subset to ensure that the application reaches a minimal quality requirement and it is worth spending the money on the functional testing.</p>
<p class="calibre2">There may be some test cases that are not envisioned when the application was designed and thus there is no test case in the functional test plan. It may be some weird user action, a user pressing a button on the screen when nobody thought it was possible. Users, even if benevolent, happen to press or touch anything and enter all possible unrealistic inputs into a system. Ad-hoc testing tries to amend this shortage. A tester during ad-hoc testing tries all the possible ways of use of the application that he or she can imagine at the moment the test is executed.</p>
<p class="calibre2">This is also related to security testing, also called penetration testing when the vulnerabilities of the system are discovered. These are special types of tests that are performed by professionals who have their core area of expertise in security. Developers usually do not have that expertise, but at least, the developers should be able to discuss issues that are discovered during such a test and amend the program to fix the security holes. This is extremely important in the case of Internet applications.</p>
<p class="calibre2">Performance testing checks that the application, in a reasonable environment, can handle the expected load that the user puts on the system. A load test emulates the users who attack the system and measures the response times. If the response time is appropriate, that is, lower than the required maximum under the maximum load, then the test passes; otherwise, it fails. If a load test fails, it is not necessarily a software error. It may so happen that the application needs more or faster hardware. Load tests usually test the functionality of the application in only a limited way and only test use scenarios that pose read load on the application.</p>
<div class="packtinfobox">Many years ago, we were testing a web application that had to have a response time of 2 seconds. The load test was very simple: issue <kbd class="calibre22">GET</kbd> requests so that there are a maximum of 10,000 requests active at the same time. We started with 10 clients, and then a script was increasing the concurrent users to 100, then 1,000, and then stepping up by thousand every minute. This way, the load test was 12 minutes long. The script printed the average response time, and we were ready to execute the load test at 4:40 pm on a Friday.<br class="calibre23"/>
The average response time started from a few milliseconds and went up to 1.9 seconds as the load was increased to 5,000 concurrent users, and from there, it was descending down to 1 second as the load was increased to 10,000 users. You can understand the attitude of the people on a Friday afternoon, being happy that we met the requirements. My colleagues left for the weekend happily. I remained testing a bit more because I was bothered by the phenomenon that the response time decreases as we increase the load above 5,000. First, I reproduced the measurement and then started looking at the log files. At 7 pm, I already knew what the reason was.<br class="calibre23"/>
When the load went above 5,000, the connections the Apache server was managing started to exhaust and the web server started to send back 500 internal error codes. That is something that Apache can very effectively do. It is very fast in telling you that you cannot be served. When the load was around 10,000 concurrent users, 70% of the responses already had 500 errors. The average went down, but the users were actually not served. I reconfigured the Apache server so that it could serve all the requests and forward each to our application just to learn that the response time of our application was around 10 seconds at the maximum load. Around 10 pm, when my wife was calling my mobile the third time, I also knew how large a memory I should set in the Tomcat startup file in the options for the JVM to get the desired 2-second response time in case of 10,000 concurrent users.</div>
<p class="calibre2">Stress test is also a type of performance test that you may also face. This type of test increases the load on the system until it cannot handle the load. That test should ensure that the system can recover from the extreme load automatically or manually but, in no case, will do something that it shouldn't at all. For example, a baking system should not ever commit an unconfirmed transaction, no matter how big the load there is. If the load is too high, then it should leave the dough raw but should not bake extra bread.</p>
<p class="calibre2">The most important test at the top of the hierarchy is the user acceptance test. This is usually an official test that the customer, who buys the software, executes and in the case of successful execution, pays the price for the software. Thus, this is extremely important in professional development.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Test automation</h1>
            

            <article>
                
<p class="calibre2">Tests can be automated. It is not a question of whether it is possible to automatize a test, only whether it is worth doing so. Unit tests and integration tests are automated, and as time advances, more and more tests get automated as we move along to higher steps towards the <strong class="calibre1">user acceptance test</strong> (<strong class="calibre1">UAT</strong>). UAT is not likely to be automated. After all, this test checks the integration between the application and the user. While the user, as an external module, can be mocked using automation in lower levels, we should reach the level when the integration test happens without mocks.</p>
<p class="calibre2">There are many tools that help the automation of tests. The blocker for test automation, these days, is the cost of the tools to do so, the cost of learning and developing the tests, and the fear that the automated tests are not discovering some of the errors.</p>
<p class="calibre2">It is true that it is easier to do something wrong with a program than without. This is so true for almost anything not only for testing. And still we do use programs; why else would you read this book? Some of the errors may not be discovered during automated functional testing, which would otherwise have been discovered using manual tests. At the same time, when the same test is executed the hundredth time by the same developer, it is extremely easy to skip an error. An automated test will not ever do that. And most importantly, the cost of the automated test is not 100 times the cost of running it once.</p>
<p class="calibre2">We have used test automation tools in this book. <strong class="calibre1">SoapUI</strong> is a tool that helps you create tests that can be executed automatically. Other testing tools that are worth looking at are <strong class="calibre1">Cucumber</strong>, <strong class="calibre1">Concordion</strong>, <strong class="calibre1">Fintnesse</strong>, and <strong class="calibre1">JBehave</strong>. There is a good comparison of tools at <a href="https://www.qatestingtools.com/" class="calibre6"><span>https://www.qatestingtools.com/</span></a>.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Black box versus white box</h1>
            

            <article>
                
<p class="calibre2">You may have heard many times that a test is a black box test. This simply means that the test does not know anything about how the system under test (SUT) is implemented. The test relies only on the interface of the SUT that is exported for the outside world. A white box test, on the other end of the scale, tests the internal working of the SUT and very much relies on the implementation:</p>
<div class="packtfigure"><img class="image-border50" src="../images/00067.gif"/></div>
<p class="calibre2">Both the approaches have advantages and disadvantages. We should use one, or the mixture of the two approaches, a way that fits the purpose of the actual testing needs the most. A black box test not relying on the implementation does not need to change if the implementation changes. If the interface of the tested system changes, then the test should also be changed. A white box test may need changes if the implementation changes, even if the interface remains the same. The advantage of the white box test is that, many times, it is easier to create such a test and the testing can be more effective.</p>
<p class="calibre2">To get the best of both worlds, systems are designed to be testable. Be careful, though. It means many times that the functionality internal to the tested system is propagated to the interface. That way, the test will use only the interface and, thus, can be declared to be a black box, but it does not help. If something changes in the internal working of the tested system, the test has to follow it. The only difference is that you may call it a black box test if the interface also changes. That does not save any work. Rather, it increases it: we have to check all the modules that rely on the interface if they also need any change.</p>
<p class="calibre2">I do not say that we should not pay attention to creating testable systems. Many times making a system testable results in cleaner and simpler code. If the code, however, gets messier and much longer because we want to make it testable, then we are probably not going in the right way.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Selecting libraries</h1>
            

            <article>
                
<p class="calibre2">Programming for the enterprise or even programming a moderately sized project cannot be done without the use of external libraries. In the Java world, most of the libraries that we use are open source and, more or less, free to use. When we buy a library that is sold for money, there is usually a standard process enforced by the purchasing department. In such a case, there is a written policy about how to select the vendor and the library. In the case of "free" software, they do not usually care, though they should. In such a case, the selection process mainly lies with the IT department and it is therefore important to know the major points to be considered before selecting a library even if for free.</p>
<p class="calibre2">In the previous paragraph, I put the word free between quotes. That is because there is no software, which is free. There is no such thing as a free lunch, as they say. You have heard this many times but it may not be obvious in the case of an open source code library or framework you are going to select. The major selection factor for any purchase or implementation is the cost, the price. If the software is free, it means that you do not need to pay an upfront fee for the software. However, there is a cost in integrating it and using it. Support costs money. Somebody may say that the support is community support and also available free of charge. The thing is that the time you spend hunting for a workaround that helps you to get over a bug is still money. It is your time, or in case you are a manager, it is the time of the professional in your department whose time you pay for, or, as a matter of fact, it can be an external contractor who will hand you a huge bill in case you do not have the expertise in-house to solve the issue.</p>
<p class="calibre2">Since free software does not have a price tag attached, we have to look at the other factors that are important in the selection process. At the end of the day, they all will affect the cost in some way. Sometimes, the way a criterion alters the cost is not obvious or easily calculable. However, for each one, we can set no-go levels that are based on technology decisions, and we can compare libraries for being better or worse along with each of the criteria.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Fit for the purpose</h1>
            

            <article>
                
<p class="calibre2">Perhaps, this is the most important factor. Other factors may be argued about in terms of the scale of importance, but if a library is not appropriate for the purpose we want to use, then this is certainly not something to select, no matter what. It may be obvious in many cases, but you may be surprised how many times I have seen a product selected because it was the favorite of a person in some other project and the library was forced for use in the new project even though the requirements were totally different.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">License</h1>
            

            <article>
                
<p class="calibre2">The license is an important question as not all free software is free for all uses. Some of the licenses allow free use for hobby projects and education but require you to purchase the software for professional, profit-oriented use.</p>
<p class="calibre2">The most widely used licenses and their explanation (and the whole text of the license) is available on the web page of the <strong class="calibre1">Open Source Initiative</strong> (<a href="https://opensource.org/licenses" class="calibre6"><span>https://opensource.org/licenses</span></a>). It lists nine different licenses, and to make the situation a bit more complex, these licenses have versions.</p>
<p class="calibre2">One of the oldest licenses is the <strong class="calibre1">General Public License</strong> (<strong class="calibre1">GPL</strong>) standing for GNU. This license contains the following sentences:</p>
<div class="packtquote">For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code.</div>
<p class="calibre2">If you create software for a for-profit enterprise and the company intends to sell software, you probably cannot use any line of code that is from a GPL-licensed software. It would imply that you are required to pass on your own source code, which may not be the best sales strategy. Apache license, on the other hand, may be okay for your company. This is something that the lawyers should decide.</p>
<p class="calibre2">Even though this is the lawyers' work, there is one important point that we developers must be aware of and pay close attention to. Sometimes, the libraries contain code from other projects and their license, as advertised, may not be the real one. A library may be distributed under the Apache license but contains code that is GPL-licensed. This is obviously a violation of the GPL license, which was committed by some open source developers. Why would you care? Here comes the explanation via an imagined situation.</p>
<p class="calibre2">You develop software for an enterprise. Let's say that this company is one of the largest car manufacturers of the world, or it is one of the largest banks, pharma, whatever. The owner of the GPL software seeks remedies for the misuse of her software. Will she sue the software developer, John Doe, who has a total wealth of 200K, or your company, claiming that you did not duly check the license of the code? She certainly will not dig for gold where there is none. Suing the company you work for may not be successful, but certainly not a good process you or anyone at the company wants.</p>
<p class="calibre2">What can we as software professionals do?</p>
<p class="calibre2">We have to use libraries that are well known, used widely. We can check the source code of the library to see whether there is some copied code. Some package names may present some clue. You can Google some part of the source code to find matches. Last but not least, the company can subscribe to services that provide similar research for the libraries.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Documentation</h1>
            

            <article>
                
<p class="calibre2">Documentation is an important aspect. If the documentation is not appropriate, it will be hard to learn how to use the library. Some of the team members may have already known the library, but, again, this may not be the case for later team members. We should consider our colleagues, who are expected to be average programmers, and they will have to learn the use of the library. Thus documentation is important.</p>
<p class="calibre2">When we speak about documentation, we should not only think about the <em class="calibre12">JavaDoc</em> reference documentation but also tutorials and books if they are available.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Project alive</h1>
            

            <article>
                
<p class="calibre2">It is important not to select a library for use that is not alive. Have a look at the roadmap of the library, the last time a release was shipped, and the frequency of the commits. If the library is not alive, we should consider not using it. Libraries work in an environment and the environment changes. The library may connect to a database. The new version of the database may provide new features that give us better performance only if the library is modified to accommodate these new features. The library communicates over HTTP; will it support the new 2.0 version of the protocol? If nothing else, the version of the Java environment will change over the years and the library we use should sooner or later follow it to leverage the new features.</p>
<p class="calibre2">There is no guarantee that an alive library will always stay alive. However, a library that is already dead will certainly not resurrect.</p>
<p class="calibre2">Even if the project is alive at the moment, there are some points that may give some hints about the future of the library. If the company developing it is well-established and financially stable, and the library is developed with a reasonable business model, then there is a low risk that the project dies. If there are a lot of companies who use the library, then it is likely that the project will stay alive even if the original team stops working on it or the original financing structure changes. However, these are only small factors and not well-established facts. There is no guarantee, and telling the future is more an art than a science.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Maturity</h1>
            

            <article>
                
<p class="calibre2">Maturity is similar to the previous criterion. A project may very well be alive just starting up, but if it is in its infancy, we better not use the library for a large project. When a project is in its early phase, a lot of bugs can be in the code, the API may change radically, and presumably, there may only be a small number of companies relying on the code. This also means that the community support is lower.</p>
<p class="calibre2">Of course, if all the projects select only mature open source code, then no open source project would ever get to the mature state. We should assess the importance of the project. Is the project business-critical? Will the project become business-critical?</p>
<p class="calibre2">If the project is not business-critical, the company may afford to invent a fresh library that is not that mature. It may be reasonable if there are no mature libraries for the purpose because the technology you are going to use is relatively new. In such a case, the project in the company is probably also new and not business-critical yet. It will be business-critical, we hope, after some time, but by that time, the library will be mature, or may just die and we can select a competing solution before the project becomes too expensive to switch.</p>
<p class="calibre2">Judging the maturity of a library is always difficult and has to be aligned with the maturity and importance of the project that we want to use the library for.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Number of users</h1>
            

            <article>
                
<p class="calibre2">If the library is alive and mature but there are not many users, then something is smelly. Why don't people use the library if it is good? If the number of users for a library or framework is low and there are no large corporations among the users, then it is probably not a good one. Nobody using it may signal that our assessment of the other criteria may not be appropriate.</p>
<p class="calibre2">Also note that if there are only a few users of the library, then the knowledge in the community is also scarce and we may not be able to get community support.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">The "I like it" factor</h1>
            

            <article>
                
<p class="calibre2">Last but not least, the <em class="calibre12">I like it</em> factor is extremely important. The question is not whether you like the library but rather how much the developers like it. Developers will like a library that is easy to use and fun to work with, and this will result in low cost. If the library is hard to use and developers do not like it, then they will not learn to use it to the level of profession required for good quality, only to the level that is just needed. The end result will be suboptimal software.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Continuous integration and deployment</h1>
            

            <article>
                
<p class="calibre2">Continuous integration means that whenever a new version is pushed to the source code repository, the continuous integration server kicks in, pulls the code to its disk, and starts the build. It compiles the code first, then runs the unit tests, fires the static code analysis tools, and, if all goes right, it packages a snapshot release and deploys it on a development server.</p>
<p class="calibre2">CI servers have web interfaces that can be used to create a release. In such a case, the deployment can even go to the test servers or even to production depending on local business needs and on the policy that was created accordingly.</p>
<p class="calibre2">Automating the build and deployment process has the same advantages as any other automation: repeated tasks can be performed without manual intervention, which is tedious, boring, and, thus, error-prone if done by a human. The outstanding advantage is that if there is some error in the source code that can be discovered by the automated build process, it will be discovered. Novice developers say that it is cheaper and easier to build the code locally, which the developers do anyway, and then push the code to the server if the build process is already checked. It is partly true. Developers have to check that the code is of good quality and builds well, before sending it to the central repo. However, this cannot always be achieved. Some errors may not manifest on local environments.</p>
<p class="calibre2">It may so happen that one developer accidentally uses a newer version of Java than the one supported and uses a new feature of the new version. Enterprises do not generally use the latest technology. They tend to use versions that are proven, have many users, and are mature. This year, in 2017, when Java 9 is going to be released in July, huge enterprises still use Java 1.6 and 1.7. Since Java 9 has many new features that are not trivial to implement, I expect that the adoption of the technology may take even longer than the adoption of Java 1.8, which gave us functional programming and lambda.</p>
<p class="calibre2">It may also happen that a new library is added to the dependencies of the build and the developer who added it to the build file (<kbd class="calibre11">pom.xml</kbd>, or <kbd class="calibre11">build.gradle</kbd>) could use it without any problem on her local machine. It does not mean that the library is officially added to the project, and it may not be available in the central code repository (Artifactory, Nexus, or other implementations of the code repository). The library may have only been on the local repository of the developer, and she may have assumed that since the code compiles, the build is OK.</p>
<div class="packtinfobox">Some large organizations use different code repositories for different projects. The libraries get into these repositories following meticulous examination and decisions. Some libraries may get there, while others may not. The reason to have different repositories could be numerous. Some project is developed for one customer who has a different policy about an open source project than the other. If the enterprise develops code for itself, it may so happen that some library is phased out or not supported anymore, and can only be used for projects that are old. A maintenance release may not need to replace a library, but new projects may be not be allowed to use a dying software library.</div>
<p class="calibre2">The CI server can run on a single machine or it can run on several machines. In case it serves many projects, it may be set up as a central server with many agents running on different machines. When some build process has to be started, the central server delegates this task to one of the agents. The agents may have different loads, running several different build processes, and may have different hardware configuration. The build process may have requirements regarding the speed of the processor or about the available memory. Some agent may run simpler builds for smaller projects but would fail to execute the build of a large project or of some small project that still has a huge memory requirement to execute some tests.</p>
<p class="calibre2">When a build fails, the build server sends out e-mails to the developers, and the person who sent the last update to the code repository is obligated to fix the bug without delay. This encourages the developers to commit frequently. The smaller the change, the fewer chances there are of a build problem. The build server web interface can be used to see the actual state of the projects, which project is failing to build, and which is just fine. If a build fails, there is a red sign in the line of the build, and if the build is OK, the sign is green.</p>
<p class="calibre2">Many times, these reports are continually displayed on some old machine using a huge display so that every developer or just anybody who enters the room can see the actual state of the builds. There is even special hardware that you can buy that has red, yellow, and green lamps to follow the state of the build and ring a bell when the build fails.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Release management</h1>
            

            <article>
                
<p class="calibre2">Developing software means a continuously changing code base. Not every version of the software is supposed to be installed in production. Most of the versions are pushed to the repository on a branch half complete. Some versions are meant only for testing and a few are meant to be installed in production even if only some of those will finally get to production.</p>
<p class="calibre2">Almost all the time, the releases follow the semantic versioning that we discussed in an earlier section. The versions that are meant only to be tested usually have the <kbd class="calibre11">-SNAPSHOT</kbd> modifier at the end of the version number. For example, the <kbd class="calibre11">1.3.12-SNAPSHOT</kbd> version is the version that was once debugged, and is going to become the <kbd class="calibre11">1.3.12</kbd> version. The snapshot versions are not definite versions. They are the code as it is by then. Because a snapshot release never gets installed in production, it is not needed to reproduce a snapshot version for maintenance. Thus, the snapshot versions are not increased continually. Sometimes, they may be changed, but that is a rare exception.</p>
<p class="calibre2">It may so happen that we work on a bug fix, <kbd class="calibre11">1.3.12-SNAPSHOT</kbd>, and during the development, we change so much code that we decide that it has to be <kbd class="calibre11">1.4.0</kbd> when it is released, and we rename the snapshot as <kbd class="calibre11">1.4.0-SNAPSHOT</kbd>. This is a rare case. Many times, the release creation creates a <kbd class="calibre11">1.4.0</kbd> version from <kbd class="calibre11">1.3.12-SNAPSHOT</kbd> as the decision about the new release number is taken by the time the release is created.</p>
<p class="calibre2">When the release process is started, usually from the web interface of the CI server, the developer creating the release has to specify the release version. This is usually the same as the snapshot version without the <kbd class="calibre11">-SNAPSHOT</kbd> postfix. The build process not only creates the build in this case but also tags the source code repository version it was using and loads the packaged program (artifact) to the code repository. The tag can be used later to access the exact version of the source code that was used to create the release. If there is a bug in a specific version, then this version has to be checked out on a developer machine to reproduce the bug and find the root cause.</p>
<p class="calibre2">If the build of a release fails, it can be rolled back, or you better just skip that release number and note it as a failed release build. An existing release can never have two versions. The source code is the only one that is for that release and the generated code has to be exactly the one in any storage. Subsequent compilation of the same source may result in slightly different code, for example, if a different version of Java is used to create the latter one. Even in such a case, the one that was created by the build server in the first place is the version that belongs to the release. When a bug is reproduced and the code is recompiled from the exact same source, it is already a snapshot version. Multiple releases may be possible from the same source version, for example, compiled with Java versions from 1.5 to 1.8 and version 9 but a single release always belongs to the exact same source code.</p>
<p class="calibre2">If a release that was supposed to be a release version fails during QA checks, then a new release has to be created and the failed release has to be noted as such. The version that marketing uses to name the different versions should not have a relation to the technical version numbers we work with. Many times, it is, and it causes much headache. If you realize that the two are totally different things and one does not have to do anything with the other, life gets simpler. Look at the different versioning of the Windows operating system or Java. As marketing, Java used 1.0 then 1.1, but Java 1.2 was advertised as Java 2 and still the code contained 1.2 (which now seven major releases later also becomes 9 instead of 1.9)</p>
<p class="calibre2">The last part of release management is that deployments should register the version numbers. The company has to know which release is installed on which server, and of which client.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Code repository</h1>
            

            <article>
                
<p class="calibre2">Code repository stores the libraries and helps manage the dependencies of the different libraries. In the old times, when Java projects used ANT as a build tool and without the later added Ivy dependency management, the libraries that were needed by a project were downloaded to the source code, usually to the <kbd class="calibre11">lib</kbd> library. If a library needed another library, then those were also downloaded and stored manually, and this continued until all the libraries that one of the already downloaded libraries needed were copied to the source code tree.</p>
<p class="calibre2">This was a lot of manual work and, also, the library code was stored in the source code repository in many copies. A compiled library is not source code and has nothing to do in the source code repository. Manual work that can be automated has to be automated. Not because developers are lazy (yes, we are and we have to be) but because manual work is error prone and, thus, expensive.</p>
<p class="calibre2">This was when Apache Ivy was invented and Maven, following ANT, already supported repository management built in. They all stored the libraries structured in directories and supported metadata that described the dependencies to other libraries. Lucky that Gradle did not invent its own code repository. Instead, it supports both Maven and Ivy repositories.</p>
<p class="calibre2">Using the repository, the build tools automatically download the libraries that are needed. In case a library has a new version, then the developer only has to update the version of the needed library in the build configuration and all tasks, including downloading all the new versions of the other libraries that are needed by that version, are done automatically.</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Walking up the ladder</h1>
            

            <article>
                
<p class="calibre2">At this point, you have got a lot of information that will rocket your start as an enterprise Java developer. You have got a base knowledge that you can build on. There is a long way to become a professional Java developer. There is a lot of documentation to read, a lot of code to scan and understand, and also a lot of code to write till you can claim to be a professional Java developer. You may probably face many years of continuous education. The good thing is that even after that, you can continue your journey and you can educate yourself, as being a professional Java developer is rarely a job people retire from. No, no! Not because they die while at it! Rather, professional software developers gaining experience start to code less and less and support the development process in different ways, which leverages more of their experience. They can become business analysts, project managers, test engineers, subject-matter experts, architects, scrum masters, automation engineers, and so on. Is it a familiar list? Yes, these are the people you will work with as a developer. Many of them may have started as a developer themselves.<br class="title-page-name"/>
The following diagram shows the relative position of these roles:</p>
<div class="packtfigure"><img class="image-border51" src="../images/00068.gif"/></div>
<p class="calibre2">Let's take a bit more detailed look into what these roles perform in enterprise development:</p>
<ul class="calibre14">
<li class="calibre15">Business analysts work with the client and create the documents, specifications, use cases, and user stories needed by the developers to develop the code.</li>
<li class="calibre15">Project managers administer the projects and help the team in getting things done in cooperation with other teams, caring for all the project matters that developers cannot attend to or would unnecessarily burn their time that they should have devoted to coding.</li>
<li class="calibre15">Subject-matter experts are more advanced in knowing the business needs, so it is a bit rare for a developer to become one, but in case the industry you work in is technology oriented, it may not be incredible to become one.</li>
<li class="calibre15">Test engineers control the QA process and understand not only the test methodologies and requirements of testing but also the development process so that they can support bug fixes and not only identify them, which would be poor.</li>
<li class="calibre15">Architects work with BAs and design a high-level structure of the applications and code, and document it in a way that helps the developers to focus on the actual tasks they have to perform. Architects are also responsible for the solution to use technologies, solutions, and structures which fit the purpose, are future proof, affordable, and so on.</li>
<li class="calibre15">Scrum mates help the development team to follow the agile methodology and help the team in controlling the administration and resolving problems.</li>
</ul>
<p class="calibre2">There are many ways to go as a software developer and I only listed some of the positions that you can find in an enterprise today. As technology develops, I can imagine that in 20 years from today, software developers will teach and curate artificial intelligence systems and that will be what we refer to as programming today. Who can tell?</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    

        <section>

            <header>
                </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
            

            <article>
                
<p class="calibre2">Going in this direction is a good choice. Being a Java developer and becoming a professional in it is a profession that will pay well in the coming 10 to 20 years for sure and perhaps even later. At the same time, I personally find this technology fascinating and interesting, and after more than 10 years of Java programming and more than 35 years of programming, I still learn something new in it every day.</p>
<p class="calibre2">In this book, you learned the basics of Java programming. From time to time, I also mentioned issues, suggested directions, and warned you about pitfalls that are not Java-specific. However, we also did the homework of learning the Java language, the infrastructure, the libraries, development tools, and networking in Java. You also learned the most modern approaches that came only with Java 8 and 9, such as functional programming in Java, streams, and reactive programming. If you know all that I have written in this book, you can start working as a Java developer. What's next? Go, and find your treasure in programming and in Java!</p>


            </article>

            <footer class="calibre4">
                
            </footer>

        </section>
    </body></html>