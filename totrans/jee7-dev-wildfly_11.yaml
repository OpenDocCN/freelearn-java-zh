- en: Chapter 11. Clustering WildFly Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we went through the most interesting aspects of developing
    Java Enterprise applications. Once you are ready to roll out your applications,
    it is important that you guarantee your customers a responsive and fault-tolerant
    environment. This requirement can be achieved through application server clustering.
  prefs: []
  type: TYPE_NORMAL
- en: WildFly clustering is not the product of a single library or specification,
    but rather a blend of technologies. In this chapter, we will first introduce some
    of the basics of clustered programming. Then, we will quickly move on to the cluster
    configuration and its setup, which will be required to deploy some clustered applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list is a preview of the topics that will be covered in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What clustering is and how WildFly implements it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up clusters in the standalone and domain mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing clustered Java EE 7 applications in order to achieve load balancing
    and high availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cluster of application servers consists of multiple server instances (cluster
    nodes) running simultaneously and working together to provide increased scalability
    and reliability. The nodes that make up a cluster can be located either on the
    same machine or different machines. From the client's point of view, this is irrelevant
    because the cluster appears as a single server instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing clustering in your applications will produce the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal scalability (scaling out)**: Adding a new node to a cluster should
    allow the overall system to service a higher client load than that provided by
    a simple basic configuration. Ideally, it should be possible to service any given
    load simply by adding the appropriate number of servers or machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: In a clustered environment, the individual nodes that compose
    the cluster should each process a fair share of the overall client load. This
    can be achieved by distributing client requests across multiple servers, which
    is also known as load balancing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: Applications running in a cluster can continue to do
    so when a server instance fails. This is achieved because applications are deployed
    on multiple nodes of the cluster, and so if a server instance fails, another server
    instance on which that component is deployed can continue with application processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WildFly clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is available in WildFly out of the box. There is no all-in-one library
    that deals with clustering, but rather a set of libraries that cover different
    kinds of aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the basic clustering architecture adopted by WildFly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![WildFly clustering](img/00089.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The backbone of JBoss clustering is the **JGroups** library, which provides
    communication between members of the cluster using a multicast transmission.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Multicast** is a protocol where data is transmitted simultaneously to a group
    of hosts that have joined the appropriate multicast group. You can think about
    multicast as a radio or television streaming where only those tuned to a particular
    frequency receive the streaming.'
  prefs: []
  type: TYPE_NORMAL
- en: The next building block is **Infinispan**, which handles the consistency of
    your application across the cluster by means of a replicated and transactional
    JSR-107-compatible cache.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**JSR-107** specifies the API and semantics for temporary in-memory caching
    of Java objects, including object creation, shared access, spooling, invalidation,
    and consistency across JVMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into some cluster examples, we will first need to describe how
    to set up a cluster of WildFly nodes using the two available nodes: **standalone
    cluster** and **domain cluster**. If you don''t remember the difference between
    the **standalone** and **domain** mode or what core **domain** elements are, you
    can revise the material from [Chapter 2](part0017_split_000.html#page "Chapter 2. Your
    First Java EE Application on WildFly"), *Your First Java EE Application on WildFly*.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting a cluster of standalone nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A standalone server starts as a single JVM process; therefore, we need to start
    each server using the `standalone.bat/standalone.sh` command, passing all the
    required parameters. In the following example, we are starting a cluster of two
    server nodes on two different boxes that are bound to the IP addresses `192.168.1.10`
    and `192.168.1.11`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `-c` parameter specifies the server configuration to be used; out of the
    box, the application server includes two standalone clustering configurations:
    `standalone-ha.xml` and `standalone-full-ha.xml`. The latter one also includes
    the messaging subsystem and other elements of the Java EE full profile; therefore,
    it has been named the *full* configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: The other parameter (`-b`) should sound familiar to older JBoss users, as it's
    still used to specify the server-binding address, which needs to be unique in
    order to avoid port conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this other example, we are starting another cluster of two nodes on the
    same box using some additional parameters in order to avoid port conflicts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we had to specify two additional parameters: `jboss.node.name`
    in order to assign a unique server name to each node and a socket-binding port,
    which uses an offset of `200`. So, for example, the second node would respond
    to the HTTP channel on port `8280` instead of port `8080`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Don't be surprised if you don't see any message about clustering on your server
    console. Clustering modules are activated on demand, so first you need to deploy
    an application that is cluster-aware. In a few minutes, we will show you how.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a cluster of domain nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to configure a cluster running on a domain of server nodes, you need
    to configure the main `domain.xml` file for your domain controller. Then, for
    every WildFly host that is a part of the cluster, you need to provide a `host.xml`
    configuration file, which describes the configuration of a single-server distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The domain controller configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `domain.xml` file is located at `JBOSS_HOME/domain/configuration/`. It
    includes the main domain configuration, which is shared by all server instances.
    In the `domain.xml` file, we will define the server group configurations specifying
    a profile that is compatible with clustering. Out of the box, a WildFly domain
    ships with four different profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`default`: This profile has the support of Java EE Web Profile and some extensions,
    such as RESTful web services, or support for **Enterprise JavaBeans** (**EJB**)
    3 remote invocations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`full`: This profile supports all the default subsystems contained in the default
    profile and the messaging subsystem'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ha`: This profile corresponds to the `default` profile extended with clustering
    capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`full-ha`: This is the `full` profile with clustering capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, first specify a cluster-aware profile for your server groups in your `domain.xml`
    file. In our example, we have adopted the `full-ha` profile for both the server
    groups so that you can run the full Java EE stack on all your domain servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When using a `full-ha` profile, you need to configure HornetQ clustering security.
    You can just disable it, or you need to additionally set a completely random user
    credential for a JMS cluster. Find the profile settings in `domain.xml`, and add
    the following code to the messaging subsystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the `domain.xml` file, you need to check whether your domain
    controller''s `host.xml` file contains a reference to the local host, as shown
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `local` stanza means that this host controller will take the role of a domain
    controller. For all other hosts controllers, you must specify the remote domain
    controller host and its port (in this example, we added some variables as placeholders).
    We will cover them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you need to create a management user that will be used to establish
    a connection between the slave nodes and the domain controller. For this purpose,
    launch the `add-user.sh/add-user.cmd` script, which is located in the `JBOSS_HOME/bin`
    directory of your distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding listing, you have to create a management user
    by specifying a username and password for it. You should answer the previous question
    with either `yes` or `y` to indicate that the user will be used to connect to
    the domain controller from the host controller. The generated secret value is
    the Base64-encoded password of the newly created user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can start the domain controller by specifying the address that will
    be used for public and management interfaces (in our example, `192.168.1.10`)
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We have set the bind address of the physical network to the host configuration
    with the `jboss.bind.address.management` property. The management interface must
    be reachable for all the hosts in the domain in order to establish a connection
    with the domain controller.
  prefs: []
  type: TYPE_NORMAL
- en: Host configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the domain controller is configured and started, the next step is to set
    up the other hosts that will connect to the domain controller. On each host, we
    also need an installation of WildFly, where we will configure the `host.xml` file.
    (As an alternative, you can name the host file as you like and start the domain
    with the `-host-config` parameter, for example, `./domain.sh -host-config=host-slave.xml`.)
  prefs: []
  type: TYPE_NORMAL
- en: The first thing is to choose a unique name for each host in our domain in order
    to avoid name conflicts. Otherwise, the default is the hostname of the server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, you have to choose a unique name for the other host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we need to specify that the host controller will connect to a remote domain
    controller. We will not specify the actual IP address of the domain controller
    but leave it as a property named `jboss.domain.master.address`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we need to specify the username that will be used to connect
    to the domain controller. So let''s add the user `admin1234`, which we created
    on the domain controller machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to specify the Base64 password for the server identity that
    we included in the `remote` element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to configure the server nodes inside the `host.xml` file
    on both the hosts. So, on the first host, we will configure `server-one` and `server-two`
    to add them to `main-server-group`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'On the second host, we will configure `server-three` and `server-four` to add
    them to `other-server-group`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the `auto-start` flag value indicates that the server instances
    will not be started automatically if the host controller is started.
  prefs: []
  type: TYPE_NORMAL
- en: 'For `server-two` and `server-four`, a `port-offset` value of `150` is configured
    to avoid port conflicts. Okay, now we are done with our configuration. Assuming
    that the first host has an IP address of `192.168.1.10`, we can start the first
    host with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The second host (`192.168.1.11`) can be started with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Deploying clustered applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have tried starting your standalone or domain set of cluster nodes,
    you will be surprised that there is no information at all about clustering in
    your server logging. Believe me, it is not a bug but a feature! One of the key
    features of WildFly is that only a minimal set of services is started; therefore,
    in order to see a cluster''s live demonstration, you need to deploy a cluster-aware
    application. In order to trigger clustering libraries in your application, you
    can follow two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: If your application uses Enterprise JavaBeans, you don't have to do anything
    more. This area brings some important changes in WildFly. Now, by default, the
    data of all stateful session beans is replicated in HA profiles, and all stateless
    beans are clustered. If your application is deployed on a container started with
    the `standalone-ha.xml` configuration, all remote **Stateless Session Bean** (**SLSB**)
    support failover capabilities by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your application includes a web application archive, you can use the portable
    `<distributable />` element in your `web.xml` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's have a look at both the approaches, starting from clustering EJBs.
  prefs: []
  type: TYPE_NORMAL
- en: Creating HA Stateful Session Beans
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clustered **Stateful Session Beans** (**SFSB**) have built-in failover capabilities.
    This means that the state of `@Stateful` EJBs is replicated across the cluster
    nodes so that if one of the nodes in the cluster goes down, some other node will
    be able to take over the invocations addressed to it. It is possible to disable
    this feature for specific beans using the `@Stateful(passivationCapable=false)`
    annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts a typical exchange of information between the
    EJB client application and the remote EJB component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating HA Stateful Session Beans](img/00090.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, after a successful lookup of an SFSB via **Java Naming and Directory
    Interface** (**JNDI**), a proxy is returned to the client for subsequent method
    invocations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the EJB is clustered, it will return a session ID and along with it the
    *affinity* of that session, that is, the name of the cluster to which the stateful
    bean belongs to on the server side. This affinity will later help the EJB client
    to route the invocations on the proxy appropriately to a specific node in the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: While this session creation request is going on, **NodeA** will also send back
    an asynchronous message that contains the cluster topology. The JBoss **EJB Client**
    implementation will take note of this topology information and will later use
    it to create connections to the nodes within the cluster and route invocations
    to those nodes, whenever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s assume that **NodeA** goes down and the client application subsequently
    invokes on the proxy. At this stage, the **JBoss EJB Client** implementation will
    be aware of the cluster topology; therefore, it knows that the cluster has two
    nodes: **NodeA** and **NodeB**. Now when the invocation arrives, it detects that
    **NodeA** is down, so it uses a selector to fetch a suitable node from among the
    cluster nodes. This exchange is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating HA Stateful Session Beans](img/00091.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If a suitable node is found, the **JBoss EJB Client** implementation creates
    a connection to that node (in our case **NodeB**) and creates an EJB receiver
    out of it. At the end of this process, the invocation has now been effectively
    failed over to a different node within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering the Ticket example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [Chapter 3](part0023_split_000.html#page "Chapter 3. Introducing Java EE
    7 – EJBs"), *Introducing Java EE 7 – EJBs*, we discussed our ticket system example,
    which was built around the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A stateful EJB to hold the session data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A singleton EJB to store the cache of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A stateless EJB to perform some business methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's see how to apply the necessary changes to start our application in a cluster
    context.
  prefs: []
  type: TYPE_NORMAL
- en: The stateless and stateful beans are ready to be clustered—no additional code
    is required; however, there's a pitfall. As a matter of fact, the singleton EJB
    that is used to hold the cache of a seat will be instantiated once in each JVM
    of the cluster. This means that if there's a server failure, the data in the cache
    will be lost and new data (inconsistent) will be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several alternatives to set up a cache in a clustered environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a JBoss proprietary solution that deploys a clustered version of `SingletonService`,
    which exposes an HA singleton of `org.jboss.msc.service.Service` (an example of
    this approach is contained in the WildFly quickstart demo at [https://github.com/wildfly/quickstart/tree/master/cluster-ha-singleton](https://github.com/wildfly/quickstart/tree/master/cluster-ha-singleton))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move your cache to a persistent storage, which means using JPA to store and
    read data from the cache (see [Chapter 5](part0030_split_000.html#page "Chapter 5. Combining
    Persistence with CDI"), *Combining Persistence with CDI*, which includes a JPA-based
    example of our application)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a distributed data cache such as Infinispan to store data, providing a failover
    and data consistency to your cache
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Showing all the possible solution implementations would, however, make this
    section excessively long; therefore, we will illustrate how to use the last option,
    which can provide a good architectural pattern with the least amount of effort.
  prefs: []
  type: TYPE_NORMAL
- en: Turning your cache into a distributed cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Infinispan is a distributed data grid platform that exposes a JSR-107-compatible
    cache interface in which you can store data and enhance it by providing additional
    APIs and features (such as transactional cache, data eviction and expiration,
    asynchronous operations on the cache, and more). Its primary interface is `javax.cache.Cache`,
    which is similar to the `java.util.ConcurrentMap` Java SE, with some modifications
    for distributed environments. In particular, it adds the ability to register,
    deregister, and list event listeners. Also, it defines a `CacheLoader` interface
    to load/store cached data. Cache instances can be retrieved using an appropriate
    `CacheManager` interface, which represents a collection of caches.
  prefs: []
  type: TYPE_NORMAL
- en: 'So here''s our singleton `TheatreBox` class rewritten using the Infinispan
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first thing we want to stress on is the `@Resource` annotation, which injects
    an `EmbeddedCacheManager` instance. When the WildFly deployer encounters this
    annotation, your application will include a dependency on the requested cache
    container. Consequently, the cache container will automatically start during deployment
    and stop (including all caches) during undeployment of your application.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, when the EJB is instantiated (see the method `start`, which is
    annotated as `@PostConstruct`), `org.infinispan.Cache` is created using `EmbeddedCacheManager`
    as a factory. This cache will be used to store our highly available set of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The operations performed against the distributed cache are quite intuitive:
    the `put` method is used to store instances of the `Seat` object in the cache
    and the corresponding `get` method is used to retrieve elements from it, just
    what you would do using an ordinary hashmap. The only difference is that in our
    clustered cache, every entry must be serializable. Be sure to mark `Seat` as `Serializable`
    and create a default constructor for it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As far as application deployment is concerned, you need to state a dependency
    to the Infinispan API explicitly, which is not included as an implicit dependency
    in WildFly''s class-loading policy. This is most easily done by adding the following
    line to your application''s `META-INF/MANIFEST.MF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We additionally need to add the new cache container to the appropriate profile
    in our `domain.xml` file (in the Infinispan subsystem):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our sample, we are using the `seats.values()` call to get all the elements
    from our distributed map, which is in fact an instance of `org.infinispan.Cache`.
    This operation is normally discouraged in distributed caches (not replicated)
    and has its own limitations. Check out the Javadoc for this method at [https://docs.jboss.org/infinispan/6.0/apidocs/org/infinispan/Cache.html#values()](https://docs.jboss.org/infinispan/6.0/apidocs/org/infinispan/Cache.html#values())
    for more information. This is however, no longer the case for the newest version
    of Infinispan: [http://infinispan.org/infinispan-7.0/](http://infinispan.org/infinispan-7.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: Coding the cluster-aware remote client
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The remote EJB client will not need any particular change in order to be able
    to achieve high availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will only need to prepare a `jboss-ejb-client.properties` file, which will
    contain the list of servers that will be initially contacted (via remoting) by
    our client application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this file, we assume that you are running a two-node cluster
    on the `localhost` address: the first one running the default port settings and
    the second one using an offset of `200` (just as shown in the second paragraph
    of the *Starting a cluster of standalone nodes* section).'
  prefs: []
  type: TYPE_NORMAL
- en: Replace the `remote.connection.nodeX.host` variable's value with the actual
    IP or host if you are running your server nodes on different machines from your
    client.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying and testing high availability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying an application to a cluster can be achieved in several ways; if you
    prefer automation instead of manually copying each archive into the `deployments`
    folder, you can reuse the CLI deployment script contained in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if you are using the WildFly Maven plugin to deploy, you can
    parameterize its configuration, including the hostname and the port as variables,
    which will be passed to the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, you will use the following shell to compile the package and deploy
    the application on the first node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For the second node, you will use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deploying the application in the domain node works the same as mentioned in
    the preceding example, except that you need to add the `domain` tag to your configuration
    and need to specify at least one server group. Visit [https://docs.jboss.org/wildfly/plugins/maven/latest/examples/deployment-example.html](https://docs.jboss.org/wildfly/plugins/maven/latest/examples/deployment-example.html)
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have deployed both the applications on your server node, you should
    be able to see the cluster view in the server console logs and also see that the
    Infinispan cache has been started and has discovered other nodes in the cluster.
    You should see something similar to the following on one of the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying and testing high availability](img/00092.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Before you launch your application, update Maven''s `exec` plugin information,
    which should now reference our remote EJB client application as shown in the highlighted
    section of the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the client will show the evidence that we have successfully
    completed the first transaction. On the client console, you will see the return
    value of the booking transaction and the `Seat` list, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying and testing high availability](img/00093.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the server node where our EJB client landed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying and testing high availability](img/00094.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now shut down the preceding server node (*Ctrl* + *C* would suffice if you are
    starting it as a foreground process) and press *Enter* (or *Return* on a Mac)
    on the client application.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the following screenshot, you should see that the session
    continues to run on the survivor node and correctly displays the session values
    (the money left). Your client window should also display the updated cache information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Deploying and testing high availability](img/00095.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Web application clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Web application clustering involves two aspects: setting up an HTTP load balancer
    and telling WildFly to make the application''s user sessions as HA. How to do
    the former depends on what load balancer you would choose (`mod_cluster` is our
    suggested choice—it is preconfigured and integrates with WildFly out of the box);
    the latter could not be simpler—just add the `<distributable/>`tag to your application''s
    `web.xml` file. Whenever a node fails, the user''s HTTP session will be handled
    by another one. If everything goes well, the end user will not know that there
    was a failure—everything will be handled behind the scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to action both these steps in concrete terms.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing your web applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have several choices available in order to achieve load balancing of your
    HTTP requests. You can opt for a hardware load balancer that sits in front of
    your cluster of servers or you can choose from the many available software solutions
    for WildFly, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Apache Tomcat's `mod_jk` module to route your requests to your nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Apache `mod_proxy` that configures Apache to act as a proxy server and forwards
    requests to WildFly nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use WildFly's built-in solution `mod_cluster` to achieve dynamic load balancing
    of your requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we will illustrate how to get started with `mod_cluster`—a module for
    the Apache HTTP server. The advantage of using `mod_cluster` against other options
    can be summarized in the following key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic clustering configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server-side pluggable load metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Life cycle notifications of the application status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a matter of fact, when using a standard load balancer such as `mod_jk`, you
    have to provide a static list of nodes that is used to spread the load. This is
    a very limiting factor, especially if you have to deliver upgrades to your configuration
    by adding or removing nodes; alternatively, you simply need to upgrade software
    used by single nodes. Besides this, using a flat cluster configuration can be
    tedious and it is prone to errors, especially if the number of cluster nodes is
    high.
  prefs: []
  type: TYPE_NORMAL
- en: When using `mod_cluster`, you can dynamically add or remove nodes from your
    cluster because cluster nodes are discovered through an advertising mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the `mod_cluster` libraries on the HTTP side send UDP messages
    on a multicast group, which is subscribed by WildFly nodes. This allows WildFly
    nodes to automatically discover HTTP proxies when application life cycle notifications
    are sent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next diagram illustrates this concept better:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Load balancing your web applications](img/00096.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Installing mod_cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `mod_cluster` module is implemented as a core WildFly module, which is a
    part of the distribution. On the HTTP side, it is available as a set of libraries
    installed on the Apache web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the WildFly side, you can find the `mod_cluster` module''s subsystem already
    bundled as part of the clustered configuration file. You can locate it in either
    the `standalone-ha.xml` file or the `standalone-full-ha.xml` (and of course in
    the `domain.xml` file) configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The subsystem contains just a bare-bones configuration that references its
    socket binding through the `advertise-socket` element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: On the Apache web server side, we have to install the core libraries that are
    used to interact with `mod_cluster`. This is a very simple procedure; just point
    the browser to the latest `mod_cluster` release at [http://www.jboss.org/mod_cluster/downloads](http://www.jboss.org/mod_cluster/downloads).
    Be sure to choose a version that suits your operating system and architecture
    (x86 or x64).
  prefs: []
  type: TYPE_NORMAL
- en: Once the binaries are downloaded, extract the archive to a folder; then, navigate
    to the extracted folder. The `mod_cluster` binaries essentially consist of a bundled
    Apache web server with all the required libraries installed. To preconfigure your
    installation, be sure to run the `\httpd-2.2\bin\installconf.bat` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible to use your own Apache web server 2.2 installation; just pick
    up the modules from the `mod_cluster` bundle and copy them to the `modules` folder
    of your Apache web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you choose to use your own Apache web server over the bundled one, you have
    to load the following libraries into your `httpd.conf` file (the same set is used
    in the bundled Apache HTTP):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these modules covers an important aspect of load balancing, listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mod_proxy`, `mod_proxy_http`, and `mod_proxy_ajp`: These are the core modules
    that forward requests to cluster nodes using either the HTTP/HTTPS or AJP protocol'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mod_manager`: This module reads the information from AS 7 and updates the
    shared memory information in conjunction with `mod_slotmem`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mod_proxy_cluster`: This module contains the balancer for `mod_proxy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mod_advertise`: This is an additional module that allows HTTP to advertise
    via multicast packets—the IP and port—where the `mod_cluster` module is listening'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next part of the configuration that we need to add is the core load balancing
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Basically, you have to replace the `192.168.10.1` IP address with the one that
    your Apache web server listens for requests and the port value of `8888` with
    the one you want to use to communicate with WildFly.
  prefs: []
  type: TYPE_NORMAL
- en: As it is, the Apache virtual host allows you to have incoming requests from
    the subnetwork `192.168.10`.
  prefs: []
  type: TYPE_NORMAL
- en: The `KeepAliveTimeout` directive allows you to reuse the same connection within
    60 seconds. The number of requests per connection is unlimited since we are setting
    `MaxKeepAliveRequests` to `0`. The `ManagerBalancerName` directive provides the
    balancer name for your cluster (defaults to `mycluster`).
  prefs: []
  type: TYPE_NORMAL
- en: What's most important for us is the `ServerAdvertise` directive that is set
    to `On` and uses the advertise mechanism to tell WildFly whom it should send the
    cluster information to.
  prefs: []
  type: TYPE_NORMAL
- en: This option is disabled in the bundled server by default. Be sure to uncomment
    the `ServerAdvertise` directive in its `httpd.conf` file.
  prefs: []
  type: TYPE_NORMAL
- en: Now, restart the Apache web server and the single application server nodes.
    If you have correctly configured the mode cluster on the HTTP side, you will see
    that each WildFly node will start receiving UDP multicast messages from `mod_cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are running on a Windows machine, be sure to run your web server as an
    administrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'If everything goes well, you can visit `http://127.0.0.1:6666/mod_cluster_manager`
    to see the status of your load balancer and the interconnected nodes. Be sure
    to not use Google Chrome for this because it considers the `6666` port as an unsecure
    one (it is an IRC port by default). You should see the following information on
    the simple webpage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**If you don''t have a running WildFly instance** now, be sure to start it
    with one of the full-HA configuration files. Refresh Apache''s configuration web
    page after the server is up.'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering your web applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Clustering web applications requires the least amount of effort to be put in
    by the developer. As we have just discussed, all you need to do to switch on clustering
    in a web application is to add the following directive to the `web.xml` descriptor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Once your application ships with the distributable stanza in it, the cluster
    will start, and provided you have correctly designed your session layer, it will
    be load balanced and fault tolerant as well.
  prefs: []
  type: TYPE_NORMAL
- en: You could check it out by pointing the browser to your HTTP proxy. For the default
    setup, it would be `http://localhost:6666/your_web_application/`.
  prefs: []
  type: TYPE_NORMAL
- en: Programming considerations to achieve HA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to support in-memory replication of HTTP session states, all servlets
    and JSP session data must be serializable.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Serialization** is the conversion of an object to a series of bytes so that
    the object can be easily saved to a persistent storage or streamed across a communication
    link. The byte stream can then be deserialized, converting the stream into a replica
    of the original object.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in an HTTP servlet that implements `javax.servlet.http.HttpSession`,
    you need to use the `setAttribute` method to change the attributes in a session
    object. If you set the attributes in a session object with `setAttribute`, by
    default the object and its attributes are replicated using the Infinispan API.
    Every time a change is made to an object that is in the session, `setAttribute`
    should be called to update that object across the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Likewise, you need to use `removeAttribute` to remove an attribute from a session
    object.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving HA in JSF applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the applications included in this book, we have used JSF and the CDI API
    to manage the web session. In this case, we transparently replicate the other
    server nodes to the beans, which are marked as `@SessionScoped`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering JSF-based applications requires special attention if you are dealing
    with both HTTP and EJB sessions created by SFSB. In the earlier servlet-centric
    frameworks, the usual approach was to store references of Stateful Session Beans
    in `javax.servlet.http.HttpSession`. When dealing with high-level JSF and CDI
    Beans, it is vital to provide a `@SessionScoped` controller to your application,
    which gets injected in the SFSB reference; otherwise, you will end up creating
    a new Stateful Session Beans upon each request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of how to adapt your Ticket CDI application (described
    in [Chapter 4](part0028_split_000.html#page "Chapter 4. Learning Context and Dependency
    Injection"), *Learning Context and Dependency Injection*) to a clustered environment.
    At first, as we said, we need to include the distributable stanza in your `web.xml`
    file to trigger clustering modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, apply the same changes to the `TheatreBox` singleton that we described
    in the *Turning your cache into a distributed cache* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our controller component is bound to a `@SessionScoped` state, you don''t
    need to apply any changes in order to propagate your session across server nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, remember to include the Infinispan dependency in your `META-INF/MANIFEST.MF`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Once your application is deployed on both the nodes of your cluster, you can
    test it by hitting the Apache web server (`http://localhost:6666/ticket-agency-cluster`
    in our example) and start booking tickets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Achieving HA in JSF applications](img/00097.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Since the `mod_cluster` subsystem is configured to use **sticky web sessions**
    by default, all subsequent requests from the same client will be redirected to
    the same server node. Therefore, by shutting down the sticky server node, you
    will get evidence that a new cluster view has been created and you can continue
    shopping on the other server node.
  prefs: []
  type: TYPE_NORMAL
- en: '![Achieving HA in JSF applications](img/00098.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was all about the world of clustered applications. Here, we took
    you through the robust clustering features of WildFly and applied them to some
    of the examples discussed in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The number of topics related to clustering might be expanded to cover a full
    book of its own; however, we decided to stress only on some features. In particular,
    we learned how to cluster EJBs and achieve fault tolerance in case there is a
    change in the server topology.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed clustering web applications and the integration with load
    balancing solutions such as the Apache web server and `mod_cluster`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will focus on some new topics added to Java EE 7 related
    to long-term task execution: batch processing and concurrency utilities'' usage.'
  prefs: []
  type: TYPE_NORMAL
