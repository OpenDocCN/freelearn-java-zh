- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Basics of Modern Data Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of the 21st century, due to more and more internet usage and
    more powerful data insight tools and technologies emerging, there has been a data
    explosion, and data has become the new gold. This has implied an increased demand
    for useful and actionable data, as well as the need for quality data engineering
    solutions. However, architecting and building scalable, reliable, and secure data
    engineering solutions is often complicated and challenging.
  prefs: []
  type: TYPE_NORMAL
- en: A poorly architected solution often fails to meet the needs of the business.
    Either the data quality is poor, it fails to meet the SLAs, or it’s not sustainable
    or scalable as the data grows in production. To help data engineers and architects
    build better solutions, every year, dozens of open source and preoperatory tools
    get released. Even a well-designed solution sometimes fails because of a poor
    choice or implementation of the tools.
  prefs: []
  type: TYPE_NORMAL
- en: This book discusses various architectural patterns, tools, and technologies
    with step-by-step hands-on explanations to help an architect choose the most suitable
    solution and technology stack to solve a data engineering problem. Specifically,
    it focuses on tips and tricks to make architectural decisions easier. It also
    covers other essential skills that a data architect requires such as data governance,
    data security, performance engineering, and effective architectural presentation
    to customers or upper management.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the landscape of data engineering and the basic
    features of data in modern business ecosystems. We will cover various categories
    of modern data engineering problems that a data architect tries to solve. Then,
    we will learn about the roles and responsibilities of a Java data architect. We
    will also discuss the challenges that a data architect faces while designing a
    data engineering solution. Finally, we will provide an overview of the techniques
    and tools that we’ll discuss in this book and how they will help an aspiring data
    architect do their job more efficiently and be more productive.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the landscape of data engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Responsibilities and challenges of a Java data architect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques to mitigate those challenges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the landscape of data engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn what data engineering is and why it is needed.
    You will also learn about the various categories of data engineering problems
    and some real-world scenarios where they are found. It is important to understand
    the varied nature of data engineering problems before you learn how to architect
    solutions for such real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: What is data engineering?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By definition, **data engineering** is the branch of software engineering that
    specializes in collecting, analyzing, transforming, and storing data in a usable
    and actionable form.
  prefs: []
  type: TYPE_NORMAL
- en: With the growth of social platforms, search engines, and online marketplaces,
    there has been an exponential increase in the rate of data generation. In 2020
    alone, around 2,500 petabytes of data was generated by humans each day. It is
    estimated that this figure will go up to 468 exabytes per day by 2025\. The high
    volume and availability of data have enabled rapid technological development in
    AI and data analytics. This has led businesses, corporations, and governments
    to gather insights like never before to give customers a better experience of
    their services.
  prefs: []
  type: TYPE_NORMAL
- en: However, raw data usually is seldom used. As a result, there is an increased
    demand for creating usable data, which is secure and reliable. Data engineering
    revolves around creating scalable solutions to collect the raw data and then analyze,
    validate, transform, and store it in a usable and actionable format. Optionally,
    in certain scenarios and organizations, in modern data engineering, businesses
    expect usable and actionable data to be published as a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive deeper, let’s explore a few practical use cases of data engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use case 1**: **American Express** (**Amex**) is a leading credit card provider,
    but it has a requirement to group customers with similar spending behavior together.
    This ensures that Amex can generate personalized offers and discounts for targeted
    customers. To do this, Amex needs to run a clustering algorithm on the data. However,
    the data is collected from various sources. A few data flows from MobileApp, a
    few flows from different Salesforce organizations such as sales and marketing,
    and a few data flows from logs and JSON events will be required. This data is
    known as raw data, and it can contain junk characters, missing fields, special
    characters, and sometimes unstructured data such as log files. Here, the data
    engineering team ingests that data from different sources, cleans it, transforms
    it, and stores it in a usable structured format. This ensures that the application
    that performs clustering can run on clean and sorted data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use case 2**: A health insurance provider receives data from multiple sources.
    This data comes from various consumer-facing applications, third-party vendors,
    Google Analytics, other marketing platforms, and mainframe batch jobs. However,
    the company wants a single data repository to be created that can serve different
    teams as the source of clean and sorted data. Such a requirement can be implemented
    with the help of data engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand data engineering, let’s look at a few of its basic concepts.
    We will start by looking at the dimensions of data.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions of data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any discussion on data engineering is incomplete without talking about the dimensions
    of data. The dimensions of data are some basic characteristics by which the nature
    of data can be analyzed. The starting point of data engineering is analyzing and
    understanding the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To successfully analyze and build a data-oriented solution, the four *V*s of
    modern data analysis are very important. These can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Dimensions of data ](img/B17084_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Dimensions of data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at each of these *V*s in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: This refers to the size of data. The size of the data can be as
    small as a few bytes to as big as a few hundred petabytes. Volume analysis usually
    involves understanding the size of the whole dataset or the size of a single data
    record or event. Understanding the size is essential in choosing the type of technologies
    and infrastructure sizing decisions to process and store the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: This refers to the speed at which data is getting generated.
    High-velocity data requires distributed processing. Analyzing the speed of data
    generation is especially critical for scenarios where businesses require usable
    data to be made available in real-time or near-real-time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: This refers to the various variations in the format in which the
    data source can generate the data. Usually, they can be one of the three following
    types:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured**: Structured data is where the number of columns, their data
    types, and their positions are fixed. All classical datasets that fit neatly in
    the relational data model are perfect examples of structured data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unstructured**: These datasets don’t conform to a specific structure. Each
    record in such a dataset can have any number of columns in any arbitrary format.
    Examples include audio and video files.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.json` or a `.xml` file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: This refers to the trustworthiness of the data. In simple terms,
    it is related to the quality of the data. Analyzing the noise of data is as important
    as analyzing any other aspect of the data. This is because this analysis helps
    create a robust processing rule that ultimately determines how successful a data
    engineering solution is. Many well-engineered and designed data engineering solutions
    fail in production due to a lack of understanding about the quality and noise
    of the source data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a fair idea of the characteristics by which the nature of data
    can be analyzed, let’s understand how they play a vital role in different types
    of data engineering problems.
  prefs: []
  type: TYPE_NORMAL
- en: Types of data engineering problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Broadly speaking, the kinds of problems that data engineers solve can be classified
    into two basic types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing problems**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Publishing problems**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at these problems in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Processing problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problems that are related to collecting raw data or events, processing them,
    and storing them in a usable or actionable data format are broadly categorized
    as processing problems. Typical use cases can be a data ingestion problem such
    as **Extract, Transform, Load** (**ETL**) or a data analytics problem such as
    generating a year-on-year report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, processing problems can be divided into three major categories, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Near real-time processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Categories of processing problems ](img/B17084_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Categories of processing problems
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at each one of these categories in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the SLA of processing is more than 1 hour (for example, if the processing
    needs to be done once in 2 hours, once daily, once weekly, or once biweekly),
    then such a problem is called a batch processing problem. This is because, when
    a system processes data at a longer time interval, it usually processes a batch
    of data records and not a single record/event. Hence, such processing is called
    **batch processing**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Batch processing problem ](img/B17084_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Batch processing problem
  prefs: []
  type: TYPE_NORMAL
- en: Usually, a batch processing solution depends on the volume of data. If the data
    volume is more than tens of terabytes, usually, it needs to be processed as big
    data. Also, since big data processes are schedule-driven, a workflow manager or
    schedular needs to run its jobs. We will discuss batch processing in more detail
    later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A **real-time processing** problem is a use case where raw data/events are to
    be processed on the fly and the response or the processing outcome should be available
    within seconds, or at most within 2 to 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, a real-time process receives data in the
    form of an event stream and immediately processes it. Then, it either sends the
    processed event to a sink or to another stream of events to be processed further.
    Since this kind of processing happens on a stream of events, this is known as
    real-time stream processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Real-time stream processing ](img/B17084_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Real-time stream processing
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 1.4*, event E0 gets processed and sent out by the streaming
    application, while events E1, E2 and E3 are waiting to be processed in the queue.
    At t1, event E1 also gets processed, showing continuous processing of events by
    streaming application
  prefs: []
  type: TYPE_NORMAL
- en: An event can generate at any time (24/7), which creates a new kind of problem.
    If the producer application of an event directly sends the event to a consumer,
    there is a chance of event loss, unless the consumer application is running 24/7\.
    Even bringing down the consumer application for maintenance or upgrades isn’t
    possible, which means there should be zero downtime for the consumer application.
    However, any application with zero downtime is not realistic. Such a model of
    communication between applications is called **point-to-point** communication.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge in point-to-point communication for real-time problems is
    the speed of processing as this should be always equal to or greater than that
    of a producer. Otherwise, there will be a loss of events or a possible memory
    overrun of the consumer. So, instead of directly sending events to the consumer
    application, they are sent asynchronously to an **Event Bus** or a **Message Bus**.
    An Event Bus is a high availability container that can hold events such as a queue
    or a topic. This pattern of sending and receiving data asynchronously by introducing
    a high availability Event Bus in between is called the **Pub-Sub framework**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some important terms related to real-time processing problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Events**: This can be defined as a data packet generated as a result of an
    action, a trigger, or an occurrence. They are also popularly known as **messages**
    in the Pub-Sub framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Producer**: A system or application that produces and sends events to a Message
    Bus is called a **publisher** or a **producer**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer**: A system or application that consumes events from a Message Bus
    to process is called a **consumer** or a **subscriber**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Queue**: This has a single producer and a single consumer. Once a message/event
    is consumed by a consumer, that event is removed from the queue. As an analogy,
    it’s like an SMS or an email sent to you by one of your friends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic**: Unlike a queue, a topic can have multiple consumers and producers.
    It’s a broadcasting channel. As an analogy, it’s like a TV channel such as HBO,
    where multiple producers are hosting their show, and if you have subscribed to
    that channel, you will be able to watch any of those shows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A real-world example of a real-time problem is credit card fraud detection,
    where you might have experienced an automated confirmation call to verify the
    authenticity of a transaction from your bank, if any transaction seems suspicious
    while being executed.
  prefs: []
  type: TYPE_NORMAL
- en: Near-real-time processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Near-real-time processing**, as its name suggests, is a problem whose response
    or processing time doesn’t need to be as fast as real time but should be less
    than 1 hour. One of the features of near-real-time processing is that it processes
    events in micro batches. For example, a near-real-time process may process data
    in a batch interval of every 5 minutes, a batch size of every 100 records, or
    a combination of both (whichever condition is satisfied first).'
  prefs: []
  type: TYPE_NORMAL
- en: At time tx, all events (E1, E2 and E3) that are generated between t0 and tx
    are processed together by near real-time processing job. Similarly all events
    (E4, E5 and E6) between time tx and tn are processed together.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Near-real-time processing ](img/B17084_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Near-real-time processing
  prefs: []
  type: TYPE_NORMAL
- en: Typical near-real-time use cases are recommendation problems such as product
    recommendations for services such as Amazon or video recommendations for services
    such as YouTube and Netflix.
  prefs: []
  type: TYPE_NORMAL
- en: Publishing problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Publishing problems deal with publishing the processed data to different businesses
    and teams so that data is easily available with proper security and data governance.
    Since the main goal of the publishing problem is to expose the data to a downstream
    system or an external application, having extremely robust data security and governance
    is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, in modern data architectures, data is published in one of three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Sorted data repositories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look at each.
  prefs: []
  type: TYPE_NORMAL
- en: Sorted data repositories
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sorted data repositories is a common term used for various kinds of repositories
    that are used to store processed data. This is usable and actionable data and
    can be directly queried by businesses, analytics teams, and other downstream applications
    for their use cases. They are broadly divided into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **data warehouse** is a central repository of integrated and structured data
    that’s mainly used for reporting, data analysis, and **Business Intelligence**
    (**BI**). A **data lake** consists of structured and unstructured data, which
    is mainly used for data preparation, reporting, advanced analytics, data science,
    and **Machine Learning** (**ML**). A **data hub** is the central repository of
    trusted, governed, and shared data, which enables seamless data sharing between
    diverse endpoints and connects business applications to analytic structures such
    as data warehouses and data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: Web services
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another publishing pattern is where data is published as a service, popularly
    known as **Data as a Service**. This data publishing pattern has many advantages
    as it enables security, immutability, and governance by design. Nowadays, as cloud
    technologies and GraphQL are becoming popular, Data-as-a-Service is getting a
    lot of traction in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two popular mechanisms of publishing Data as a Service are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: REST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraphQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss these techniques in detail later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There’s a popular saying: *A picture is worth a thousand words*. Visualization
    is a technique by which reports, analytics, and statistics about the data are
    captured visually in graphs and charts.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualization is helpful for businesses and leadership to understand, analyze,
    and get an overview of the data flowing in their business. This helps a lot in
    decision-making and business planning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few of the most common and popular visualization tools are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tableau** is a proprietary data visualization tool. This tool comes with
    multiple source connectors to import data into it and create easy fast visualization
    using drag-and-drop visualization components such as graphs and charts. You can
    find out more about this product at [https://www.tableau.com/](https://www.tableau.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microsoft Power BI** is a proprietary tool from Microsoft that allows you
    to collect data from various data sources to connect and create powerful dashboards
    and visualizations for BI. While both Tableau and Power BI offer data visualization
    and BI, Tableau is more suited for seasoned data analysts, while Power BI is useful
    for non-technical or inexperienced users. Also, Tableau works better with huge
    volumes of data compared to Power BI. You can find out more about this product
    at [https://powerbi.microsoft.com/](https://powerbi.microsoft.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticsearch-Kibana** is an open source tool whose source code is open source
    and has free versions for on-premise installations and paid subscriptions for
    cloud installation. This tool helps you ingest data from any data source into
    Elasticsearch and create visualizations and dashboards using Kibana. Elasticsearch
    is a powerful text-based **Lucene** search engine that not only stores the data
    but enables various kinds of data aggregation and analysis (including ML analysis).
    Kibana is a dashboarding tool that works together with Elasticsearch to create
    very powerful and useful visualizations. You can find out more about these products
    at [https://www.elastic.co/elastic-stack/](https://www.elastic.co/elastic-stack/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A Lucene index is a full-text inverse index. This index is extremely powerful
    and fast for text-based searches and is the core indexing technology behind most
    search engines. A Lucene index takes all the documents, splits them into words
    or tokens, and then creates an index for each word.
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache Superset** is a completely open source data visualization tool (developed
    by Airbnb). It is a powerful dashboarding tool and is completely free, but its
    data source connector support is limited, mostly to SQL databases. A few interesting
    features are its built-in role-based data access, an API for customization, and
    extendibility to support new visualization plugins. You can find out more about
    this product at [https://superset.apache.org/](https://superset.apache.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we have briefly discussed a few of the visualization tools available in
    the market, there are many visualizations and competitive alternatives available.
    Discussing data visualization in more depth is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have provided an overview of data engineering and the various types
    of data engineering problems. In the next section, we will explore what role a
    Java data architect plays in the data engineering landscape.
  prefs: []
  type: TYPE_NORMAL
- en: Responsibilities and challenges of a Java data architect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data architects are senior technical leaders who map business requirements to
    technical requirements, envision technical solutions to solve business problems,
    and establish data standards and principles. Data architects play a unique role,
    where they understand both the business and technology. They are like the *Janus*
    of business and technology, where on one hand they can look, understand, and communicate
    with the business, and on the other, they do the same with technology. Data architects
    create processes that are used to plan, specify, enable, create, acquire, maintain,
    use, archive, retrieve, control, and purge data. According to DAMMA’s data management
    body of knowledge, *a data architect provides a standard common business vocabulary,
    expresses strategic requirements, outlines high-level integrated designs to meet
    those requirements, and aligns with the enterprise strategy and related business
    architecture.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the cross-cutting concerns that a data architect
    handles:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Cross-cutting concerns of a data architect ](img/B17084_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Cross-cutting concerns of a data architect
  prefs: []
  type: TYPE_NORMAL
- en: 'The typical responsibilities of a Java data architect are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting business requirements into technical specifications, which includes
    data storage and integration patterns, databases, platforms, streams, transformations,
    and the technology stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing the architectural framework, standards, and principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing and designing reference architectures that are used as patterns that
    can be followed by others to create and improve data systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining data flows and their governance principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending the most suitable solutions, along with their technology stacks,
    while considering scalability, performance, resource availability, and cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinating and collaborating with multiple departments, stakeholders, partners,
    and external vendors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the real world, a data architect is supposed to play a combination of three
    disparate roles, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Multifaced role of a data architect ](img/B17084_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Multifaced role of a data architect
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at these three architectural roles in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data architectural gatekeeper**: An architectural gatekeeper is a person
    or a role that ensures the data model is following the necessary standards and
    that the architecture is following the proper architectural principles. They look
    for any gaps in terms of the solution or business expectations. Here, a data architect
    takes a negative role in finding faults or gaps in the product or solution design
    and delivery (including a lack of or any gap in best practices in the data model,
    architecture, implementation techniques, testing procedures, **continuous integration/continuous
    delivery** (**CI/CD**) efforts, or business expectations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data advisor**: A data advisor is a data architect that focuses more on finding
    solutions rather than finding a problem. A data advisor highlights issues, but
    more importantly, they show an opportunity or propose a solution for them. A data
    advisor should understand the technical as well as the business aspect of a problem
    and solution and should be able to advise to improve the solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business executive**: Apart from the technical roles that a data architect
    plays, the data architect needs to play an executive role as well. As stated earlier,
    the data architect is like the Janus of business and technology, so they are expected
    to be a great communicator and sales executive who can sell their idea or solution
    (that is technical) to nontechnical folks. Often, a data architect needs to present
    elevator speeches to higher leadership to show opportunities and convince them
    of a solution for business problems. To be successful in this role, a data architect
    must think like a business executive – *What is the ROI? Or what is there for
    me in it? How much can we save in terms of time and money with this solution or
    opportunity?* Also, a data architect should be concise and articulate in presenting
    their idea so that it creates immediate interest among the listeners (mostly business
    executives, clients, or investors).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s understand the difference between a data architect and data engineer.
  prefs: []
  type: TYPE_NORMAL
- en: Data architect versus data engineer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data architect and data engineer are related roles. A data architect visualizes,
    conceptualizes, and creates the blueprint of the data engineering solution and
    framework, while the data engineer takes the blueprint and implements the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Data architects are responsible for putting data chaos in order, generated by
    enormous piles of business data. Each data analytics or data science team requires
    a data architect who can visualize and design the data framework to create clean,
    analyzed, managed, formatted, and secure data. This framework can be utilized
    further by data engineers, data analysts, and data scientists for their work.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of a data architect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data architects face a lot of challenges in their day-to-day work. We will
    be focusing on the main challenges that a data architect faces on a day-to-day
    basis:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architectural pattern
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the best-fit technology stack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of actionable data governance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommending and communicating effectively to leadership
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architectural pattern
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A single data engineering problem can be solved in many ways. However, with
    the ever-evolving expectations of customers and the evolution of new technologies,
    choosing the correct architectural pattern has become more challenging. What is
    more interesting is that with the changing technological landscape, the need for
    agility and extensibility in architecture has increased many folds to avoid unnecessary
    costs and sustainability of architecture over time.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the best-fit technology stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the complex problems that a data architect needs to figure out is the
    technology stack. Even when you have created a very well-architected solution,
    whether your solution will fly or flop will depend on the technology stack you
    are choosing and how you are planning to use it. As more and more tools, technologies,
    databases, and frameworks are developed, a big challenge remains for data architects
    to choose an optimum tech stack that can help create a scalable, reliable, and
    robust solution. Often, a data architect needs to take into account other non-technical
    factors as well, such as the future growth prediction of the tool, the market
    availability of skilled resources for those tools, vendor lock-in, cost, and community
    support options.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of actionable data governance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data governance is a buzzword in data businesses, but what does it mean? Governance
    is a broad area that includes both workflows and toolsets to govern data. If either
    the tools or the workflow process has limitations or is not present, then data
    governance is incomplete. When we talk about actionable governance, we mean the
    following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating data governance with all data engineering systems to maintain standard
    metadata, including traceability of events and logs for a standard timeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating data governance concerning all the security policies and standards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role-based and user-based access management policies on all data elements and
    systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adherence to defined metrics that are tracked continually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating data governance and the data architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data governance should always be aligned with strategic and organizational goals.
  prefs: []
  type: TYPE_NORMAL
- en: Recommending and communicating effectively to leadership
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Creating an optimal architecture and the correct set of tools is a challenging
    task, but it never is enough, unless and until they are not put into practice.
    One of the hats that a data architect often needs to wear is that of a sales executive
    who needs to sell their solution to the business executive or upper leadership.
    These are not usually technical people and they don’t have a lot of time. Data
    architects, most of whom have strong technical backgrounds, face the daunting
    task of communicating and selling their idea to these people. To convince them
    about the opportunity and the idea, a data architect needs to back them up with
    proper decision metrics and information that can align that opportunity to the
    broader business goals of the organization.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen the role of a data architect and the common problems that
    they face. In the next section, we will provide an overview of how a data architect
    mitigates those challenges on a day-to-day basis.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques to mitigate those challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss how a data architect can mitigate the aforementioned
    challenges. To understand the mitigation plan, it is important to understand what
    the life cycle of a data architecture looks like and how a data architect contributes
    to it. The following diagram shows the life cycle of a data architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Life cycle of a data architecture ](img/B17084_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Life cycle of a data architecture
  prefs: []
  type: TYPE_NORMAL
- en: The data architecture starts with defining the problem that the business is
    facing. Here, this is mainly identified or reported by business teams or customers.
    Then, the data architects work closely with the business to define the business
    requirements. However, in a data engineering landscape, that is not enough. In
    a lot of cases, there are hidden requirements or anomalies. To mitigate such problems,
    business analysts team up with data architects to analyze data and the current
    state of the system, including any existing solution, the current cost, or loss
    of revenue due to the problem and infrastructure where data resides. This helps
    refine the business requirements. Once the business requirements are more or less
    frozen, the data architects map the business requirements to the technical requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the data architect defines the standards and principles of the architecture
    and determines the priorities of the architecture based on the business need and
    budget. After that, the data architect creates the most suitable architectures,
    along with their proposed technology stack. In this phase, the data architects
    closely work with the data engineers to implement **proof of concept** (**POCs**)
    and evaluate the proposed solution in terms of feasibility, scalability, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the architects recommend solutions based on the evaluation results
    and architectural priorities defined earlier. The data architects present the
    proposed solutions to the business. Based on priorities such as cost, timeline,
    operational cost, and resource availability, feedback is received from the business
    and clients. It takes a few iterations to solidify and get an agreement on the
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Once an agreement has been reached, the solution is implemented. Based on the
    implementation challenges and particular use cases, the architecture may or may
    not be revised or tweaked a little. Once an architecture is implemented and goes
    to production, it enters the maintenance and operations phase. During maintenance
    and operations, sometimes, feedback is provided, which might result in a few architectural
    improvements and changes, but they are often seldom if the solution is well-architected
    in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the blue boxes indicate major involvement from a customer,
    a green box indicates major involvement from a data architect, a yellow box means
    a data architect equally shares involvement with another stakeholder, and a gray
    box means the data architect has the least involvement in that scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have understood the life cycle of the data architecture and a data
    architect’s role in various phases, we will focus on how to mitigate those challenges
    that are faced by a data architect. This book covers how to mitigate those challenges
    in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Understanding the business data, its characteristics, and storage options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data and its characteristics were covered earlier in this chapter; it will also
    be covered partly in [*Chapter 2*](B17084_02.xhtml#_idTextAnchor025), *Data Storage
    and Databases*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage options will also be discussed in [*Chapter 2*](B17084_02.xhtml#_idTextAnchor025),
    *Data Storage and Databases*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyzing and defining the business problem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the various kinds of data engineering problems (covered in this
    chapter)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have provided a step-by-step analysis of how an architect should analyze
    a business problem, classify, and define it in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062),
    *ETL Data Load – A Batch-Based Solution to Ingest Data in a Data Warehouse*, [*Chapter
    5*](B17084_05.xhtml#_idTextAnchor074), *Architecting a Batch Processing Pipeline*,
    and [*Chapter 6*](B17084_06.xhtml#_idTextAnchor092), *Architecting a Real-Time
    Processing Pipeline*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The challenge of choosing the right architecture. To choose the right architectural
    pattern, we should be aware of the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of data engineering problems and the dimensions of data (we discussed
    this in this chapter)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The different types of data and various data storage available ([*Chapter 2*](B17084_02.xhtml#_idTextAnchor025),
    *Data Storage and Databases*)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to model and design different kinds of data while storing it in a database
    ([*Chapter 2*](B17084_02.xhtml#_idTextAnchor025), *Data Storage and Databases*)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding various architectural patterns for data processing problems ([*Chapter
    7*](B17084_07.xhtml#_idTextAnchor110), *Core Architectural Design Patterns*)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the architectural patterns of publishing the data (*Section 3*,
    *Enabling Data as a Service*)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The challenge of choosing the best-fit technology stack and data platform.
    To choose the correct set of tools, we need to know how to use a tool and when
    to use what tools we have:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to choose the correct database will be discussed in [*Chapter 2*](B17084_02.xhtml#_idTextAnchor025),
    *Data Storage and Databases*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to choose the correct platform will be discussed in [*Chapter 3*](B17084_03.xhtml#_idTextAnchor043),
    *Identifying the Right Data Platform*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A step-by-step hands-on guide to using different tools in batch processing will
    be covered in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load
    – A Batch-Based Solution to Ingest Data in a Data Warehouse*, and [*Chapter 5*](B17084_05.xhtml#_idTextAnchor074),
    *Architecting a Batch Processing Pipeline*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A step-by-step guide to architecting real-time stream processing and choosing
    the correct tools will be covered in [*Chapter 6*](B17084_06.xhtml#_idTextAnchor092),
    *Architecting a Real-Time Processing Pipeline*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The different tools and technologies used in data publishing will be discussed
    in [*Chapter 9*](B17084_09.xhtml#_idTextAnchor144), *Exposing MongoDB Data as
    a Service*, and [*Chapter 10*](B17084_10.xhtml#_idTextAnchor157), *Federated and
    Scalable DaaS with GraphQL*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The challenge of creating a design for scalability and performance will be
    covered in [*Chapter 11*](B17084_11.xhtml#_idTextAnchor168), *Measuring Performance
    and Benchmarking Your Applications*. Here, we will discuss the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance engineering basics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The publishing performance benchmark
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance optimization and tuning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge of a lack of data governance. Various data governance and security
    principles and tools will be discussed in [*Chapter 8*](B17084_08.xhtml#_idTextAnchor130),
    *Enabling Data Security and Governance*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge of evaluating architectural solutions and recommending them to
    leadership. In the final chapter of this book ([*Chapter 12*](B17084_12.xhtml#_idTextAnchor180),
    *Evaluating, Recommending, and Presenting Your Solution*), we will use the various
    concepts that we have learned throughout this book to create actionable data metrics
    and determine the most optimized solution. Finally, we will discuss techniques
    that an architect can apply to effectively communicate with business stakeholders,
    executive leadership, and investors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed how this book can help an architect overcome the
    various challenges they will face and make them more effective in their role.
    Now, let’s summarize this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what data engineering is and looked at a few practical
    examples of data engineering. Then, we covered the basics of data engineering,
    including the dimensions of data and the kinds of problems that are solved by
    data engineers. We also provided a high-level overview of various kinds of processing
    problems and publishing problems in a data engineering landscape. Then, we discussed
    the roles and responsibilities of a data architect and the kind of challenges
    they face. We also briefly covered the way this book will guide you to overcome
    challenges and dilemmas faced by a data architect and help you become a better
    Java data architect.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the basic landscape of data engineering and what this
    book will focus on, in the next chapter, we will walk through various data formats,
    data storage options, and databases and learn how to choose one for the problem
    at hand.
  prefs: []
  type: TYPE_NORMAL
