- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced API Concepts and Implementations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating robust and high-performance RESTful APIs involves more than just setting
    up endpoints and handling CRUD operations. As applications grow and user demands
    increase, incorporating advanced strategies that keep your APIs efficient, reliable,
    and resilient under various conditions becomes essential. This chapter explores
    these strategies, focusing on data-handling techniques such as pagination, filtering,
    and file uploads and downloads, as well as resilience mechanisms crucial for modern
    API development.
  prefs: []
  type: TYPE_NORMAL
- en: On the data management front, we will examine key practices such as pagination,
    filtering, and the efficient uploading and downloading of files through REST APIs.
    These methods are designed to boost the responsiveness and scalability of your
    APIs, ensuring they can seamlessly manage higher loads and more complex data interactions.
    Some of these topics are related to performance, but we will cover performance
    optimization in greater detail in [*Chapter 10*](B21843_10.xhtml#_idTextAnchor284)
    .
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will explore critical resilience mechanisms, including timeouts,
    retry strategies, rate limiting, throttling, idempotency keys, circuit breakers,
    and bulkheads. These approaches help safeguard your APIs against failures, manage
    traffic effectively, and maintain stability even under unpredictable or adverse
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a developer’s perspective, anticipating how systems will behave in real-world
    scenarios can be challenging. During development, we often operate in ideal settings:
    running the API locally, being the sole user, and conducting tests with minimal
    variables that might cause failures. This controlled environment can obscure the
    complexities and potential issues that emerge in production, where the API must
    support multiple users, handle varying loads, and respond to unexpected events.
    By incorporating the advanced concepts and implementations discussed in this chapter,
    you will be better equipped to build APIs that perform reliably and efficiently
    in any environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will find several different approaches that will help
    you to improve your API depending on your requirements. For this chapter, we recommend
    you catch up with the code from the previous chapter; some small enhancements
    for this chapter are available at the following link: [https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter6](https://github.com/PacktPublishing/Mastering-RESTful-Web-Services-with-Java/tree/main/chapter6)
    . We use the `curl` command-line tool as a client to test the APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: Data handling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data handling** is the cornerstone of building robust and user-friendly APIs.
    In this section, we will explore advanced data-handling techniques that address
    common challenges faced in real-world API development.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring that an API delivers reliable performance is crucial for several interconnected
    reasons, including enhancing user experience, enabling system scalability, optimizing
    resource efficiency, and influencing the success of the business behind API development.
  prefs: []
  type: TYPE_NORMAL
- en: A high-performing API must respond quickly and communicate clearly with clients.
    This means returning appropriate HTTP status codes (such as 200 for success or
    404 for not found) along with descriptive messages that explain what happened.
    Low latency is crucial because users expect fast responses, and slow APIs can
    create system bottlenecks that frustrate users.
  prefs: []
  type: TYPE_NORMAL
- en: When APIs take too long to respond, they cause delays throughout the entire
    application. Modern systems often need real-time data updates, so APIs must handle
    frequent requests efficiently while maintaining consistent response times. Clear
    communication is equally important—when errors occur, clients need informative
    messages that explain the problem and suggest solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Effective APIs balance speed with clarity. They process requests quickly while
    providing meaningful feedback through proper status codes and helpful messages.
    This approach ensures both end users and developers can work with the API effectively,
    reducing confusion and debugging time.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing response speed and message quality helps APIs handle increased traffic
    without compromising performance or raising operational costs. These improvements
    support both user satisfaction and business growth.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore data-handling techniques that improve
    API performance. We will start with pagination for managing large datasets, then
    cover filtering and efficient file operations through REST APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Paginatio n
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieving a large number of records from one or multiple database tables and
    converting them into a data structure compatible with your language/framework,
    such as in Java with Spring Boot, consumes resources, including memory and CPU
    processing power. This process continues until the data is transformed into a
    standard format such as JSON for communication and is ready to be sent to the
    client. Additionally, even after the data is prepared, resources are still used
    to transmit it over the network to the client.
  prefs: []
  type: TYPE_NORMAL
- en: Often, it is unnecessary to send all the data represented by a resource in a
    single, large payload to the client. Doing so consumes more resources, degrades
    the system performance, and causes a poor user experience. These may be accompanied
    by the increased costs associated with cloud computing, which are billed based
    on usage. Processing and transferring larger amounts of data also takes more time.
    Instead, the data can be divided into smaller chunks, which require less time
    and fewer resources, thereby making the API more reliable and cost-effective to
    maintain. This approach is known as **pagination** , which enhances API responsiveness
    and conserves resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example: our API contains 100,000 products, and we have
    a **single-page application** ( **SPA** ) that consumes this API to display products
    in a data table on the **Products** page. Imagine that every time a user opens
    this page, the API is called to access the database, load these records into memory,
    convert them to JSON, and then transfer this large amount of data via an internet
    request each time a user accesses the page. Now, imagine that this system is used
    by numerous users daily. Without pagination, each user action would trigger resource-intensive
    processes, leading to increased costs, slower response times, and a diminished
    user experience. By implementing pagination, the API can deliver smaller, more
    manageable datasets, improving performance and scalability while reducing operational
    expenses. All of that can be done without limiting the user interface view of
    the application, because the user will never see all the products displayed at
    the same time.'
  prefs: []
  type: TYPE_NORMAL
- en: Different pagination approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some common approaches to REST API pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: Offset-based pagination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page-based pagination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cursor-based pagination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keyset pagination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at each of them
  prefs: []
  type: TYPE_NORMAL
- en: Offset-based pagination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Offset-based pagination** is a straightforward and widely adopted technique.
    It works by defining an offset (the starting position) and a limit (the number
    of records to retrieve per page) to navigate through the dataset. This is the
    approach that we can find in the APIs of several different companies, such as
    OpenWeatherMap, Stripe, Adidas, and Mailchimp.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of offset-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity** : The method is straightforward to implement and easy to understand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility** : By adjusting the offset, you can directly jump to any position
    that you may want'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of offset-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance issues** : As the offset increases, query performance declines
    because the database must scan and skip over a growing number of records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency** : Data changes such as inserts or deletes between requests
    can lead to inconsistent results, causing duplicate or missing records across
    pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clients can manipulate the business logic, which does not guarantee a single
    experience for all clients. It can be considered a benefit, though, if the API
    is meant to be very flexible.
  prefs: []
  type: TYPE_NORMAL
- en: Most likely, this will be passed to a database query such as SQL, which, without
    proper input sanitization and parameterization, can cause SQL injection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of an API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Page-based pagination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Page-based pagination** divides a dataset into pages, allowing clients to
    navigate the data by specifying a page number. This approach is simple and hence
    widely adopted by companies in the API market, for example, Salesforce and Microsoft.'
  prefs: []
  type: TYPE_NORMAL
- en: To use it, you need to specify the page you want to retrieve ( `page` ) and
    the number of records per page ( `page_size` ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of page-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User-friendliness** : Intuitive for users to navigate to specific pages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplicity** : Easy to implement and understand'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of page-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance issues** : Like offset-based pagination, large page numbers can
    cause performance degradation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less flexibility** : The returned list of records must start at a page boundary.
    This may not be convenient for user interfaces using scrollbars instead of the
    classical pagination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of an API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Page-based pagination for the `GET /products` endpoint of our example Product
    API was implemented while demonstrating API evolution in [*Chapter 5*](B21843_05.xhtml#_idTextAnchor116)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Cursor-based pagination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Cursor-based pagination** uses a pointer (cursor) to keep track of the current
    position in the dataset. Instead of specifying an offset, clients use a cursor
    to request the next set of records and a limit that represents the number of records
    we want to return per chunk of data. That is the approach implemented by the X
    (formerly Twitter) API to paginate the data of tweets, followers, and other resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of cursor-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance efficiency** : Maintains reliable performance regardless of dataset
    size since it does not require skipping records'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data consistency** : Less susceptible to data inconsistencies between requests,
    ensuring more reliable results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of cursor-based pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity** : The implementation is more complex when compared to offset-based
    pagination'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Less flexibility** : It is suited for sequential navigation and may not be
    a good option if you need to jump to arbitrary pages'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of an API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Keyset pagination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Keyset pagination** is a variation of cursor-based pagination; it leverages
    a unique key—typically a timestamp or a **universally unique identifier** ( **UUID**
    )—to navigate through records. Unlike offset-based pagination, which relies on
    numerical offsets that can become increasingly inefficient with larger datasets,
    keyset pagination uses the unique key to mark the position in the dataset. This
    approach ensures faster query performance and more consistent response times,
    making it particularly well suited for applications that require real-time data
    access and scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the advantages of keyset pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance** : Keyset pagination offers better performance for large datasets
    by eliminating the need for the database to count or skip over large numbers of
    records, as is necessary with offset-based pagination'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability** : As datasets grow, keyset pagination maintains consistent
    response times, making it a scalable solution for high-traffic applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability** : By relying on a unique key for navigation, keyset pagination
    reduces the risk of encountering missing or duplicate records, ensuring data integrity
    across paginated results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the disadvantages of keyset pagination:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential navigation only** : Keyset pagination requires sequential data
    access. Unlike offset-based pagination, which allows users to jump directly to
    any page by specifying an offset, keyset pagination requires navigating through
    records in a linear order. This limitation can be restrictive for applications
    where users need to access non-sequential pages or perform random access within
    the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bookmark dependency** : To access a specific page, the client must retain
    the unique key (cursor) from the preceding pages. This dependency can complicate
    client-side logic, especially in scenarios where users might want to revisit or
    share specific pages without maintaining a history of cursors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is an example of an API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After selecting the pagination strategy to apply to all our endpoints and APIs,
    we should also return the information about the pagination. Providing information
    such as total items, pages, and the current page helps clients better navigate
    large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Different approaches to return pagination information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are many ways to return pagination information to the client. The most
    common are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using response headers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Including pagination information in the response body
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using hypermedia (HATEOAS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using response headers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This strategy involves embedding pagination information directly within the
    HTTP headers, such as details about the total number of pages, the current page,
    and other relevant information. This approach is not commonly used in isolation,
    and it is often combined with other strategies to enhance API functionality. For
    instance, GitLab employs this method alongside hypermedia strategies to provide
    a more comprehensive user experience. We can combine filtering with pagination
    to retrieve specific data, allowing us to manage large datasets by breaking them
    into paginated sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we can see four custom headers to inform the client
    about how to navigate through the API effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Including pagination information in the response body
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this approach, the API includes paginated data within the response body alongside
    relevant pagination metadata, such as total pages and the current page. For example,
    the Stripe API employs this strategy by returning both the data and a `has_more`
    flag that indicates whether additional results are available. This method offers
    clarity by directly integrating pagination details with the response, letting
    clients understand their data context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some APIs separate the response data from the metadata, which can also be effective.
    An examp le structure is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this structure, the `data` field contains the actual results from the API,
    while the `pagination` section provides essential metadata about the paginated
    response. This approach enhances usability by clearly organizing both the data
    and its associated pagination information, making it easier for clients to navigate
    and process the results.
  prefs: []
  type: TYPE_NORMAL
- en: Using hypermedia (HATEOAS)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: HATEOAS enhances API responses by embedding navigational links that guide clients
    on how to navigate and perform actions based on the server’s response. As we discussed
    in [*Chapter 1*](B21843_01.xhtml#_idTextAnchor015) , HATEOAS represents the fourth
    level of maturity in REST architecture, helping to decouple the client from the
    server. We will dive deep into this subject in the *HATEOAS* subsection later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This strategy empowers clients to interact with the API dynamically, without
    requiring prior knowledge of the structure or additional documentation. For example,
    APIs such as GitHub incorporate HATEOAS to make navigation through resources,
    such as paginated data, straightforward and intuitive. In the following code block,
    we can see an example of using this strategy to help the client move through the
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding code, the `_links` attribute provides key navigational
    details, including information about the current request and links to the previous
    and next pages, as well as the first and last pages. With these comprehensive
    links, clients can effortlessly navigate through the API without needing to construct
    additional requests manually.
  prefs: []
  type: TYPE_NORMAL
- en: This structure streamlines the client’s interaction with the data, embedding
    the necessary navigation information within each response. By supplying these
    links, the API enables developers to create more intuitive and seamless user experiences.
  prefs: []
  type: TYPE_NORMAL
- en: In API development, efficient data handling and robust pagination strategies
    are not merely supplementary features—they are essential points that underpin
    the efficacy, scalability, and overall user experience of your applications. We
    have outlined some different pagination approaches with their details, to ensure
    that your APIs deliver reliable and high-performing services. Regardless of your
    chosen approach, the key is maintaining consistency across all endpoints. Ensuring
    that every API follows the same pagination standard helps simplify integration
    for clients and promotes a more cohesive experience.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore how to filter data returned by the API
    by applying some strategies that allow us to combine pagination and filtering.
  prefs: []
  type: TYPE_NORMAL
- en: While pagination alone enables us to manage large datasets by breaking them
    into manageable pages, it may not fully address the client’s needs when specific
    subsets of data are required. To improve data handling further, we can incorporate
    filtering, which will allow clients to retrieve only the data they need within
    each page. By combining pagination with filtering, we offer a more flexible and
    efficient approach to data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Filtering** allows API clients to inform our API that we need some specific
    subset of the data on that resource, which means that it will not waste resources
    processing data that we are not interested in at that moment; we can concentrate
    exclusively on the data that we need. We can combine filtering with pagination
    to retrieve specific data, allowing us to manage large datasets by breaking them
    into paginated sections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some principles for filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statelessness** : The REST API should be stateless, meaning that every request
    from a client must contain all the necessary information that will give the server
    the capability to fulfill the request. Filtering is achieved by including filter
    parameters in the query string of the request URL, without the need to save this
    state on the API server side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency and predictability** : The filtering syntax should be consistent
    across all API endpoints. This ensures predictability across different endpoints
    and makes it easy for clients to understand the filtering by looking at the request.
    For example, if we are filtering by a first name parameter using `` `?firstName=Peter`
    `` in one endpoint, it should work similarly across other endpoints where such
    filtering is relevant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Granularity and flexibility** : An effective filtering mechanism allows clients
    to precisely define their requirements while being adaptable enough to handle
    different scenarios. For instance, filters should accommodate different data types
    (such as strings, numbers, and dates) and a range of operators (such as equals,
    not equals, greater than, less than, in, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different filtering approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Filtering can be implemented via several different approaches. It can be implemented
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Basic field filtering** : This is the most straightforward approach, where
    you can pass the field name to the resource and the value that you want to filter
    by, which is usually done by passing these values as a query parameter on the
    URL:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Multiple field filtering** : This is the same as the previous one, with the
    only difference being that we will pass multiple field filters and their respective
    values for filtering the data. To separate the `field=value` set, we will use
    the `''&''` character:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is possible to achieve filtering by a range of values, combining the same
    field name preceded by the `min` and `max` prefixes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Filtering with comparison operators** : Some APIs also provide the capability
    of handling comparison operators; this gives a chance to build a more sophisticated
    set of filtering combinations than a simple field equals value:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**List filtering (inclusion/exclusion)** : Using this approach, we can use
    an operator of `in` or `nin` (not in) on a list:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you will realize by now, pagination and filtering are two concepts that
    you cannot miss in your API implementation. Furthermore, they need to be combined
    to exploit their advantages on the endpoints of our APIs that are candidates to
    return lists of data. These are some best practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Use significant, clear, and intuitive parameter names that guide the user to
    avoid errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate and sanitize the inputs to avoid attacks and typos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate and implement default limits to avoid the user requesting large values
    and the server having to pass excessive volumes of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keep consistent naming, response metadata, and approaches across the system
    to provide a good user/developer experience, making the structure and parametrization
    standardized and easy to consume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering is a critical aspect of robust API design, enabling clients to retrieve
    the data they need without overburdening the server or the network. By implementing
    well-structured filtering mechanisms, APIs can achieve higher performance, scalability,
    and user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Employing various filtering approaches—from basic field filters to advanced
    comparison operators and list-based criteria—allows APIs to cater to diverse client
    requirements and complex querying needs. Moreover, integrating filtering with
    pagination not only enhances data management but also ensures that responses remain
    swift and manageable, even when dealing with extensive datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore a practical example that builds upon our paginated endpoint.
    Here, we add filtering capabilities to allow clients to specify criteria such
    as categories or price ranges. Combining pagination and filtering not only enhances
    data management but also is a best practice that provides a streamlined experience
    for clients, keeping responses fast and relevant even as datasets grow.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an e-commerce API endpoint that returns a list of products. This endpoint
    allows clients to request specific pages of data and apply filters to retrieve
    only the products they need, reducing data load on both the client and server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of an API call using pagination and filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It should return something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Best practices when paginating and filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s take a look at some of the best practices to follow when paginating and
    filtering:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use clear and intuitive parameter names** : The parameter names, such as
    `page` , `limit` , `category` , `priceMin` , and `priceMax` , should be simple,
    self-explanatory, and aligned with common API design conventions, making the endpoint
    easy to use and understand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply input validation and sanitization** : The API validates all parameters
    to ensure they are correctly formatted. For instance, `page` and `limit` must
    be integers, while `priceMin` and `priceMax` must be decimal numbers. Additionally,
    the `sort` parameter only accepts specific values, preventing injection attacks
    and typos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apply default limits and constraints** : In our example, a default limit
    of 10 items per page is applied, and the `limit` parameter cannot exceed `100`
    . These measures protect the server from handling overly large requests, enhancing
    performance and preventing abuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Employ consistent naming and metadata** : The response includes metadata—
    `page` , `limit` , `totalPages` , and `totalItems` —that provides clients with
    essential information about the pagination state, ensuring a standardized and
    predictable experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Combine pagination and filtering** : This example demonstrates how pagination
    and filtering work together. The client can specify page numbers and filters simultaneously,
    allowing them to retrieve exactly the data they need without overloading the server
    or client with unnecessary information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By applying these best practices, the API achieves a balance of flexibility,
    performance, and user-friendliness, ensuring that clients can access data efficiently
    while maintaining robust, scalable API design.
  prefs: []
  type: TYPE_NORMAL
- en: With pagination and filtering, we have seen how to manage large datasets efficiently
    by selectively retrieving information. However, in many applications, handling
    data is not limited to managing structured information alone. Often, APIs need
    to support the uploading and downloading of files, allowing clients to transfer
    data in formats such as images, documents, and reports.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into best practices and techniques for handling
    file uploads and downloads via REST APIs. These strategies will help ensure secure,
    reliable file transfers that integrate smoothly with the rest of the API.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading and downloading files via the REST API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Web applications often need to support file uploads and downloads. This functionality
    is usually accompanied by additional requirements, such as validating file types,
    names, and maximum sizes, determining optimal storage solutions, providing meaningful
    responses, and handling large files efficiently through streaming. In this section,
    we will discuss these common topics in detail.
  prefs: []
  type: TYPE_NORMAL
- en: When designing controllers to handle file uploads, it is essential to ensure
    that the parameter for the uploaded file is defined using the Spring Web interface,
    that is, the Spring Boot interface that represents an uploaded file in a multi-part
    request— `org.springframework.web.multipart.MultipartFile` . Additionally, the
    request must have its `Content-Type` set to `multipart/form-data` , the standard
    format used by web browsers for uploading files, so this format is also expected
    by an API service using `MultipartFile` .
  prefs: []
  type: TYPE_NORMAL
- en: We can demonstrate the upload and download functionality by adding a subresource,
    `/products/{productId}/photo` , to our example Product API, which will represent
    a photo image of the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example HTTP request and method to upload a single file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `multipart/form-data` format represents a form that can contain multiple
    fields. It is also possible to use it to upload multiple files in one request.
    This is why the `"file"` parameter name in the preceding example is important
    for identifying the file our code wants to work with.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example HTTP request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the method to upload multiple files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Our example code stores the uploaded file content as a **large object** ( **LOB**
    ) in the relational database for simplicity. Relational databases are not optimized
    for this kind of data, so in large-scale systems, other types of storage are commonly
    used.
  prefs: []
  type: TYPE_NORMAL
- en: The file could be saved in an external storage provider, such as AWS S3. This
    cloud-based storage solution efficiently manages files, providing a link for accessing
    the stored content. This approach is increasingly popular in the industry as it
    optimizes resource management and leverages the scalability of cloud infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: By saving files in cloud data storage and exposing only the link through the
    API, developers can avoid loading large resources directly. This not only enhances
    performance but also improves response times for clients, allowing them to retrieve
    files as needed without overloading the API.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding examples, the API returns a string in the response, so the
    returned string could be used to pass the cloud storage URL.
  prefs: []
  type: TYPE_NORMAL
- en: It is advisable to validate uploaded files to ensure security, data integrity,
    and performance. Some of the possible validations are described in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the uploaded files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The validation step is critical not only for RESTful APIs but also for any system
    that you may be developing, to maintain the integrity and security of your application.
    The inputs you receive can make the application vulnerable to several distinct
    kinds of attacks. Let us see some examples of important validations to be performed
    when uploading files.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the file content type
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accepting files of only a certain type would be a good measure to avoid receiving
    malicious files or even data that does not comply with the data you need to store.
    In the following example, you ensure that you will only receive PNG and JPEG files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the content type of the whole HTTP request body will be `multipart/form-data`
    , but each of the files it contains also has its own content type. This is why
    we need to check the content type using the `getContentType` method of the particular
    `MultipartFile` object we are going to process.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the content of the file, our example Product API stores the content
    type of the uploaded file. The same content type is returned by the `GET` endpoint
    when the file is downloaded. Without the correct `Content-Type` header, opening
    the URL in a web browser would display gibberish because the browser would not
    know how to interpret the downloaded file bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the file size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Validating the size of a file is a measure to avoid processing files that are
    bigger than expected, which can cause denial of service by exhaustion of memory
    due to the overloading of the system with excessively large files, irrespective
    of whether the upload is intentional or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the file size programmatically using the respective method of
    the `MultipartFile` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'File uploads are also limited for the whole application by Spring configuration
    properties specifying the maximum acceptable multipart request size and the maximum
    size of one file within such a request. You can set the properties in the `application.yml`
    or `application.properties` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Validating filenames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To improve file upload security on your server, validating filenames is essential
    to prevent attacks such as path traversal, where attackers attempt to access restricted
    areas on your server’s filesystem. For example, an attacker might use a filename
    such as `../../etc/passwd` to try to access sensitive files outside the intended
    upload directory.
  prefs: []
  type: TYPE_NORMAL
- en: Path traversal attacks can be particularly dangerous in file uploads. By manipulating
    the file path, an attacker may exploit weak validation to save files in unintended
    locations on the server, potentially leading to the exposure of sensitive data
    or even overwriting critical files. For example, if the server directly appends
    a user-provided filename to an upload path, an attacker could use `../` sequences
    to navigate up the directory structure, allowing them to access or modify restricted
    files. This is why validation is crucial when handling file paths, especially
    in upload scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are several ways to prevent path traversal attacks, here are some
    effective strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set user and file permissions carefully** : Restrict permissions to limit
    access only to necessary users and directories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store files separately** : Consider storing uploaded files on a different
    server or in a secure storage service such as AWS S3. By keeping files off the
    main server, you reduce the risk of unauthorized access to your system’s file
    structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validate the file path** : Ensure that the file path points to a specific
    folder that is expected for the files to be sent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use secure file upload resolvers** : In Spring, classes such as `CommonsMultipartResolver`
    and `StandardServletMultipartResolver` , as resolvers, are responsible for resolving
    or interpreting multipart data within a request and help manage file uploads securely.
    They separate file parts from other form data and make them accessible as `MultipartFile`
    objects within Spring’s request-handling framework. Both of these classes are
    responsible for parsing multipart file requests, which are commonly used in file
    uploads. They handle the file data separately from user input, which helps prevent
    injection attacks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CommonsMultipartResolver** : This class, part of the Apache Commons `FileUpload`
    library, allows you to configure file size limits and temporary storage directories.
    It offers flexibility for applications that require strict control over file storage
    and performance. By setting up limits and constraints through `CommonsMultipartResolver`
    , you can mitigate risks such as excessive file uploads or unauthorized access
    to temporary storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StandardServletMultipartResolver** : This is a resolver in Spring that leverages
    the built-in multipart support provided by Servlet 3.0, making it a reliable and
    efficient choice for handling file uploads in Spring applications. By relying
    on the servlet container’s native multipart processing, this resolver avoids the
    need for additional libraries, simplifying setup and maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These resolvers not only simplify file handling but also help mitigate potential
    security vulnerabilities by handling multipart file requests in a standardized
    way. This ensures that files are processed according to predefined rules, which
    reduces the risk of path traversal and other injection attacks.
  prefs: []
  type: TYPE_NORMAL
- en: When working with files stored on your application’s server, remember not to
    rely solely on methods such as `StringUtils.cleanPath` . Although commonly used,
    it should not be the only security measure because it does not fully protect against
    path traversal. As noted in the Spring documentation (v6.1.12), “ `cleanPath`
    *should not be depended upon in a security context* .”
  prefs: []
  type: TYPE_NORMAL
- en: To enhance filename sanitization, you can use the `FilenameUtils` class from
    the Apache Commons IO library. This utility class provides methods to manipulate
    and sanitize filenames safely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of codes that would help to cover some file upload validations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach provides a thorough sanitization and validation of filenames,
    ensuring that they do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Conform to expected characters and formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have reasonable length limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are restricted to allowed file types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are stored only within the intended directory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are optionally assigned a unique name to prevent conflicts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining these security measures and utilizing `FilenameUtils` for safe
    file name manipulation, you can ensure that file uploads remain secure. This reduces
    the chance of path traversal attacks and protects your application from unauthorized
    access. Even if your application stores files on a service such as AWS S3, validating
    filenames contributes to a consistent, secure, and user-friendly experience.
  prefs: []
  type: TYPE_NORMAL
- en: Using storage services such as AWS S3 can simplify file upload handling by offloading
    certain security concerns from your application server. Since S3 and similar services
    store files in a managed environment separate from your server’s filesystem, they
    reduce the risk of path traversal attacks, where an attacker might attempt to
    save or access files in unauthorized locations. By isolating storage, these services
    protect your core infrastructure from direct interaction with uploaded files,
    reducing the potential impact of improperly validated file paths or names.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, AWS S3 automatically generates unique URLs or identifiers for
    uploaded files, which minimizes the need for name conflict resolution and additional
    filename validation. S3 enforces secure naming conventions and ensures that uploaded
    files are stored safely without affecting other areas of your application. This
    means your application can focus on verifying basic attributes, such as file type
    or size, without worrying as much about path traversal or directory restrictions,
    streamlining the file upload process and enhancing overall security.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing thorough validation to secure filenames and mitigate risks
    such as path traversal attacks, it is crucial to consider how our API communicates
    the outcome of the file upload process. Providing clear, meaningful responses
    ensures that clients are informed about the status of their upload attempts, whether
    successful or not. Effective responses do more than acknowledge success—they also
    guide users when an error occurs, helping them understand what went wrong and
    how to correct it. By returning specific HTTP status codes and relevant metadata,
    we can make the file upload process more transparent, reliable, and user-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: Providing meaningful responses on our API service for file upload
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Provide clear feedback to the client regarding the result of the upload by
    using appropriate HTTP status codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`201 Created` : When a file is successfully uploaded'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`415 Unsupported Media Type` : For invalid file types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`413 Payload Too Large` : If the file size exceeds the allowed limit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`400 Bad Request` : For other validation errors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include metadata in the response, such as the file URL or identifier for future
    downloads.
  prefs: []
  type: TYPE_NORMAL
- en: After covering these fundamental yet essential details and highlighting specific
    precautions for handling files within your API, it becomes clear that even seemingly
    minor implementations can lead to potential vulnerabilities. Such weaknesses may
    act as critical points of failure, potentially destabilizing the entire API. For
    instance, a single malformed file could disrupt API operations, or an attack exploiting
    file-naming conventions might provide an entry point for malicious actors. The
    intention of covering this topic was to call your attention to these scenarios
    that you may encounter when dealing with files on your API.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier in this chapter, we introduced HATEOAS while discussing pagination to
    demonstrate how hypermedia links can enhance navigation through large datasets.
    Now, we’ll explore HATEOAS in greater detail as it represents a broader architectural
    pattern that goes beyond pagination, offering a more sophisticated approach to
    API interaction and resource discovery.
  prefs: []
  type: TYPE_NORMAL
- en: HATEOAS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HATEOAS, as mentioned in the *Pagination* section, is a key principle of RESTful
    APIs that improves the client’s interaction by including navigational links within
    the response. These hyperlinks guide the client on how to access related resources
    without needing prior knowledge of the API structure. As we saw before, in a paginated
    response, HATEOAS can provide links to the next and previous pages, enabling smooth
    navigation through data without requiring hardcoded logic.
  prefs: []
  type: TYPE_NORMAL
- en: By using HATEOAS, APIs become more self-explanatory and adaptable to changes.
    Clients can discover available actions, such as editing or deleting, directly
    from the response, based on the provided links. This simplifies API usage and
    ensures flexibility, as clients do not have to rely on external documentation
    to understand the API’s behavior, reducing the chance of errors when updates occur.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see an example of using HATEOAS to expose available operations on a
    resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, the response contains three elements in the `links`
    section. The first element, `"rel": "self"` , indicates the link to the current
    resource itself. This allows the client to retrieve or interact with a specific
    user using the `GET` method at the URL provided in the `href` (which stands for
    hypermedia reference) attribute. The second element, `"rel": "edit"` , provides
    a link for updating the user’s details with the `PUT` method. Lastly, the `"rel":
    "delete"` element allows the client to remove the user with the `DELETE` method.
    Each of these links guides the client through interactions with the resource,
    making the API self-explanatory and easier to use.'
  prefs: []
  type: TYPE_NORMAL
- en: HATEOAS, as outlined in RFC 8288, formalizes the use of web links in APIs, ensuring
    consistency and clarity in how these links are structured and interpreted. This
    RFC defines key components such as the `rel` attribute, which specifies the relation
    type of a link (e.g., `"self"` or `"edit"` ), and `href` , which defines the target
    URL for the interaction. By adhering to this standard, APIs become more robust,
    with clients able to navigate resources and perform actions without needing to
    hardcode endpoint paths or refer to external documentation. This approach enhances
    flexibility and future-proofs API interactions, as the server can evolve while
    still maintaining compatibility with existing clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same example we saw before can also have a different response, where certain
    actions—such as deleting the element—are not available. For instance, in the following
    example, the response for the user `"Jane Smith"` does not include a `"delete"`
    link in the `links` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the absence of the `delete` link indicates that the `delete` operation
    is not available for this resource, due to permission restrictions or business
    rules. HATEOAS helps make these limitations clear to the client, ensuring that
    the client does not attempt unsupported actions. This allows the API to dynamically
    control which actions are allowed, preventing misuse and guiding the client’s
    behavior based on the current state of the resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the implementation of HATEOAS, modern API frameworks offer built-in
    tools to support this pattern. For example, in Spring Boot, you can leverage the
    `spring-boot-starter-hateoas` dependency to easily incorporate HATEOAS into your
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: HATEOAS enhances the dynamism of the API, advancing it to a higher level of
    maturity by incorporating all the benefits we have mentioned so far. However,
    it is important to evaluate whether this resource is necessary for your use case,
    as HATEOAS is not a mandatory component for every API. It requires another dependency,
    additional development, and maintenance, and then requires attention to understand
    whether it makes sense to have it.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered essential practices for building a reliable and user-friendly
    API by employing efficient data-handling techniques. Now, as we transition to
    resilience patterns, we will explore strategies that help APIs maintain stability
    and robustness, even in the face of unexpected challenges. Resilience is crucial
    for ensuring that an API continues to function reliably under various adverse
    conditions, from network issues to system overloads. In the upcoming section,
    we will dive into patterns that enable APIs to handle such scenarios gracefully,
    enhancing their dependability and user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Resilience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before discussing how to make an API resilient, let’s look at the Oxford English
    dictionary’s definition for resilience: “ *(of a person or animal) able to withstand
    or recover quickly from difficult conditions.* ” If we try to apply the same definition
    to our RESTful APIs or systems in general, we could say that a resilient API is
    an API that can withstand failures and disruptions while maintaining its functionality
    and performance, recovering from situations that could shut it down or cause a
    degradation in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that an API serves as a reliable component of a larger, robust system,
    it must handle excess spikes, performance degradation of services it depends on,
    and infrastructure failures effectively. For that, we need to observe several
    aspects of the API design that we are going to approach. We will explore strategies
    and techniques to achieve these capabilities, ensuring your APIs can deliver consistent
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding API design, the key principles that we need to observe are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Redundancy** : Have multiple instances of our components/services to avoid
    a single point of failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoupling** : Minimize dependencies in the design, aiming to limit impact
    during any system part failure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimize direct dependencies within a system’s design by reducing the interconnections
    between its components, which helps limit the impact of any single component’s
    failure. When an API is tightly coupled to other services or components, an issue
    in one part can cascade throughout the system, leading to widespread failures
    or degraded performance. By contrast, a decoupled system allows components to
    operate more independently, isolating faults and preventing them from affecting
    other parts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault isolation** : Use techniques such as bulkheads and circuit breakers
    to confine failures to isolated parts of the system, preventing error states from
    propagating throughout the entire system, as would occur with a “poison pill.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graceful degradation** : Keep limited functionality even when parts of the
    system fail or are underperforming. This way prioritizes the main functionalities,
    preventing the entire system from stopping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us list some concepts and patterns that we will talk about and the problems
    that each one of them can help us to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: Timeouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retry mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting and throttling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Idempotency key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bulkhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timeouts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Establishing appropriate timeout settings is critical when designing APIs to
    ensure system resilience. API interactions often involve synchronous remote calls
    between services, whether they reside on the same network or across different
    networks. This synchronous communication means that the client remains unaware
    of the server’s processing status, maintains the network connection, and waits
    until the call either succeeds or fails. Without well-defined timeouts, clients
    might experience prolonged wait times for responses, which can degrade user experience,
    introduce security vulnerabilities, and cause system instability or downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The timeout can be set from two perspectives:'
  prefs: []
  type: TYPE_NORMAL
- en: Client configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Server configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Configuring timeouts on the client side is essential to prevent clients from
    waiting indefinitely for a server response. By setting a specific timeout duration,
    the client ensures that it does not become unresponsive due to delayed server
    replies. When the server fails to respond within the designated timeout period,
    the client should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alert users** : Inform users about the delay to maintain transparency and
    manage expectations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log the event** : Record the timeout occurrence in logs for monitoring and
    troubleshooting purposes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Initiate retry mechanisms** : Depending on the application requirements,
    the client may attempt to resend the request to recover from transient issues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement the timeout for the API client using Spring Boot’s `RestClient`
    . In our example, we will configure a timeout of six seconds, striking a balance
    between allowing sufficient time for normal operations and preventing excessive
    waiting periods. Here is how to configure `RestClient` with the appropriate timeout
    settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This example demonstrated how to configure `RestClient` with appropriate timeout
    settings to establish connections and receive data. Let us set up these timeouts
    to ensure our client avoids waiting too long for server responses.
  prefs: []
  type: TYPE_NORMAL
- en: Server configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When a server receives a request, it begins processing the necessary information,
    which may involve interacting with other servers, executing database queries,
    and performing various computational tasks. If these operations exceed the predefined
    timeout threshold, the server should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Terminate processing** : Stop any ongoing operations related to the request
    to free up resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Release allocated resources** : Ensure that resources such as memory, threads,
    and database connections are properly released to prevent leaks and bottlenecks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notify the client** : Inform the client that a timeout has occurred using
    the appropriate HTTP error code, `408 Request Timeout` , allowing the client to
    handle the situation appropriately (e.g., by retrying the request or notifying
    the user)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before setting a timeout, it is important to observe the typical responses of
    your service and network conditions; this will allow you to have a proper notion
    of what is a “normal” expected time to respond, because setting a very short timeout
    can cause unnecessary failures, whereas a very long timeout can lead to poor user
    experience. The best application of this practice requires constant monitoring
    of the timeout metrics and adjusting configurations, both client and server, to
    maintain a healthy, resilient, and sustainable API.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate client timeout settings in a Spring Boot application, you can
    configure the connection timeout and read timeout within `ClientHttpRequestFactorySettings`
    . This class is responsible for setting up the HTTP request factory with specific
    timeout parameters. These settings are then used to create a `RestClient` instance,
    which is a component for executing HTTP requests to other services. For more detailed
    information on this configuration, please refer to the `ProductsApiConfiguration`
    class in the `chapter6` folder of the book’s source code repository.
  prefs: []
  type: TYPE_NORMAL
- en: While the last example focuses on client-side configuration, it’s worth noting
    that server-side timeout configuration in Spring Boot typically involves setting
    properties in your `application.properties` or `application.yml` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This sets the server’s connection timeout to five seconds. The specific properties
    and their effects can vary depending on the server you’re using (e.g., Tomcat
    or Jetty).
  prefs: []
  type: TYPE_NORMAL
- en: The previous examples show basic client-side and server-side timeout configurations.
    However, in more complex applications, you might need more advanced timeout management.
    The example in this book is intentionally simple to focus on key concepts. For
    real-world applications, especially those with complex database operations or
    microservices, you might find more sophisticated libraries, such as Resilience4j,
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Resilience4j offers a `TimeLimiter` module that provides detailed control over
    timeouts and works well with Spring Boot applications. While this level of complexity
    is not necessary for our basic example, understanding these advanced techniques
    can be valuable as your applications become more complex. Here’s an example using
    Resilience4j version 2.2.0 to manage timeouts in a more advanced scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the Resilience4j dependency to your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configure `TimeLimiter` in your `application.yml` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the service with `TimeLimiter` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the service in a controller:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this example, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We have an instance of `TimeLimiter` that is built by a named `TimeLimiterRegistry`
    using the configuration named `productServiceGetById` , which has a configuration
    for a five-second timeout.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ProductsQueryUseCaseImpl.getProductById()` method uses the `timeLimiter.executeCompetionStage()`
    method, which wraps the repository call in a `CompletableFuture` to make it asynchronous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a real scenario, if a complex database query or high server load causes a
    delay, it will be detected by this `timeLimiter` call, throwing one exception.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That will be detected by the controller’s `try/catch` and return a timeout HTTP
    code.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how to apply timeouts to specific service methods, such as
    those involving database operations. Although it is more complex than the book’s
    main example, it demonstrates how you can improve your timeout strategies as your
    application’s needs grow.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you would choose the appropriate level of timeout management based
    on your specific requirements. You might start with simpler approaches and move
    to more advanced solutions, such as Resilience4j, as your system becomes more
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'While timeouts are crucial for maintaining system responsiveness, they often
    work hand in hand with another important resilience pattern: retry mechanisms.
    When a timeout occurs, it may be due to temporary issues that could be resolved
    quickly. In such cases, automatically retrying the operation can help maintain
    service continuity without user intervention. Let’s explore how retry mechanisms
    complement timeout strategies and enhance overall system resilience.'
  prefs: []
  type: TYPE_NORMAL
- en: Retry mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing a retry mechanism is an effective strategy to prevent the temporary
    unavailability of a system component from disrupting the entire chain of dependent
    processes. While retries can mitigate temporary failures, excessive or inappropriate
    retries can exacerbate issues, leading to resource exhaustion and degraded performance.
    We will present how to properly deal with this here.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are countless reasons why an API might fail to complete a process cycle,
    affecting various tasks within a request. Here are some cases that you can handle
    with retries:'
  prefs: []
  type: TYPE_NORMAL
- en: Timeouts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5xx server errors (e.g., `500 Internal Server Error` or `503 Service Unavailable`
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting (e.g., `429 Too Many Requests` )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider an e-commerce platform where verifying the stock count of a product
    is a critical step in finalizing a sale. Suppose the product stock verification
    API typically responds within 2 to 5 seconds. If this API experiences a temporary
    delay or failure, implement a retry mechanism to ensure that the transaction can
    still proceed without unnecessary abandonment. Take the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial request** : The system requests the stock count'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First failure** : The API does not respond within the expected period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retry attempt** : The system retries the request after a brief delay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Successful response** : On the second attempt, the API responds successfully,
    allowing the sale to proceed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach minimizes the risk of losing transactions due to temporary API
    unavailability. However, retries can become problematic if the product stock API
    is down and recovery takes longer. Continuous retries without communicating with
    other systems involved in the transaction can lead to a poor customer experience
    and cause considerable damage and loss to the company.
  prefs: []
  type: TYPE_NORMAL
- en: 'To not get trapped in the pitfalls that we mentioned, you need to consider
    some best practices when implementing your retry policy:'
  prefs: []
  type: TYPE_NORMAL
- en: Set a maximum retry limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use exponential backoff with jitter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have idempotent requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement timeout mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Log retry attempts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set a maximum retry limit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Without a maximum retry limit, your system could enter an endless loop of trying
    to resend a failed request. Imagine a scenario where a server is down for an extended
    period. If there is no limit on retries, the system will keep trying to resend
    the request forever, wasting resources and potentially causing other issues.
  prefs: []
  type: TYPE_NORMAL
- en: Use exponential backoff with jitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Exponential backoff is a method used to manage the timing of retry attempts
    after a failure. Instead of trying to resend the message at regular intervals,
    the system waits longer each time it fails to resend. The wait time increases
    exponentially, meaning it increases quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First attempt** : If sending the message fails, wait for a brief period of
    time (e.g., one second) before trying again.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Second attempt** : If it fails again, wait twice as long (e.g., two seconds).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Third attempt** : Double the wait time, that is, four seconds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Continue depending on the limit of tries** : At every new retry, the wait
    time doubles (8 seconds, 16 seconds, etc.).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach helps prevent the system from repeatedly trying to send messages
    too quickly, which can cause more problems, such as overloading the server or
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Jitter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Jitter** is a random time used as a time interval for retrying failed requests.
    Instead of having the retry mechanism make all the calls at a fixed time or even
    with exponential progress, we have a random time for each new retry. Instead of
    waiting exactly 2 seconds, for example, it might wait between 1 and 3 seconds.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining exponential backoff and jitter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using only exponential backoff can still cause issues. If many devices or systems
    are trying to resend messages at the same time, they might all wait the same amount
    and then retry together, creating a sudden peak of traffic. This can make the
    network or server even more overwhelming, which is why it is best practice to
    combine exponential backoff and jitter.
  prefs: []
  type: TYPE_NORMAL
- en: Adding jitter helps by spreading out the retry attempts. Each device waits for
    a slightly different time, reducing the chance that they all send requests at
    the same time. This makes the system more stable and increases the chances that
    messages will be successfully sent.
  prefs: []
  type: TYPE_NORMAL
- en: Have idempotent requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having idempotent requests when retrying API calls is vital for building resilient
    and reliable systems. It ensures that multiple attempts to perform the same operation
    do not lead to unintended consequences, such as duplicate actions or inconsistent
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Implement timeout mechanisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You are already aware of the importance of timeouts from the previous topic.
    A retry involves sending a new request to the service, and it is essential to
    manage how long we wait before taking the next action.
  prefs: []
  type: TYPE_NORMAL
- en: Log retry attempts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logging each failure during retry attempts will help us understand recurring
    patterns or frequent issues causing the retries, and it will ease system troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: A well-implemented retry mechanism significantly enhances the resilience of
    APIs by gracefully handling transient failures. By adhering to best practices—such
    as limiting retry attempts, employing exponential backoff, guaranteeing idempotent
    retries, and distinguishing error types—developers can ensure that retries contribute
    to system stability without introducing new vulnerabilities. Thoughtful retry
    strategies lead to improved user experiences, higher system reliability, and sustained
    operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although retry mechanisms help handle temporary failures, unrestricted retries
    could overwhelm your system. This brings us to another crucial resilience pattern:
    rate limiting. By controlling the frequency and volume of requests, rate limiting
    helps maintain system stability even during high-load situations.'
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rate limiting is a critical strategy used to protect APIs from abuse and excessive
    resource access. It ensures the stability, reliability, and security of API systems
    by restricting the number of requests an API can handle from a user (user, IP,
    application, token, etc.) within a specified period. This approach helps to prevent
    **denial of service** ( **DoS** ) attacks and ensures the fair distribution of
    services among clients, preventing exhaustion of resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply rate limiting on distinct levels of context; it can be based on
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Global level** : In this context, we can limit the total number of requests
    the API can handle by period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP level** : By IP, we limit the number of requests based on the IP address
    of the client'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User level** : Limits requests based on individual users or clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application level** : Limits based on a specific application using the API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will explore some of the aspects of implementing
    rate limiting, such as quota management, time windows, and rate-limiting headers.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting key aspects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To effectively manage rate limiting, it is essential to understand its core
    components and how they contribute to overall API resilience. Rate limiting is
    not just about restricting requests; it is about controlling access fairly, transparently,
    and scalably, ensuring a stable experience for all users while protecting resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will explore three critical aspects of rate limiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quota management** : Defines the maximum number of requests allowed per client
    within a given period (e.g., 1,500 requests per hour).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time windows** : The period during which the request count is measured. Common
    intervals include per second, minute, hour, or day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate limit headers** : APIs often include headers in responses to inform
    clients about their current usage and remaining quota. Examples include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X-Rate-Limit-Limit` : The maximum number of requests allowed'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X-Rate-Limit-Remaining` : The number of requests left in the current window'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X-Rate-Limit-Reset` : The time when the rate limit will reset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Provide the proper response HTTP code when it reaches the rating limit**
    : A `429 Too Many Requests` error is advised by RFC 6585.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you know what these aspects are and understand what they mean, let
    us talk about different implementations of rate limiting on your API.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting implementation strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Implementing rate limiting requires careful consideration of various strategies,
    each with its advantages and trade-offs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fixed window** : This strategy has the time split into fixed intervals (windows),
    such as one minute. The count of requests is reset at the start of each new window.
    This is the simplest implementation, although it might be susceptible to bursts
    at window boundaries (e.g., a client could send double the allowed requests in
    a brief period and have multiple requests rejected). Depending on the application,
    it may be acceptable to employ this simple strategy to implement and have occasional
    failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sliding window** : Unlike fixed windows, sliding windows move continuously.
    For each request, the system checks the number of requests in the past defined
    period for readjusting itself. It will result in a smoother distribution of allowed
    requests over time, reducing the chance of overloaded periods and periods without
    any requests. It has a more complex implementation but shows precise control over
    request rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token bucket** : A bucket holds tokens representing the number of allowed
    requests. Tokens are added, refilling the bucket based on the number of allowed
    requests per configured time. Each request consumes a token. If no tokens are
    available, the request is rejected. It has a slightly more complex logic, but
    will support the APIs that need to handle occasional spikes without compromising
    overall rate limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leaky bucket** : This strategy is like a token bucket but processes requests
    at a fixed rate, queuing requests where the bucket limit is the mark that determines
    whether we accept new requests. If the queue/bucket is full, additional requests
    are rejected. Every time a request is processed from the queue, it opens space
    for a new, incoming request. It can cause an increase in latency due to the queuing.
    However, it will ensure a steady processing rate. This is advantageous for systems
    where maintaining a consistent processing rate is critical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rate limiting implementation will protect the API against abuse by malicious
    actors overwhelming the API, support resource management, ensure a fair distribution
    of resources among the clients, and maintain the API’s responsiveness by controlling
    the traffic flow.
  prefs: []
  type: TYPE_NORMAL
- en: While rate limiting focuses on restricting the total number of requests, a related
    but distinct concept is throttling. Understanding the difference between these
    two approaches is crucial for implementing effective request management strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Gateway-level rate limiting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While rate limiting inside individual API services provides good protection,
    implementing rate limiting at the gateway level offers important benefits that
    experienced developers should consider for production systems.
  prefs: []
  type: TYPE_NORMAL
- en: Single point of control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: API gateways act as the main entry point for all client requests. This makes
    them perfect places to set up rate-limiting rules. Instead of adding rate-limiting
    code to each service separately, you can manage all policies from one central
    location. This approach reduces complexity and makes maintenance easier across
    your entire system.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping requests early
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gateway-level rate limiting blocks excessive requests before they reach your
    application services. When clients send too many requests, the gateway immediately
    returns a 429 error code without using any backend resources, such as database
    connections or server processing power. This early blocking protects your system
    during traffic spikes and saves computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: System-wide rate limiting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In complex systems where clients use multiple services, gateway rate limiting
    lets you set limits across your entire API. For example, you can limit a user
    to 5,000 requests per hour across all services, not just individual ones. This
    prevents clients from avoiding limits by spreading requests across different endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation options
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Popular gateway tools such as Kong, AWS API Gateway, and Nginx offer built-in
    rate-limiting features. These tools support different strategies, such as the
    token bucket and sliding window methods. When using multiple gateway servers,
    consider using shared storage such as Redis to track request counts accurately
    across all servers.
  prefs: []
  type: TYPE_NORMAL
- en: Gateway-level rate limiting works best when combined with service-level rate
    limiting. Use gateways for general limits and overall system protection, while
    keeping specific business rules and detailed limits within individual services.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Throttling** , on the other hand, manages the rate at which requests are
    handled. Instead of limiting the total number of requests, throttling makes sure
    that requests are processed at a steady pace. This is often done by queuing or
    delaying requests to keep the system stable.'
  prefs: []
  type: TYPE_NORMAL
- en: For real-time data, the API limits how frequently each user can make requests
    to prevent server overload. In this case, each user can make only one request
    per second. If a user sends requests faster than this, throttling will slow down
    their requests, but will not block them completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: User X sends three requests in one second.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API processes the first request immediately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API delays the second and third requests, ensuring each request is spaced
    one second apart.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Throttling controls the pace of requests, allowing the server to handle high-frequency
    requests without being overwhelmed.
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between rate limiting and throttling, then?
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate limiting** : Controls the total number of requests over a period (e.g.,
    500 requests per hour)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throttling** : Controls the speed of requests, slowing them down if they
    come in too quickly (e.g., one request per second)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using both together ensures that the API remains responsive and fair, even with
    high demand.
  prefs: []
  type: TYPE_NORMAL
- en: Throttling implementation strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some of the throttling implementation strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic throttling** : Adjusts limits based on real-time system performance
    and load. This implementation is more responsive to changing conditions, although
    it requires sophisticated monitoring and adjustment mechanisms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Priority-based throttling** : Assigns priorities to different types of requests
    or clients. In this way, it ensures critical operations have access to necessary
    resources; however, this approach is complex to manage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graceful degradation** : Reduces functionality or performance in a controlled
    manner when under high load. During peak usage, this strategy maintains some level
    of service. This strategy may come at the cost of some periods of bad user experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circuit breaker pattern** : Temporarily stop requests flowing to some specific
    route, to prevent system overload and allow recovery. It will protect the system
    against cascading failures. As a side effect, it may require careful configuration
    to avoid unnecessary shutdowns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throttling helps ensure the reliability of APIs, maintaining stability under
    varying conditions. It allocates resources efficiently based on current demands,
    preventing slowdowns and outages, and maintaining the consistency of the service.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy is used by many large API providers, such as Microsoft’s GitHub
    API, which has a detailed rate-limiting system based on distinct levels. The level
    is determined by the number of simultaneous requests you make.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify things, let us assume we are at the most basic level. In this case,
    rate limits depend on our authentication. For example, unauthenticated requests
    that can access public data have their specific limits. Authenticated requests,
    on the other hand, have different rate limits (bigger) depending on the type of
    authentication, such as using a token or OAuth.
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean you need to implement something that complex. This is an
    example of how rate limits can help manage the demand on your API, depending on
    the level of implementation. It depends only on your needs and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: A quite simple example where you can find rate limits widely used on the market
    of API providers is the limiting of the usage for the free user plan, as you can
    see on different Google APIs, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting and throttling are essential mechanisms for API management, helping
    to maintain the integrity and reliability of services. Rate limiting focuses on
    controlling the number of requests per client within a specific time, while throttling
    employs strategies to manage overall traffic flow and system performance.
  prefs: []
  type: TYPE_NORMAL
- en: For API providers, implementing effective rate limiting and throttling ensures
    fair usage, protects against abuse, and maintains optimal performance. For API
    consumers, understanding these mechanisms allows for the design of resilient applications
    that interact smoothly with APIs, handle limitations gracefully, and provide a
    seamless user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Managing the request flow through throttling is important, but ensuring request
    consistency is equally crucial. This is where idempotency comes into play, particularly
    when dealing with retried requests that might have succeeded but failed to communicate
    their success.
  prefs: []
  type: TYPE_NORMAL
- en: Idempotency key
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand why an idempotency key is necessary, it is essential to first
    understand the concept of idempotency. If you do not recall this from the second
    chapter, now is a good time to revisit that section. In simple terms, idempotency
    means calling the API multiple times with the same parameters while ensuring that
    the state remains unchanged and no unintended side effects occur.
  prefs: []
  type: TYPE_NORMAL
- en: According to the idempotency key RFC (there is no defined number yet) about
    the idempotency key header, “The HTTP Idempotency-Key request header field can
    be used to carry an idempotency key in order to make non-idempotent HTTP methods
    such as `POST` or `PATCH` fault-tolerant.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the following scenario: we have a frontend that sends an order to our
    management order API, and this happens when the user clicks on the **Save Order**
    button; this will trigger the code that will make a `POST` call with all the data
    required to create a new order. Let us imagine that due to internet instability,
    the user does not see the result on the screen as fast as usual and hits this
    button multiple times. This means we will receive multiple `POST` calls with the
    same data. As an undesirable side effect, we would have the same order repeated
    multiple times, which causes an inconsistency in our system.'
  prefs: []
  type: TYPE_NORMAL
- en: If our frontend added on each call a header item `"Idempotency-Key"` , a key
    that satisfies the requirement that its value is the same if and only if it is
    the same request (order) from a business perspective, it would prevent this undesirable
    side effect of having multiple wrong records stored on the management API because
    `"Idempotency-Key"` would serve as a unique key to avoid these repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of one call to an API that requires the usage of an idempotency
    key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we have the `"Idempotency-Key"` header, and this header
    is sent by the request client. The Stripe documentation recommends generating
    a key with V4 UUIDs, or another random string with enough entropy to avoid collisions.
    By doing so, we can guarantee idempotency even for methods that are not idempotent.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, using an idempotency key is crucial for maintaining the reliability
    and consistency of APIs, especially when handling non-idempotent requests such
    as `POST` or `PATCH` . By assigning a unique key to each request, developers can
    prevent duplicate actions, such as multiple orders being created unintentionally.
    This approach ensures that even if a user sends the same request multiple times
    due to network issues or repeated clicks, the system remains stable and accurate.
    Implementing idempotency keys not only safeguards the integrity of the data but
    also enhances the user experience by avoiding errors and inconsistencies. Adhering
    to best practices, such as generating unique and secure keys, further strengthens
    the API’s resilience and fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: '![img](img/Information_Box_Icon.png)'
  prefs: []
  type: TYPE_IMG
- en: '**UUID V4**'
  prefs: []
  type: TYPE_NORMAL
- en: A UUID V4 is a 128-bit value used to uniquely identify information in computer
    systems. It is randomly generated and follows a specific format to ensure uniqueness.
    Unlike other versions, a V4 UUID relies on random numbers, which makes it highly
    unlikely to generate duplicates. This type of identifier is often used when a
    unique reference is needed, such as in database entries or API requests, to ensure
    data consistency and prevent collisions.
  prefs: []
  type: TYPE_NORMAL
- en: While idempotency ensures request consistency, it does not address the broader
    challenge of handling failing downstream services. The circuit breaker pattern
    offers a solution by preventing cascading failures in distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by electrical circuit breakers, which protect electrical systems from
    damage caused by overloads or short circuits, the circuit breaker pattern in software
    works analogously. It helps prevent system failures by stopping repeated attempts
    at unsuccessful operations until the system recovers. Just like in an electrical
    circuit, when the switch is closed, energy flows through, but opening the switch
    disrupts the flow.
  prefs: []
  type: TYPE_NORMAL
- en: When a certain number or percentage of failures is detected, the pattern opens
    the faulty circuit or component and redirects all requests to a fallback routine.
    After some time, it tries again, and if not all but some requests succeed, then
    it assumes the state of half open. Then, depending on the stability of the subsequent
    requests, it can open or close, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Some frameworks also allow you to ignore certain types of failures (Java exceptions)
    and only count specific ones, offering more control over how the system responds
    to avoid issues. Depending on the library or framework you choose, the circuit
    breaker configuration can have an extensive list of parameters to customize how
    the circuit breaker needs to behave. In the following figure, we have an illustration
    of the workflow and an example of the Resilience4j configuration for our circuit
    breaker.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Circuit breaker flow](img/B21843_06_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Circuit breaker flow
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of implementation using Resilience4j:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This code sets up a Resilience4j circuit breaker to monitor a downstream service
    call. It does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Opens after the service fails more than 50% of the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remains open for 10 seconds before allowing test requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Treats calls taking longer than five seconds as failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using the circuit breaker pattern, the API becomes more resilient, as it
    limits repeated calls to failing services and recovers once the service stabilizes.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we saw how a circuit breaker can help manage failures
    in downstream services, protecting the API by isolating and handling service disruptions.
    However, implementing circuit breakers within each API service can lead to duplicated
    code and maintenance challenges, especially in larger systems with multiple services.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore this further by understanding a different way of implementing
    a circuit breaker that is implemented at the level of the system gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breaker implemented in the gateway
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A gateway functions as a central control point for managing communication between
    various services or systems. This makes it an optimal location for implementing
    patterns such as circuit breakers, rate limiting, and file upload validation.
    By incorporating these mechanisms at the gateway level, operational concerns are
    separated from business logic, allowing for isolation and promoting the standardization
    of system configurations, all managed through the gateway.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some benefits of implementing a circuit breaker on a gateway:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centralized control** : A single circuit breaker can protect multiple downstream
    services, simplifying management and monitoring'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved resilience** : By isolating failing services, a circuit breaker
    can prevent cascading failures and maintain overall system stability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced latency** : When a service is deemed unhealthy, the circuit breaker
    can quickly return a fallback response, reducing latency for clients'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced reliability** : A circuit breaker can help ensure that critical
    services remain available even in the face of temporary failures'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The circuit breaker pattern is a crucial tool for building reliable and stable
    APIs, especially those that rely on multiple services or external APIs. You should
    consider using this pattern in several key situations to enhance resilience and
    stability.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, when building distributed systems or microservices, distinct parts
    of your application communicate with each other through APIs. In such setups,
    if one service becomes slow or fails, it can negatively impact the entire system.
    By implementing a circuit breaker, you can monitor these API calls and stop attempts
    to communicate with the failing service temporarily. This prevents one service’s
    issues from causing a domino effect, ensuring that other parts of the system continue
    to function smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: Another important scenario is when your application relies on external services.
    Many applications depend on third-party APIs, such as payment gateways, social
    media platforms, or data providers. These external services can sometimes experience
    downtime or slow responses due to several reasons, such as maintenance or high
    traffic. A circuit breaker helps by detecting these failures early and avoiding
    long waits for responses that may never come. Instead, your application can provide
    fallback options, such as default messages or alternative actions, maintaining
    a good user experience even when external services are unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent transaction failures, systems often cache frequently calculated values
    such as delivery tax, which may depend on external services. In situations where
    a quick response is crucial, a cached value can serve as a fallback option, avoiding
    lost transactions. A circuit breaker pattern can be effectively used to manage
    these scenarios. Promote a fallback solution until the part of the circuit that
    is opened gets to a normal state again.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breakers help prevent system failures by stopping repeated attempts
    at unsuccessful operations. However, sometimes we need to isolate different parts
    of our system to prevent resource exhaustion. This is where the bulkhead pattern
    becomes valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Bulkhead
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have a system where one small part fails, but you do not want the
    entire system to go down because of it. This is where the bulkhead pattern comes
    in. Let us explore how this pattern helps improve fault tolerance and keeps your
    system running smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: The bulkhead pattern is inspired by ships. Ships have bulkheads, which are walls
    that divide the ship into separate sections. If one section takes on water, the
    bulkheads prevent the water from spreading to other parts of the ship. This keeps
    the ship from sinking entirely.
  prefs: []
  type: TYPE_NORMAL
- en: In software, the bulkhead pattern works similarly by dividing an application
    into various parts or services that operate independently. If one part fails or
    slows down, the other parts continue to function without any issues. This isolation
    ensures that a problem in one area does not affect the entire system.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned, the bulkhead pattern is a structural design pattern used to partition
    a system into isolated units or modules. Each module can handle its own load and
    failures independently, ensuring that an issue in one does not cascade to others.
    This isolation can be applied at various levels, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service level** : Separating different microservices within an architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource level** : Allocating separate thread pools, memory, or database
    connections to different components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional level** : Dividing functionalities within a service to prevent
    one failing feature from affecting others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key characteristics of this pattern include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Isolation** : Ensures that failures in one compartment do not impact others'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource allocation** : Allocates separate resources to each compartment
    to prevent resource exhaustion from affecting the entire system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fault tolerance** : Enhances the system’s ability to handle partial failures
    gracefully'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A common example of using the bulkhead pattern is managing “hot paths” in your
    system. Hot paths are parts of your system that receive more traffic than others.
    If a hot path gets too many requests, it can overload that part of the system,
    causing it to slow down or crash. This can make the entire system unresponsive,
    affecting users who are not using the hot path.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how the bulkhead pattern enhances resilience, we will dive into
    different areas where this approach can be applied effectively. By isolating components,
    we can prevent failures in one part of the system from affecting the entire application.
    This flexibility allows us to apply the bulkhead pattern across various levels,
    such as services, resources, and functionalities, making it a versatile solution
    for fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore some practical ways to implement bulkheads in different system
    architectures and at different levels, from monolithic systems to microservices,
    as well as through database and queue management.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing bulkhead in different architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can apply the bulkhead pattern whether you are using a monolithic architecture
    or microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monolithic systems** : Even if your system is a single application, you can
    create isolation by directing heavy traffic to specific instances. For example,
    you can have certain instances handle the hot paths while other instances manage
    different parts of the system. This way, if the hot path gets overloaded, the
    rest of the system remains unaffected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microservices** : In a microservices architecture, each service can act as
    a separate bulkhead. If one service fails, the others continue to operate normally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the bulkhead pattern through database isolation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another way to implement the bulkhead pattern is by isolating your databases.
    Instead of using a single database that handles all requests, you can create separate
    database instances for different parts of your system. For example, a high-traffic
    feature can have its own database, preventing it from overwhelming the main database
    and affecting other features.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the bulkhead pattern through queue management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also apply isolation to message queues by creating different queues
    with varying priorities. High-priority queues can be monitored and scaled independently
    to ensure they process messages quickly, while standard queues operate normally.
    This prevents a surge in one queue from impacting others.
  prefs: []
  type: TYPE_NORMAL
- en: External services and throttling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When your system makes calls to external services, latency and timeouts can
    cause problems. By using the bulkhead pattern, you can limit the number of external
    requests, ensuring that slow responses from third-party services do not overwhelm
    your system. Implementing throttling in your code can help manage the flow of
    these requests effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Using the bulkhead pattern increases your system’s resilience and availability.
    It makes your system easier to manage by isolating different parts. However, adding
    bulkheads also introduces some complexity. You need to carefully design your system
    to ensure that the added complexity is justified by the improved fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: The bulkhead resilience pattern is a powerful technique for building robust
    and reliable systems. By creating isolation between different parts of your architecture,
    you can prevent small failures from cascading and affecting the entire system.
    Whether you are working with monolithic applications or microservices, the bulkhead
    pattern can enhance your system’s stability and performance.
  prefs: []
  type: TYPE_NORMAL
- en: The bulkhead pattern is a strong architectural method used for designing and
    building APIs. It helps make systems more resilient, scalable, and able to handle
    faults. Separating distinct parts of the system keeps failures limited, uses resources
    efficiently, and ensures the system stays strong even if some parts go down. However,
    it is important to weigh these benefits against the extra complexity and resource
    use it can bring. Careful planning, managing resources well, and continuous monitoring
    are key to successfully using the bulkhead pattern in your API design.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter covered two important areas for building strong RESTful APIs:
    data handling and resilience.'
  prefs: []
  type: TYPE_NORMAL
- en: The *Data handling* section focused on managing large amounts of data efficiently.
    Key techniques include pagination, which splits data into smaller pages to improve
    performance, and filtering, which allows clients to request only the data they
    need. The chapter also explained how to handle file uploads and downloads securely,
    ensuring that APIs can manage large files without slowing down. Additionally,
    we introduced HATEOAS, which adds navigational links to API responses, making
    it easier for clients to interact with the API.
  prefs: []
  type: TYPE_NORMAL
- en: The *Resilience* section showed us how we can keep APIs stable and reliable
    under various conditions. Important strategies included setting timeouts to avoid
    long wait times, using retry mechanisms to handle temporary failures, and implementing
    rate limiting to prevent abuse and manage traffic. Throttling helps control the
    flow of requests, while idempotency keys ensure that repeated requests do not
    cause errors. Advanced patterns, such as circuit breakers, stop failing requests
    to protect the system, and bulkheads isolate different parts of the API to prevent
    failures from spreading. By applying these resilience techniques, APIs can maintain
    high performance and reliability even during unexpected challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, by focusing on effective data handling and robust resilience strategies,
    developers can create APIs that are both efficient and dependable, capable of
    supporting growing applications and delivering a consistent user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have mastered advanced API concepts and implementations, the journey
    ahead will focus on securing, testing, and deploying your APIs in production environments.
    [*Chapter 7*](B21843_07.xhtml#_idTextAnchor176) will guide you through securing
    your RESTful APIs against common vulnerabilities and attacks, teaching you to
    implement HTTPS encryption, proper authentication and authorization mechanisms,
    JWT tokens, and protection against OWASP vulnerabilities and CORS issues. [*Chapter
    8*](B21843_08.xhtml#_idTextAnchor223) shifts focus to comprehensive testing strategies,
    covering everything from Spring MVC unit testing to integration testing, GenAI-assisted
    test creation, and contract testing to ensure API reliability.
  prefs: []
  type: TYPE_NORMAL
- en: These upcoming chapters will transform your APIs from functional prototypes
    into secure, well-tested services ready for production deployment. By the end
    of *Part 2* , you will have the knowledge to build APIs that not only perform
    well but also protect user data and maintain reliability under various conditions,
    preparing you for the final part of the book, which covers deployment and performance
    optimization.
  prefs: []
  type: TYPE_NORMAL
