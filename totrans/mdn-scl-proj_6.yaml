- en: Build Flights Performance Prediction Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flight delays and cancellations are travel annoyances. Will a Chicago-bound
    flight arrive late causing a traveler to miss their connecting flight to Denver?
    Another traveler at Chicago Airport just learned that their connecting flight
    to Philly was delayed, perhaps even canceled. If both travelers could predict
    the odds of their respective experiences actually occurring, travel would get
    so much better.
  prefs: []
  type: TYPE_NORMAL
- en: That said, implementing a flight delay pipeline that can predict outcomes on
    the lines just described is the overarching learning objective of this chapter.
    The next section lists all the learning objectives in terms of topics covered
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: All learning objectives in this chapter depend on the following datasets compiled
    by the United States Department of Transportation. These are flight data, airline
    carrier data, and flight performance data, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each topic covered in this chapter has specific learning objectives, broken
    down into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Background theory, starting with coverage of the years 2007 and 2008 flights,
    carrier, and flight performance datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Spark-Scala implementation of a flight delay prediction model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That said, the immediate learning objective is to gain an understanding of
    the flight on-time performance dataset for 2007 and 2008\. A good place to start
    is the *Flight dataset at a glance* section:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding background theory relevant to understanding flight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulating the flights performance problem by applying the background theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learn which dataset to pick from the US Department of Transportation website
    the dataset we pick belong to the years 2007 and 2008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to learn what we can from the data, by conducing data exploratory steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing data into test and training datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of a model in Scala and Spark to predict flights performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of flight delay prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will implement a logistic regression-based machine learning
    model to predict flight delays. This model learns from flight data described in
    the next section, *Flight dataset at a glance*.
  prefs: []
  type: TYPE_NORMAL
- en: A real-life situation goes like this—travel company T has a new prediction feature
    in their booking system that is designed to enhance a customer's travel experience.
    How so? For example, say traveler *X* wants to get on Southwest flight *SW1* from
    origin *A* (St Louis) to destination *C* (Denver) with a connection at city *B*
    (Chicago). If T's flight booking system could predict the odds of *X*'s flight
    arriving late at Chicago, and furthermore the odds of missing the connecting flight
    as well, *X* has information at their disposal that lets him or her decide the
    next course of action.
  prefs: []
  type: TYPE_NORMAL
- en: With these opening point made, let's take a look at our flight dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The flight dataset at a glance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data analysis in this chapter relies on a flight dataset, a dataset consisting
    of the following individual datasets. Download these datasets from the `ModernScalaProjects`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Airports.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AirlineCarriers.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flights.csv`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnTime2007Short.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OnTime2008Short.xlsx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot is an overall view of the airports and airline carrier''s
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/788e8fd3-2ab7-4026-89e1-ddd8cac17941.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The airport and airline dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table describes the structure of the on-time dataset (`OnTime2008Short.xlsx`).
    It lists all the 28 fields. The table consists of denormalized, semi-structured
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6d43da8-0a38-4d61-9ce3-4dcb0c09b318.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The OnTime2008Short file dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The description of the fields are into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Causes of delay on account of the airline (carrier) (in minutes)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightCarrierDelay`: It denotes the delay caused by the carrier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightWeatherDelay`: It denotes the delay caused by weather'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightNASDelay`: It denotes the delay caused by the National Air System'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightSecurityDelay`: It denotes the delay on account of security checks or
    other security reasons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightLateAircraftDelay`: It denotes that the aircraft arrives late for reasons
    other than the preceding causes described'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flight aircraft data**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlightUniqueCarrier`: A unique two-letter sequence in uppercase, or a one-number-one-letter sequence
    (for example, US, DL, 9E)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The section represents a comprehensive overview of the project. To start with,
    we formulate at a high level the nature of the underlying problem we want to solve.
    The problem formulation step paves the way for implementation. First, let's formulate
    the flight delay prediction problem.
  prefs: []
  type: TYPE_NORMAL
- en: Problem formulation of flight delay prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A high-level description of the problem of flight delays is summed up in one
    statement—we want to implement a prediction mode that will make predictions on
    flight delays. In short, a traveler with an itinerary wants to know whether his/her
    flight or flights are running late.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section starts by laying out the implementation infrastructure for [Chapter
    4](9608dfbf-246b-4df2-83f6-9e4967b99b0a.xhtml), *Building a Spam Classification
    Pipeline*. The goal of this section will be to get started on developing one data
    pipeline to analyze the flight-on-time dataset. The first step is to set up prerequisites,
    before implementation. That is the goal of the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up prerequisite software
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following prerequisites or prerequisite checks are recommended. A new prerequisite
    on this list is MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase Java memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review JDK version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-contained Scala application based on **Simple Build Tool** (**SBT**), where
    all dependencies are wired into the `build.sbt` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start by detailing the steps to increase the memory available to the Spark
    application. Why would we want to do that? This and other points related to Java
    heap space memory are explored in the following topic.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Java memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flight on-time records, compiled over a period of time, say, month by month,
    become big or medium data. Processing such volumes of data on a local machine
    is not trivial. In most cases, a local machine with limited RAM simply won't cut
    it.
  prefs: []
  type: TYPE_NORMAL
- en: As challenging as this situation can be, we want to make the best use of our
    local machine. That brings us to why we want to increase Java memory. For example,
    trying to process a
  prefs: []
  type: TYPE_NORMAL
- en: typical one-time dataset file of 27 columns and 509,520 rows, is enough to cause
    Java
  prefs: []
  type: TYPE_NORMAL
- en: 'to run out of memory (see the following screenshot):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80d225f7-60c9-4119-9b95-05de182cee0d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: GC overhead limit exceeded
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, `java.lang.OutOfMemory` occurs when the Java VM on your machine tries
    to go over its threshold memory allocation, as set by the `-Xmx` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The `-Xmx` parameter has to do with memory management. It is used to set the
    maximum Java heap size. From Java 1.8 onwards, the JVM will allocate heap size
    proportional to the physical memory on the machine
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this situation, here are a few different ways to increase Java memory:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: On the command line, we pass into SBT the following runtime parameters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum allowable heap size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java thread stack size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial heap size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Method 2: Setting maximum Java heap size in the Java control panel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Method 3: Globally setting these parameters in the environment variable, `JAVA_OPTS`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address the `GC Overhead Limit exceeded` problem illustrated in the preceding
    screenshot, we can quickly allocate more heap space right on the command line,
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed2792bb-ecf1-4803-ba03-2450ba0eca15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Allocating heap space
  prefs: []
  type: TYPE_NORMAL
- en: Note the `-Xmx2G` setting. We set the `SBT_OPTS` environment variable with the
    value of `-Xmx2G`, the maximum allocated Java heap space memory. We set that and
    then run SBT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the next method, it might be useful to know the following
    JVM heap allocation statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: Total memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is useful. Heap memory utilization numbers are revealing. The following
    screenshot shows how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44620101-b200-4a99-9ac6-103abb14e30d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Heap memory
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will talk about method 2, where we go through the steps to set Java
    runtime parameters globally.
  prefs: []
  type: TYPE_NORMAL
- en: The following steps apply to Windows machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to Start | Control Panel, and under Category, choose Small icons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a137b5f1-f4fb-4f1d-b1c1-1c35aedf2b3d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Control Panel
  prefs: []
  type: TYPE_NORMAL
- en: 'The ensuing panel allows you to make changes to your computer''s settings.
    The Java setting is one of those. Locate Java in the Control Panel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4f83b333-f01f-48a6-a6a8-6643788465b3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: All Control Panel Items
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on Java, as in the preceding screenshot, will take you to the Java
    Control Panel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/09ee4547-315c-48d0-870b-0ad59cdb8eed.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Java Control Panel
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the Java tab results in the Java Runtime Environment Settings panel,
    where you may inspect the Runtime Parameters, such as the Java heap size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/71434e7d-31c0-463e-aabb-21668310285c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Java Control Panel's User tab
  prefs: []
  type: TYPE_NORMAL
- en: Referring to the Java Control Panel representations, we want to set the maximum
    Java heap size in the Runtime Parameters box. `Xmx2048m` is the new value of the
    maximum heap space, where `m` stands for megabytes. It is easy to modify the value
    of the `-Xmx` parameter. Click on it, then change the value to `2048` and click
    OK.
  prefs: []
  type: TYPE_NORMAL
- en: There is no space between `-Xmx` and `2048m` or `2 GB`.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s it. Exit the Control Panel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2264256-f855-49a9-9aa9-933f3d8e6814.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Java Control Panel Runtime Parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of Java memory management and the settings that are available to help
    us manage Java memory usage in our Spark application, here is a list of command
    line options available on running the `java -X` command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10a42cc6-32f3-4477-8d18-f9d24b89c9d7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: java -X command line
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot illustrates a comprehensive list of command line options.
    These options let you tweak different Java environment settings related to memory
    usage of your JVM-based Spark application. We are interested in the Xmx setting.
  prefs: []
  type: TYPE_NORMAL
- en: We just described method 2, where we outlined how to set the Java runtime parameter,
    `-Xmx`, in the Java Control Panel.
  prefs: []
  type: TYPE_NORMAL
- en: 'That leaves us with method 3, where we describe how to set three runtime parameters
    globally. In reference to the preceding screenshot, these are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-Xmx`: Sets (or allocates) the size in megabytes that the Java heap space
    is allowed to grow to. A typical default setting is `64m`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Xms`: Sets the initial Java heap size. The default is 2 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Xss`: Sets the Java thread stack size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will set these parameters in an environmental variable called `JAVA_OPTS`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps illustrate how to do just this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, we right-click on This PC and select Properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5810c859-4525-4e59-86de-f7717d9479b8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Properties option tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on Properties takes us to the following screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/990e58e7-3925-4b97-a2cb-bfe84ed9ce96.jpg)'
  prefs: []
  type: TYPE_IMG
- en: System tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on Advanced system settings takes us to the following screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f61bc110-b8e4-4e70-860a-27a48517f73c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: System Properties tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Environment Variables... next. In the ensuing screen, we will be able
    to set `JAVA_OPTS`. If `JAVA_OPTS` is not present, create a new one. Click on
    New and enter the appropriate values in the Variable name and Variable value boxes.
    Dismiss the box by clicking OK:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0e16ed0c-52c5-4dbc-898d-854149cdb9b1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: New System Variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Your new `JAVA_OPTS` variable is now ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/4e479571-ecb3-48e1-a3f9-810d316f3c4f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Environment Variables
  prefs: []
  type: TYPE_NORMAL
- en: In the environment setting we just made, set the `JAVA_OPTS` environment variable
    to the value `JAVA_OPTS = =Xmx2048M -Xms64M -Xss16M`.
  prefs: []
  type: TYPE_NORMAL
- en: Refer back to the preceding screenshot for a quick refresher on what those settings
    are.
  prefs: []
  type: TYPE_NORMAL
- en: 'To take stock of all environment variables, launch the Windows PowerShell (there
    should be a PowerShell app on the desktop). The following is a complete listing
    of all the environment variables. Note the ones that are relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e83668e4-5e6b-4b19-aed7-db322d4e7926.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hadoop Environment settings
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, here is a list of considerations when selecting an appropriate Java
    (maximum Java heap size):'
  prefs: []
  type: TYPE_NORMAL
- en: Setting maximum heap space, in bytes, and initial heap size, also in bytes.
    These are appropriate memory allocation pool values that help control the amount
    of memory usage for our JVM-based Spark application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-Xmx` option changes the maximum heap space for the VM. Some example settings
    are `-Xmx2048`, `-Xmx81920k`, and  `-Xmx1024m`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-Xmx10G` is the same as `-Xmx1024m` or `-Xmx1024g`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `-Xms` option allows us to set an initial heap size. The default value
    is 64 MB or 640 KB, for example, `Xms64m`. Consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To determine how much can be a higher heap size setting, we recommend increasing
    the Java heap space to no more than 50% of the total RAM available. For example,
    if your machine has 32 GB of available RAM, we recommend setting the maximum heap
    no higher than 16 GB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting the maximum heap space to a value above 16 GB in our example would cause
    problems with performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will review your system JDK.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the JDK version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have JDK 8, that is all you need to safely skip this section. If you
    want to install JDK 9, do not. Spark is incompatible with any JDK version greater
    than 8\. Also, please ensure that you did not install the JDK into a path that
    has spaces in it. This is a minor detail, but we want to make sure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we get into the installation of MongoDB. We will talk about
    the why and the how.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is MongoDB and why do we even need it? Firstly, MongoDB's document model
    makes it easy to map objects in application code to equivalent JSON representations
    in MongoDB. There is more to this. Spark has good integration with MongoDB. One
    clear advantage is being able to publish our on-time dataframe into MongoDB as
    a document. Fetching a dataframe document from MongoDB is good from a performance
    standpoint too.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two prerequisites to installing MongoDB (on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: Only 64-bit machines are able to support MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to get the latest Windows updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get started, download the latest stable version of the MongoDB Community
    Server from the MongoDB Download Center page on the [mongodb.com](https://www.mongodb.com/)
    website. That will be 4.0\. Depending on whichever operating system you have,
    download the appropriate version. The instructions here are for Windows 10, 64-bit
    users.
  prefs: []
  type: TYPE_NORMAL
- en: The MongoDB product no longer supports 32-bit x86 operating system platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next few steps, we will install MongoDB as a service:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the MongoDB installer, an MSI file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9db19f8d-88e0-42b3-aac8-5f6b010d74bd.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MSI file of MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: 'Click Install, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b847d3ed-63ee-4bee-b681-ae039692673d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Install screen of MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: 'Click Next and proceed with the complete setup type of installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d3fd7825-33b7-4013-b8d4-4ffdc84b47ea.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Next button of MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Complete and proceed with the complete setup type of installation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a60e2eda-f2af-40c6-8d32-3c6fcf447d39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Complete option of MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: 'As already stated, we will choose not to install MongoDB as a service. Therefore,
    uncheck the Install MongoDB as a Service option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/01655a8d-35da-4e57-b947-c6c6a44e7860.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Service Configuration MongoDB
  prefs: []
  type: TYPE_NORMAL
- en: Note where you are installing MongoDB into. The server is installed at `C:\MongoDB\Server\4.0`.
    The data folder is at `C:\MongoDB\Server\4.0\data`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will see the screen of the MongoDB Compass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e997fbf2-b701-4632-bd3a-69e50af17951.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Install MongoDB Compass
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will show you how and why we put MongoDB to work. With
    the prerequisites out of the way and the application building infrastructure in
    place, we proceed to the *Implementation and deployment* section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementation depends on setting up the big data infrastructure. Please verify
    that your MongoDB installation is running properly. Now we shall list implementation
    objectives as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data into test, train and validation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The overall objective is to perform data analysis on an on-time flight dataset
    corresponding to the year 2007-2008\. Of the 2007 flight data, 80% will be used
    as the training dataset and the rest as a validation dataset. In so far as model
    performance evaluation is concerned, 100% of the 2008 flight data becomes the
    testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the implementation objectives required to implement the flight
    prediction model:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the flight dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may develop the pipeline in four ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incrementally in your local Spark shell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By firing up your Horton Sandbox on your host machine managed virtual machine,
    and developing code in a powerful Zeppelin Notebook environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing everything on the Azure Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the application as a self-contained SBT application and deploying
    it to your local Spark cluster using `spark-submit`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flesh out your code in IntelliJ and wire up all the necessary dependencies in
    the `build.sbt` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the application and interpret the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next subsection, we will document step-by-step instructions for implementing
    the project. In the succeeding step, we will create a new Scala project in IntelliJ
    and call it `Chapter6`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new Scala project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a Scala project called `Chapter6`, with the following artifacts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`AirlineWrapper.scala`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Aircraft.scala`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot is representative of what our project looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c73be01-6e7d-4cc0-8a80-b4087781895f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: IntelliJ project structure
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break down the project structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.idea`: These are the generated IntelliJ configuration files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`project`: Contains a `build.properties` and `plugins.sbt`. For example, `plugins.sbt`
    may be used to specify the SBT assembly plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/scala`: A folder that houses Scala source files in the `com.packt.modern.chapter6`
    package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src/main/resources`: Any data or configuration files; for example, a log4j
    configuration file called `log4j.xml`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`: This is where artifacts of the compile process are stored. Any generated
    assembly JAR files go there.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`build.sbt`: This is the main SBT configuration file. Spark and its dependencies
    are specified here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we will start developing. We start with the `AirlineWrapper.scala`
    file and end with the deployment of the final application JAR into Spark with
    `spark-submit`.
  prefs: []
  type: TYPE_NORMAL
- en: Building the AirlineWrapper Scala trait
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `AirlineWrapper` contains code to create a `SparkSession` instance called
    `session`. It also declares case classes to represent our flights dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create the `trait` definition first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The entry point to programming is as follows: The first thing we do in the
    `trait` is to declare a `lazy val` called `session`. This is where we lazily create
    an instance of `SparkSession`. Lazily implies that the `val` is only executed
    when it is encountered the first time around. The session is our entry point to
    programming Spark with the `DataSet` and `DataFrame` API is `SparkSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code snippet, `CarrierCode` is an identification number assigned
    by US DoT to identify a unique airline (carrier):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, `originOfFlight` is the origin of the flight (IATA airport
    code) and `destOfFlight` is the destination of the flight (IATA airport code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code snippet, `iataAirportCode` is the international airport
    abbreviation code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and create a `File` object out of the airport''s dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Load and create a `File` object out of the airline carrier dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `File` object out of the main FAA dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This method takes in a relative path to the data inside the `resources` folder
    of your folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will write a method called `buildDataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember to update your import statements. The necessary input statements look
    like the following code. This is all we need to be able to compile all of the
    code that we developed up until now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the MongoDB packages, including the connector package, in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `Aircraft` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `main` method inside the `Aircraft` object, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `object` now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `case class` to represent carefully selected features in the dataset
    that we decide will contribute most to this data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, create a dataframe to represent the `FlightData`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We just loaded the dataset and created a dataframe. Now, we are able to print
    the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `printschema()` method displays the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53042a67-d6a3-4ab1-8562-2c1e5893552b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will need a cast on some fields. To call the `cast` method, we call in the
    following import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create a local temporary view and give it the name `airline_onTime`.
    This temporary view only exists for as long as the lifespan of the `SparkSession`
    that we used to create our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a `count` on the number of rows in the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Create a local temporary view using the given name. The lifetime of this temporary
    view is
  prefs: []
  type: TYPE_NORMAL
- en: 'tied to the `SparkSession` that was used to create this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a local temporary view using the given name. The lifetime of this temporary
    view is tied to the `SparkSession` that was used to create this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Having trimmed and cast our fields and made sure the numeric columns work,
    we can now save our data as JSON lines and parquet. Call the `toJSON` method to
    return the content of the dataset as a dataset of JSON strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the new dataset in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Save our JSON airline dataframe as a `.gzip` JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to convert our dataframe to `parquet` records. The following
    code does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read our newly created JSON archive and display the first 20 rows of
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the `parquet` version as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the `parquet` version of the airline dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, write to the MongoDB database, `airlineOnTimeData`. The call to the `save`
    method produces a `DataFrameWriter` that contains a `.mode` method; `mode` takes
    in an `"overwrite"` parameter. Thus, if the `collection` already exists in Mongo,
    the new records will still be written into the MongoDB database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: To confirm that the data was written into MongoDB, launch the MongoDB Compass
    Community app. In the Connect to Host opening screen, click on Connect and in
    the resulting screen click on database test. The benefit of writing to MongoDB
    is that, it gives us a easy way to retrieve our data and import it into Spark
    if something were to corrupt our data `airlineOnTimeData` collection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, submit the application into a Spark local cluster using the `spark-submit`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we carried out **machine learning** (**ML**) data analysis
    tasks on flight performance data. One such task is the implementation of a regression
    model fitted on a training subset of data. Given a new or unknown flight with
    delayed departure data, this model was able to predict whether the flight under
    investigation made up for time lost and arrived at the destination on time. One
    important takeaway from this ML exercise is this—the origin to destination distance
    contributed most toward predicting time gained. Carrier delays contributed least
    toward a prediction. A longer flight, it turns out, is able to gain more time.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provided the foundation to build more sophisticated models. A model
    with more predictor variables (for example, taking into account, the weather and
    security delays) could yield deeper, sharper predictions. That said, this chapter
    hopefully opens a window for opportunity readers to understand how flight performance
    insights could help travelers snag an optimal travel experience in terms of money
    and time spent.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter and the last one, we will develop a recommender system.
    Get inspired by Amazon's recommendation algorithms and Netflix's ratings system
    for bringing us relevant movies. The recommendation system that we build will
    take advantage of all our accumulated skills in Spark ML this far.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before readers head to the next chapter, we invite readers to attempt an upgrade
    on the flight performance model. The idea is this—feed in a couple more predictors
    that enhance the flight delay ML process in a way that makes predictions deeper
    and more incisive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few questions to open further vistas of learning:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a `parquet` file and what are its advantages, especially when a dataset
    becomes larger, and data shuffling between nodes becomes necessary?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of data compressed in a columnar format?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Occasionally, you might run into this error: "`Unable to find encoder stored
    in Dataset. Primitive types (Int, String, and so on) and Product types (case classes)
    are supported by importing spark.implicits._`". How do you get around this error?
    What is the root cause? Hint—build a simple dataframe with a dataset from the
    first chapter. Use the `spark.read` approach and attempt a `printSchema` on it.
    If that produces the aforementioned error, investigate if it could be that an
    explicit schema is required'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As an alternative to MongoDB, would you rather submit flight performance data
    to HDFS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why did MongoDB prove to be useful in this chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is semi-structured data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name one big benefit of Spark that sets it apart from Hadoop? For example, think
    programming paradigms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you read in the flight's data from Kafka? If so, how and what might be a
    reason to do this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is data enrichment and how is it related to munging if both the terms are
    related?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataframe with two case classes, each with a small subset from the
    carriers CSV and airports CSV datasets respectively. How would you write this
    to MongoDB?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following article on *Introduction to Multivariate Regression Analysis*
    is about the importance of regression analysis: [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3049417/)
  prefs: []
  type: TYPE_NORMAL
