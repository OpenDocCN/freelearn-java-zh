- en: Schema Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we saw how to produce and consume data in JSON format.
    In this chapter, we will see how to serialize the same messages with Apache Avro.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Avro in a nutshell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starting the Schema Registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Schema Registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to build a Java `AvroProducer`, a consumer, and a processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to run the Java `AvroProducer` and the processor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avro in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Avro is a binary serialization format. The format is schema-based so,
    it depends on the definition of schemas in JSON format. These schemas define which
    fields are mandatory and their types. Avro also supports arrays, enums, and nested
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: One major advantage of Avro is that it supports schema evolution. In this way,
    we can have several historical versions of the schema.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, the system must adapt to the changing needs of the business. For this
    reason, we can add or remove fields from our entities, and even change the data
    types. To support forward or backward compatibility, we must consider which fields
    are indicated as optional.
  prefs: []
  type: TYPE_NORMAL
- en: Because Avro converts the data into arrays of bytes (serialization), and Kafka's
    messages are also sent in binary data format, with Apache Kafka, we can send messages
    in Avro format. The real question is, where do we store the schemas for Apache
    Avro to work?
  prefs: []
  type: TYPE_NORMAL
- en: Recall that one of the main functions of an enterprise service bus is the format
    validation of the messages it processes, and what better if it has a historical
    record of these formats?
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka Schema Registry is the module responsible for performing important
    functions. The first is to validate that the messages are in the appropriate format,
    the second is to have a repository of these schemas, and the third is to have
    a historical version format of these schemas.
  prefs: []
  type: TYPE_NORMAL
- en: The Schema Registry is a server that runs in the same place as our Kafka brokers.
    It runs and stores the schemas, including the schema versions. When messages are
    sent to Kafka in Avro format, the messages contain an identifier of a schema stored
    in the Schema Registry.
  prefs: []
  type: TYPE_NORMAL
- en: There is a library that allows for message serialization and deserialization
    in Avro format. This library works transparently and naturally with the Schema
    Registry.
  prefs: []
  type: TYPE_NORMAL
- en: When a message is sent in Avro format, the serializer ensures that the schema
    is registered and obtains the schema ID. If we send an Avro message that is not
    in the Schema Registry, the current version of the schema is registered automatically
    in the Registry. If you do not want the Schema Registry to behave in this way,
    you can disable it by setting the `auto.register.schemas` flag to `false`.
  prefs: []
  type: TYPE_NORMAL
- en: When a message is received in Avro format, the deserializer tries to find the
    schema ID in the Registry and fetch the schema to deserialize the message in Avro
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Both the Schema Registry and the library for the serialization and deserialization
    of messages in Avro format are under the Confluent Platform. It is important to
    mention that when you need to use the Schema Registry, you must use the Confluent
    Platform.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to mention that with the Schema Registry, the Confluent
    library should be used for serialization in Avro format, as the Apache Avro library
    doesn't work.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to define the Avro schema. As a reminder, our `HealthCheck`
    class looks like *Listing 5.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.1: HealthCheck.java'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with the representation of this message in Avro format, the schema (that
    is, the template) of all the messages of this type in Avro would be *Listing 5.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.2: healthcheck.avsc'
  prefs: []
  type: TYPE_NORMAL
- en: This file must be saved in the `kioto` project in the `src/main/resources` directory.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that there are the types `string`, `float`, and `double`.
    But, in the case of `Date`, it can be stored as a `long` or as a `string`.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will serialize `Date` as a `long`. Avro doesn't have a
    dedicated `Date` type; we have to choose between a `long` and a `string` (an ISO-8601
    `string` is usually better), but the point with this example is to show how to
    use different data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about Avro schemas and how to map the types, check the
    Apache Avro specification at the following URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://avro.apache.org/docs/current/spec.html](http://avro.apache.org/docs/current/spec.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Starting the Schema Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, we have our Avro schema; now, we need to register it in the Schema Registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we start the Confluent Platform, the Schema Registry is started, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want just to start the Schema Registry, we need to run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is similar to the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using the Schema Registry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, the Schema Registry is running on port `8081`. To interact with the Schema
    Registry, there is a REST API. We can access it with `curl`. The first step is
    to register a schema in the Schema Registry. To do so, we have to embed our JSON
    schema in another JSON object, and we have to escape some special characters and
    add a payload:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning, we have to add `{ \"schema\": \"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the double quotation marks (`"`) should be escaped with a backslash (`\"`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end, we have to add `\" }`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, as you can guess, the API has several commands to query the Schema Registry.
  prefs: []
  type: TYPE_NORMAL
- en: Registering a new version of a schema under a – value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To register the Avro schema `healthcheck.avsc`, located in the `src/main/resources/`
    path listed in *Listing 5.2*, using the `curl` command, we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This means that we have registered the `HealthChecks` schema with the version
    `"id":1` (congratulations, your first version).
  prefs: []
  type: TYPE_NORMAL
- en: Note that the command registers the schema on a subject called `healthchecks-avro-value`.
    The Schema Registry doesn't have information about topics (we still haven't created
    the `healthchecks-avro` topic). It is a convention, followed by the serializers/deserializers,
    to register schemas under a name following the <topic>-value format. In this case,
    since the schema is used for the message values, we use the suffix-value. If we
    wanted to use Avro to identify our messages keys, we would use the <topic>-key
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to obtain the ID of our schema, we use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output is the schema ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the schema ID, to check the value of our schema, we use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is the schema value shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Registering a new version of a schema under a – key subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an example, to register a new version of our schema under the `healthchecks-avro-key`
    subject, we would execute the following command (don''t run it; it is just to
    exemplify):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Registering an existing schema into a new subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's suppose that there is an existing schema registered on a subject called
    `healthchecks-value1`, and we need this schema available on a subject called `healthchecks-value2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command reads the existing schema from `healthchecks-value1`
    and registers it to `healthchecks-value2` (assuming that the `jq` tool is already
    installed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Listing all subjects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To list all the subjects, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Fetching a schema by its global unique ID
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To fetch a schema, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Listing all schema versions registered under the healthchecks–value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To list all schema versions registered under the `healthchecks-value` subject,
    you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Fetching version 1 of the schema registered under the healthchecks-value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To fetch version 1 of the schema registered under the `healthchecks-value`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Deleting version 1 of the schema registered under the healthchecks-value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delete version 1 of the schema registered under the `healthchecks-value`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Deleting the most recently registered schema under the healthchecks-value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delete the most recently registered schema under the `healthchecks-value`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Deleting all the schema versions registered under the healthchecks–value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To delete all the schema versions registered under the `healthchecks-value`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Checking whether a schema is already registered under the healthchecks–key subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To check whether a schema is already registered under the `healthchecks-key`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Testing schema compatibility against the latest schema under the healthchecks–value
    subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the schema compatibility against the latest schema under the `healthchecks-value`
    subject, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Getting the top-level compatibility configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To get the top level compatibility configuration, you can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Globally updating the compatibility requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To globally update the compatibility requirements, you can use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Updating the compatibility requirements under the healthchecks–value subject
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To update the compatibility requirements under the `healthchecks-value` subject,
    you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Java AvroProducer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we should modify our Java Producer to send messages in Avro format. First,
    it is important to mention that in Avro there are two types of messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specific records**: The file with the Avro schema (avsc) is sent to a specific
    Avro command to generate the corresponding Java classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generic records**: In this approach, a data structure similar to a map dictionary
    is used. This means that you set and get the fields by their names and you must
    know their corresponding types. This option is not type-safe, but it offers much
    more flexibility than the other, and here the versions are much easier to manage
    over time. In this example, we will use this approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start with the code, remember that in the last chapter we added the
    library to support Avro to our Kafka client. If you recall, the `build.gradle`
    file has a special repository with all this libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Confluent''s repository is specified in the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'In the dependencies section, we should add the specific Avro libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Do not use the libraries provided by Apache Avro, because they will not work.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we already know, to build a Kafka message producer, we use the Java client
    library; in particular, the producer API. As we already know, there are two requisites
    that all the Kafka producers should have: to be a `KafkaProducer` and to set the
    specific `Properties`, such as *Listing 5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.3: AvroProducer Constructor'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the `AvroProducer` constructor shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In line `//1`, the values now are of type `org.apache.avro.generic.GenericRecord`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//2`, the constructor now receives the Schema Registry URL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//3`, the Serializer type for the messages' keys remains as `StringSerializer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//4`, the Serializer type for the messages' values now is a `KafkaAvroSerializer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//5`, the Schema Registry URL is added to the Producer properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//6`, the avsc file with the schema definition is parsed with a Schema
    Parser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we have chosen the use of generic records, we have to load the schema.
    Note that we could have obtained the schema from the Schema Registry, but this
    is not safe because we do not know which version of the schema is registered.
    Instead of this, it is a smart and safe practice to store the schema along with
    the code. In this way, our code will always produce the correct data types, even
    when someone else changes the schema registered in the Schema Registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/avro` directory, create a file called `AvroProducer.java`
    with the contents of *Listing 5.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.4: AvroProducer.java'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the `AvroProducer` class shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In line `//1`, `ratePerSecond` is the number of messages to send in a 1-second
    period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//2`, to simulate repetition, we use an infinite loop (try to avoid
    this in production)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//3`, now we can create `GenericRecord` objects using `GenericRecordBuilder`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//4`, we use a Java Future to send the record to the `healthchecks-avro`
    topic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//5`, we wait this time to send messages again
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//6`, we read the result of the Future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//7`, everything runs on the broker on the localhost in port `9092`,
    and with the Schema Registry running on the localhost in port `8081`, sending
    two messages in an interval of 1 second
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the AvroProducer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run the following command from the `kioto` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is OK, the output is something like the one shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'If it is not running yet, go to Confluent''s directory and start it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The broker is running on port `9092`. To create the `healthchecks-avro` topic,
    execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are just creating a normal topic and nothing indicates the messages'
    format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run a console consumer for the `healthchecks-avro` topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: From our IDE, run the main method of the `AvroProducer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer should be similar to the one shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Binary is a horrible format for humans to read, isn't it? We can just read the
    strings but not the rest of the record.
  prefs: []
  type: TYPE_NORMAL
- en: To solve our readability problem, we should use `kafka-avro-console-consumer`
    instead. This fancy consumer deserializes the Avro records and prints them as
    human-readable JSON objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the command line, run an Avro console consumer for the `healthchecks-avro`
    topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output on the console consumer should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are finally producing Kafka messages in Avro format. With the help of
    the Schema Registry and the Confluent library, this task is quite simple. As described,
    after much frustration in productive environments, the generic records scheme
    is better than the specific records scheme, because it is better to know specifically
    with which schema we are producing data. Keeping a copy of the schema along with
    the code gives you that guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if we fetch the schema from the Schema Registry before producing
    the data? The correct answer is it depends, and it depends on the
  prefs: []
  type: TYPE_NORMAL
- en: '`auto.register.schemas` property. If this property is set to true, when you
    request a schema that is not in the Schema Registry, it will automatically be
    registered as a new schema (this option in production environments is not recommended,
    as it is prone to error). If the property is set to false, the schema will not
    be stored and, since the schema will not match, we will have a nice exception
    (do not believe me, reader; go and get proof of this).'
  prefs: []
  type: TYPE_NORMAL
- en: Java AvroConsumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a Kafka `AvroConsumer` that we will use to receive the input
    records. As we already know, there are two prerequisites that all the Kafka Consumers
    should have: to be a `KafkaConsumer` and to set specific properties, such as in
    *Listing 5.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.5: AvroConsumer constructor'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the changes in the `AvroConsumer` constructor shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In line `//1`, the values now are of type `org.apache.avro.generic.GenericRecord`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//2`, the constructor now receives the Schema Registry URL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//3`, the deserializer type for the messages' keys remains as `StringDeserializer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//4`, the deserializer type for the values is now `KafkaAvroDeserializer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//5`, the Schema Registry URL is added to the consumer properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In line `//6`, with these `Properties`, we build a `KafkaConsumer` with string
    keys and `GenericRecord` values: `<String, GenericRecord>`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that when defining the Schema Registry URL for the deserializer
    to fetch schemas, the messages only contain the schema ID and not the schema itself.
  prefs: []
  type: TYPE_NORMAL
- en: Java AvroProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/avro` directory, create a file called `AvroProcessor.java`
    with the contents of *Listing 5.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.6: AvroProcessor.java (part 1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the first part of the `AvroProcessor` class shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we declare an `AvroConsumer`, as in *Listing 5.5*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second section, we declare an `AvroProducer`, as in *Listing 5.4*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, in the `src/main/java/kioto/avro` directory, let''s complete the `AvroProcessor.java`
    file with the contents of *Listing 5.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing 5.7: AvroProcessor.java (part 2)'
  prefs: []
  type: TYPE_NORMAL
- en: 'An analysis of the `AvroProcessor` shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In line `//1`, the consumer is subscribed to the new Avro topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//2`, we are consuming messages of type `GenericRecord`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//3`, the Avro record is deserialized to extract the `HealthCheck`
    object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//4`, the start time is transformed into the format in the current
    time zone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//5`, the uptime is calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//6`, the uptime is written to the `uptimes` topic, using the serial
    number as the key and the uptime as the value. Both values are written as normal
    strings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In line `//7`, everything runs on the broker on the localhost in port `9092`
    and with the Schema Registry running on the localhost in port `8081`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As mentioned previously, the code is not type-safe; all the types are checked
    at runtime. So, be extremely careful with that. For example, the strings are not
    `java.lang.String`; they are of type `org.apache.avro.util.Utf8`. Note that we
    avoid the cast by calling the `toString()` method directly on the objects. The
    rest of the code remains equal.
  prefs: []
  type: TYPE_NORMAL
- en: Running the AvroProcessor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build the project, run the following command from the `kioto` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is correct, the output will be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a console consumer for the `uptimes` topic, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: From the IDE, run the main method of the `AvroProcessor`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the IDE, run the main method of the `AvroProducer`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed, instead of sending data in JSON format, how to use
    AVRO as the serialization format. The main benefit of AVRO (over JSON, for example)
    is that the data must conform to the schema. Another advantage of AVRO over JSON
    is that the messages are more compact when sent in binary format, although JSON
    is human readable.
  prefs: []
  type: TYPE_NORMAL
- en: The schemas are stored in the Schema Registry, so that all users can consult
    the schema version history, even when the code of the producers and consumers
    for those messages is no longer available.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Avro also guarantees backward and forward compatibility of all messages
    in this format. Forward compatibility is achieved following some basic rules,
    such as when adding a new field, declaring its value as optional.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka encourages the use of Apache Avro and the Schema Registry for the
    storage of all data and schemas in Kafka systems, instead of using only plain
    text or JSON. Using that winning combination, you guarantee that your system can
    evolve.
  prefs: []
  type: TYPE_NORMAL
