- en: '*Chapter 6*: Observing and Monitoring Cloud-Native Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two chapters, we discussed and explained the varying capabilities
    of the MicroProfile 4.1 platform for building and enhancing your cloud-native
    application. At this point, your cloud-native application is built on a strong
    fundamental core with much credit due to the tried and true components of the
    Jakarta EE platform. On top of that, you've added a few bells and whistles to
    make your application more resilient, secure, configurable, and documentable.
    For all intents and purposes, you have yourself a fully capable cloud-native application
    that's ready for deployment. But as the savvy developer that you are, you know
    that the story of your cloud-native application is not complete once you deploy
    it. Nothing is truly perfect and, depending on the complexity of your application
    ecosystem, letting your applications run wild can be disastrous.
  prefs: []
  type: TYPE_NORMAL
- en: This brings forth the important task of monitoring your applications. You, your
    team, or your operations team will need to be able to monitor the activity and
    performance of your applications to identify any potential problems. Effective
    monitoring can be used as an early warning for impending trouble, shedding light
    on areas that may need optimizing, or aiding in the post-mortem analysis to see
    what might have gone wrong. Alternatively, from a more optimistic perspective,
    effective monitoring can simply provide beautiful data on the performance of your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to this chapter, where we will cover the last three specifications
    that are included in the MicroProfile platform's release scope. For observing
    and monitoring your cloud-native applications, the MicroProfile platform provides
    the **MicroProfile Health**, **MicroProfile Metrics**, and **MicroProfile OpenTracing**
    technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Determining the health of your cloud-native application using MicroProfile Health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumenting and using metrics on your cloud-native application using MicroProfile
    Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracing your cloud-native application using MicroProfile OpenTracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build and run the samples mentioned in this chapter, you will need a Mac
    or PC (Windows or Linux) with the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Java Development Kit, version 8 or higher: [https://adoptium.net/](https://adoptium.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apache Maven: [https://maven.apache.org/](https://maven.apache.org/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Git client: [https://git-scm.com/](https://git-scm.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the source code used in this chapter is available on GitHub at
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have cloned the GitHub repository, you can start the Open Liberty
    server that these code samples will execute in, by entering the `ch6` directory
    and entering the following command from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You can then stop the server in the same command window by pressing *Ctrl* +
    *C*.
  prefs: []
  type: TYPE_NORMAL
- en: The application that's deployed to the Open Liberty server will be given a context
    root of `ch6`. For example, the full URL to a JAX-RS resource would be `http://localhost:9080/ch6/path/to/resource`.
    This will be reflected in the code samples in this chapter that illustrate sending
    requests to an endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the health of your cloud-native application using MicroProfile Health
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To begin our three-part journey of looking at the MicroProfile observability
    toolkit, we'll examine the MicroProfile Health technology. We chose to examine
    this technology first as its benefits and use cases are much broader in scope
    compared to the other two technologies in this chapter. The MicroProfile Health
    technology reports information regarding the health, or status, of your microservice.
    The expected health status is either *UP* or *DOWN*.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of MicroProfile Health in a cloud-native application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now know what the MicroProfile Health technology can do. But what purpose
    does it serve? To find out, we must take a step back. The driving force for developing
    applications with the MicroProfile technologies is that they will be cloud native.
    And if you recall [*Chapter 1*](B17377_01_Final_SB_epub.xhtml#_idTextAnchor021),
    *Cloud-Native Application*, the important distinction between a cloud-native application
    and a non-cloud-native application is its ability to take advantage of the capabilities
    provided by the cloud. The MicroProfile Health technology is a perfect example
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: At its very core, the MicroProfile Health technology strives to report the health
    of your application to some external observer. Since we are developing a cloud-native
    application, the application will live its life in a container that's been deployed
    on your cloud platform. Whether its tenure is short-lived or not is what the health
    statuses serve to dictate. In effect, these health statuses report the health
    of your container to your cloud platform. Using the status report of the container,
    the cloud platform's monitor service can use that data to make decisions to terminate
    and replace any troublesome containers. The circumstances of when your containers
    are terminated and restarted ultimately dependent on you, the developer. Your
    cloud platform may have rules on what to do, given the health of a container,
    but the context of how these health statuses are reported depends on how the MicroProfile
    Health technology is used throughout your application.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will examine a sample scenario of using the health
    status of your application/container with **Kubernetes**. Kubernetes is an open
    source project that provides a container orchestration solution for deploying,
    scaling, and managing your containers. As one of the more well-known container
    orchestration platforms, it will provide an excellent vehicle for demonstrating
    the benefits of using the MicroProfile Health technology. Kubernetes, along with
    other cloud infrastructure topics, will be covered in more detail in [*Chapter
    7*](B17377_07_Final_SB_epub.xhtml#_idTextAnchor146)*,* *MicroProfile Ecosystem
    with Open Liberty, Docker, and Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile Health technology overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MicroProfile Health provides three types of health, or status, checks. They
    are the **liveness**, **readiness**, and **startup** health checks. We will explain
    these health checks in detail shortly, but for now, know that their purpose is
    to report whether an application is alive, ready, or if it has even completed
    starting up. The instrumentation and existence of a health check in an application
    is defined as a **procedure**. This procedure is called upon to check if the application
    has started, as well as its liveness or readiness for the instrumented components
    of the application. Moving forward, we will refer to health checks as either a
    health check, a procedure, or a combination of both.
  prefs: []
  type: TYPE_NORMAL
- en: Health check procedures can be instrumented throughout your microservice and
    will return an *UP* or *DOWN* status to indicate the liveness or readiness of
    the various components of your application, as well as if the application has
    completed initialization. The liveness, readiness, and startup health checks will
    be reported through the `http://host:port/health/live`, `http://host:port/health/ready`,
    and `http://host:port/health/started` endpoints, respectively. An overall status
    is provided for each endpoint and is the logical conjunction of all procedures.
    If there are five readiness procedures instrumented in your application and all
    but one procedure returns *UP*, then the overall readiness of your application
    is *DOWN*. There is also the `http://host:port/health` endpoint, which provides
    an overall status of the whole application from the conjunction of the health
    check procedures for the liveness, readiness, and startup health checks. When
    using the `http://host:port/health` endpoint, there is no distinction between
    liveness, readiness, or startup health checks. All health check procedures, regardless
    of their type, must return *UP* for the `http://host:port/health` endpoint to
    return *UP*. It should be noted that the order in which procedures are invoked
    is arbitrary, so they can be invoked in any order. Moving forward, when referring
    to the health endpoints, we will omit `http://host:port` for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: Before we move on, let's learn a little bit more about these three types of
    health checks.
  prefs: []
  type: TYPE_NORMAL
- en: Liveness health check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of the liveness health check procedure, as its name suggests, is
    to report whether the application is alive. In a cloud environment, this status
    can be used by the monitoring service to determine if the application is running
    as expected. Failing this health check may prompt your cloud platform's monitoring
    service to terminate the application's container. Depending on the policy that
    you've configured, this may cause the application's container to be restarted.
  prefs: []
  type: TYPE_NORMAL
- en: Note that a failing liveness procedure does not mean that the application is
    no longer running. Instead, it means that the strategy that's been employed for
    instrumenting the check has deemed that the application has suffered or is suffering
    from a deterioration of service and can no longer be considered operationally
    effective. For example, a liveness procedure can be used to detect a memory leak
    in the JVM and at the rate of memory loss, so it would be prudent to terminate
    this container now instead of later. As a result, a *DOWN* status will be returned.
  prefs: []
  type: TYPE_NORMAL
- en: Startup health check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of the startup health check is to provide an intermediary check
    that is a precursor to the liveness health check. In a container environment,
    not all containers are made equal. Understandably, certain containers may be slow
    to start and initialize due to the complexity of the application that is running
    within them. In a compatible cloud environment, the startup check can be carried
    out for an *UP* before liveness checks are carried out.
  prefs: []
  type: TYPE_NORMAL
- en: Readiness health check
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The purpose of a readiness health check procedure is to allow external observers
    (for example, the cloud monitoring service) to identify if the application is
    ready to receive and conduct business logic. The application may have effectively
    started and be alive and running with no issues, as indicated by the liveness
    checks, but it isn't quite ready to receive traffic yet. This could be because
    the application is still attempting to initialize a resource or connect to another
    application that it depends on. The readiness check will report *DOWN* as it continues
    its attempt to secure a connection.
  prefs: []
  type: TYPE_NORMAL
- en: Special Note About Default Health Checks
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your underlying MicroProfile runtime, your runtime may provide
    `mp.health.disable-default-procedures` Config element and setting its value to
    `true`.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting health check procedures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A health check procedure is called upon by the MicroProfile runtime to find
    out the health of a particular component of an application, whether it be a liveness,
    readiness, or startup procedure. But it can also be the case that the health check
    reports on the liveness, readiness, and startup procedures simultaneously. This
    is due to how the MicroProfile Health runtime operates under the hood. Like the
    other MicroProfile technologies, MicroProfile Health is intrinsically integrated
    with `@Liveness`, `@Readiness`, and `@Startup` qualifier annotations. Using any
    of these annotations lets the MicroProfile Health runtime know what health statuses
    are being reported. But before we get too ahead of ourselves, what exactly is
    a health check in the application code?
  prefs: []
  type: TYPE_NORMAL
- en: The basis for every health check procedure is the functional interface, called
    `HealthCheck`. This consists of a single function, `call()`, that returns a `HealthCheckResponse`.
    In an application, the `HealthCheck` implementation is annotated with at least
    one of the `@Liveness`, `@Readiness`, or `@Startup` annotations. Remember that
    MicroProfile Health's integration with CDI means that every health check procedure
    (that is, the `HealthCheck` implementation) is a CDI bean and has a place in the
    application's life cycle context. It would be prudent to also define a CDI scope
    for your health check procedure. We will be using `@ApplicationScoped` in our
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how to instrument the liveness, readiness,
    and startup health checks in the same procedure. You can instrument a singular
    health check by using one annotation instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, now that we know how to create and define the different types of health
    checks, we can learn how to build the health check response, whose data will be
    consumed by an external observer through one of the `/health/*` endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, we will be returning a `HealthCheckResponse` object.
    This data object contains all the information we need for uniquely identifying
    the health check and, most importantly, the health of your cloud-native application.
  prefs: []
  type: TYPE_NORMAL
- en: '`HealthCheckResponse` consists of three fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`String` field that distinguishes this health check procedure from other health
    check procedures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enum` field with either an *UP* or *DOWN* value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Map<String, Object>`. When `String` key and its value can be either `String`,
    `long`, or `boolean`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's look at the different ways of constructing health check procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Using HealthCheckResponseBuilder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create a `HealthCheckResponse`, you can call one of two static methods in
    `HealthCheckResponse` that will return a `HealthCheckResponseBuilder`. These two
    methods are `builder()` and `named(String name)`. The latter creates a `HealthCheckResponseBuilder`
    with a name that's already specified, whereas the former provides a *clean* instance.
  prefs: []
  type: TYPE_NORMAL
- en: '`HealthCheckResponseBuilder` provides a builder pattern for constructing a
    `HealthCheckResponse` containing the required and optional fields. This is the
    preferred method if you intend to provide optional data.'
  prefs: []
  type: TYPE_NORMAL
- en: The following code example shows a scenario where we are performing a liveness
    health check based on the heap memory usage of the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `LivenessCheck` can be found at [https://bit.ly/2WbiVyV](https://bit.ly/2WbiVyV):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used the `named(String name)` static method to provide the
    health check with a name. We then used the `HealthCheckResponseBuilder` class's
    `up()`, `down()`, and `withData(String key, String value)` methods to specify
    the status of the health check and to provide any additional contextual data.
    The `withData(…)` method is an overloaded method and can accept `String`, `long`,
    and `boolean` values. In this example, if the memory usage is less than 90% (that
    is, a value less than `0.9` is returned by the `getMemUsage()` method), we will
    return an *UP* status. Otherwise, we will return a *DOWN* status.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, if you're using `HealthCheckResponse.builder()` instead, you
    will need to use the `HealthCheckResponseBuilder` class's `name(String name)`
    to provide a name for the health check.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, instead of having a bulky `if`-`else` block, we can use `HealthCheckResponseBuilder.status(boolean
    status)` in one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we've reduced eight lines of code to one line!
  prefs: []
  type: TYPE_NORMAL
- en: Using HealthCheckResponse
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using `HealthCheckResponseBuilder`, we can also use the `HealthCheckResponse`
    class's two static methods, which conveniently create an *UP* or *DOWN* `HealthCheckResponse`,
    as illustrated in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `ReadinessCheck` can be found at [https://bit.ly/3iV3WBP](https://bit.ly/3iV3WBP):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The methods that are being used here are aptly named `up(String name)` and `down(String
    name)` and accept a `String` parameter that defines the name of the health check.
    This approach assumes that there is no additional optional data to be incorporated
    with this health check procedure. In the following example, we will retrieve the
    current system time and if it is an even number, we will return a *DOWN* status
    (otherwise, it will be an *UP* status).
  prefs: []
  type: TYPE_NORMAL
- en: CDI producers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With MicroProfile Health's implicit reliance on CDI, health checks procedures
    can also be instrumented using a CDI method producer. You can instrument multiple
    health check procedures in one class by using CDI method producers. The following
    example shows the liveness, readiness, and startup health check procedures being
    instrumented as CDI method producers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `CDIMethodProducerCheck` can be found at [https://bit.ly/3k9GrUT](https://bit.ly/3k9GrUT):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The liveness procedure that's encapsulated by the `livenessCDIMethodProducer`
    method will return *UP* if the memory usage is below 90% (that is, a value less
    than 0.9 is returned by the `getMemUsage()` method). The readiness procedure that's
    encapsulated by the `readinessCDIMethodProducer` method will return *UP* if the
    CPU usage is below 90% (that is, a value less than 0.9 is returned by the `getCpuUsage()`
    method). The startup procedure that's encapsulated by the `startupCDIMethodProducer`
    method will execute the `getStatus()` business method to evaluate the condition
    of the application's startup state, and will return either `true` or `false` to
    invoke an *UP* or *DOWN* status, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving health check data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned previously, we can view the data through the `/health`, `/health/liveness`,
    `/health/readiness`, and `/health/started` endpoints. Due to this, these health
    checks can be consumed by requests over HTTP/REST. Through an HTTP/REST call,
    the health check procedures are presented in JSON format. The root level contains
    the overall health status with a `status` field, and it is calculated from the
    conjunction of all health check procedures defined in the `checks` JSON list.
  prefs: []
  type: TYPE_NORMAL
- en: The overall status dictates the HTTP response code. An *UP* status returns HTTP
    200, while a *DOWN* status returns HTTP 500\. Any failures or errors that are
    encountered by the health check procedure will result in an HTTP 503 return code,
    which equates to a *DOWN* status. Remember that if any health check reports are
    down, then the overall status is *DOWN*. Each health check JSON object in the
    list displays the contents of `HealthCheckReponse` (that is, its name, status,
    and optional key-value map). If there a no health check procedures, then an automatic
    *UP* (that is, HTTP 200) is returned. The formatting structure and behaviors listed
    previously apply to all four endpoints. The use of response codes is important
    as this may be the method in which the external observer is determining the health
    of your application (that is, the cloud platform).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example output can be applied to any of the four health endpoints,
    so we will not define which endpoint it is from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output reports that we have a health check procedure named `"goodCheck"`
    that reports *UP*. We also have a procedure named `"questionableCheck"` that is
    reporting *DOWN*. This causes the overall status to report *DOWN* and will result
    in an HTTP 500 error being returned. As shown in the output, the `"questionableCheck"`
    procedure has incorporated the additional contextual map data; that is, `"application":
    "backend"` and `"locale": "en"`.'
  prefs: []
  type: TYPE_NORMAL
- en: Special Note About Default Readiness and Startup Procedures
  prefs: []
  type: TYPE_NORMAL
- en: The MicroProfile Health runtime provides a configuration value (through MicroProfile
    Config) called `mp.health.default.readiness.empty.response`. Its value can be
    either *UP* or *DOWN*. The default value is *DOWN*. This value is used to report
    the readiness of your microservice if the application is still starting up and
    the readiness procedures cannot be called yet. This is not applicable if no readiness
    health check procedures have been defined in the application code. If that is
    the case, then the default behavior for no health check procedures is to return
    an HTTP 200 response with an *UP* status on the `/health/readiness` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a configuration value that exists for the startup health check
    called `mp.health.default.startup.empty.response`. If there are no startup health
    checks at all, then the default *UP* status is returned on the `/health/started`
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Liveness checks, on the other hand, do not have configurable values. They follow
    the simple rule of returning an HTTP 200 response with an *UP* status if the application
    is still starting, and the liveness checks are not ready to be invoked yet.
  prefs: []
  type: TYPE_NORMAL
- en: Other connections and payload formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Depending on the runtime you chose, it may be the case that the results of the
    health check procedures can be obtained through additional means (for example,
    TCP or JMX). We're using the word *additional* here because, at the very minimum,
    a MicroProfile Health runtime must support an HTTP/REST request. However, as a
    cloud-native technology, MicroProfile Health understands that other strategies
    for obtaining the data may be preferred. The MicroProfile Health specification
    defines a set of protocols and wire format rules for how the data can be consumed
    and presented. Health check data should be presented in JSON format whenever possible.
    But failing that, the same payload of data must be made available.
  prefs: []
  type: TYPE_NORMAL
- en: This book will not discuss the intricacies of the protocol and the wire format
    semantics defined in the MicroProfile Health specification. You can review the
    MicroProfile Health specification for such information at [https://bit.ly/3ecI6Gz](https://bit.ly/3ecI6Gz).
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile health checks with Kubernetes' liveness, readiness, and startup
    probes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now look at how the health check data that's reported by health check
    procedures can be consumed in a real-life scenario. To do this, we will be using
    Kubernetes. As this is one of the more well-known cloud container orchestration
    platforms, this will serve as an excellent vehicle of demonstration. We will be
    using Kubernetes terminology and will try our best to describe these terms in
    this section. We will explore Kubernetes and the cloud infrastructure in more
    depth in [*Chapter 7*](B17377_07_Final_SB_epub.xhtml#_idTextAnchor146), *MicroProfile
    Ecosystem with Docker, Kubernetes, and Istio*.
  prefs: []
  type: TYPE_NORMAL
- en: In a cloud environment, the containers you deploy exist on an interconnected
    web of physical or virtual machines. Kubernetes serves to seamlessly manage and
    integrate your container deployments that reside in Kubernetes' **Pods**. Pods
    can contain one or more containers. To get a pulse on the activity of the Pods
    in this network (that is, your cloud), a **kubelet** is present on each machine.
    This acts as a node agent that manages the Pods on the machine and communicates
    with the central Kubernetes management facilities. As part of its duties, it can
    determine when the containers within these Pods are stale or broken, and it has
    the power to stop and restart them if the need arises. Kubelets are also given
    the task of evaluating when a container is ready to receive traffic or not. And,
    most fundamentally, they can check if the container has completed initializing.
    They accomplish these tasks by checking the liveness, readiness, and startup statuses
    of the containers within the Pods using liveness, readiness, and startup probes.
  prefs: []
  type: TYPE_NORMAL
- en: This behavior is container-specific in that it must be enabled on a per-container
    basis. This is achieved when configuring the container in the Pod's configuration
    YAML file. The following example uses a snippet from the `broker.yaml` file, which
    configures the Broker microservice from the StockTrader application that we introduced
    in [*Chapter 3*](B17377_03_Final_SB_epub.xhtml#_idTextAnchor049), *Introducing
    the IBM Stock Trader Cloud-Native Application*, and will look at again in [*Chapter
    8*](B17377_08_Final_SB_epub.xhtml#_idTextAnchor159), *Step-by-Step Stock Trader
    Development*. The YAML file contains a Kubernetes `Deployment` definition, which
    provides the configuration needed to deploy the container(s) onto a Pod, including
    the container image to use, environment variables, and, of course, the liveness,
    readiness, and startup probes, which can be configured for each container that
    is defined. We've omitted the other parts of the file to only show the configuration
    of the liveness, readiness, and startup probes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `broker.yaml` can be found at [https://bit.ly/3sEvHAa](https://bit.ly/3sEvHAa):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The liveness, readiness, and startup endpoints are defined in the `livenessProbe`,
    `readinessProbe`, and `startupProbe` sections, respectively. The probes are configured
    to use HTTP/S with `httpGet`. In our example, we will be using an unsecured HTTP
    endpoint. You will need to add a new field under `httpGet` named `scheme` and
    set the value to `HTTPS` if you want a secure connection. We specify `/health/live`,
    `/health/ready`, and `/health/started` with the `path` field and the port to reach
    it on with `port`. An initial delay of 60 seconds is configured with the `initialDelaySeconds`
    field for the readiness probe. This prevents the readiness probe from firing until
    this time has elapsed, to allow the container and its application to start up.
    When the probes are firing, requests will be sent every 15 seconds for the readiness
    and liveness probes and 30 seconds for the startup probe, as configured through
    `periodSeconds`. However, what's not defined in this example is the `timeoutSeconds`
    field. By default, the value is 1 second and it defines the period in which the
    kubelet should wait before it times out. `failureThreshold` defines how many times
    the probes will retry before it is considered a failure.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that the liveness probe does not specify the `initialDelaySeconds`
    field. You can do so, but this is unnecessary as we are using `startUpProbe`.
    Remember that (if defined) the startup probe will be queried first until it provides
    the *UP* status, and then the liveness probe will be checked. This is the behavior
    that Kubernetes provides.
  prefs: []
  type: TYPE_NORMAL
- en: If any of the probes fail completely, as in all attempts have transpired, then
    the container is subject to being restarted.
  prefs: []
  type: TYPE_NORMAL
- en: It should be of no surprise now that the MicroProfile Health technology had
    the Kubernetes platform in mind with its availability of liveness, readiness,
    and startup endpoints, all of which match the specific Kubernetes liveness, readiness,
    and startup probes. However, the existence of the simple `/health` endpoint allows
    it to be used by other platforms that only care about a single health endpoint.
    But remember that when using the `/health` endpoint, the concept of liveness,
    readiness, and startup maybe no longer be applicable. On top of that, MicroProfile
    Health's straightforward protocol and wire format rules allow its health check
    data to be easily consumed by any external observer, sentient or not.
  prefs: []
  type: TYPE_NORMAL
- en: We've now come to the end of the MicroProfile Health section. As we mentioned
    earlier, in the introduction to MicroProfile Health, this technology serves to
    satisfy a broad monitoring scope. In the next section, we'll start to cover a
    more detailed monitoring scope with MicroProfile Metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting metrics on your cloud-native application using MicroProfile Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the second part of our MicroProfile observability trilogy, where we
    find ourselves in the thick of things with MicroProfile Metrics. The previous
    technology we discussed – MicroProfile Health – strived to report on the overall
    health of your cloud-native application by allowing you, the developer, to strategically
    place health checks throughout your application. MicroProfile Metrics, on the
    other hand, strives to report on the performance and inner workings of the application
    and its environment using metrics instrumented in the application by you, as well
    as metrics provided by the MicroProfile Metrics runtime. This provides real-time
    statistics that can be recorded and/or aggregated to be analyzed with specialized
    monitoring tools. To accomplish this, the MicroProfile Metrics technology comes
    well-equipped with seven types of metrics that range in complexity and functionality.
    As we progress through this section of this chapter, we will come to know them
    very well.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of MicroProfile Metrics in a cloud-native application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Being able to monitor the statistics and performance data of specific components
    throughout your application is not a cloud-native, development-specific idea.
    This should be a healthy practice, regardless of whether your endeavors are on
    the cloud. However, being able to monitor your microservices is critical when
    we're talking about a topology of highly scalable and diverse applications. Even
    if you don't command a sprawl of applications, but rather a few applications,
    the benefits of gathering metrics are indisputably invaluable. This is a way for
    your microservice to talk to you and tell you how it's feeling. This provides
    you with the opportunity to identify any patterns of concern before your application's
    liveness health checks unexpectedly decrees that it is *DOWN*. For example, in
    the previous section, we demonstrated a scenario where the liveness health check
    procedure was dependent on how much memory was being used. After it surpassed
    a certain threshold, it would fail and report *DOWN*. By just using MicroProfile
    Health, we wouldn't know anything was wrong until it was too late and by then,
    your cloud platform would have already restarted the container. And perhaps you
    may be blissfully unaware that anything has happened at all.
  prefs: []
  type: TYPE_NORMAL
- en: Having MicroProfile Metrics report on such statistics allows you to anticipate
    such disasters ahead of time and to understand the performance of your application.
    As another example, we can have metrics reporting on the number of requests that
    have been made to the REST endpoints in your microservice and how long it took,
    on average, for the requests to be fulfilled. This metric information can shed
    light on how popular your microservice is and how well – or how poorly – your
    microservice is performing. This can then prompt the necessary steps for revising
    and modifying the deployment environment or even the application itself.
  prefs: []
  type: TYPE_NORMAL
- en: However, MicroProfile Metrics can only report on the instantaneous value of
    the metrics. To properly harness this stream of information, we need to aggregate
    the metric data over time and, in effect, transform it into a **time series metric**.
    Neither MicroProfile Metrics itself, nor any other MicroProfile technology, serves
    to accomplish this task. MicroProfile Metrics is only here to provide a seamless
    and effective way of instrumenting metrics into your microservices. There is already
    an established ecosystem of tools and platforms that specializes in aggregating
    metrics and visualizing them. A popular monitoring stack to use is one that utilizes
    **Prometheus** and **Grafana**.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus is an open source monitoring solution for gathering, storing, and
    querying time series metrics. Prometheus is often combined with the use of another
    tool, called Grafana. Grafana is another open source monitoring solution that
    serves to display and visualize time series metrics through graphs, tables, and
    other types of visualizations by using customized queries that have been made
    to the time series database (for example, Prometheus). This can provide you or
    your operations team with the ability to monitor the performance of your microservices
    through meaningful visualizations in a human-friendly way.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this section, we will demonstrate how to use Grafana to visualize
    the metric data that's been gathered by the MicroProfile Metrics runtime. Being
    able to strategically instrument metrics to provide meaningful information is
    half the battle; effectively using that information is how the battle is won.
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile Metrics technology overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may have noticed that, in the introduction to this section, we mentioned
    that metrics can come from the application or the runtime itself. Just like MicroProfile
    Health, where default health checks may be provided, the MicroProfile Metrics
    runtime can provide default out-of-the-box metrics as well. The runtime must provide,
    for the most part, a certain set of metrics on top of any optional metrics that
    it wishes to provide. These metrics are referred to as **base metrics** and **vendor
    metrics**, respectively. However, not all base metrics are strictly required,
    and we will explain this shortly. The metrics that are instrumented in the application
    by the developer are referred to as **application metrics**. All these different
    sets of metrics live separately, agnostic of each other, under different **metric
    registries**. The metric registry is the control center and the heart of the MicroProfile
    Metrics technology. The metric registry is where metrics are registered, stored,
    retrieved, and deleted. This logical grouping of the different types of metrics
    into their own unique metric registries simplifies handling different scopes of
    metrics and, most importantly, avoids any metric name collisions that could occur
    if they resided together in one single metric registry.
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve the metric data, the MicroProfile Metrics runtime provides four
    HTTP/REST endpoints. The first one is a general `http://host:port/metrics` endpoint,
    which displays all the metrics from all the scopes and registries. Metrics are
    prefixed with the name of their respective metric registries to avoid confusion.
    The other three endpoints are sub-resources of the `http://host:port/metrics`
    endpoint, and they report on the metrics in each specific registry. They are the
    `http://host:port/metrics/base`, `http://host:port/metrics/vendor`, and `http://host:hort/metrics/application`
    HTTP/REST endpoints. Metrics can be reported in either JSON or Prometheus exposition
    format. We will cover these two formats in detail later. Moving forward, when
    referring to the metrics endpoints, we will omit `http://host:port` for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the following diagram illustrates the general flow of the metric
    life cycle. First, metrics are instrumented into your microservice (or provided
    by the runtime!). These metrics are reported on the `/metrics` endpoint. Some
    monitoring tool or platform (for example, Prometheus) is then used to retrieve
    the metric data and store it, thus transforming it into time series metrics. Another
    monitoring tool or platform (for example, Grafana) is then used to visualize that
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Life cycle of a metric'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17377_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – Life cycle of a metric
  prefs: []
  type: TYPE_NORMAL
- en: We will now describe the three different metric scopes in more detail; that
    is, **base metrics**, **vendor metrics**, and **application metrics**.
  prefs: []
  type: TYPE_NORMAL
- en: Base metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Base metrics are a set of metrics that must be provided by all MicroProfile
    Metrics runtimes. There are, however, a few exceptions, where the metrics can
    be optionally implemented instead. This slight variability is due to what base
    metrics were meant to achieve. The list of base metrics was created in the hopes
    of capturing and reporting metrics that each runtime would have. Having base metrics
    defined and implemented by the runtime relieves the developer from the burden
    of having to instrument their own metrics to capture basic and/or common statistics.
    By providing these base metrics, they would always be available, regardless of
    whether they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: The obvious target for base metrics would be to encompass the **Java Virtual
    Machine** (**JVM**) statistics. Base metrics cover a long list of metrics that
    target memory statistics, garbage collection statistics, thread usage, thread
    pool statistics, class loading statistics, as well as operating system statistics.
    However, not every JVM is made equal, and a few of these metrics are optional
    as the JVM under the hood may not hold such statistics. Base metrics also include
    optional REST metrics that track the request count, unmapped exceptions count,
    and time spent on each REST/JAX-RS endpoint. We encourage you to review the list
    of base metrics and their definitions by looking at the MicroProfile Metrics specification
    at [https://bit.ly/3mXpL42](https://bit.ly/3mXpL42).
  prefs: []
  type: TYPE_NORMAL
- en: The MicroProfile Metrics specification only defines the aforementioned JVM and
    REST metrics explicitly as base metrics, but the metrics generated by MicroProfile
    Fault Tolerance are classified as base metrics as well. We covered the MicroProfile
    Fault Tolerance metrics in the *Fault Tolerance metrics* section of the previous
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Vendor metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vendor metrics are metrics that are provided by the vendor for their implementation
    of MicroProfile Metrics. Different implementations of MicroProfile Metrics will
    contain different sets of vendor metrics. Vendor metrics are completely optional,
    and it can be the case that your chosen MicroProfile Metrics runtime does not
    supply any vendor metrics at all. The purpose of vendor metrics is to allow the
    vendor's implementation to provide any metrics that can enhance the monitoring
    capabilities of the end user for the specific MicroProfile Metrics runtime. For
    example, if the runtime you are using is also Jakarta EE compliant, then it may
    be possible for it to provide metrics related to components under that platform.
    The vendor metrics can then be exclusively accessed on the `/metrics/vendor` endpoint
    or combined with metrics from other scopes on the `/metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Application metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Application metrics are metrics that have been instrumented by you, the developer,
    in your application. These metrics report on statistics that interest you and
    your team for observing and monitoring the performance of the application. This
    is the metrics scope that you will be primarily interacting with when instrumenting
    metrics. The application metrics can then be exclusively accessed on the `/metrics/application`
    endpoint or combined with metrics from other scopes on the `/metrics` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The seven metric types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we understand the different scopes of the available metrics, we can
    list the seven types of application metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Counter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gauge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concurrent Gauge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple Timer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the names, it's easy to deduce what the different types of metrics serve
    to achieve. If not, don't worry – we'll cover these metrics in detail when we
    cover how to instrument the different metrics later, in the *Instrumenting metrics*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The metric model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know what type of metrics there are, and under what scopes they
    may live, it's time for us to understand the underlying metric model. This may
    sound like a dull topic, and you may be tempted to skip this, but understanding
    this is crucial if you wish to know how to instrument and handle metrics effectively.
  prefs: []
  type: TYPE_NORMAL
- en: A metric, besides being one of the seven metric types, consists of a name, a
    set of optional key-value `/metrics/*` endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the name is rather obvious: it is to uniquely identify the metric
    from others. However, that may not be sufficient in some cases because different
    metrics can share the same name. This is because MicroProfile Metrics supports
    **multi-dimensional metrics** with key-value tags.'
  prefs: []
  type: TYPE_NORMAL
- en: The combination of the metric's name and its tags is encapsulated in a `MetricID`
    object in the metric registry. `MetricID` is the primary identifier of a metric.
    It is tightly coupled with the metric instance itself in the metric registry.
    The use of tags for a metric is optional, and it can be the case that the metrics
    in your application all use distinct metric names with no tags. This results in
    a `MetricID` with just a name and no tags. However, you may find it useful to
    leverage the power of multi-dimensional metrics, if the need arises. Such a need
    may come if you are attempting to record the same type of data (for example, a
    counter to count something) from multiple similar sources. You can use the same
    metric name and provide a tag that uniquely identifies it from the other sources.
    An example of this would be if you are using metrics to count how many times the
    methods in a specific class are being invoked. You can name the metrics `classMethodInvocations`
    and provide each method with a tag, where the key is `method` and the value is
    the name of the method.
  prefs: []
  type: TYPE_NORMAL
- en: This use of multi-dimensional metrics is best taken advantage of when using
    one of the available visualization monitoring tools, such as Grafana. You can
    quickly retrieve and display all metrics with the same name, regardless of what
    their tags are, in one simple query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last item that is used to identify a metric is its metadata. The metadata
    consists of the metric''s name, its type, the metric''s unit of measurement (if
    applicable), an optional description, and an optional human-readable display name.
    For each unique metric name, there is only one piece of metadata. As a result,
    there can be multiple MetricID linked to one piece of metadata. Being able to
    reuse the metric''s name in the metadata helps correlate the MetricIDs and the
    metadata as they are loosely coupled in the metric registry. The relationships
    described previously are illustrated in the following diagram. ***** denotes 0
    to many:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Metric registry metric model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17377_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Metric registry metric model
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving metric data
  prefs: []
  type: TYPE_NORMAL
- en: Before we continue with the topic of instrumenting metrics, we will cover how
    metrics are made available. In the *Instrumenting metrics* section, we will be
    covering each metric individually and providing examples of its output, specifically
    its Prometheus output. Therefore, first, we must understand what we will be looking
    at.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, metrics are available through HTTP/REST requests
    to either the `/metrics`, `/metrics/base`, `/metrics/vendor`, or `/metrics/application`
    endpoint in either JSON or Prometheus exposition format. The metric output for
    a specific metric name can be retrieved by sending a request to `/metrics/<scope>/<metric_name>`.
  prefs: []
  type: TYPE_NORMAL
- en: JSON format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of the metrics in JSON format comes in two parts. We can obtain the
    metric and its data by invoking a `GET` request with the `Accept` header by specifying
    `application/json`. If we issue an `OPTION` request instead, we will be able to
    retrieve the metadata associated with the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at what sending a `GET` request to `/metrics` will return. Notice
    that the metrics from the different scopes are in their own JSON array lists.
    We will only show the base metrics and hide any vendor or application metrics
    in the sample output. We will also look at an example of multi-dimensional metrics
    by using the first two metrics listed in the base scope. There are two `gc.total`
    metrics whose key-value pairs are `"name=scavenge"` and `"name=global"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'To find out what the `gc.total` metric was meant for, we can obtain the metadata
    of the metrics by sending an `OPTIONS` request to `/metrics`. Since the output
    of this request will be lengthy, we will only show the `gc.total` metric and obfuscate
    the rest. Like the `GET` request, the metrics from each scope/registry are separated
    into their own JSON arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the metadata, the `gc.total` metric is a counter that'sused
    to count the number of garbage collections that have occurred in this JVM. The
    tags are used to identify two different garbage collectors on the system that
    the two metrics monitor.
  prefs: []
  type: TYPE_NORMAL
- en: A request was made to `/metrics` to demonstrate how the metrics are partitioned
    from different scopes. We could have also invoked `/metrics/base/gc.total` to
    specifically retrieve the metadata of the `gc.total` metric.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus exposition format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the Prometheus exposition format, all metric data is provided together
    with a `GET` request to the `/metrics/*` endpoints. If `application/json` is not
    specified, the Prometheus format will be returned by default. As its name suggests,
    this format to be used directly by the Prometheus monitoring tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'A specific template must be followed for formatting the metrics. To describe
    this, we''ll only take a look at the output from the `gc.total` metrics. We''re
    only using a snippet here as the full output would be too lengthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In Prometheus exposition format, metrics are organized by their metric name.
    The first grouping is for the `base_gc_total` metric. This corresponds to the
    `gc.total` metric we saw in the aforementioned JSON format examples. The true
    metric name is `gc.total`, but it must be transformed into `gc_total` as Prometheus
    formatted metrics are alphanumeric characters with underscores (`_`). The MicroProfile
    Metrics runtime also prepends the name of the registry scope that the metric belongs
    to. This is either `base_`, `vendor_`, or `application_`. Notice that the tags
    are appended to the end of the metric's name, within squiggly brackets.
  prefs: []
  type: TYPE_NORMAL
- en: Each unique grouping of metrics by metric name is preceded by a `# TYPE` line
    and a `# HELP` line. These two lines define the metric's type and a description,
    if available. Remember that the description is an optional field in the metric's
    metadata.
  prefs: []
  type: TYPE_NORMAL
- en: There are additional formatting rules for certain metrics. We will cover this
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MicroProfile Metrics technology provides a rich Java API for programmatically
    instrumenting metrics, as well as providing CDI annotations for easily instrumenting
    metrics for methods, fields, and even entire classes. We will not cover all the
    possible scenarios of using the Java API and its annotations, particularly regarding
    the usage of the `MetricRegistry` class. Instead, this section will explain the
    main uses of the API and its annotations to allow you to understand how to use
    the technology with confidence. We encourage you to review the Java documentation
    for MicroProfile Metrics if you wish to completely master everything.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll cover how to instrument each metric programmatically
    and with annotations. This will be followed by an example of the output of the
    `/metrics` endpoint in Prometheus exposition format. Preceding that, we will cover
    the technical aspects of the metric registry, metadata, tags, and `MetricID`.
    They provide the fundamental knowledge needed to effectively instrument metrics.
  prefs: []
  type: TYPE_NORMAL
- en: As you may recall from the *MicroProfile Metrics technology overview* section,
    the metric registry is the nexus of operation for the MicroProfile Metrics runtime.
    Unless you are strictly using annotations to instrument metrics in your microservice,
    you will need to obtain a `MetricRegistry` (CDI) bean. It is through this `MetricRegistry`
    that we can create, register, and retrieve metrics programmatically. Even if you
    are strictly using annotations to instrument metrics, you will be interacting
    with `MetricRegistry` under the covers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section contains a large amount of content. The following is a summary
    of what we will be covering:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a metric registry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creating, registering, and retrieving metrics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Metatadata, tags, and MetricIDs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Counter
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) Concurrent gauge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) Meter
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: e) Timer and simple timer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f) Gauge
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `@Metric` annotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining a metric registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To obtain `MetricRegistry`, we can use injection, as demonstrated in the following
    code sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that there are three types of metric registry scopes: the base metric
    registry, the vendor metric registry, and the application metric registry. By
    default, when you inject a `MetricRegistry` into your application, the MicroProfile
    Metrics runtime will provide an application registry. You can inject the other
    types of registries if you wish. You will need to annotate your injection with
    a `@RegistryType` and with an annotation parameter specifying the type of registry
    to inject. The following example illustrates the usage of `@RegistryType`, where
    we specify the type as `MetricRegistry.Type.Application`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can inject a base metric registry and a vendor metric registry if you specify
    a `@RegistryType(type=MetricRegistry.Type.BASE)` or `@RegistryType(type=MetricRegistry.Type.VENDOR)`
    annotation, respectively. However, in your application, you should NOT be registering
    metrics or manipulating the base or vendor metrics. These two metric registries
    should only be used to retrieve the metrics so that you can view their data.
  prefs: []
  type: TYPE_NORMAL
- en: Note About `MetricRegistry` and Annotation Usage
  prefs: []
  type: TYPE_NORMAL
- en: When using annotations to instrument metrics, you will only be interacting with
    the application metric registry. You will not be able to choose which `MetricRegistry`
    the metric annotations apply to.
  prefs: []
  type: TYPE_NORMAL
- en: Creating, registering, and retrieving metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using `MetricRegistry`, you can create and register metrics using specific
    methods for each metric type. Each metric type, except for Gauge, will have the
    following method signatures. Invoking such methods from `MetricRegistry` will
    create, register, and return an instance of that metric if a metric with the given
    name, metadata, and tags does not already exist in the registry. If one does exist,
    then that existing metric is returned. It should be noted that using metric annotations,
    except for the gauge annotation, works similarly. We will demonstrate the method''s
    signature pattern with the `Counter` metric type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The method names for the other metric types are `concurrentGauge`, `timer`,
    `simpleTimer`, `histogram`, and `meter`. We will demonstrate the various usages
    of these methods in the metric-specific sections. Gauge also has its own set of
    methods that `MetricRegistry` provides, but we will cover those in the *Gauge*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Note on Metric Reusability
  prefs: []
  type: TYPE_NORMAL
- en: Whether you're using `MetricRegistry` or the metric annotations to instrument
    your metrics, you can reuse an existing metric by specifying matching metadata
    or `MetricID` values.
  prefs: []
  type: TYPE_NORMAL
- en: To only retrieve metrics, you can call one of the `getMetrics()`, `getCounters()`,
    `getGauges()`, `getConcurrentGauges()`, `getHistograms()`, `getMeters()`, `getTimers()`,
    or `getSimpleTimers()` methods from `MetricRegistry`. These calls will return
    a map of the desired metrics, with `MetricID` as the key.
  prefs: []
  type: TYPE_NORMAL
- en: There are other methods for creating, registering, retrieving, and deleting
    metrics from the metric registry, some of which use a `MetricFilter`, as well
    as other methods concerning retrieving metadata and metric IDs. You can even create
    your own implementation of the metrics and register that over the instances provided
    by the MicroProfile Metrics runtime. However, these methods will not be covered
    as there a simply too many! We encourage you to review the Java documentation
    of the `MetricRegistry` class. The information we've provided so far regarding
    the use of `MetricRegistry` s is to help you understand the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata, tags, and MetricIDs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you may have noticed in the previous section, metadata, tags, and MetricIDs
    can and will be used in your application code by the metric registry. However,
    we must understand how to create and use them before we can learn how to instrument
    and utilize the different metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Every metric must contain metadata information. As you may recall, metadata
    information consists of its name, metric type, unit of measurement, a description,
    and a display name. The required fields from this set are the name and the metric
    type. The other metadata fields are optional. All this information is encompassed
    in a `Metadata` object. Every field in the `Metadata` object is a `String`. For
    the metric type field, you will need to specify an `enum` value from a `MetricType`
    `enum`. For the unit field, you will need to specify one of the static fields
    in `MetricUnits`.
  prefs: []
  type: TYPE_NORMAL
- en: If you are instrumenting multi-dimensional metrics, then you will also need
    to provide tags for your metrics. Every tag is a key-value pair of `String` values
    and is represented by a `Tag` object. The tag's name must match the `[a-zA-Z_][a-zA-Z0-9_]*`
    regex. The value of the tag can be anything. A metric can contain 0 or more tags.
    This `Tag` is then set into a `MetricID` that also contains the `String` name
    of the metric.
  prefs: []
  type: TYPE_NORMAL
- en: Note About Configurable Tags
  prefs: []
  type: TYPE_NORMAL
- en: Using MicroProfile Config, we can define two config values for setting tag values
    to all the metrics in the MicroProfile Metrics runtime. `mp.metrics.appName` takes
    a single string value that is used to identify the application's name. This will
    be appended to all the metrics as a key-value tag in the form of `_app=<application_name>.`
    The `mp.metrics.tags` config allows a comma-separated list of key-value tags to
    be defined in the form of `tag1=value1,tag2=value2`. These tags will then be applied
    to all metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Using metadata and tags programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When instrumenting metrics programmatically, we need to create a `Metadata`
    object. To accomplish this, we need to retrieve `MetadataBuilder` by invoking
    the static `Metadata.builder()` method. Using this `MetadataBuilder`, we can construct
    a `Metadata` object using a builder pattern. At the very minimum, we will want
    to specify its name and metric type. In the following example, we won''t be registering
    any metrics, so we will use the `MetricType.INVALID` metric type. When we demonstrate
    how to instrument each of the individual metrics in the rest of this section,
    we will use the appropriate `MetricType` there:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To create a `Metadata` object with all the fields specified, you can do the
    following. Once again, as this example is for the sake of demonstration, we will
    use the `MetricUnits.NONE` value. Since the upcoming sections will not be using
    the unit field heavily, we encourage you to explore the available unit values
    available by reviewing the source file at [https://bit.ly/3ds4IDK](https://bit.ly/3ds4IDK).
    The following example also includes the use of tags and a `MetricID`. Creating
    a `Tag` is a simple process in that you invoke the `Tag` constructor with the
    `String` name and value parameters. You can then construct a `MetricID` by passing
    the metric name and a variable-length amount of `Tag` parameters into the `MetricID`
    constructor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `MetricsResource` can be found at [https://bit.ly/2UzoczI](https://bit.ly/2UzoczI):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using a combination of MetricIDs, tags, and metadata, you can create, register,
    and retrieve metrics from `MetricRegistry`. As you may recall from the previous
    section, which listed the different method signatures, `MetricID` and `Metadata`
    are never used together as arguments. However, we know that the metric registry
    uses both to classify and identify the registered metrics. This is because the
    metric registry will be able to infer the minimum necessary data to construct
    the other object, whether it be `MetricID` or metadata, during processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using metadata and tags with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When instrumenting metrics with annotations, the metadata and tags are provided
    through the annotation parameters. It is possible to not have to specify any parameters
    at all. The MicroProfile Metrics runtime, when using CDI, can infer the necessary
    information. This type of annotation already provides a metric type and if no
    name is provided, then a name is generated using the package name, class name,
    and method name. Alternatively, in a situation where the annotation is used on
    a constructor, it will be a combination of the package name, class name, and constructor
    name (that is, the class name again!).
  prefs: []
  type: TYPE_NORMAL
- en: Even if a name is supplied, the full metric name is a combination of the class
    name and the metric name. However, this may prove undesirable. To address this,
    each metric annotation parameter contains an `absolute` parameter, which you can
    set to `true` so that the metric uses the provided metric name.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how to provide the metadata information with annotations, the
    following code snippet will use the Counter class'' `@Counted` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, `name`, `displayName`, and `description` parameters exist that
    accept `String` values. The `absolute` parameter accepts a `Boolean` value. The
    unit accepts a static field from `MetricUnits`, and tags are accepted as a list
    of `String` values in *key-value* format.
  prefs: []
  type: TYPE_NORMAL
- en: Counter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We''ve finally arrived at our first metric: the counter. The counter metric,
    as its name suggests, is a metric that keeps count of the metrics. The counter
    can only monotonically increase. You can use this to keep track of how many times
    a method or block of business logic has been invoked, or the number of times a
    request was received or sent.'
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting counters programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following code sample demonstrates how to create and retrieve a counter
    metric named `counterMetric` using two `GET` requests. In the first `GET` resource,
    the `/counter1` URI, we create `counterMetric` by invoking `MetricRegistry.counter(Metadata
    metadata, Tags… tags)`. This will return a new counter metric that we can increment
    by calling `counter.inc()`, which increments the counter by 1\. In the second
    `GET` resource, the `/counter2` URI, we do something different and call `MetricRegistry.counter(MetricID
    metricID)`. Here, `MetricID` matches the `MetricID` property that was generated
    by the metric registry when we first created and registered `counterMetric`. Since
    it already exists, we are returned the existing `counterMetric` using the metric
    registry. We then increment it by calling the `inc(long value`) method to increment
    the counter by a specified amount. In our example, we increment it by 3\. In both
    `GET` resources, we return a string that includes the current count of the counter
    by invoking `getCount()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `CounterResource` can be found at [https://bit.ly/2XGDDXZ](https://bit.ly/2XGDDXZ):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see what happens when we send requests to both `GET` resources
    and then view the results through `/metrics/application/counterMetric` directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the output, we issue `GET` requests to the `/ch6/counterResource/counter1`
    and `/ch6/counterResource/counter2` endpoints and the counter metric is incremented
    by 1 and 3, respectively. We then issue a `GET` request to `/metrics/application/counterMetric`
    to view the Prometheus formatted output of our counter metric directly. `application_counterMetric_total{metricType="counter"}`
    is returned, which represents the counter metric with its tag of `metricType="counter"`.
    It holds a value of 4, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Note About Prometheus Formatting with Counters
  prefs: []
  type: TYPE_NORMAL
- en: Counter metrics in Prometheus exposition format will have the `_total` suffix
    appended to the metric name.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting counters with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using annotations is a much simpler affair. You can annotate the `@Counted`
    annotation on either a method, constructor, or even a whole class. When the annotated
    element is invoked, the counter is incremented by 1.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we'll annotate the `MetricsResource` class with `@Counted`.
    When a metric annotation is annotated on a class, it will apply to all applicable
    targets in the class for that annotation. For `@Counted`, this means that all
    the constructors and methods will be instrumented. This example will also demonstrate
    the metric names that are generated. Note that since we are using annotations,
    we do not need to inject `MetricRegistry`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `CounterAnnotatedResource` can be found at [https://bit.ly/3iZiL6D](https://bit.ly/3iZiL6D):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take the application for a drive. We''ll omit showing the `curl` command
    to the application''s REST endpoint and just show the output of querying `/metrics/application`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: After issuing a single `GET` request to `/ch6/counterResource/getResource`,
    we should see the aforementioned values when viewing the metric data on the `/metrics/application`
    endpoint. `application_metrics_demo_CounterAnnotatedResource_getResource_total`
    is the counter metric that was created for the `getResource()` method, while `application_metrics_demo_CounterAnnotatedResource_CounterAnnotatedResource_total`
    is the counter metric that was created for the constructor of the class. Both
    values are *1*, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent gauge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concurrent gauge metric is a metric that's used to count the parallel invocation
    of the instrumented component. Its values can increase or decrease. This metric
    can be used to count the number of parallel invocations of a method, business
    logic, requests, and more. Besides counting parallel invocations, the concurrent
    gauge metric also keeps track of the highest and lowest count that's been recorded
    within the previously **completed full minute**. A completed full minute denotes
    the period from 0:00:00.9999999 to 0:00:59.99999999 on the clock. A completed
    full minute does not mean the last 60 seconds from the current instantaneous time.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting concurrent gauges programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we'll demonstrate how to use a concurrent gauge. They are typically
    invoked in parallel using a `Runnable` named `sleeper`. This creates – and subsequently
    retrieves – a concurrent gauge named `concurrentGaugeMetric`. In this example,
    we will use `MetricRegistry.concurrentGauge(String name)` in our interaction with
    the metric registry. This is the simplest creation or retrieval method provided
    by the metric registry as you only need to provide the name. This infers that
    there are no tags associated with this metric. The sleeper `Runnable` will then
    increment the concurrent gauge (for example, with `inc()`), sleep for 10 seconds,
    and then decrement it (for example, with `dec()`). You can only increment or decrement
    by 1\. We'll make parallel invocations using a `for` loop and an `ExecutorService`.
    However, what's not shown in this code example are the getter methods for the
    three values; that is, `getCount()`, `getMin()`, and `getMax()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `ConcurrentGaugeResource` can be found at [https://bit.ly/3ghFyZz](https://bit.ly/3ghFyZz):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we will send a `GET` request called `/ch6/concurrentGaugeResource/concurrentGauge`.
    Once the current minute has completed, we will view the output via `/metrics/application`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we issued a `GET` request to `/ch6/concurentGaugeResource/concurrentGauge`.
    We then followed up with a `GET` request, `/metrics/application`, to view the
    output. `application_concurrentGaugeMetric_current` shows the current value, which
    is `10`, as expected. `application_concurrentGaugeMetric_max` and `application_concurrentGaugeMetric_min`,
    which show the maximum and minimum recorded values of the previous full minute,
    are `0`, as expected. After the current full minute has completed, we view the
    results again and we see that the current, max, and min values are `0`, `0`, and
    `10`, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Note About Metrics with Multiple Values
  prefs: []
  type: TYPE_NORMAL
- en: The concurrent gauge is our first metric with multiple output values. To display
    all the values with the same metric name, each value of the metric is given its
    own suffix. We will see this pattern in other complex metrics later.
  prefs: []
  type: TYPE_NORMAL
- en: Immediately after our `GET` request to `/ch6/concurrentGaugeResource/concurrentGaugeParallel`,
    we will see that the current count for the concurrent gauge is `10`. When the
    10 seconds have elapsed for each thread and a full minute has passed, we will
    see that the current value is `0` and that the maximum value is `10`.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting concurrent gauges with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To instrument a concurrent gauge with annotations, you must use the `@ConcurrentGauge`
    annotation. This applies to methods, constructors, and classes.The concurrent
    gauge annotation will increment when the target is invoked and decrement when
    it is finished.
  prefs: []
  type: TYPE_NORMAL
- en: We'll demonstrate the usage of `@ConcurrentGauge` in a similar fashion to the
    programmatic example. The `sleeper` runnable will invoke the `sleeper()` method,
    which is annotated with the `@ConcurrentGauge` annotation. In this example, we
    will specify `absolute=true`, which will then cause the MicroProfile Metrics runtime
    to use the metric's name. The `/metrics/*` output will be the same as it was for
    the programmatic example, so it will not be shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `ConcurrentGaugeAnnotatedResource` can be found at
    [https://bit.ly/3xZZhD0](https://bit.ly/3xZZhD0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Histogram
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The histogram metric, like a histogram graph, processes the data it has been
    provided with in a statistical distribution. A histogram metric outputs 12 values:
    count, sum, minimum value, maximum value, mean, standard deviation, and the 50th,
    75th, 95th, 98th, 99th, and 99.9th percentiles. Unlike the other metrics, the
    histogram metric can only be instrumented programmatically. There is no annotation
    support. You might use a histogram metric to record and calculate the distribution
    of the sizes of data that your application receives for processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our demonstration, we''ll generate 1,000 random numbers within the range
    *0-999* and feed them into our histogram. This time, we will use `metricRegistry.histogram(Metadata
    metadata)` to create our histogram. We won''t be showing the `getCount()`, `getSum()`,
    and `getSnapshot()` getter methods here, which return `Snapshot` objects that
    contain the getter methods for the remaining statistical values. As this would
    be too lengthy to list, you can view the `Snapshot` class and its methods here:
    [https://bit.ly/2QndNFf](https://bit.ly/2QndNFf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `HistogramResource` can be found at [https://bit.ly/3y4AoWK](https://bit.ly/3y4AoWK):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what results we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we issued a `GET` request to `/ch6/histogramResource/histogram`
    and we followed up with a `GET` request to `/metrics/application` to view the
    results. As expected, the count is 1,000, as reported by the `application_histogramMetric_seconds_count`
    value. The remaining metric values are the calculated values. As there are a large
    number of values, we will not be explicitly covering all of them. The names provided
    for the metric values are self-explanatory to indicate what values they represent.
  prefs: []
  type: TYPE_NORMAL
- en: Note About Prometheus Formatting with Histograms
  prefs: []
  type: TYPE_NORMAL
- en: If a unit has been defined, the metric name is appended to the unit as `_<unit>`.
    Prometheus only accepts certain **base units**, so the MicroProfile Metrics runtime
    will scale the value to the appropriate base unit. For example, if milliseconds
    were specified as the unit, the values will be scaled to a base unit of seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Also, notice that the quantile metric values share the same name but use tags
    to identify which percentile it is representing.
  prefs: []
  type: TYPE_NORMAL
- en: Meter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The meter metric, like the histogram metric, aggregates input values and performs
    calculations to produce results. Instead of statistical distributions, the meter
    calculates rates in units per second. The unit that's specified for the metric
    will be ignored. This only applies to the Prometheus output. The meter outputs
    the mean rate and the 1, 5, and 15-minute exponentially weighted moving average
    rates. Meter can be useful for monitoring the traffic on a specific method or
    component in your microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting meters programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our example, we''ll demonstrate using the meter metric to monitor the rate
    of requests to two `GET` resources: `/meter` and `/meter2`. With the first `GET`
    resource, we will use the last variant of the register/retrieve methods that we
    have yet to use with `MetricRegistry.meter(String metricName, Tags… tags)`. Once
    the metric has been created or retrieved, we will invoke the `mark()` method,
    which increases the meter''s recorded hits by 1\. With the second `GET` resource,
    we can pass a long parameter value so that we can invoke `mark(long value)`, which
    increments the number of hits to the meter by the specified value. Notice that
    we use `MetricID` in the `/meter2` `GET` resource to retrieve the metric we created
    and registered in the `/meter` resource.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `MeterResource` can be found at [https://bit.ly/3ASCV8j](https://bit.ly/3ASCV8j):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Not shown are the getter methods for the values; that is, `getCount()`, `getMeanRate()`,
    `getOneMinuteRate()`, `getFiveMinuteRate()`, and `getFifteenMinuteRate()`. Let''s
    run through hitting both `GET` resources and view the result at `/metrics/application`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we issued a `GET` request to `/ch6/meterResource/`
    meter, which increments the meter by 1, followed by a `GET` request to `/ch6/meterResource/meter2`,
    supplying it with a parameter value to increment the meter by 3\. We then viewed
    the resulting output in `/metrics/application`. `application_histogramMetric_total`
    shows that the count is 4, as expected, and that the remaining values are the
    calculated values. Once again, the names associated with the remaining metric
    values are self-explanatory and will not be explicitly explained.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting meters with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To instrument a meter metric with annotations, you must use the `@Metered` annotation.
    This annotation applies to methods, constructors, and classes. Like other annotated
    metrics, only a single value is incremented by using the annotation. We'll demonstrate
    a sample that uses the `@Metered` annotation and omit showing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `MeterAnnotatedResource` can be found at [https://bit.ly/3mhnHpk](https://bit.ly/3mhnHpk):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Timer and simple timer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since both the timer and simple timer metrics are very similar, we will demonstrate
    how to use both metrics together.
  prefs: []
  type: TYPE_NORMAL
- en: The **timer** metric, as its name implies, records the time spent going through
    the instrumented component. At its core, it keeps track of the total elapsed time.
    Additionally, it provides the throughput/rate from the hits recorded, as well
    as the statistical distribution of the recorded times. These outputted values
    are the same as they are for the histogram and meter metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The **simple timer** metric, on the other hand, is a timer but with the extra
    bells and whistles stripped off. It only reports on the count, total elapsed time,
    and, like the concurrent gauge, the highest and lowest recorded time of the previous
    complete full minute. If you don't require all the extra values that the timer
    provides, or intend to calculate them yourself later, the simple timer should
    be your metric of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting timers and simple timers programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In our example, we'll instrument a timer and a simple timer in their own `GET`
    resources. In both resources, we will provide an example of how to record time
    using the `Context` object. This allows us to explicitly mark the beginning and
    end of what we want to time by calling the `time()` method from either the timer
    or simple timer, to start timing, and then calling the `Context` object's `close()`
    method to stop timing. Note that the `Context` object is an inner interface of
    both the `Timer` and `SimpleTimer` classes, and that you will need to use the
    appropriate `Context` object. Both the timer and simple timer metrics can time
    the execution of a `Runnable` or `Callable` object or lambda expression. The following
    two code snippets are from the same `TimersResource` class, and the full source
    code can be found at [https://bit.ly/37YaWYy](https://bit.ly/37YaWYy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the `GET` resource known as `/timer`, which
    demonstrates timing with a `Runnable` object with the timer metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet shows the `GET` resource called `/simpleTimer`,
    which demonstrates timing with a `Callable` object with the simple timer metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Not shown are the getter methods for the metric values. For the timer, you can
    call `getCount()`, `getElapsedTime()`, `getSnapshot()`, `getMeanRate()`, `getOneMinuteRate()`,
    `getFiveMinuteRate()`, and `getFifteenMinuteRate()`. For the simple timer, you
    can call `getCount()`, `getElapsedTime()`, `getMinTimeDuration()`, and `getMaxTimeDuration()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call both `GET` resources and see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: First, we issue `GET` requests to `/ch6/timersResource/timer` and `/ch6/timersResource/simpleTimer`
    to invoke both of our timers. We then send a request to `/metrics/application`
    to view the results. As we have already demonstrated the similar max and min behavior
    of the concurrent gauge, we will not be demonstrating that behavior for the simple
    timer here. Additionally, as the timer metric outputs a statistical distribution
    of the recorded times (which includes the total recorded durations) and the throughput
    of requests, similar to the histogram and meter metrics, the timer metric's output
    will be omitted. What remains is the output for the simple timer. Notice that
    the values for `application_simpleTimerMetric_maxTimeDuration_seconds` and `application_simpleTimerMetric_minTimeDuration_seconds`
    report `NaN`. This is because there are no recorded values for the previously
    completed minute. If you would like to view the full output, we encourage you
    to try out the samples directly. Take a look at the *Technical requirements* section
    at the beginning of this chapter for instructions on how to run the samples.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting timers and simple timers with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To instrument the timer and simple timer metric, you will need to use `@Timed`
    and `@SimplyTimed`, respectively. These annotations apply to methods, constructors,
    and classes. They will both record how long it takes to execute the target annotated
    element.
  prefs: []
  type: TYPE_NORMAL
- en: We will show a simple example demonstrating how to annotate `@Timed` and `@SimplyTimed`
    on a JAX-RS endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `TimersAnnotatedResource` can be found at [https://bit.ly/3xVroDb](https://bit.ly/3xVroDb):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Gauges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A gauge metric serves to report on some value that is provided by the application.
    This can be any value, but it is highly recommended that the value is a number
    as Prometheus only supports numeric gauges. This is not a limitation of JSON output.
    Additionally, you can only create a numeric gauge using the metric registry's
    methods for creating a gauge.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting gauges programmatically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we mentioned earlier, the gauge metric does not follow the same pattern of
    the registration and retrieval method signatures like the other metrics do. This
    is due to the nature of what the gauge metric does. When registering or retrieving
    a gauge, you will need to specify a `Supplier` or `Function` object or lambda
    expression.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the method signatures for registering or retrieving a gauge
    metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The only notable method you can call with the gauge metric is `getValue()`.
    Since you should be familiar with the usage of the `MetricID` class, the `Metadata`
    class, how to create a metric, and Java functions (which we assume you are familiar
    with), we will not be providing any example code for instrumenting the gauge metric
    programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting gauges with annotations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To instrument a gauge metric, you will need to use the `@Gauge` annotation.
    This annotation can only be applied to a method. With the gauge annotation, you
    must specify the unit parameter. We will show a simple example where the method,
    and therefore the gauge, will return the current millisecond since the last epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `GaugeResource` can be found at [https://bit.ly/3mfj6Ux](https://bit.ly/3mfj6Ux):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll assume that a `GET` request invokes this method, so we''ll just show
    the resulting `/metrics/application` output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Note About Prometheus Formatting with Gauges
  prefs: []
  type: TYPE_NORMAL
- en: The unit that's defined for the gauge is appended as `_<unit>` and is scaled
    to the appropriate base unit.
  prefs: []
  type: TYPE_NORMAL
- en: The @Metric annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `@Metric` annotation is a unique annotation that allows you to inject a
    metric that corresponds to the field or parameter type that it is being annotated
    on. The `@Metric` annotation contains the same annotation parameters as the other
    metric annotations. It will return a metric with the matching metadata, if it
    exists; otherwise, a new metric of the specified type will be created, registered,
    and injected. Let's look at an example of using both injection strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `MetricsResource` can be found at [https://bit.ly/3iBAz7E](https://bit.ly/3iBAz7E):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the aforementioned example, `fieldInjectedCounter` is injected with field
    injection and `parameterInjectedCounter` is injected with parameter injection.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing metric data with Prometheus and Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MicroProfile Metrics runtime can only report on instantaneous metric values.
    To effectively use this data for monitoring purposes, we need to aggregate that
    data with a tool such as Prometheus. Then, using a tool such as Grafana, we can
    create a wide variety of visualizations that present the metric data over a configurable
    time period. Prometheus can scrape data from multiple sources and Grafana will
    then pull the data from it by performing queries against Prometheus using the
    `REST.request`, from the Broker microservice of the StockTrader application.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the `REST.request` Metric
  prefs: []
  type: TYPE_NORMAL
- en: The `REST.request` metric is a simple timer that is automatically instrumented
    to all REST endpoints by the MicroProfile Metrics runtime. The instrumented `REST.request`
    metrics are differentiated from each other with tags related to the class name
    and the method signature.
  prefs: []
  type: TYPE_NORMAL
- en: The Broker microservice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Broker service contains multiple JAX-RS/REST endpoints for creating, retrieving,
    and deleting Broker objects, as well as retrieving portfolio returns with `GET`,
    `POST`, and `DELETE` requests. This all occurs in the `AccountService` class.
    The full source code can be found at [https://bit.ly/3sBGvPE](https://bit.ly/3sBGvPE).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll look at the sample output of a `REST.request` metric so that
    we can understand the format of the metric name and its tags, before we demonstrate
    querying it with Grafana. We''ll show the output of a `GET` endpoint that queries
    all accounts whose method is `getAccounts()`. The other base metrics – the max
    time and min time values and the metric description for `REST.request` – have
    been omitted from the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing with Grafana
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Grafana, we can create a visualization out of each metric by querying the
    metric name. For example, we can simply query `base_REST_request_total` and Grafana
    will display all instances of that metric, which counts the request invocations
    to a REST endpoint. Alternatively, if we want to see only the metrics from a single
    microservice, such as `AccountService`, we can issue the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'However, just the total count of a counter doesn''t tell us much. We would
    be more interested in knowing how many times the metrics have increased in the
    past 10 minutes instead. Here, we can perform the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Or perhaps we want to know the rate at which requests have increased in the
    past 10 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'When using a simple timer, what we would be most interested in is the timing
    data. However, the elapsed time by itself is nothing significant, but we can calculate
    a new value that may prove more useful. Using the elapsed time and count, we can
    calculate the average duration per request with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a graphical visualization of the aforementioned query. The
    details of the snapshot are not important; the snapshot serves to illustrate the
    layout of what you would expect to see when using Grafana. The query is entered
    at the top, with the visualization displayed in the middle, and a table or list
    of the queried metrics at the bottom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – A Grafana graph visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17377_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – A Grafana graph visualization
  prefs: []
  type: TYPE_NORMAL
- en: These examples only show a sliver of the potential of using Prometheus and Grafana.
    In the preceding figure, we only used a graph visualization. There are a wide
    variety of visualizations that exist that suit any specific visualization needs
    you may have. On top of that, there is a vast array of functions available to
    use with PromQL to calculate any specific values that you and your team may find
    useful. It should also be noted that the preceding figure only shows a direct
    view of a single visualization. Remember that you can build dashboards with multiple
    visualizations displayed all at once.
  prefs: []
  type: TYPE_NORMAL
- en: We've now come to the end of the MicroProfile Metrics section. With the metrics
    instrumented in your microservice, you can monitor different parts of your application
    in detail. In the next section, we'll learn how to observe requests that span
    multiple microservices with MicroProfile OpenTracing.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing your cloud-native application using MicroProfile OpenTracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will conclude our MicroProfile observability journey by looking at the MicroProfile
    OpenTracing technology. Unlike the other two technologies we've examined in this
    chapter, MicroProfile OpenTracing is much more lightweight in comparison. We'll
    cover the importance of this technology while overviewing it and jump straight
    into learning how to use MicroProfile OpenTracing.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of and an overview of MicroProfile OpenTracing in a cloud-native
    application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MicroProfile OpenTracing technology ties in with the concept of **distributed
    tracing**. In a cloud environment, applications or microservices communicate and
    interact with one another, which, in turn, can interact with other microservices.
    This chain of interactions can be quite lengthy, depending on the nature and context
    of your application deployments. When something unexpectantly fails, it can be
    a difficult and troublesome task to diagnose where things have gone wrong in such
    a complex and distributed topology.
  prefs: []
  type: TYPE_NORMAL
- en: This is where distributed tracing comes in. Distributed tracing allows us to
    track and monitor requests or processes as it navigates from one application to
    another. Throughout its journey, which is referred to as a **trace**, performance
    data (for example, time spent), contextual data in the form of tags, and any important
    logs are retrieved for a **span**. A span defines the individual hierarchal segments
    that make up a **trace**. Each span can be identified by name.
  prefs: []
  type: TYPE_NORMAL
- en: For example, invocating a method creates a span named *method1*. This method
    can then invoke another method, which then creates a new **child span**, named
    *method2*, that is under the scope of the **parent span** from the first method.
    When the child span is complete (that is, the method finishes invoking), it returns
    to the first method and when the first method finishes, the trace is completed.
    There is no limit to how many child spans there can be. The resulting trace records
    are sent to an external tool or platform that gathers and stores these records,
    and can provide a way for us to view all the traces and the spans that it comprises.
  prefs: []
  type: TYPE_NORMAL
- en: It is through this that we can analyze and understand the performance and latency
    of a request, and any additional contextual information from individual spans,
    as it navigates through multiple microservices. With distributed tracing, we can
    easily profile the performance and latency of a request and diagnose any errors
    or failures that occur.
  prefs: []
  type: TYPE_NORMAL
- en: For distributed tracing to be effective in a system, all applications must use
    the same distributed tracing library. Now, you may be thinking that this is what
    MicroProfile OpenTracing serves to satisfy. This is not the case. The MicroProfile
    OpenTracing technology operates on top of the existing **OpenTracing** technology.
    The OpenTracing technology is a façade that defines a vendor-neutral API for instrumenting
    distributed tracing in an application. This OpenTracing technology is incorporated
    into the MicroProfile OpenTracing runtime. To be able to apply to trace instrumentation
    to your application, you will need to use a compatible **tracer** implementation.
    You can view the compatible tracers at [https://opentracing.io/docs/supported-tracers/](https://opentracing.io/docs/supported-tracers/).
    However, note that the different MicroProfile OpenTracing runtimes are compatible
    with different sets of tracer libraries. Consult the documentation of your chosen
    runtime for more details. It can even be the case that your chosen runtime may
    support a tracer that isn't on OpenTracing's list of officially supported tracers.
  prefs: []
  type: TYPE_NORMAL
- en: Each application in the system will need to be configured to use the same tracer
    library. Different tracer libraries may differ in how they communicate the contextual
    identification data of a trace, which is called the **span context**. The span
    context contains stateful information that is accompanied by a request as it navigates
    through the network of microservices. This allows the OpenTracing technology to
    link spans together into a singular trace when they transcend application boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile OpenTracing amends the OpenTracing technology by defining an additional
    `@Traced` annotation that complements the use of the OpenTracing technology. However,
    the main benefit of MicroProfile OpenTracing is that you can automatically instrument
    traces on inbound and outbound JAX-RS requests. Any JAX-RS application will be
    traced without the developer having to deal with the MicroProfile OpenTracing
    API or the OpenTracing API. We will not cover how to use the OpenTracing API in
    this book, only the amendments that MicroProfile OpenTracing provides. We leave
    it up to you to explore the OpenTracing API and its documentation at [https://bit.ly/3gEHLis](https://bit.ly/3gEHLis).
  prefs: []
  type: TYPE_NORMAL
- en: The producer of the implementation library may also provide a platform/server
    that aggregates the tracing records. We will demonstrate this with the Jaeger
    Tracing platform at the end of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Special Note on OpenTracing
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the OpenTracing project has combined with OpenCensus
    to form OpenTelemetry. OpenTelemetry is an all-in-one technology that will satisfy
    your monitoring needs for tracing, logging, and metrics. Future iterations of
    the MicroProfile platform may see the incorporation of OpenTelemetry and its subcomponents.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-instrumenting JAX-RS requests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MicroProfile OpenTracing allows you to automatically instrument tracing on JAX-RS
    requests on both the client and server side. When a request is sent through a
    JAX-RS client or using MicroProfile Rest Client, a span will be automatically
    created. If an active span already exists, then it will be a child span of the
    active span. This span begins when the request is sent by the client.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, when an incoming JAX-RS request is received, a span will be created.
    If the request is part of a trace, the MicroProfile OpenTracing runtime will automatically
    determine that by attempting to extract span context information from the incoming
    request. If such data exists, then the new span is a child span of a preceding
    span in this trace. If there are no active spans or extractable span context information,
    then a new span and, subsequently, a new trace is created. This span begins when
    the request is received and correlates with the JAX-RS resource method. This default
    behavior of auto-instrumentation on the JAX-RS resource method can be overridden
    with the use of the `@Traced` annotation, which will be covered in the *Instrumenting
    the @Traced annotation and injecting a Tracer* section.
  prefs: []
  type: TYPE_NORMAL
- en: There are some additional rules regarding names and tags that we will cover
    once we have described how to auto-instrument outbound and inbound JAX-RS requests.
  prefs: []
  type: TYPE_NORMAL
- en: Outbound JAX-RS requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With outbound JAX-RS requests, the span that is created is given the name of
    the HTTP method to be invoked. For example, a `GET` request results in a span
    named `GET`.
  prefs: []
  type: TYPE_NORMAL
- en: Note on Using JAX-RS Clients
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a JAX-RS client to create outbound requests, you will need
    to pass the ClientBuilder you've created to `ClientTracingRegistrar` for the MicroProfile
    OpenTracing runtime to create a span for it. You can invoke either the `configure(ClientBuilder
    clientBuilder)` or `configure(ClientBuilder clientBduilder, ExecutorService executorService)`
    static methods, which will then return a `ClientBuilder` object that you can use.
    The implementation of the MicroProfile OpenTracing runtime may have been already
    configured so that any ClientBuilders used will create a span, thus not needing
    to invoke the `configure(…)` methods. Consult the documentation of your MicroProfile
    OpenTracing runtime for details.
  prefs: []
  type: TYPE_NORMAL
- en: Inbound JAX-RS requests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With inbound JAX-RS requests, the span that is created is given the name in
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'This is referred to as the **class-method** naming format and is the default
    format. Alternatively, you can use the **http-path** naming format, which uses
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: To enable the http-path format, use the MicroProfile Config configuration element
    known as `mp.opentracing.server.operation-name-provider` and specify `http-path`.
  prefs: []
  type: TYPE_NORMAL
- en: Span tags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spans that are created in both inbound and outbound requests use the following
    tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Tags.SPAN_KIND_CLIENT"`. An inbound request has a value of `"Tags.SPAN_KIND_SERVER"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tags.HTTP_METHOD**: The value of this is the HTTP method that has been invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tags.HTTP_URL**: This is the value of the HTTP URL that the request has been
    sent to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tags.HTTP_STATUS**: This is the status of the HTTP request. It specifies
    what response was received by the client or what response the server is returning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jaxrs.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`event=error` and `error.object=<error object instance>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumenting the @Traced annotation and injecting a tracer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Provided with MicroProfile OpenTracing is the `@Traced` CDI annotation. This
    annotation can be applied to methods and classes. When applied to a class, every
    method in the class is annotated with `@Traced`. The `@Traced` annotation can
    be used to further fine-tune the spans that make up the trace. It can also be
    used to override the default auto-instrumentation of JAX-RS resource methods,
    such as disabling or renaming the span or to further specify spans on other methods
    in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `@Traced` annotation contains two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**value**: This is a boolean parameter. It is true by default, which implies
    that the annotated method will be automatically instrumented for tracing. A false
    value disables automatic tracing for the method. This can be used to disable automatic
    instrumentation on JAX-RS endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`String` and defines the name of the span that would be created when the method
    is invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that when both a class and a method inside the class use the `@Traced`
    annotation, the method annotation and its parameters take priority.
  prefs: []
  type: TYPE_NORMAL
- en: The MicroProfile OpenTracing runtime can also inject an optional `io.opentracing.Tracer`
    object. Using this OpenTracing object, you can programmatically create and manipulate
    spans using the OpenTracing API. You can add your own tags, logs, and baggage.
    We will not cover how to use the OpenTracing API in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows how to inject the OpenTracing's `Tracer` object,
    as well as how to use `@Traced` on both a JAX-RS endpoint and a normal business
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full source code for `TraceResource` can be found at [https://bit.ly/3AXmiIr](https://bit.ly/3AXmiIr):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In the aforementioned example, `doNotTraceMe()` is annotated with `@Traced(value=false)`,
    which alerts the OpenTracing runtime to not trace this JAX-RS endpoint. `traceMe()`
    is a normal business method and is annotated with `@Traced(operationName="traceMe")`
    to alert the OpenTracing runtime to trace this as a span if the code path travels
    to this method. The span is called `"traceMe"`.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing traces with Jaeger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this demonstration, we''ll use a simple application consisting of two JAX-RS
    resources called `OutboundRequestResource` and `InboundRequestResource`. We''ll
    issue a GET request to `OutboundRequestResource` at [http://localhost:9080/outbound/tracing](http://localhost:9080/outbound/tracing),
    which will then create a `ClientBuilder` to send a `GET` request to `InboundRequestResource`.
    This, in turn, will invoke the `epoch()` method in a `TracedExample` class that''s
    been annotated with `@Traced(operationName="epoch")`. The resulting trace visualization
    can be seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 6.4 – Trace inspection in Jaeger'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17377_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Trace inspection in Jaeger
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full source for `OutBoundTraceResource` at [https://bit.ly/3swFZEb](https://bit.ly/3swFZEb).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full source for `InBoundTraceResource` at [https://bit.ly/3xZxrXz](https://bit.ly/3xZxrXz).
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full source for `TracedExample` at [https://bit.ly/3y6pHmM](https://bit.ly/3y6pHmM).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a snapshot of what you may expect when inspecting a trace on the Jaeger
    web client. The preceding figure may be hard to discern, so we will describe it.
    The top left shows the name of the trace. The trace is named *book: GET:com.packt.microprofile.book.ch6.opentracing.OutBoundTraceResource.tracing*.
    The trace is given the name of the first span in this trace, which is the `GET`
    request that we issued to the `/tracing` endpoint in `OutBoundTraceResource`.'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the interface consists of the sequential listing of the spans that
    make up the trace. When minimized, it will display the duration of each span and
    their active durations compared to the other spans as solid horizontal bars. When
    you click on a span entry, it will expand to show more details, such as its contextual
    data. In the aforementioned figure, the span that was created from the inbound
    JAX-RS request from `InBoundTraceResource`, as well as the span that was instrumented
    from the `@Traced` annotation on the `epoch()` method, have been expanded.
  prefs: []
  type: TYPE_NORMAL
- en: Let's describe the first expanded span, which is the span that was created by
    the inbound request. It is called *GET:com.packt.microprofile.book.ch6.opentracing.InBoundTraceResource.waiting*.
    Included in the details are its tags, which we discussed earlier in this section;
    that is, *component*, *http.method*, *http.status_code*, *http.url*, and *span.kind*.
    A tag that's appended by Jaeger is in *internal.span.format*. The instrumented
    span does not contain any tags other than the ones provided by Jaeger.
  prefs: []
  type: TYPE_NORMAL
- en: With the combination of a summary view of a trace and having the ability to
    view the individual spans that make up the trace, using distributed tracing to
    profile the paths that a request takes is very useful for analyzing performance
    and latency. In the aforementioned example, we demonstrated distributed tracing
    with the Jaeger platform. Another distributed tracing platform that provides an
    instrumentation library and the facilities to view and analyze the traces is **Zipkin**.
    Zipkin isn't included on the list of official tracers in the OpenTracing documentation,
    but you may find that the MicroProfile OpenTracing runtime you chose supports
    it. Consult the documentation of your runtime for their list of supported libraries
    and the necessary steps for configuring it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the three observability technologies offered by
    the MicroProfile platform; that is, MicroProfile Health, MicroProfile Metrics,
    and MicroProfile OpenTracing. From reporting the overall health of your application
    with health checks to the detailed statistical data that metrics provide, to tracking
    and profiling requests as they travel through your microservices with distributed
    tracing, each technology has an invaluable purpose that satisfies the important
    task of monitoring and observing your cloud-native application. Your application
    has now harnessed all the features and capabilities that the MicroProfile release
    platform has to offer. There are additional technologies that come with MicroProfile's
    standalone releases. We will cover these in the final chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the topic of deploying your cloud-native
    application onto the cloud. We'll see how it interacts with cloud infrastructures
    such as Docker, Kubernetes, and Istio.
  prefs: []
  type: TYPE_NORMAL
