<html><head></head><body>
		<div id="_idContainer037">
			<h1 id="_idParaDest-137"><em class="italic"><a id="_idTextAnchor146"/>Chapter 7</em>: MicroProfile Ecosystem with Open Liberty, Docker, and Kubernetes</h1>
			<p>So far, in the previous chapters of this book, we focused on using the MicroProfile APIs to write a cloud-native application. In this chapter, we will look at how to run a cloud-native application. One of the features of MicroProfile that sets it apart from some other cloud-native application frameworks is that MicroProfile offers multiple implementations of the APIs. This reduces the chance of ending up locked into a particular implementation or finding that the open source community behind the APIs you were utilizing wasn't as vibrant as you thought and the maintainers disappear. In addition, different implementations tend to take different design decisions, which may better suit your needs. At the time of writing, there were four implementations of the most recent release of the MicroProfile APIs: <strong class="bold">Open Liberty</strong>, <strong class="bold">Payara</strong>, <strong class="bold">WebSphere Liberty</strong>, and <strong class="bold">WildFly</strong>. In addition, <strong class="bold">Helidon</strong>, <strong class="bold">JBoss EAP</strong>, <strong class="bold">KumuluzEE</strong>, and <strong class="bold">Quarkus</strong> implement previous versions.</p>
			<p>Once you have chosen an implementation, you need to deploy the application into production. Increasingly, this is achieved using technologies such as <strong class="bold">Docker</strong>, <strong class="bold">Kubernetes</strong>, and a <strong class="bold">Service Mesh</strong>. This will be the focus of this chapter. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Deploying cloud-native applications to Open Liberty</li>
				<li>Containerizing cloud-native applications using Docker</li>
				<li>Deploying cloud-native applications to Kubernetes</li>
				<li>MicroProfile and Service Mesh</li>
			</ul>
			<p>In this chapter, you will learn how to configure a MicroProfile application to run on Open Liberty, packaging it as a container and deploying it into a Kubernetes runtime like Red Hat OpenShift.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor147"/>Technical requirements</h1>
			<p>To build and run the samples mentioned in this chapter, you will need a Mac or PC (Windows or Linux) with the following software:</p>
			<ul>
				<li><strong class="bold">Java Development Kit</strong> (<strong class="bold">JDK</strong>) – Java 8 or later: <a href="http://ibm.biz/GetSemerut">http://ibm.biz/GetSemerut</a></li>
				<li>Apache Maven: <a href="https://maven.apache.org">https://maven.apache.org</a></li>
				<li>A Git client: <a href="https://git-scm.com">https://git-scm.com</a></li>
				<li>A Docker client: <a href="https://www.docker.com/products ">https://www.docker.com/products</a></li>
				<li>The OpenShift client: <a href="https://docs.openshift.com/container-platform/4.6/cli_reference/openshift_cli/getting-started-cli.html ">https://docs.openshift.com/container-platform/4.6/cli_reference/openshift_cli/getting-started-cli.html</a></li>
			</ul>
			<p>All of the source code used in this chapter is available on GitHub at <a href="https://github.com/PacktPublishing/Practical-Cloud-Native-Java-Development-with-MicroProfile/tree/main/Chapter07">https://github.com/PacktPublishing/Practical-Cloud-Native-Java-Development-with-MicroProfile/tree/main/Chapter07</a>.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor148"/>Deploying cloud-native applications to Open Liberty</h1>
			<p>In this section, we will look at<a id="_idIndexMarker829"/> how to deploy a MicroProfile application using <strong class="bold">Open Liberty</strong>. We have chosen Open Liberty because we are<a id="_idIndexMarker830"/> committers on Open Liberty, but its focus on being current with the latest releases of MicroProfile, performance, and ease of use make it a good option for anyone.</p>
			<p>As you might expect from the name, Open Liberty is an open source Java runtime for building and deploying cloud-native applications. It is designed around the idea of components called <strong class="bold">features</strong> that can be <a id="_idIndexMarker831"/>configured to provide just enough runtime for your application needs. This means if your application doesn't use or need MicroProfile OpenTracing, then you don't need to configure the MicroProfile OpenTracing feature and the runtime will be smaller, faster, and leaner – it'll be better sized for what your application needs. Open Liberty has a feature for programming APIs such as MicroProfile APIs, Java EE, Jakarta EE, and gRPC. It also has features for runtime capabilities such as a feature for integrating with OpenID Connect for authentication. </p>
			<p>Open Liberty is configured primarily using a simple XML file format referred to as a <strong class="source-inline">server.xml</strong> file. XML is used for a few reasons:</p>
			<ul>
				<li>The fact that Java has baked-in support for parsing XML is one of the primary reasons.</li>
				<li>XML models hierarchical configuration very well (unlike a properties format)</li>
				<li>Space characters do not affect the semantic interpretation of the file format (unlike YAML).</li>
			</ul>
			<p>When parsing the <a id="_idIndexMarker832"/>configuration file, Open Liberty takes the approach of ignoring any configuration it doesn't understand. This has several advantages. It means that a <strong class="source-inline">server.xml</strong> file can contain a configuration that isn't valid for the release of Open Liberty being used without causing a startup <a id="_idIndexMarker833"/>failure. It also means a simple typo in the configuration won't prevent the server from starting the application.</p>
			<p>One of the core responsibilities of the <strong class="source-inline">server.xml</strong> file is to configure which features to load. By using the <strong class="source-inline">server.xml</strong> file to configure which features are to be used and by ensuring behavior changes are only introduced via new features, Open Liberty guarantees that the behavior of the configuration remains unchanged from one release to the next. A simple server configuration that enables all the MicroProfile APIs is as follows:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;featureManager&gt;</p>
			<p class="source-code">        &lt;feature&gt;microProfile-4.1&lt;/feature&gt;</p>
			<p class="source-code">    &lt;/featureManager&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>Open Liberty configuration can be centralized in a single <strong class="source-inline">server.xml</strong> file, or it can be split up among many configuration files. This both facilitates both sharing of configuration and also separates configuration based on the environment the server configuration is deployed to. An example of this might be the use of an in-memory database in a development environment, but in production, a database such as DB2, Oracle, or MariaDB might be used instead. This is facilitated by two mechanisms. The first is a <strong class="source-inline">server.xml</strong> file that can explicitly include another <strong class="source-inline">server.xml</strong> file. The second is the use of <a id="_idIndexMarker834"/>something referred to as <strong class="bold">config dropins</strong>. Config dropins consist of two directories, called <strong class="source-inline">defaults</strong> and <strong class="source-inline">overrides</strong>, that are read before and after the main server configuration file. The files in these directories are read in alphabetical order, providing predictability in how configuration is read. Configuration files can also be parameterized<a id="_idIndexMarker835"/> using variable replacement syntax. Variables can be defined in the <strong class="source-inline">server.xml</strong> file, as Java system properties, or using <a id="_idIndexMarker836"/>environment variables. Variables in <strong class="source-inline">server.xml</strong> can be defined multiple times and the last definition of the variable will be used for variable resolution. A variable might be defined as follows:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;variable name="microProfile.feature" </p>
			<p class="source-code">        value="microProfile-4.1" /&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>It can then be referenced elsewhere like this:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;featureManager&gt;</p>
			<p class="source-code">        &lt;feature&gt;${microProfile.feature}&lt;/feature&gt;</p>
			<p class="source-code">    &lt;/featureManager&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>Variables can also have a default value, which allows configuration to be written that will always work while allowing it to be overridden in production:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;variable name="http.port" defaultValue="9043" /&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>Variables have different precedents based on where they are defined from, allowing them to be easily overridden. The precedence order (later precedence overrides previous precedence) is as follows:</p>
			<ol>
				<li><strong class="source-inline">server.xml</strong> default values</li>
				<li>Environment variables</li>
				<li>The <strong class="source-inline">bootstrap.properties</strong> file</li>
				<li>Java system properties</li>
				<li>Variables defined in s<strong class="source-inline">erver.xml</strong></li>
				<li>Variables defined on server startup</li>
			</ol>
			<p>This provides multiple simple ways to change the behavior of Open Liberty based on the environment Open Liberty is deployed into. </p>
			<p>Open Liberty allows you to <a id="_idIndexMarker837"/>package your MicroProfile application as a <strong class="source-inline">WAR</strong> file for deployment into the server. The MicroProfile specifications do not have an opinion on how an application is packaged and deployed, so Open Liberty reuses the <strong class="source-inline">WAR</strong> packaging model from Jakarta EE as the way to package the <a id="_idIndexMarker838"/>application. This makes a lot of sense because MicroProfile makes use of several Jakarta EE programming models, and it makes it easier for a MicroProfile application to make use of parts of Jakarta EE that are not in MicroProfile, such as concurrency utilities for Jakarta EE. It also allows you to reuse the existing Maven and Gradle build tools for packaging a MicroProfile application.</p>
			<p>There are two ways to deploy a <strong class="source-inline">WAR</strong> file into Open Liberty. The first is by dropping the WAR file into the <strong class="source-inline">dropins</strong> folder, and the second is through the <strong class="source-inline">server.xml</strong> file:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;webApplication location="myapp.war" /&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>The primary reason for using the server configuration approach over the <strong class="source-inline">dropins</strong> folder is that it allows you to customize how the application is run, for example, setting the application's <strong class="source-inline">contextRoot</strong>, configuring classloading, or configuring security role bindings.</p>
			<p>Open Liberty supports several mechanisms for packaging applications for deployment. The simplest is to package the application as a <strong class="source-inline">WAR</strong> file. This is the least likely in a cloud-native environment though. Open Liberty also supports packaging a server as a <strong class="source-inline">zip</strong> file, an executable <strong class="source-inline">JAR</strong> file, and a Docker container (described in the next section). Open Liberty provides plugins for Maven and Gradle that make it simple to build applications that will run on Open Liberty. One of the features of these plugins is the Open Liberty <strong class="bold">dev mode</strong>. Dev mode watches the application code and recompiles it as it changes and deploys it into a running Open <a id="_idIndexMarker839"/>Liberty server, providing<a id="_idIndexMarker840"/> a fast turnaround time for making changes. It also monitors the Open Liberty configuration so if you add new features or other configuration changes, the server is also updated. To use <strong class="source-inline">dev</strong> mode in Maven, simply add the Open Liberty Maven plugin to the <strong class="source-inline">plugin</strong> section of your <strong class="source-inline">pom.xml</strong> as shown here:</p>
			<p class="source-code">&lt;plugin&gt;</p>
			<p class="source-code">    &lt;groupId&gt;io.openliberty.tools&lt;/groupId&gt;</p>
			<p class="source-code">    &lt;artifactId&gt;liberty-maven-plugin&lt;/artifactId&gt;</p>
			<p class="source-code">    &lt;version&gt;[3.3.4,)&lt;/version&gt;</p>
			<p class="source-code">&lt;/plugin&gt; </p>
			<p>This configures the plugin to use the 3.3.4 version of the plugin or a more recent release if one exists. </p>
			<p>When you run the <strong class="source-inline">liberty:dev</strong> Maven goal, the plugin will compile the application, downloading any dependencies required to run the application, deploy it into Liberty, and run the server with Java debugger support. This allows you to make changes to the application in any code editor, whether it is a simple editor such as <strong class="bold">vi</strong> or a full-fledged IDE such as <strong class="bold">IntelliJ IDEA</strong> or <strong class="bold">Eclipse IDE</strong>. </p>
			<p>Liberty's design makes it very simple to build applications that will run in a container environment such as Docker. There is even a <strong class="source-inline">dev</strong> mode for containers that can be run using <strong class="source-inline">liberty:devc</strong> for this. The next section will discuss how to create a container as the deployment artifact.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor149"/>Containerizing cloud-native applications using Docker</h1>
			<p>In this section, we <a id="_idIndexMarker841"/>will look at how to containerize a<a id="_idIndexMarker842"/> MicroProfile application. There are two key parts to containerizing anything: the first is the creation of an image, and the second is running that image. While Docker was not the first product to do containerization, it did popularize it in a way that developers and operators could understand. Cloud Foundry was a common early alternative that had similar concepts but hid them <a id="_idIndexMarker843"/>as internal implementation details rather than making them first-class concepts. With Docker, these two concepts were broken into two parts, exposed by the <strong class="source-inline">docker build</strong> command used to<a id="_idIndexMarker844"/> create the image, and the <strong class="source-inline">docker run</strong> command used to run the image. These concepts were further expanded to become standardized, meaning there are now multiple alternatives to <strong class="source-inline">docker build</strong> and <strong class="source-inline">docker run</strong>. </p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor150"/>The container image</h2>
			<p>A <strong class="bold">container image</strong> is the container deployment artifact. A container image contains everything required to run the application. This means that a container image can be moved from one environment to <a id="_idIndexMarker845"/>another with confidence that it will run in the same way. The motto is to think, create once, run everywhere; however, there are some limitations to this. A container is tied to a CPU architecture, so a container designed for x86 CPUs wouldn't run on ARM or Power ones without a translation layer for the CPU instructions (such as Rosetta 2, which translates Mac x86 instructions to Mac ARM ones to support x86 Mac applications on Macs with an M series ARM processor).</p>
			<p>A <strong class="bold">Dockerfile</strong> is a set of instructions for how to create a container image. A Dockerfile starts by declaring a named image that it is based on and then identifies a series of steps to add additional content into the container image. A common practice might be for it to be based on an<a id="_idIndexMarker846"/> image containing an <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>), a Java image, or an image that pre-packages the application runtime such as Open Liberty. </p>
			<p>While it is convenient to think of a container image as a single large file containing everything in the image, this is not how a container image works. A container image is made up of a series of layers that are identified by an SHA hash. This provides three key advantages:</p>
			<ul>
				<li>It reduces the storage space required to store images. If you have 30 images based on a common base image, you only store that common base image once, not 30 times. While storage space is relatively cheap, if you have a large number of applications, file duplication between the containers would soon add up to large numbers.</li>
				<li>It reduces bandwidth requirements. When you transfer container images to and from a <strong class="bold">container registry</strong>, you do not need to upload or download the layers you already have. It is likely you will have many container images, but one common OS image, and a common JVM image.</li>
				<li>It reduces build time. If the input to a layer hasn't changed, there is no need to rebuild it. The input to a layer is any change you made in that line, plus the output of the prior line. </li>
			</ul>
			<p>Each layer has a pointer to the <a id="_idIndexMarker847"/>layer it was built on top of. When you create a container image, it consists of all the layers from the base image and one new layer for every single line in the Dockerfile. A simple Dockerfile for packaging a simple MicroProfile application might look like this:</p>
			<p class="source-code">FROM open-liberty:full-java11-openj9</p>
			<p class="source-code">ADD src/main/config/liberty/server.xml /config</p>
			<p class="source-code">ADD target/myapp.war /config/dropins</p>
			<p>This creates a container image based on Ubuntu 20.04 with Java SE 11 using the OpenJ9 JRE implementation with all Open Liberty features available to it. It then copies <strong class="source-inline">server.xml</strong> from the default location in a Maven project and the application to the <strong class="source-inline">dropins</strong> folder. This will create an image with all the layers associated with the <strong class="source-inline">open-liberty:full-java11-openj9</strong> image, and two layers associated with this image.</p>
			<p class="callout-heading">Best practice: using multiple layers</p>
			<p class="callout">The advantage of this approach is when you push or pull an image, only layers that are not already present are transferred. In the simple MicroProfile example stated previously, when you build and push the image, only the layers associated with the application and server configuration will be transferred. Think of it this way: if you have a base image that is 500 MB in size, and the layers of your image are a total of 5 MB, your image would be a total of 505 MB, but when you push that back to the container registry, only 5 MB would need to be sent since the base image is already there. </p>
			<p>This leads to some interesting design questions when designing Docker images. The purpose of a Docker image is clearly to get it running somewhere, and preferably to get that to happen as quickly as possible. This makes it faster to deploy a new image production, to scale it up, or in the event of a problem, to replace the container with a new one. A simple way to<a id="_idIndexMarker848"/> build Docker images is to package your application in a single JAR file, add it to the Dockerfile, and then run it:</p>
			<p class="source-code">FROM adoptopenjdk:11-openj9</p>
			<p class="source-code">ADD target/myapp.jar /myapp.jar</p>
			<p class="source-code">CMD ["java", "-jar", "/myapp.jar"]</p>
			<p>This is a popular way to create Docker images and works well if the application is small, but many applications built this way are not small. Consider the case where that <strong class="source-inline">jar</strong> contains Open Liberty, the application code, and some open source dependencies. That means that every time the application is modified, a new layer containing all that code has to be re-created and deployed. If, on the other hand, the application was split up, then a change to the application would require a much smaller upload:</p>
			<p class="source-code">FROM open-liberty:full-java11-openj9</p>
			<p class="source-code">ADD target/OS-dependencies /config/library</p>
			<p class="source-code">ADD target/myapp.war /config/apps/myapp.war</p>
			<p>In this example, a change to the application code would only rebuild the last layer, meaning the upload will be smaller and distributed faster. A change to the open source dependencies would of course result in that layer being rebuilt, but those tend to change less frequently than the application. If there are many applications that share a common set of libraries (whether open source or not), it might make sense to create a named base image that all applications can use. This would be especially useful if the container images are commonly run on the same host.</p>
			<p>A significant thing to understand about the layers is once created, they are immutable. That means if you delete a file created in an earlier layer, it doesn't remove the files from the image; it just marks them as deleted so they cannot be accessed. This means that when transferring the container image around, you will copy the bytes for the file, but it won't ever be accessible. If the file is contributed to by a base image, this will be unavoidable, but if you control the image, then it is something to avoid.</p>
			<h3>Dockerfile instructions</h3>
			<p>As previously stated, a Dockerfile is<a id="_idIndexMarker849"/> a series of instructions detailing how to create a Docker image. There are several instructions for <a id="_idIndexMarker850"/>creating an image. The first instruction in all the examples so far is the <strong class="source-inline">FROM</strong> instruction.</p>
			<h4>FROM</h4>
			<p>The <strong class="source-inline">FROM</strong> instruction <a id="_idIndexMarker851"/>defines the container image that is the base of the image you are creating. All Dockerfiles are required to start with <strong class="source-inline">FROM</strong>. A Dockerfile can have multiple <strong class="source-inline">FROM</strong> lines: these are commonly used to create a multi-stage build. An example of this might be if you need some kind of extra tools to build your image that you don't want to be present when you run it. An example of this might be <strong class="source-inline">wget</strong> or <strong class="source-inline">curl</strong> for downloading files and <strong class="source-inline">unzip</strong> for expanding a zip file. A simple multi-stage build might look like this:</p>
			<p class="source-code">FROM ubuntu:21.04 as BUILD</p>
			<p class="source-code">RUN apt-get update</p>
			<p class="source-code">RUN apt-get install -y unzip wget</p>
			<p class="source-code">RUN wget https://example.com/my.zip -O my.zip</p>
			<p class="source-code">RUN unzip my.zip /extract</p>
			<p class="source-code">FROM ubuntu:21.04</p>
			<p class="source-code">COPY /extract /extract --from=BUILD</p>
			<p>In this example, the first stage installs <strong class="source-inline">wget</strong> and <strong class="source-inline">unzip</strong>, downloads a file, and unzips it. The second stage starts from the base image and then copies the extracted files into the new image layer. If a single-stage Dockerfile were created, this would have resulted in an image with three additional layers containing the binaries for <strong class="source-inline">unzip</strong> and <strong class="source-inline">get</strong>, the <strong class="source-inline">zip</strong> file, and <strong class="source-inline">extract</strong>. The multi-stage build only contains <strong class="source-inline">extract</strong>. To do this with a single-stage Docker build is less readable and would look like this:</p>
			<p class="source-code">FROM ubuntu:21.04 </p>
			<p class="source-code">RUN apt-get update &amp;&amp; \</p>
			<p class="source-code">    apt-get install -y unzip wget &amp;&amp; \</p>
			<p class="source-code">    wget https://example.com/my.zip -O my.zip &amp;&amp; \</p>
			<p class="source-code">    unzip my.zip /extract &amp;&amp; \</p>
			<p class="source-code">    rm my.zip &amp;&amp; \</p>
			<p class="source-code">    apt-get uninstall -y unzip wget &amp;&amp; \</p>
			<p class="source-code">    rm -rf /var/lib/apt/lists/*    </p>
			<p>This Dockerfile uses a single <strong class="source-inline">RUN</strong> command to run multiple commands to create only a single layer and it has to undo each step before the end. The last line is required to tidy up files created by <strong class="source-inline">apt</strong>. The multi-stage Dockerfile is much simpler. Another common use for multi-stage builds<a id="_idIndexMarker852"/> is to use the first stage to build the application, and then the second stage for running:</p>
			<p class="source-code">FROM maven as BUILD</p>
			<p class="source-code">COPY myBuild /build</p>
			<p class="source-code">WORKDIR build</p>
			<p class="source-code">RUN mvn package</p>
			<p class="source-code">FROM open-liberty:full-java11-openj9</p>
			<p class="source-code">COPY /target/myapp.war /config/apps/ --from=BUILD</p>
			<h4>COPY and ADD</h4>
			<p>The <strong class="source-inline">COPY</strong> and <strong class="source-inline">ADD</strong> instructions<a id="_idIndexMarker853"/> perform a similar function. <strong class="source-inline">ADD</strong> has a superset of the function of <strong class="source-inline">COPY</strong> so it is generally advised to only use <strong class="source-inline">ADD</strong> if you need the<a id="_idIndexMarker854"/> extended function. The first argument for both instructions specifies the source file (or directory) and by default is interpreted to be copying from the machine running the build. The command is always relative to the directory the build is run from and you cannot use <strong class="source-inline">..</strong> to navigate to the parent directory. The use of the <strong class="source-inline">from</strong> argument, as shown in the previous section, redirects the copy to be from another container image. The second argument is the location in the container the file should be copied to. </p>
			<p>The <strong class="source-inline">ADD</strong> command provides some additional features over and above the <strong class="source-inline">COPY</strong> command. The first is it allows you to specify a <strong class="source-inline">URL</strong> to download the file from as the first argument. The second <a id="_idIndexMarker855"/>feature is it will unzip a <strong class="source-inline">tar.gz</strong> file into a directory. To go<a id="_idIndexMarker856"/> back to the first multi-stage build example, if the output was a <strong class="source-inline">tar.gz</strong> file, it would mean it could be simplified to just be the following:</p>
			<p class="source-code">FROM ubuntu:21.04 as BUILD</p>
			<p class="source-code">ADD https://example.com/my.tar.gz /extract</p>
			<h4>RUN</h4>
			<p>The <strong class="source-inline">RUN</strong> instruction simply executes one or more commands using the shell from the OS layer. This allows you to do pretty much anything you want or need to provide the command available in the base OS image. For example, it is uncommon for <strong class="source-inline">unzip</strong> or <strong class="source-inline">wget</strong> to be in the base Linux OS images, so those commands will fail unless action is taken to install them. Each <strong class="source-inline">RUN</strong> instruction creates a new layer, so if you create a file in one <strong class="source-inline">RUN</strong> command and delete it in another, due to the immutability of the layers, the file will exist but not be visible anymore. For this reason, it is often important to use the <strong class="source-inline">&amp;&amp;</strong> operator to string multiple commands together in a single layer. An example of this was shown previously, but is repeated here:</p>
			<p class="source-code">FROM ubuntu:21.04 </p>
			<p class="source-code">RUN apt-get update <strong class="bold">&amp;&amp;</strong> \</p>
			<p class="source-code">    apt-get install -y unzip wget &amp;&amp; \</p>
			<p class="source-code">    wget https://example.com/my.zip -O my.zip &amp;&amp; \</p>
			<p class="source-code">    unzip my.zip /extract &amp;&amp; \</p>
			<p class="source-code">    rm my.zip &amp;&amp; \</p>
			<p class="source-code">    apt-get uninstall -y unzip wget &amp;&amp; \</p>
			<p class="source-code">    rm -rf /var/lib/apt/lists/*    </p>
			<h4>ARG and ENV</h4>
			<p><strong class="source-inline">ARG</strong> defines a build argument that can be specified at build time. The <strong class="source-inline">ARG</strong> values are set when running <strong class="source-inline">docker build</strong> using the <strong class="source-inline">build-arg</strong> argument. <strong class="source-inline">ARG</strong> can have a default value, in case <a id="_idIndexMarker857"/>it isn't provided at build time. These<a id="_idIndexMarker858"/> build arguments are not persisted after the build finishes, so they are not available at runtime and are not persisted in the image.</p>
			<p><strong class="source-inline">ENV</strong> defines an environment variable that is available both at build and runtime. </p>
			<p>Both of these are referenced in the same way, so the key difference is the visibility of the value.</p>
			<h4>ENTRYPOINT and CMD</h4>
			<p>When running a container, you <a id="_idIndexMarker859"/>need something to happen, such as starting the Open Liberty server. What happens can be defined by the Dockerfile using the instructions <strong class="source-inline">ENTRYPOINT</strong> and <strong class="source-inline">CMD</strong>. The difference<a id="_idIndexMarker860"/> between the two instructions is how they interact with the <strong class="source-inline">docker run</strong> command. When running a Docker container, any arguments after the Docker image name are passed into the container. <strong class="source-inline">CMD</strong> provides a default value in case no command-line arguments are provided. <strong class="source-inline">ENTRYPOINT</strong> defines a command that will be run and any command-line arguments provided to <strong class="source-inline">docker run</strong> are passed in after <strong class="source-inline">ENTRYPOINT</strong>. Both <strong class="source-inline">CMD</strong> and <strong class="source-inline">ENTRYPOINT</strong> have the same syntax. The Open Liberty container specifies both of these, so images based on them do not tend to specify them.</p>
			<h4>WORKDIR</h4>
			<p>The <strong class="source-inline">WORKDIR</strong> instruction is used to<a id="_idIndexMarker861"/> change the current directory for future <strong class="source-inline">RUN</strong>, <strong class="source-inline">CMD</strong>, <strong class="source-inline">COPY</strong>, and <strong class="source-inline">ENTRYPOINT</strong> instructions. </p>
			<h4>USER</h4>
			<p>When building an image, the default <a id="_idIndexMarker862"/>user account used for executing commands is the <strong class="source-inline">root</strong> one. For some operations, this is reasonable and fair. If doing an OS update, you typically need to execute as <strong class="source-inline">root</strong>. However, when running the containers using, the <strong class="source-inline">root</strong> account is a clear security issue. The Dockerfile has a <strong class="source-inline">USER</strong> instruction that sets the user account used for <strong class="source-inline">RUN</strong> instructions as well as the process that executes in the container when it is run. This makes it simple to set the account to a non-root account. The Open Liberty images in previous examples set <strong class="source-inline">USER</strong> to <strong class="source-inline">1001</strong>, which means that any of the previous examples based on it will not run using the <strong class="source-inline">root</strong> account, but the one based on the Java image would. One problem with the previous Dockerfile examples is that the <strong class="source-inline">ADD</strong> and <strong class="source-inline">COPY</strong> instructions write to<a id="_idIndexMarker863"/> the files so they are owned by the <strong class="source-inline">root</strong> user, which can cause issues at runtime. This can be resolved by updating the <strong class="source-inline">ADD</strong> or <strong class="source-inline">COPY</strong> instructions to change the ownership as they are written:</p>
			<p class="source-code">FROM open-liberty:full-java11-openj9</p>
			<p class="source-code">ADD --chown=1001:0 src/main/config/liberty/server.xml/config</p>
			<p class="source-code">ADD --chown=1001:0 target/myapp.war /config/dropins</p>
			<p>Alternatively, the <strong class="source-inline">RUN</strong> instruction can be used to execute the <strong class="source-inline">chown</strong> command-line tool. This will create a new layer but might be required if the <strong class="source-inline">ADD</strong> or <strong class="source-inline">COPY</strong> instruction moves multiple files and only some should have their ownership changed.</p>
			<p>Although Dockerfiles are the most used way to create a container image, there are alternative ways to build container images. A few examples follow.</p>
			<h3>Source-to-Image</h3>
			<p><strong class="bold">Source-to-Image</strong> (<strong class="bold">S2I</strong>) is a technology<a id="_idIndexMarker864"/> that converts an application source into a container image. Instead of creating a <strong class="bold">Dockerfile</strong>, an <strong class="bold">S2I</strong> builder ingests the source code, runs the build, and <a id="_idIndexMarker865"/>encodes it in a container image. This allows the developer to focus on the application code and not the creation of the container. By encoding the best practices for building a container in an <strong class="bold">S2I</strong> builder, it can be reused across applications, making it more likely that a set of applications all have well-designed container images. There are <strong class="bold">S2I</strong> builders for many languages and frameworks, including Open Liberty. <strong class="bold">S2I</strong> is an open source technology created by Red Hat to help developers adopt OpenShift, although it can be, and is, used to create containers that can run anywhere.</p>
			<h3>Cloud Native Buildpacks</h3>
			<p><strong class="bold">Buildpacks</strong> were originally created by Heroku to simplify deploying applications into their <strong class="bold">Platform as a Service</strong> (<strong class="bold">PaaS</strong>). It was picked<a id="_idIndexMarker866"/> up and used by Cloud Foundry. Buildpacks predate container technology, meaning they <a id="_idIndexMarker867"/>don't create <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>) compatible <a id="_idIndexMarker868"/>container images. Recently, though, an effort has been underway to shift Buildpacks into the container world with <strong class="bold">Cloud Native Buildpacks</strong>. These work similarly to <strong class="bold">S2I</strong> in that they encode the <a id="_idIndexMarker869"/>creation of the container image in code rather than a Dockerfile, in a way that can be used between and across applications. Unlike <strong class="bold">S2I</strong> builders, Buildpacks do not start from the application source, but the application artifact, for example, a <strong class="source-inline">WAR</strong> file.</p>
			<p>Having created a container image, the next step is to run it. While a developer might use Docker to run the image on their desktop when running in production, the most common way to run container images is with Kubernetes, which we will discuss in the next section.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor151"/>Deploying cloud-native applications to Kubernetes</h1>
			<p><strong class="bold">Kubernetes</strong> started as a project in Google to <a id="_idIndexMarker870"/>allow them to manage software at scale. It has since moved to become an open source<a id="_idIndexMarker871"/> project managed by the <strong class="bold">Cloud Native Computing Foundation </strong>(<strong class="bold">CNCF</strong>) and has contributors from all over the industry. Every major (and most minor) public cloud provider <a id="_idIndexMarker872"/>uses Kubernetes to manage the deployment of containers. There are also private cloud products such as Red Hat OpenShift that provide a distribution of Kubernetes for deployment either on-premises or on a public cloud but dedicated to a single company.</p>
			<p>A Kubernetes deployment is<a id="_idIndexMarker873"/> known as a <strong class="bold">cluster</strong>. To run containers and provide a highly available, scalable environment a cluster consists of a control plane and a set of key resources that provide it with the ability to run or manage containers, scale them, and keep the containers running in the event of any failures. When running a container in Kubernetes, the container is placed in a Pod, which is then run on a node based on decisions made by the control plane.</p>
			<p>A <strong class="bold">Pod</strong> provides a shared <a id="_idIndexMarker874"/>context for running a set of containers. All the containers in a Pod run on the same node. Although <a id="_idIndexMarker875"/>you can run multiple containers in a Pod, normally a Pod will contain a single application container, and any <a id="_idIndexMarker876"/>other containers running in the Pod will be sidecars providing some administrative or support function to the application container.</p>
			<p>In a traditional automated operation environment, the automation will describe how to set the environment up. When using Kubernetes instead of describing how to set up the environment, the description describes the desired end state, and it is up to the control plane to decide how to make this happen. The configuration for this is provided as one (or, more often, a set of) YAML document(s). The net effect of this is when deploying to Kubernetes, you do not describe the deployment by defining pods on a node and putting a container into it. Instead, you <a id="_idIndexMarker877"/>define a <strong class="bold">Deployment</strong> that will express the container you want to deploy and how many replicas of the container should be created. A simple deployment of a single instance of Open Liberty can be done with this YAML:</p>
			<p class="source-code">apiVersion: apps/v1</p>
			<p class="source-code">kind: Deployment</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: demo</p>
			<p class="source-code">  name: demo</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  replicas: 1</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    matchLabels:</p>
			<p class="source-code">      app: demo</p>
			<p class="source-code">  template:</p>
			<p class="source-code">    metadata:</p>
			<p class="source-code">      labels:</p>
			<p class="source-code">        app: demo</p>
			<p class="source-code">    spec:</p>
			<p class="source-code">      containers:</p>
			<p class="source-code">      - image: openliberty/open-liberty:full-java11-openj9-</p>
			<p class="source-code">  ubi</p>
			<p class="source-code">        name: open-liberty</p>
			<p>This YAML can then be <a id="_idIndexMarker878"/>deployed using the <strong class="source-inline">kubectl apply</strong> command. This results in a single Pod running Open Liberty being deployed. While the container is running and could respond to HTTP requests, there is no route for<a id="_idIndexMarker879"/> network traffic to get to the container. The key to enabling network traffic to reach a deployment is the Kubernetes <strong class="bold">Service</strong>. The <a id="_idIndexMarker880"/>Service defines the port the process in the container is listening on and the port it should be accessed via the Kubernetes networking stack. A Service for this can be defined using this YAML:</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">kind: Service</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: demo</p>
			<p class="source-code">  name: demo</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  ports:</p>
			<p class="source-code">  - name: 9080-9080</p>
			<p class="source-code">    port: 9080</p>
			<p class="source-code">    protocol: TCP</p>
			<p class="source-code">    targetPort: 9080</p>
			<p class="source-code">  selector:</p>
			<p class="source-code">    app: demo</p>
			<p class="source-code">  type: ClusterIP</p>
			<p>A Service allows other containers running Kubernetes to access it, but it doesn't allow services outside the cluster to access it. There are several options for how to expose the container externally, for <a id="_idIndexMarker881"/>example, port forwarding, an ingress controller, or OpenShift has the concept of <strong class="bold">routes</strong>. A route essentially just <a id="_idIndexMarker882"/>exposes a service<a id="_idIndexMarker883"/> externally to the cluster. You can specify the host and path, or you can let Kubernetes default it. To expose this Open Liberty server externally, you can define a route using this YAML:</p>
			<p class="source-code">kind: Route</p>
			<p class="source-code">apiVersion: route.openshift.io/v1</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo</p>
			<p class="source-code">  labels:</p>
			<p class="source-code">    app: demo</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  to:</p>
			<p class="source-code">    kind: Service</p>
			<p class="source-code">    name: demo</p>
			<p class="source-code">    weight: 100</p>
			<p class="source-code">  port:</p>
			<p class="source-code">    targetPort: 9080-9080</p>
			<p>These three YAML files have deployed a container and exposed it externally so it can be accessed and used. The three YAML files can be placed in a single file using <strong class="source-inline">---</strong> as a separator, but there is an alternative option for managing deployment than using YAML to configure everything, and that option is to use an Operator. </p>
			<p>An <strong class="bold">Operator</strong> is a way of <a id="_idIndexMarker884"/>packaging, deploying, and managing a set of resources related to an application. They were originally intended to help manage stateful applications where just throwing away and starting a new Pod might result in data loss; however, they can also be used to simplify the deployment of applications. Operators watch for the<a id="_idIndexMarker885"/> definition of a <strong class="bold">Custom Resource</strong> that it understands and configures the relevant Kubernetes resources to run that application. An Operator can do things such as managing the deployment and updating of an application when new images become available. Open Liberty<a id="_idIndexMarker886"/> provides an Operator that <a id="_idIndexMarker887"/>can manage the deployment of applications built on Open Liberty. As an example, all the previous YAML files can be simply replaced with this YAML:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: my-liberty-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-</p>
			<p class="source-code">    openj9-ubi</p>
			<p class="source-code">  service:</p>
			<p class="source-code">    type: ClusterIP</p>
			<p class="source-code">    port: 9080</p>
			<p class="source-code">  expose: true</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor152"/>MicroProfile Health in Kubernetes</h2>
			<p>The MicroProfile Health <a id="_idIndexMarker888"/>specification allows you to configure support for liveness and<a id="_idIndexMarker889"/> readiness checks. These probes allow the Kubernetes control plane to understand the health of the container and what action to take. An unsuccessful liveness probe will trigger the Pod to be recycled as it indicates that a problem has occurred that cannot be resolved. A readiness probe, on the other hand, will simply cause Kubernetes to stop routing traffic to the Pod. In both cases, you need to have multiple instances to ensure that during any outage of one container, the clients will remain unaware. To <a id="_idIndexMarker890"/>configure these liveness and readiness probes, you ensure that the Open Liberty server is configured to run MicroProfile Health:</p>
			<p class="source-code">&lt;server&gt;</p>
			<p class="source-code">    &lt;featureManager&gt;</p>
			<p class="source-code">        &lt;feature&gt;mpHealth-3.0&lt;/feature&gt;</p>
			<p class="source-code">    &lt;/featureManager&gt;</p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>Then, when defining the <a id="_idIndexMarker891"/>application, configure the liveness and readiness probes:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: my-liberty-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  <strong class="bold">readinessProbe</strong>:</p>
			<p class="source-code">    httpGet:</p>
			<p class="source-code">      path: /health/ready</p>
			<p class="source-code">      port: 9080</p>
			<p class="source-code">    initialDelaySeconds: 30</p>
			<p class="source-code">  <strong class="bold">livenessProbe</strong>:</p>
			<p class="source-code">    httpGet:</p>
			<p class="source-code">      path: /health/live</p>
			<p class="source-code">      port: 9080</p>
			<p class="source-code">    initialDelaySeconds: 90</p>
			<p class="source-code">  replicas: 1</p>
			<p>This configures the liveness <a id="_idIndexMarker892"/>and readiness probes to make an HTTP <strong class="source-inline">get</strong> request to the MicroProfile health endpoints for liveness and readiness. It also configures a wait period after the container starts <a id="_idIndexMarker893"/>before making the first check. This gives the container a chance to run any startup routines before it starts polling to determine the status. </p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor153"/>MicroProfile Config in Kubernetes</h2>
			<p><strong class="bold">MicroProfile Config</strong> provides a way to receive<a id="_idIndexMarker894"/> configuration in your application that can be<a id="_idIndexMarker895"/> provided in the environment. In Kubernetes, this kind of configuration is typically stored in a <strong class="bold">ConfigMap</strong> or a <strong class="bold">Secret</strong>. As discussed previously, in <a href="B17377_05_Final_SB_epub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Enhancing Cloud-Native Applications</em>, a ConfigMap is essentially a set of key/value pairs stored in Kubernetes that can be bound into a Pod so it is available to the container. To receive configuration in your application from Kubernetes, ensure that the Open Liberty server is configured to run MicroProfile Config:</p>
			<p class="source-code">&lt;server&gt; </p>
			<p class="source-code">  &lt;featureManager&gt; </p>
			<p class="source-code">    &lt;feature&gt;mpConfig-2.0&lt;/feature&gt; </p>
			<p class="source-code">  &lt;/featureManager&gt; </p>
			<p class="source-code">&lt;/server&gt;</p>
			<p>There are many ways to create a ConfigMap, and <a href="B17377_05_Final_SB_epub.xhtml#_idTextAnchor091"><em class="italic">Chapter 5</em></a>, <em class="italic">Enhancing Cloud-Native Applications</em>, demonstrated one mechanism. Another way to define a ConfigMap is to apply a ConfigMap with the following YAML:</p>
			<p class="source-code">kind: ConfigMap</p>
			<p class="source-code">apiVersion: v1</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo</p>
			<p class="source-code">data:</p>
			<p class="source-code">  example.property.1: hello</p>
			<p class="source-code">  example.property.2: world</p>
			<p>Now, when deploying your application, you can<a id="_idIndexMarker896"/> either bind a single environment variable from this ConfigMap, or all of them. To bind the value of <strong class="source-inline">example.property.1</strong> from the ConfigMap as a variable called <strong class="source-inline">PROP_ONE</strong> to an Open <a id="_idIndexMarker897"/>Liberty application, you would use the following YAML:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  env:</p>
			<p class="source-code">    - name: PROP_ONE</p>
			<p class="source-code">      valueFrom:</p>
			<p class="source-code">        configMapKeyRef:</p>
			<p class="source-code">          key: example.property.1</p>
			<p class="source-code">          name: demo</p>
			<p class="source-code">  replicas: 1</p>
			<p>A ConfigMap could (as in the aforementioned example) contain a lot of properties that the container may need to access, instead of binding a single entry, or entries one by one, you can bind all the <a id="_idIndexMarker898"/>entries. The following YAML would define an application with all the values of the ConfigMap <a id="_idIndexMarker899"/>bound:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  envFrom:</p>
			<p class="source-code">    - configMapRef:</p>
			<p class="source-code">        name: demo</p>
			<p class="source-code">  replicas: 1</p>
			<p>One of the more recent features of MicroProfile Config is the concept of configuration profiles. The idea is that you can provide the configuration for running the application in development, test, and production environments and have MicroProfile Config only load the configuration for the desired profile. To configure this, you also need to define the config profile. The MicroProfile Config specification says that a property in a profile name starts with <strong class="source-inline">%&lt;profile name&gt;</strong>; however, <strong class="source-inline">%</strong> isn't valid in an environment variable name so it is replaced with <strong class="source-inline">_</strong>. Example YAML for this is the following:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  env:</p>
			<p class="source-code">    - name: mp.config.profile</p>
			<p class="source-code">      value: dev</p>
			<p class="source-code">  envFrom:</p>
			<p class="source-code">    - configMapRef:</p>
			<p class="source-code">        name: dev</p>
			<p class="source-code">      prefix: '_dev.'</p>
			<p class="source-code">    - configMapRef:</p>
			<p class="source-code">        name: test</p>
			<p class="source-code">      prefix: '_test.'</p>
			<p class="source-code">    - configMapRef:</p>
			<p class="source-code">        name: prod</p>
			<p class="source-code">      prefix: '_test.'</p>
			<p class="source-code">  replicas: 1</p>
			<p>ConfigMaps in Kubernetes are <a id="_idIndexMarker900"/>good for storing data that isn't sensitive, but when it comes to storing API keys, credentials, and so on, Kubernetes has an <a id="_idIndexMarker901"/>alternative concept known as Secrets. Secrets can<a id="_idIndexMarker902"/> represent multiple different kinds of Secrets, but here we are just going to consider simple key/value pairs. The Kubernetes platform provides better protection for Secrets than ConfigMaps, although many people prefer to use third-party products for Secret management. It is still good to understand how Secrets work because third-party products tend to follow the same conventions to access sensitive data from within the container.</p>
			<p>Secrets are encoded using base64 encoding, which isn't fantastic protection. Open Liberty allows passwords it loads to be AES encrypted, and provides an API for decrypting protected strings, so your base64-encoded secret could be a base64-encoded AES encrypted string. However, since you would still need to provide the decryption key to Open Liberty and this is not a security hardening book, we will not go into further details here. Referencing<a id="_idIndexMarker903"/> a single key pair from a secret from deployment is done almost identically to referencing from a ConfigMap, but using <strong class="source-inline">secretKeyRef</strong> rather than <strong class="source-inline">configMapKeyRef</strong>; for example, with this YAML:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  env:</p>
			<p class="source-code">    - name: PROP_ONE</p>
			<p class="source-code">      valueFrom:</p>
			<p class="source-code">        secretKeyRef:</p>
			<p class="source-code">          key: my_secret</p>
			<p class="source-code">          name: secret.config</p>
			<p class="source-code">  replicas: 1</p>
			<p>If you deploy the secret YAML and bind to it as in the aforementioned example, your container will have an environment<a id="_idIndexMarker904"/> variable called <strong class="source-inline">PROP_ONE</strong> whose value is <strong class="source-inline">super secret</strong>.</p>
			<p>Just like with a ConfigMap, you can bind all the key/value pairs in a secret to the container, and just like with the prior example, it is done in a very similar way to ConfigMaps:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  envFrom:</p>
			<p class="source-code">    - secretRef:</p>
			<p class="source-code">        name: secret.config</p>
			<p class="source-code">  replicas: 1</p>
			<p>Secrets can also be bound as<a id="_idIndexMarker905"/> files in the container file system and this can be preferable for security-sensitive data. When you do this, the secret will be defined in the file system and the value of the secret will be the content of the file. MicroProfile Config cannot consume secrets bound<a id="_idIndexMarker906"/> this way, but it provides a way to add additional ConfigSources, allowing you to easily load the configuration. The YAML for binding a secret to the file system is to essentially mount it as a volume. The following example YAML will result in every key/value in the secret <strong class="source-inline">secret.config</strong> being mounted as a file in the file system of the container under the directory <strong class="source-inline">/my/secret</strong>:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  volumeMounts:</p>
			<p class="source-code">    - mountPath: /my/secrets</p>
			<p class="source-code">      name: secret</p>
			<p class="source-code">  volumes:</p>
			<p class="source-code">    - secret:</p>
			<p class="source-code">        secretName: secret.config</p>
			<p class="source-code">      name: secret</p>
			<p class="source-code">  replicas: 1</p>
			<p>To enable the injection of<a id="_idIndexMarker907"/> the bound Secrets, you need a <strong class="bold">ConfigSource</strong> that will read<a id="_idIndexMarker908"/> these properties. A ConfigSource is registered <a id="_idIndexMarker909"/>using the <strong class="bold">ServiceLoader</strong> pattern, so placing the classname in <strong class="source-inline">META-INF/services/org.eclipse.microprofile.config.spi.ConfigSource</strong> on the application classpath will <a id="_idIndexMarker910"/>automatically cause it to be loaded. A short example ConfigSource that will do this is the following:</p>
			<p class="source-code">public class FileSystemConfigSource implements ConfigSource {</p>
			<p class="source-code">    private File dir = new File("/my/secrets");</p>
			<p class="source-code">    public Set&lt;String&gt; getPropertyNames() {</p>
			<p class="source-code">        return Arrays.asList(dir.listFiles())</p>
			<p class="source-code">                     .stream()</p>
			<p class="source-code">                     .map(f -&gt; f.getName())</p>
			<p class="source-code">                     .collect(Collectors.toSet());</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public String getValue(String s) {</p>
			<p class="source-code">        File f = new File(dir, s);</p>
			<p class="source-code">        try {</p>
			<p class="source-code">            if (f.exists())</p>
			<p class="source-code">                Path p = f.toPath();</p>
			<p class="source-code">                byte[] secret = Files.readAllBytes(f);</p>
			<p class="source-code">                return new String(secret,                                   StandardCharsets.UTF_8);</p>
			<p class="source-code">        } catch (IOException ioe) {</p>
			<p class="source-code">        }</p>
			<p class="source-code">        return null;</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public String getName() {</p>
			<p class="source-code">        return "kube.secret";</p>
			<p class="source-code">    }</p>
			<p class="source-code">    public int getOrdinal() {</p>
			<p class="source-code">        return 5;</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>This config source will load the<a id="_idIndexMarker911"/> content of a file with the property name read from a well-defined directory. If the file cannot be read, it will act as if the property is not defined. Kubernetes will update the file content when the secret is updated, which means that updates can be visible to the application automatically since this code <a id="_idIndexMarker912"/>will reread the file each time the property is read.</p>
			<p>In the next section, we will discuss some considerations when using a Service Mesh with MicroProfile.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor154"/>MicroProfile and Service Mesh</h1>
			<p>When deploying into a Kubernetes<a id="_idIndexMarker913"/> cluster, some people choose to make use of a Service Mesh. The goal of a Service Mesh is to move certain considerations of<a id="_idIndexMarker914"/> microservices out of the application code and to place it around the application. A Service Mesh can remove some application<a id="_idIndexMarker915"/> concerns such as service selection, observability, fault tolerance, and, to a certain degree, security. One common Service Mesh technology is <strong class="bold">Istio</strong>. The way Istio works is by inserting a sidecar into the Pod for the containers and all inbound and network traffic is routed via that sidecar. This allows the sidecar to perform activities applying access control policies, routing requests to downstream services, and applying fault tolerance policies such as retrying requests or timing them out. Some of these capabilities overlap with some of the MicroProfile capabilities, for example, Istio can handle attaching and propagating OpenTracing data with requests. If you use <strong class="bold">Istio</strong>, you clearly do not need to make use of MicroProfile OpenTracing, although using both <a id="_idIndexMarker916"/>would complement each other rather than cause conflict. </p>
			<p>One area where the use of a Service Mesh and MicroProfile can conflict in negative ways is fault tolerance. For example, if you configure 5 retries in MicroProfile and 5 retries in Istio and they all fail, you will end up with a total of 25 retries. As a result, it is common when using a Service Mesh to disable the MicroProfile fault tolerance capabilities. This can be done with the environment variable of <strong class="source-inline">MP_Fault_Tolerance_NonFallback_Enabled</strong> set to <strong class="source-inline">false</strong>. This will disable all the MicroProfile fault tolerance support except for the fallback capability. This is because the logic to perform on a failure is intrinsically an application consideration and not something that can be extracted into the Service Mesh. This can simply be disabled using the following YAML:</p>
			<p class="source-code">apiVersion: openliberty.io/v1beta1</p>
			<p class="source-code">kind: OpenLibertyApplication</p>
			<p class="source-code">metadata:</p>
			<p class="source-code">  name: demo-app</p>
			<p class="source-code">spec:</p>
			<p class="source-code">  expose: true</p>
			<p class="source-code">  applicationImage: openliberty/open-liberty:full-java11-    openj9-ubi</p>
			<p class="source-code">  env:</p>
			<p class="source-code">    - name: MP_Fault_Tolerance_NonFallback_Enabled</p>
			<p class="source-code">      value: 'false'</p>
			<p class="source-code">  replicas: 1 </p>
			<p>This configures<a id="_idIndexMarker917"/> the application to have a hardcoded environment <a id="_idIndexMarker918"/>variable that disables the non-fallback MicroProfile Fault Tolerance behaviors. This could also be done with a ConfigMap.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor155"/>Summary</h1>
			<p>In this chapter, we have reviewed the MicroProfile implementation that is used in the rest of the book, some best practices for building a container for a MicroProfile application, and how to deploy that application into Kubernetes. While the chapter isn't an exhaustive review of the capabilities available in Kubernetes, it does focus on the particular considerations for deploying MicroProfile applications to Kubernetes, and why they interact with Kubernetes services. This chapter should have provided you with a good starting point for creating and deploying MicroProfile applications using Open Liberty, containers, and Kubernetes.</p>
			<p>The next chapter will describe an example application that makes use of MicroProfile for a set of microservices deployed in a container to a Kubernetes cluster. </p>
		</div>
	</body></html>