- en: Chapter 8. Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover WildFly's clustering capabilities. The term cluster
    is used to describe a system split over several machines. Having the components
    of a system synchronize over multiple machines generally improves performance
    and availability.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering serves as an essential component to providing scalability and high
    availability to your applications. One major benefit of using clustering is that
    you can spread the traffic load across several AS instances via **load balancing**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Load balancing is an orthogonal aspect of your enterprise application and is
    generally achieved by using a properly configured web server in front of the application
    server. For this reason, load balancing is discussed in the next chapter while,
    in this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: All available options to set up a WildFly cluster either using a standalone
    configuration or a domain of servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to effectively configure the various components required for clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **JGroups** subsystem, which is used for the underlying communication between
    nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Infinispan** subsystem, which handles the cluster consistency using its
    advanced data grid platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Messaging** subsystem, which uses the HornetQ clusterable implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a WildFly cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the benefit of impatient readers, we will immediately show you how to get
    a cluster of WildFly nodes up and running.
  prefs: []
  type: TYPE_NORMAL
- en: All you have to do to shape a new server profile is create a new XML configuration
    file. As the standalone server holds just a single profile, you will likely want
    to use either the configuration file named `standalone-ha.xml` or `standalone-full-ha.xml`.
    Both of these ship with WildFly. This configuration file contains all the clustering
    subsystems.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a domain server is able to store multiple profiles in the
    core `domain.xml` configuration file, hence you can use this file both for clustered
    domains and for nonclustered domain servers.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Clustering and domains are two separate concepts, the functionality of each
    does not overlap. While the aim of clustering is to provide scalability, load
    balancing, and high availability, a domain is a logical grouping of servers that
    share a centralized domain configuration and can be managed as a single unit.
  prefs: []
  type: TYPE_NORMAL
- en: We will now describe the different ways to assemble and start a cluster of standalone
    servers and domain servers.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a cluster of standalone servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Configuring WildFly clusters for standalone servers can be broken down into
    two main possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: A cluster of WildFly nodes running on different machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster of WildFly nodes running on the same machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: A cluster of nodes running on different machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you decide to install each WildFly server on a dedicated machine, you are
    *horizontally scaling* your cluster. In terms of configuration, this requires
    the least effort—all you have to do is bind the server to its IP address in the
    configuration file, and start the server using the `standalone-ha.xml` configuration.
    Let''s build an example with a simple, two-node cluster as illustrated in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A cluster of nodes running on different machines](img/6232OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Open the `standalone-ha.xml` file on each WildFly distribution, and navigate
    to the `interfaces` section. Within the nested interface element, insert the IP
    address of the standalone server. For the first machine (`192.168.10.1`), we will
    define the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'On the second machine (`192.168.10.2`), we will bind to the other IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the only thing you need to change in your configuration. To start the
    cluster, you have to start your standalone server using the `standalone-ha.xml`
    configuration file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Rather than updating the `standalone-ha.xml` file with the IP address of each
    server, you can use the `-b` option, which allows you to provide the binding IP
    address on server startup. In addition, you can use the `-bmanagement` flag to
    specify the management-interface address. Using these options, the previous configuration
    for the first server can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For the second server, it can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Within a few seconds, your servers will be running; however, we have not mentioned
    any details relating to clustering nodes in the console. This is because, in WildFly,
    the core services are only started on demand. This means the clustering services
    are started only when the server detects that they are required and are stopped
    when no longer required. Hence, simply starting the server with a configuration
    that includes the clustering subsystems will not initiate the clustering services.
    To do this, we will need to deploy a cluster-enabled application.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in order to verify our installation, we will deploy a bare-bones, cluster-enabled,
    web application named `Example.war`. To enable clustering of your web applications,
    you must mark them as *distributable* in the `web.xml` descriptor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When you have deployed the application to both machines, you will see that
    the clustering services are now started and that each machine is able to find
    other members within the cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A cluster of nodes running on different machines](img/6232OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A cluster of nodes running on the same machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second variant of the standalone configuration comes into play when your
    server nodes are located (all or some of them) on the same machine. This scenario
    generally applies when you are scaling your architecture *vertically* by adding
    more hardware resources to your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuring server nodes on the same machine obviously requires duplicating
    your WildFly distribution on your filesystem. In order to avoid port conflicts
    between server distributions, you have to choose between the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Define multiple IP address on the same machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a port offset for each server distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a cluster on the same machine using multiple IP addresses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is also known as **multihoming** and requires a small amount of configuration
    to get working. Each operating system uses a different approach to achieve this.
    Illustrating the possible ways to configure multihoming is outside the scope of
    this book but, if you are interested in multihoming, we have provided links with
    detailed instructions on how to set up multihoming on Linux and Windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Linux, this tutorial describes in detail how to assign multiple
    IPs to a single network interface, also known as **IP aliasing**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/](http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows users can refer to the following blog that details how to set up multihoming
    in Windows 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html](http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have set up your network interface correctly, you will need to update
    your `standalone-ha.xml` file. You need to bind each IP to a different WildFly
    instance, just as we did when setting up the multiple-host cluster. Within the
    configuration file, navigate to the `interfaces` section and, within the nested
    `interface` element, insert the IP address to be bound to that standalone server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the first server distribution is bound to the IP Address `192.168.10.1`
    and the second one to `192.168.10.2`. (remember that you can also use the `-b`
    and `-bmanagement` switches described earlier).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a cluster on the same machine using multiple IP addresses](img/6232OS_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a cluster on the same machine using port offset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Configuring multihoming is not always a viable choice, as it requires a relative
    amount of network administration experience. A simpler and more straightforward
    option is to define a port offset for each of your cluster members. By defining
    a port offset for each server, all the default-server binding interfaces will
    shift by a fixed number, hence you will not have two servers running on the same
    ports, causing port conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using port offset, you will bind each server to the same IP address. So,
    for all your server distributions, you will configure the `standalone-ha.xml`
    file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will then leave the first server configuration unchanged. It will use the
    default socket-binding ports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For the second server configuration, you will specify a `port-offset` value
    of `150`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Your cluster configuration is now complete. You can verify this by starting
    each server distribution by passing it as an argument to the configuration file
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the following screenshot, you can see that a port offset of 150 has been
    applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a cluster on the same machine using port offset](img/6232OS_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Setting up a cluster of domain servers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you are configuring a domain cluster, you will find that the clustering
    subsystems are already included within the main configuration file `domain.xml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a matter of fact, the WildFly domain deals with clustering just as another
    profile used by the application server. Opening the `domain.xml` file, you will
    see that the application server ships with the following four profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: The `default` profile for nonclustered environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ha` profile for clustered environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `full` profile with all the subsystems for nonclustered environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `full-ha` profile with all the subsystems for clustered environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, in order to use clustering on a domain, you have to first configure your
    server groups to point to one of the `ha` profiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example configuration that uses two server groups. The following
    code snippet is from `domain.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As highlighted in the `socket-binding-group` element, we are referencing the
    `ha-sockets` group, which contains all socket bindings used for the cluster. Have
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define the servers that are part of the domain (and of the
    cluster). To keep things simple, we will reuse the domain server list that is
    found in the default `host.xml` file, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We do not need to specify the socket-binding group for each server, as this
    was configured in the `domain.xml` file. If we want to override the socket-binding
    group, then we can add the following to the `host.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows an overview of this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up a cluster of domain servers](img/6232OS_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Your clustered domain can now be started using the standard batch script (`domain.sh`
    or `domain.bat`). The server groups will now point to the `ha` profile and form
    a cluster of two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting the cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Communication via nodes in a cluster is achieved via UDP and multicasts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A multicast is a protocol by which data is transmitted simultaneously to all
    hosts that are part of the multicast group. You can think about multicast as a
    radio channel where only those tuned to a particular frequency receive the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are having problems, typically it is due to one of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The nodes are behind a firewall. If your nodes are on different machines, then
    it is possible that the firewall is blocking the multicasts. you can test this
    by disabling the firewall for each node or adding the appropriate rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are using a home network or are behind a gateway. Typically, home networks
    will redirect any UDP traffic to the **Internet Service Provider** (**ISP**),
    which is then either dropped by the ISP or just lost. To fix this, you will need
    to add a route to the firewall/gateway that will redirect any multicast traffic
    back on to the local network instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mac OS X**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Mac, you may get a **java.io.IOException: Network is unreachable**
    error when trying to start a domain in the `ha` mode. To get around this, you
    will need to create a proper network route to use UDP as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To allow you to check whether your machine is set up correctly for multicast,
    JGroups ships with two test applications that can be used to test IP multicast
    communication. The test classes are `McastReceiverTest` and `McastSenderTest`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to test multicast communication on your server, you should first navigate
    to the location of the `jgroups` JAR within the `modules` directory, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`JBOSS_HOME/modules/system/layers/base/org/jgroups/main`'
  prefs: []
  type: TYPE_NORMAL
- en: Within this directory, you will find `jgroups-3.4.3.Final.jar`, which contains
    the test programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the `McastReceiverTest` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'On the same machine, but in a different terminal, run the `McastSenderTest`
    command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If multicast works correctly, you should be able to type in the `McastSenderTest`
    window and see the output in the `McastReceiverTest` window, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Troubleshooting the cluster](img/6232OS_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You should perform this test on each machine in the cluster. Once you have done
    this, you need to ensure that UDP communication works between each machine in
    the cluster by running `McastSenderTest` on one machine and `McastReceiverTest`
    on the other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you are experiencing issues with the default multicast address
    or port, you can change it by modifying the `jgroups-udp` socket-binding group
    within the `domain.xml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Configuring the WildFly cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'WildFly supports clustering out of the box. There are several libraries that
    work together to provide support for clustering. The following figure shows the
    basic clustering architecture adopted by WildFly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the WildFly cluster](img/6232OS_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The JGroups library is core to WildFly clustering. It provides the communication
    channels between nodes of the cluster using a multicast transmission. These channels
    are created upon deployment of a clustered application and are used to transmit
    sessions and contexts around the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Another important component of clustering is **Infinispan**. Infinispan handles
    the replication of your application data across the cluster by means of a distributed
    cache.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the JGroups subsystem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Within the realm of JGroups, nodes are commonly referred to as **members**,
    and clusters are referred to as **groups**.
  prefs: []
  type: TYPE_NORMAL
- en: A node is a process running on a host. JGroups keeps track of all processes
    within a group. When a node joins a group, the system sends a message to all existing
    members of that group. Likewise, when a node leaves or crashes, all the other
    nodes of that group are notified.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we outlined earlier in the chapter, the processes (nodes) of a group can
    be located on the same host, or on different machines on a network. A member can
    also be part of multiple groups. The following figure illustrates a detailed view
    of JGroups architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring the JGroups subsystem](img/6232OS_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A JGroups process broadly consists of three parts, namely a **Channel**, the
    **Building Blocks**, and the **Protocol Stack**.
  prefs: []
  type: TYPE_NORMAL
- en: A **Channel** is a simple socket-like interface used by application programmers
    to build reliable group communication applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Building Blocks** collectively form an abstraction interface layered on
    top of channels, which can be used instead of channels whenever a higher level
    interface is required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Protocol Stack** contains a number of protocol layers in a bidirectional
    list. All messages sent have to pass through all the protocols. A layer does not
    *necessarily* correspond to a transport protocol. For example, a fragmentation
    layer might break up a message into several smaller messages, adding a header
    with an ID to each fragment, and reassemble the fragments on the receiver's side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous figure, when sending a message, the `PING` protocol is executed
    first, then `MERGE2`, followed by `FD_SOCK`, and finally, the `FD` protocol. When
    the message is received, this order would be reversed, which means that it would
    meet the `FD` protocol first, then `FD_SOCK`, followed by `MERGE2`, and finally
    up to `PING`. In WildFly, the JGroups configuration is found within the JGroups
    subsystem in the main `standalone-ha.xml/domain.xml` configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the JGroups subsystem, you can find the list of configured transport
    stacks. The following code snippet shows the default UDP stack used for communication
    between nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: UDP is the default protocol for JGroups and uses multicast (or, if not available,
    multiple unicast messages) to send and receive messages. A multicast UDP socket
    can send and receive datagrams from multiple clients. Another feature of multicast
    is that a client can contact multiple servers with a single packet, without knowing
    the specific IP address of any of the hosts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Switching to the TCP protocol is as easy as changing the `default-stack` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: TCP stacks are typically used when IP multicasting cannot be used for some reason.
    For example, when you want to create a network over a WAN. We will cover TCP configuration
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed description of all JGroups protocols is beyond the scope of this
    book but, for convenience, you can find a short description of each in the following
    table. To find out more about these protocols, or about JGroups, you can refer
    to the JGroups site at [http://jgroups.org/manual/html/index.html](http://jgroups.org/manual/html/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Usage | Protocols |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Transport | This is responsible for sending and receiving messages across
    the network | `IDP`, `TCP`, and `TUNNEL` |'
  prefs: []
  type: TYPE_TB
- en: '| Discovery | This is used to discover active nodes in the cluster and determine
    who the coordinator is | `PING`, `MPING`, `TCPPING`, and `TCPGOSSIP` |'
  prefs: []
  type: TYPE_TB
- en: '| Failure detection | This one is used to poll cluster nodes to detect node
    failures | `FD`, `FD_SIMPLE`, `FD_PING`, `FD_ICMP`, `FD_SOCK`, and `VERIFY_SUSPECT`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Reliable delivery | This ensures that messages are actually delivered in
    the right order (FIFO) to the destination node | `CAUSAL`, `NAKACK`, `pbcast`.`NAKACK`,
    `SMACK`, `UNICAST`, and `PBCAST` |'
  prefs: []
  type: TYPE_TB
- en: '| Group membership | This is used to notify the cluster when a node joins,
    leaves, or crashes | `pbcast`.`GMS`, `MERGE`, `MERGE2`, and `VIEW_SYNC` |'
  prefs: []
  type: TYPE_TB
- en: '| Flow control | This is used to adapt the data-sending rate to the data-receipt
    rate among the nodes | `FC` |'
  prefs: []
  type: TYPE_TB
- en: '| Fragmentation | This fragments messages larger than a certain size and unfragments
    them at the receiver''s side | `FRAG2` |'
  prefs: []
  type: TYPE_TB
- en: '| State transfer | This one synchronizes the application state (serialized
    as a byte array) from an existing node with a newly-joining node | `pbcast`.`STATE_TRANSFER`
    and `pbcast`.`STREAMING_STATE_TRANSFER` |'
  prefs: []
  type: TYPE_TB
- en: Customizing the protocol stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to customize your transport configuration at a lower level, then
    you can override the default properties used by JGroups or even the single protocol
    properties. For example, the following configuration can be used to change the
    default send or receive buffer used by the JGroups UDP stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If you want to have a look at all the properties available within the JGroups
    subsystem, either at transport level or at the protocol level, you can consult
    the JGroups XSD file, `jboss-as-jgroups_2_0.xsd`, found in the `JBOSS_HOME/docs/schema`
    folder of your server distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Infinispan subsystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the requirements of a cluster is that data is synchronized across its
    members. This is because, should there be a failure of a node, the application
    and its session can continue on other members of the cluster. This is known as
    **High Availability**.
  prefs: []
  type: TYPE_NORMAL
- en: WildFly uses Infinispan as the distributed caching solution behind its clustering
    functionality. Although Infinispan is embedded within the application server,
    it can also be used as a standalone data-grid platform.
  prefs: []
  type: TYPE_NORMAL
- en: We will now quickly look at Infinispan's configuration, which is found in the
    Infinispan subsystem within the main `standalone-ha.xml` or `domain.xml` configuration
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the backbone of the Infinispan configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: One of the key differences between the standalone Infinispan configuration and
    the Infinispan subsystem within WildFly is that the WildFly configuration exposes
    multiple `cache-container` elements, while the native configuration file contains
    configurations for a single cache container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `cache-container` element contains one or more caching policies, which
    define how data is synchronized for that specific cache container. The following
    caching strategies can be used by cache containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Local**: In this caching mode, the entries are stored on the local node only,
    regardless of whether a cluster has formed. Infinispan typically operates as a
    local cache.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication**: In this caching mode, all entries are replicated to all nodes.
    Infinispan typically operates as a temporary data store and doesn''t offer increased
    heap space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution**: In this caching mode, the entries are distributed to a subset
    of the nodes only. Infinispan typically operates as a data grid providing increased
    heap space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Invalidation**: In this caching mode, the entries are stored in a cache store
    only (such as a database) and invalidated from all nodes. When a node needs the
    entry, it will load it from a cache store. In this mode, Infinispan operates as
    a distributed cache, backed by a canonical data store, such as a database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will have a more detailed look at some of the
    cache configurations, such as `session` caches (the `web` cache and the `SFSB`
    cache) and the `hibernate` cache. Understanding these is essential if you are
    to configure your clustered applications properly.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring session cache containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at the caching configuration for the HTTP session
    and for stateful and singleton-session beans. The way the caches are configured
    for these three is very similar. For this reason, we will discuss them together
    and show the similarities between them. So, here is the `cache-container` configuration
    for the `web` cache, the `ejb` cache, and the `server` cache. The `web` cache
    refers to the HTTP session cache, the `ejb` cache relates to stateful session
    beans (SFSBs), and the `server` cache relates to the singleton-session beans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The configuration for each container can contain one or more caching strategy
    elements. These elements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`replicated-cache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distributed-cache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`invalidation-cache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`local-cache`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of these cache elements can be defined zero or more times. To specify
    which cache element to use for the cache container, simply reference the name
    of the cache as the property of the `default-cache` attribute. In the next section,
    we will explore in detail the differences between these cache modes. Within each
    cache definition, you may have noticed the `locking` attribute that corresponds
    to the equivalent database isolation levels. Infinispan supports the following
    isolation levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NONE`: No isolation level means no transactional support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`READ_UNCOMMITTED`: The lowest isolation level, dirty reads are allowed, which
    means one transaction may see uncommitted data from another transaction. Rows
    are only locked during the writing of data, not for reads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`READ_COMMITTED`: The transaction acquires read and write locks on all retrieved
    data. Write locks are released at the end of the transaction, and read locks are
    released as soon as the data is selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`REPEATABLE_READ`: This is the default isolation level used by Infinispan.
    The transaction acquires read and write locks on all retrieved data and is kept
    until the end of the transaction. Phantom reads can occur. Phantom reads are when
    you execute the same query in the same transaction and get a different number
    of results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SERIALIZABLE`: The strictest isolation level. All transactions occur in an
    isolated fashion as if they are being executed serially (one after the other),
    as opposed to concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another element nested within the cache configuration is `file-store`. This
    element configures the path in which to store the cached data. The default data
    is written in the `jboss.server.data.dir` directory under a folder with the same
    name as the cache container.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following figure shows the default `file-store` path for the
    standalone `web` cache container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring session cache containers](img/6232OS_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If you wish, you can customize the `file-store` path using the `relative-to`
    and `path` elements, just as we did in [Chapter 2](ch02.html "Chapter 2. Configuring
    the Core WildFly Subsystems"), *Configuring the Core WildFly Subsystems*, for
    the path element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Before moving on, let's briefly look at the way messages are sent between each
    node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data synchronization across members can be done via synchronous messages (`SYNC`)
    or asynchronous messages (`ASYNC`), which are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous** messaging is the least efficient of the two, as each node needs
    to wait for a message acknowledgement from other cluster members. However, synchronous
    mode is useful if you have a need for high consistency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous** messaging is the quicker of the two, the flip side being that
    consistency suffers. Asynchronous messaging is particularly useful when HTTP session
    replication and sticky sessions are enabled. In this scenario, a session is always
    accessed from the same cluster node. Only when a node fails is the data accessed
    from a different node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The SYNC and ASYNC properties are set in the `mode` attribute of the cache
    element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Choosing between replication and distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using **replicated** caching, Infinispan will store every entry on every
    node in the cluster grid. This means that entries added to any one of these cache
    instances will be replicated to all other cache instances in the cluster, and
    any entry can be retrieved from any cache. The arrows indicate the direction in
    which data is being replicated. In the following figure, you can see that session
    data from Node 1 is being copied to Nodes 2, 3, and 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing between replication and distribution](img/6232OS_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The scalability of replication is a function of cluster size and average data
    size. If we have many nodes and/or large data sets, we hit a scalability ceiling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If `DATA_SIZE * NUMBER_OF_HOSTS` is smaller than the memory available to each
    host, then replication is a viable choice.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, when using **distributed** caching, Infinispan will store
    every cluster entry on a subset of the nodes in the grid.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution makes use of a consistent-hash algorithm to determine where entries
    should be stored within the cluster. You can configure how many copies of a cache
    entry are maintained across the cluster. The value you choose here is a balance
    between performance and durability of data. The more copies you maintain, the
    lower the performance, but the lower the risk of losing data due to server outages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use the `owners` parameter (with a default value of `2`) to define
    the number of cluster-wide copies for each cache entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows how the session data will be replicated across the
    nodes when the `owners` parameter is set to `2`. Each node replicates its session
    data to two other nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing between replication and distribution](img/6232OS_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The choice between replication and distribution depends largely on the cluster
    size. For example, replication provides a quick and easy way to share states across
    a cluster; however, it only performs well in small clusters (fewer than ten servers).
    This is due to the increased number of replication messages that need to be sent
    as cluster size increases. In a distributed cache, several copies of an entry
    are maintained across nodes in order to provide redundancy and fault tolerance.
    The number of copies saved is typically far fewer than the number of nodes in
    the cluster. This means a distributed cache provides a far greater degree of scalability
    than a replicated cache.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the hibernate cache
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The hibernate cache container is a key part of your configuration as it handles
    the caching of your data tier. WildFly uses hibernate as the default JPA implementation,
    so the concepts described in this chapter apply both to hibernate applications
    and to JPA-based applications. Hibernate caches are conceptually different from
    session-based caches. They are based on the assumption that you have a permanent
    storage for your data (the database) This means that it is not necessary to replicate
    or distribute copies of the entities across the cluster in order to achieve high
    availability. You just need to inform your nodes when data has been modified so
    that the entry in the cache can be invalidated. If a cache is configured for invalidation
    rather than replication, every time data is changed in a cache, other caches in
    the cluster receive a message that their data is now stale and should be evicted
    from memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefit of this is twofold. First, network traffic is minimized as invalidation
    messages are very small compared to replicating updated data. Second, caches in
    the cluster will only need to do database lookups when data is stale. Whenever
    a new entity or collection is read from database, it''s only cached **locally**
    in order to reduce traffic between nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `local-query` cache is configured by default to store up to 10,000 entries
    in an LRU vector. Each entry will be evicted from the cache automatically if it
    has been idle for 100,000 milliseconds, as per the `max-idle` attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the eviction strategies supported by Infinispan:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NONE`: This value disables the eviction thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UNORDERED`: This is now deprecated. Using this value will cause the LRU to
    be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LRU`: This value causes evictions to occur based on a **least-recently-used**
    pattern'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LIRS`: This value addresses shortcomings of LRU. Eviction relies on inter-reference
    recency of cache entries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read more about how LIRS works, see the Infinispan documentation at [http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies](http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the local cache entity is updated, the cache will send a message to other
    members of the cluster, telling them that the entity has been modified. This is
    when the `invalidation-cache` comes into play. Take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The default configuration for the invalidation cache uses the same eviction
    and expiration settings as for local query cache. The maximum number of entries
    is set to 10,000 and the idle time before expiration to 100,000 milliseconds.
    The invalidation cache can also be configured to be synchronous (`SYNC`) or asynchronous
    (`ASYNC`). If you configure your invalidation cache to be synchronous, then your
    cache will be blocked until all caches in the cluster receive responses to invalidation
    messages. On the other hand, an asynchronous invalidation cache does not block
    and wait for a response, which results in increased performance. By default, hibernate
    is configured to use `REPEATABLE_READ` as the cache isolation level. For most
    cases, the default isolation level of `REPEATABLE_READ` will suffice. If you want
    to update it to, say, `READ_COMMITTED`, then you will need to add the following
    to your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The last bit of configuration we are going to look at within the Infinispan
    subsystem is the `timestamp` cache. The `timestamp` cache keeps track of the last
    time each database table was updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `timestamp` cache is strictly related to the query cache. It is used to
    store the result set of a query run against the database. If the query cache is
    enabled, before a query run, the query cache is checked. If the timestamp of the
    last update on a table is greater than the time the query results were cached,
    the entry is evicted from the cache and a fresh database lookup is made. This
    is referred to as a cache miss. Have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: By default, the `timestamps` cache is configured with asynchronous replication
    as the clustering mode. Since all cluster nodes must store all timestamps, local
    or invalidated cache types are not allowed, and no eviction/expiration is allowed
    either.
  prefs: []
  type: TYPE_NORMAL
- en: Using replication for the hibernate cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There may be situations when you want to replicate your entity cache across
    other cluster nodes, instead of using local caches and invalidation. This may
    be the case when:'
  prefs: []
  type: TYPE_NORMAL
- en: Queries executed are quite expensive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queries are likely to be repeated in different cluster nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queries are unlikely to be invalidated out of the cache (Hibernate invalidates
    query results from the cache when one of the entity classes involved in the query's
    `WHERE` clause changes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In order to switch to a replicated cache, you have to configure your `default-cache`
    attribute, as shown in the following code snippet, as well as add the relevant
    `replicated-cache` configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Advanced Infinispan configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Until now, we looked at the essential components required to get working with
    a clustered application. Infinispan has a wealth of options available to further
    customize your cache.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on advanced configuration of Infinispan via the Infinispan
    subsystem, you can check out the documentation at [http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html](http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html)
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Infinispan transport
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Infinispan subsystem relies on the JGroups subsystem to transport cached
    data between nodes. JGroups uses UDP as the default transport protocol as defined
    by the `default-stack` attribute in the JGroups subsystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can, however, configure a different transport for each cache container.
    If you want to use TCP as the transport protocol for the web cache container,
    then you can add the `stack` attribute and set it to `tcp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The default UDP transport is usually suitable for large clusters. It may also
    be suitable if you are using replication or invalidation, as it minimizes opening
    too many sockets.
  prefs: []
  type: TYPE_NORMAL
- en: To learn about the differences between TCP and UDP, please refer to this external
    link at [http://www.skullbox.net/tcpudp.php](http://www.skullbox.net/tcpudp.php).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Infinispan threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is important to note that the thread-pool subsystem has been deprecated
    in WildFly 8\. It is quite likely that it will be removed completely in WildFly
    9\. The configuration in this section can still be used in WildFly 8, but you
    will need to add the threads subsystem to your configuration file. Take a look
    at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as you can for JGroups transport, you can externalize your Infinispan
    thread configuration, moving it into the thread-pool subsystem. The following
    thread pools can be configured per cache-container:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Thread pool | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| transport | This gives the size of the bounded thread pool whose threads
    are responsible for transporting data across the network |'
  prefs: []
  type: TYPE_TB
- en: '| listener-executor | This gives the size of the thread pool used for registering
    and getting notified when some cache events take place |'
  prefs: []
  type: TYPE_TB
- en: '| replication-queue-executor | This gives the size of the scheduled replication
    executor used for replicating cache data |'
  prefs: []
  type: TYPE_TB
- en: '| eviction-executor | This gives the size of the scheduled executor service
    used to periodically run eviction cleanup tasks |'
  prefs: []
  type: TYPE_TB
- en: 'Customizing the thread pool may be required in some cases, for example, you
    may want to apply a cache replication algorithm. You may then need to choose the
    number of threads used for replicating data. In the following example, we are
    externalizing the thread pools of the web `cache-container` by defining a maximum
    of 25 threads for the bounded-queue-thread-pool and five threads for replicating
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Clustering the messaging subsystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will conclude this chapter by discussing the messaging subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: The JMS provider used in WildFly is **HornetQ**. In order to share the message
    processing load, HornetQ servers can be grouped together in a cluster. Each active
    node in the cluster contains an active HornetQ server. HornetQ manages its own
    messages and handles its own connections. Behind the scenes, when a node forms
    a cluster connection to another node, a core bridge connection is created between
    them. Once the connection has been established, messages can flow between each
    of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering is automatically enabled in HornetQ if one or more `cluster-connection`
    elements are defined. The following example is taken from the default `full-ha`
    profile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's look at how to configure the `cluster-connection`. The following
    is a typical cluster connection configuration. You can either update the default
    `cluster-connection`, or you can add your own `cluster-connection` element within
    your `<hornetq-server>` definition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `cluster-connection` `name` attribute obviously defines the cluster connection
    name, which we are going to configure. There can be zero or more cluster connections
    configured in your messaging subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: The `address` element is a mandatory parameter and determines how messages are
    distributed across the cluster. In this example, the cluster connection will only
    load balance the messages that are sent to addresses that start with `jms`. This
    cluster connection will, in effect, apply to all JMS queue and topic subscriptions.
    This is because they map to core queues that start with the substring `jms`.
  prefs: []
  type: TYPE_NORMAL
- en: The `connector-ref` element references the connector, which has been defined
    in the `connectors` section of the messaging subsystem. In this case, we are using
    the http connector (see [Chapter 3](ch03.html "Chapter 3. Configuring Enterprise
    Services"), *Configuring Enterprise Services*, for more information about the
    available connectors).The `retry-interval` element determines the interval in
    milliseconds between the message retry attempts. If a cluster connection is attempted
    and the target node has not been started, or is in the process of being rebooted,
    then connection attempts from other nodes commence only once the time period defined
    in the `retry-interval` has elapsed.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward-when-no-consumers` element, when set to `true`, will ensure that
    each incoming message is distributed round robin even though there may not be
    a consumer on the receiving node.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can actually specify the connection load-balancing policy within the `connection-factory`
    element. The out-of-the-box policies are **Round-Robin** (`org.hornetq.api.core.client.loadbalance.RoundRobinConnectionLoadBalancingPolicy`)
    and **Random** (`org.hornetq.api.core.client.loadbalance.RandomConnectionLoadBalancingPolicy`).
    You can also add your own policy by implementing the `org.hornetq.api.core.client.loadbalance.ConnectionLoadBalancingPolicy`
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example shows how to use the random policy for a connection factory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the optional `max-hops` value is set to `1` (default), which is the
    maximum number of times a message can be forwarded between nodes. A value of `1`
    means messages are only load balanced to other HornetQ serves, which are directly
    connected to this server. HornetQ can also be configured to load-balance messages
    to nodes that are indirectly connected to it, that is, the other HornetQ servers
    are intermediaries in a chain.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also refer to `jboss-as-messaging_2_0.xsd` for the full list of available
    parameters. This can be found in the `JBOSS_HOME/docs/schema` folder of your server
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring messaging credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you try to start a cluster where nodes use the `full-ha` profile, you will
    get an error logged to the console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This is because, when attempting to create connections between nodes, HornetQ
    uses a cluster user and cluster password. As you can see in the default configuration,
    you are required to update the password value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have changed this password, start your cluster, and you should see
    a successful bridge between nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Configuring messaging credentials](img/6232OS_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Configuring clustering in your applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now complete our journey through the clustering system by looking at
    how to cluster the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Session beans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering session beans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html "Chapter 3. Configuring Enterprise Services"), *Configuring
    Enterprise Services*, we discussed the difference between **Stateless Session
    Beans** (**SLSBs**), **Stateful Session Beans** (**SFSBs**), and **Singleton Session
    Beans**.
  prefs: []
  type: TYPE_NORMAL
- en: 'SLSBs are not able to retain state between invocations, so the main benefit
    of clustering an SLSB is to balance the load between an array of servers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to further specialize your SLSB, then you can choose the load-balancing
    algorithm used to distribute the load between your EJBs. The following are the
    available load-balancing policies for your SLSB:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Load-balancing policy | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `RoundRobin` | It is the default load-balancing policy. The smart proxy cycles
    through a list of WildFly Server instances in a fixed order. |'
  prefs: []
  type: TYPE_TB
- en: '| `RandomRobin` | Under this policy, each request is redirected by the smart
    proxy to a random node in the cluster. |'
  prefs: []
  type: TYPE_TB
- en: '| `FirstAvailable` | It implies a random selection of the node, but subsequent
    calls will stick to that node until the node fails. The next node will again be
    selected randomly. |'
  prefs: []
  type: TYPE_TB
- en: '| `FirstAvailableIdenticalAllProxies` | This is the same as `FirstAvailable`,
    except that the random node selection will then be shared by all dynamic proxies.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Then, you can apply the load-balancing policy as in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In JBoss AS 7, you were required to annotate your SFSB with `@Clustered` in
    order to replicate the state of the SFSB. In WildFly, this is not the case, as
    SFSB are configured to have passivation enabled by default. This means that as
    long as you annotate your bean with `@Stateful`, and you are using a server profile
    that supports high availability, your SFSB will have its state replicated across
    servers. Have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'To disable passivation/replication, you can simply set `passivationCapable`
    to `false`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, SFSBs use the cache container named `ejb`, which replicates sessions
    across all nodes. Should your application server node fail while sessions are
    running, the EJB proxy will detect it and choose another node where session data
    has been replicated. You can, however, reference a custom cache container used
    by your SFSB with the `@org.jboss.ejb3.annotation.CacheConfig` annotation. Have
    a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the corresponding cache container that uses a distributed
    cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Clustering entities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As entities sit deep in the backend, they do not need to be considered with
    regard to load-balancing logic or session replication. However, it is useful to
    cache your entities to avoid roundtrips to the database. The EJB3 persistence
    layer implementation in WildFly is Hibernate 4.3.5\. The Hibernate framework includes
    a complex cache mechanism, which is implemented both at the Session level and
    at the SessionFactory level.
  prefs: []
  type: TYPE_NORMAL
- en: The cache used in the Session level is called the first-level cache and only
    has session scope. This cache is cleared as soon as the Hibernate session using
    it is closed. Hibernate uses second-level caching to store entities or collections
    retrieved from the database. It can also store the results of recent queries.
    It is the second-level cache that we need to cluster, as this cache is used across
    sessions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling the second-level cache for your enterprise applications is relatively
    straightforward. If you are using JPA, then all you need to do to enable the second-level
    cache is add the following to your `persistence.xml` configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The first element, `shared-cache-mode`, is the JPA 2.x way of specifying whether
    the entities and entity-related state of a persistence unit will be cached. The
    `shared-cache-mode` element has five possible values, as indicated in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Shared Cache mode | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `ALL` | This value causes all entities and entity-related states and data
    to be cached. |'
  prefs: []
  type: TYPE_TB
- en: '| `NONE` | This value causes caching to be disabled for the persistence unit.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ENABLE_SELECTIVE` | This value allows caching if the `@Cacheable` annotation
    is specified on the entity class. |'
  prefs: []
  type: TYPE_TB
- en: '| `DISABLE_SELECTIVE` | This value enables the cache and causes all entities
    to be cached except those for which `@Cacheable(false)` is specified. |'
  prefs: []
  type: TYPE_TB
- en: The property named `hibernate.cache.use_minimal_puts` performs some optimization
    on the second-level cache by reducing the amount of writes in the caches at the
    cost of some additional reads. This is beneficial when clustering your entities,
    as the put operation is very expensive as it activates cache replication listeners.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, if you plan to use the Hibernate query cache in your applications,
    you need to activate it with a separate property, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of completeness, we will also include the configuration needed
    to use Infinispan as a caching provider for native Hibernate applications. This
    is the list of properties you have to add to your `hibernate.cfg.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the configuration is more verbose because you have to tell Hibernate
    to use Infinispan as a caching provider. This requires setting the correct Hibernate
    transaction factory using the `hibernate.transaction.factory_class` property.
  prefs: []
  type: TYPE_NORMAL
- en: The `hibernate.cache.infinispan.cachemanager` property exposes the cache manager
    used by Infinispan. By default, Infinispan binds the cache manager responsible
    for the second-level cache to the JNDI name `java:CacheManager/entity`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `hibernate.cache.region.factory_class` property tells Hibernate
    to use Infinispan's second-level caching integration, which uses `CacheManager`,
    as defined previously, as the source for the Infinispan cache's instances.
  prefs: []
  type: TYPE_NORMAL
- en: Caching entities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unless you have set `shared-cache-mode` to `ALL`, Hibernate will not automatically
    cache your entities. You have to select which entities or queries need to be cached.
    This is definitely the safest option since indiscriminate caching can hurt performance.
    The following example shows how to do this for JPA entities using annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Using JPA annotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `@javax.persistence.Cacheable` annotation dictates whether this entity class
    should be cached in the second-level cache. This is only applicable when the `shared-cache-mode`
    is not set to `ALL`.
  prefs: []
  type: TYPE_NORMAL
- en: Using Hibernate annotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `@org.hibernate.annotations.Cache` annotation is the older annotation used
    to achieve the same purpose as `@Cacheable`. You can still use it to define which
    strategy Hibernate should use to control concurrent access to cache contents.
  prefs: []
  type: TYPE_NORMAL
- en: The `CacheConcurrencyStrategy.TRANSACTIONAL` property provides support for Infinispan's
    fully-transactional JTA environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is a chance that your application data is read but never modified,
    you can apply the `CacheConcurrencyStrategy.READ_ONLY` property that does not
    evict data from the cache (unless performed programmatically):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the last attribute is the caching region that defines where entities
    are placed. If you do not specify a cache region for an entity class, all instances
    of this class will be cached in the `_default` region. Defining a caching region
    can be useful if you want to perform a fine-grained management of caching areas.
  prefs: []
  type: TYPE_NORMAL
- en: Caching queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The query cache can be used to cache the result set of a query. This means that
    if the same query is issued again, it will not hit the database but return the
    cached value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The query cache does not cache the state of the actual entities in the result
    set; it caches only the identifier values and results of the value type.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the query result set named `listUsers` is configured
    to be cached using the `@QueryHint` annotation inside a `@NamedQuery` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overuse of the query cache may reduce your application's performance, so use
    it wisely. First, the query cache will increase the memory requirements if your
    queries (stored as key in the query cache map) are made up of hundreds of characters.
  prefs: []
  type: TYPE_NORMAL
- en: Second, and more important, the result of the query cache is invalidated each
    time there's a change in one of the tables you are querying. This can lead to
    a very poor hit ratio of the query cache. Therefore, it is advisable to turn off
    the query cache unless you are querying a table that is seldom updated.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering web applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Clustering web applications requires the least effort. As we touched upon earlier,
    all you need to do to switch on clustering in a web application is add the following
    directive in the `web.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, clustered web applications will use the web cache contained in
    the Infinispan configuration. You also have the option of setting up a specific
    cache per deployment unit. This can be achieved by adding the `replication-config`
    directive to the `jboss-web.xml` file and specifying the cache name to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous configuration should obviously reference a cache defined in the
    main configuration file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at a lot of configuration options around clustering.
    There was a lot of information to take in but, in summary, we will mention the
    following key points.
  prefs: []
  type: TYPE_NORMAL
- en: A WildFly cluster can be composed of either standalone nodes or as part of a
    domain of servers .The clustering subsystem is defined in the `standalone-ha.xml`
    and `standalone-full-ha.xml` configurations files.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main components required for clustering: JGroups, Infinispan,
    and messaging. JGroups provides communication between nodes in a cluster. By default,
    JGroups uses UDP multicast messages to handle the cluster life cycle events.'
  prefs: []
  type: TYPE_NORMAL
- en: Within enterprise applications, there are several caches that need to be configured
    in order to achieve consistency of data. There are four cache containers configured
    by default in WildFly. These are the singleton session bean cluster `cache-container`,
    the SLSB `cache-container`, the web `cache-container`, and the Hibernate `cache-container`.
  prefs: []
  type: TYPE_NORMAL
- en: The singleton cluster (server) `cache-container` is configured to replicate
    singleton session bean data across nodes in the cluster. The SFSB's (ejb) `cache-container`
    is configured to replicate stateful session bean data across nodes in the cluster.
    The web `cache-container` is configured to replicate HTTP session data across
    nodes in the cluster. The Hibernate `cache-container` uses a more complex approach
    by defining a `local-query` strategy to handle local entities. An `invalidation-cache`
    is used when data is updated and other cluster nodes need to be informed. Finally,
    a `replicated-cache` is used to replicate the query timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we looked at the messaging subsystem, which can be easily clustered
    by defining one `cluster-connection` element. This will cause messages to be transparently
    load-balanced across your JMS servers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will look at load balancing, the other half of the story
    when it comes to configuring high availability.
  prefs: []
  type: TYPE_NORMAL
