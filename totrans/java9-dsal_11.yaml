- en: Chapter 11. Reactive Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is a little detour to reactive programming. It lets us handle
    the concurrency requirements of an application in some cases. It provides an abstraction
    to handle concurrency. Even though the concepts are old, it has gained interest
    in recent years due to the beginning of large inflow of data. In modern times,
    billions of devices generate data every day. Tapping into this data is essential
    for the growth of business; in some cases, processing the data to statistically
    analyze it or feeding it to some machine learning algorithm may be the entire
    business in itself. This makes it essential to support the processing of this
    large in-flow of data, provide a quick response, and be resilient to failures.
    Of course, one can do these things even using a traditional or imperative programming
    paradigm, just as one can, in theory, build any application using an assembly
    language. However, this makes the application extremely complex to maintain and
    impossible to modify according to business needs. In this chapter, we will discuss
    the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic idea of reactive programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an example reactive framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building example programs using our framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is reactive programming?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose we have a web server that lets us query some data or save it. This web
    server serves multiple requests at the same time, and each request is a short
    task that involves some computation. What is the usual way of achieving this?
    Well, the naive way would be to spawn a new thread for each request. But one can
    easily realize that this leads to an explosion in the number of threads in the
    application. Plus, the creation and deletion of threads are heavyweight activities;
    they slow down the entire application.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can use a thread pool so the same threads can be used over and over
    to avoid the overhead of creation and deletion of threads. However, if you want
    to serve thousands of requests at the same time, this will require a thread pool
    with thousands of threads. Thread scheduling in an operating system is complex
    and involves a lot of logic, including priority and so on. Operating systems do
    not expect threads to just run short bursts of computation; they are not optimized
    that way. Therefore, the solution is to use the same thread for multiple simultaneous
    requests. This can, in general, be done if we stop blocking for IO and use the
    same thread for another task when one task is waiting for I/O. Managing these
    things, however, is extremely complicated. Hence, we would need a framework to
    carry out these activities for us. Such a framework can be called a reactive framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reactive programming takes care of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: This property is the capability of the application to cater
    to proportional number of requests as the number of available resources increases.
    If one processor serves 500 requests per second, two processors should do 1,000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responsiveness**: We want the application to be responsive; for example,
    it should show a status when it is computing some result or fetching it from some
    other place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience**: Since we use the same thread for multiple tasks, handling errors
    is more complicated than usual. How do we let the user know of an error? So instead
    of propagating the exceptions back down the call stack, we move forward and explicitly
    tackle the error situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different techniques for using reactive programming; it depends on
    the actual problem we are trying to solve. We will not discuss all of them but
    will focus on the commonly used ones.
  prefs: []
  type: TYPE_NORMAL
- en: Producer-consumer model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A producer-consumer model is a design that divides processing into small components
    that send messages to other components. One produces a message and the other consumes
    and acts on it. It provides an abstraction to easily implement an application
    optimized to utilize all the resources. A producer-consumer model starts with
    a queue of messages. Producers publish messages in this queue and consumers receive
    them. This queue is different from the queues we have studied so far in a few
    ways. We want this queue to be thread-safe, which is required for the queue to
    work correctly in a multithreaded environment. We do not need to worry about the
    exact order of the messages being dequeued. After all, the order of messages does
    not matter when they are being received by different threads. In these conditions,
    we optimize the delivery of the messages. Before implementing this queue, let's
    discuss a few thread synchronization techniques beyond what we have learned so
    far using the synchronized keyword. These techniques are required for more optimal
    usage of resources while maintaining the correctness of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Semaphore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A semaphore is a special variable that lets us limit the number of threads
    that can use a particular resource. The following code shows an example of a semaphore
    that gives us a thread-safe counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the semaphore has been initialized to `1`, which means it will allow only
    one thread to acquire it. No other thread can acquire it until it is released.
    Unlike synchronization, there is no requirement here that the same thread has
    to call the release method that had acquired it, which makes it particularly flexible.
  prefs: []
  type: TYPE_NORMAL
- en: A call to the `acquire` method of a semaphore will be blocked until it successfully
    acquires it. This means that the calling thread will be taken off the thread scheduler
    and put aside in such a way that the operating system's thread scheduler will
    not be able to see it. Once the semaphore is ready to be acquired, this thread
    will be put back in place for the thread scheduler to see it.
  prefs: []
  type: TYPE_NORMAL
- en: Compare and set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compare and set is an atomic operation that lets you update the value of a
    variable if and only if the existing value matches a specific value. This enables
    us to update a variable based on its previous value. The CAS operation returns
    a Boolean. If the comparison is a match, which means the set operation is successful,
    it returns `true`; otherwise, it returns `false`. The idea is to keep trying until
    this set operation is successful. The following diagram shows the basic strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Compare and set](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Atomic update using the compare and set operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 1*, we are trying to increment the value of the shared variable
    **var**. The operation requires us to read the value in a thread-specific temporary
    location, then increment the temporary value and reassign it to the shared variable.
    However, this operation can cause problems if there are multiple threads trying
    to perform the update simultaneously. It can happen that both the threads read
    the value simultaneously in order to get the same temp value. Both these threads
    can update the shared variable with the incremented value. This will increment
    the value only once, but it should have actually caused two increments. To avoid
    this, we check whether the value of **var** is still the same and update only
    when it is so; otherwise, we read the value of **var** again and repeat the process.
    Since this compare and set operation is atomic, it guarantees that no increments
    will be lost. The following is the Java code that does the exact same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To use any atomic operation, we need to use classes from the `java.util.concurrent.atomic`
    package. `AtomicInteger` is a class that encapsulates an integer and enables the
    `compareAndSet` operation on it. There are other utility methods as well. In particular,
    it has methods to perform atomic increments and decrements, just like the one
    we have implemented here.
  prefs: []
  type: TYPE_NORMAL
- en: Volatile field
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a field that is being written to and read from multiple threads.
    If all the threads run on the same single CPU, the writes can simply happen on
    the CPU cache; they need not be synced to the main memory often. This would not
    be a problem as the value could also be read from the same cache. However, multiple
    CPUs can have their own caches, and in such a case, a write to a cache from one
    CPU will not be visible from another thread running on a different CPU. Most programs
    accept this and work accordingly. Java, for example, maintains a separate copy
    of a shared variable for each thread, which is occasionally synced. If we, however,
    want to mandate that the writes from one thread should be visible from another
    thread, we need to declare the field volatile. All fields involved in atomic operations
    are declared volatile.
  prefs: []
  type: TYPE_NORMAL
- en: Thread-safe blocking queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we are ready to implement our thread-safe blocking queue. Thread-safe means
    that multiple threads can share the same queue; blocking means that if a thread
    tries to dequeue an element and the queue is currently empty, the call to dequeue
    will be blocked until some other thread enqueues an element. Similarly, if a thread
    tries to enqueue a new element and the queue is full, the call to the queue will
    be blocked until another thread dequeues an element and frees some space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our queue will store elements in a fixed length array and maintain two counters
    that would store the next index for queuing and dequeuing. Two semaphores block
    threads when the queue is either empty or full. Along with this, each array position
    is provided with two semaphores that ensure that enqueuing and dequeuing operations
    do not overwrite or repeat any elements. It does this by ensuring that once a
    new element is enqueued in a particular position, it is not overwriten before
    it is dequeued. Similarly, once a particular array index is dequeued, it is never
    dequeued again before another enqueue stores another element in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `underflowSemaphore` ensures that dequeues are blocked when the queue is
    empty, and `overflowSemaphore` ensures that enqueues are blocked when the queue
    is full:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The array store is the space that holds the elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Both `enqueueLocks` and `dequeueLocks` are individual position-based locks
    that allow only a dequeue after an enqueue and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `alive` flag can be used by the dequeuing threads to know when they can
    stop running, and no more elements would be expected. This flag needs to be set
    by the enqueuing threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'All initializations are pretty much self-evident:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The enqueue operation first makes sure that the queue is not full by acquiring
    `overflowSemaphore`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `nextEnqueueIndex` is then incremented and the previous value is returned,
    which is then used to compute the index in the array where the element would be
    stored. The seemingly complicated expression ensures that the index rolls over
    properly even after the `nextEnqueueIndex` integer rolls over, provided the length
    of the queue is an integer power of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the index is selected, we must acquire an enqueue lock on the position,
    store the value, and then release the dequeue lock to mark this position as ready
    for dequeuing. At the end, we release one count on `underflowSemaphore` to mark
    the fact that there is one more element in the queue to be dequeued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The dequeue operation is very similar to the enqueue operation, just the role
    of the semaphores are reversed. There is slightly more complicated code before
    the actual operation starts. This is to enable the dequeuing threads to quit when
    no more elements are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of directly acquiring `underflowSemaphore`, we use `tryAcquire`, which
    will wake up the thread after 1 second if there are no elements are available
    to be dequeued. This gives us a chance to check the value of the `alive` Boolean
    flag and quit the dequeue operation in case it is no longer alive. If the queue
    is no longer alive, we interrupt the current thread and exit. Otherwise, we compute
    the index and dequeue the element to the enqueue operation in a similar manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a utility method to return the current number of elements in the queue.
    This is useful for knowing when to kill the queue (set the `alive` flag to `false`)
    in a producer-consumer setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Producer-consumer implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now implement a producer-consumer setup using the queue we have created.
    In simple words, the producer-consumer queue is a queue of events that the producers
    produce and the consumers consume. There are three kinds of events. The `INVOCATION`
    type refers to the regular events that propagate processing. The `ERROR` type
    event is raised when an exception needs to be propagated. The `COMPLETION` event
    is produced when it is required that the dequeue threads need to be terminated
    and the queue needs to be closed. The `ProcerConsumer` queue takes `Consumer`
    as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Event` class represents single events. Depending on the type, it can have
    a value or exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of `ProducerConsuerQueue` creates consumer threads. It also
    takes consumer code as input. The consumer must implement the `Consumer` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The consumer thread runs code that dequeues events and calls the methods on
    `consumerCode` as per the event type in the loop. The loop ends when the termination
    event is received and no more events are there in the queue to be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Consumer threads are spawned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `produce` method is invoked from a producer thread. Notice that the queue
    does not manage producer threads; they need to be managed separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a producer thread marks the stream of events to be completed, no more
    new events could be generated and the dequeuing threads will be terminated after
    they process all the events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This is to propagate an exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we need to wait for all the dequeuing threads to terminate, we use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To see how to use this producer-consumer queue to actually solve a problem,
    we will consider a dummy problem. We will work on a file—`com-orkut.ungraph.txt`—that
    is open to public and contains all the friendships between users in **Orkut**,
    which was a social networking site in the past. The file can be downloaded from
    [https://snap.stanford.edu/data/bigdata/communities/com-orkut.ungraph.txt.gz](https://snap.stanford.edu/data/bigdata/communities/com-orkut.ungraph.txt.gz).
    To protect privacy, all the users are simply referenced by some arbitrary ID and
    the mapping with the actual users is not shared. We will also use another file
    called `ulist` that would contain the list of user IDs we are interested in. Our
    task is to find the number of friends that each user in the second file has. The
    following commands show how the two files look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Each line in `com-orkut.ungraph.txt` has two IDs that are separated by a whitespace.
    The meaning is that there is friendship between these two users. It is given that
    each friendship is mentioned only once in the file and is undirected. Note that
    this means each line should increase the friend count for both the IDs. Each line
    in `ulist` has a single ID. All IDs are unique, and we must find the friend count
    of each of these IDs. Note that some of these have no friends and thus are not
    mentioned in `com-orkut.ungraph.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a utility class that will let us read integer IDs from
    the files. The purpose of this class is to read integer values from any text file
    so that not too many objects are created in the process. This is just to reduce
    garbage collection to some extent. In this case, we used file-channel-based logic
    that uses `ByteBuffer` as a buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readCount` variable keeps track of how many characters are left in the
    buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To read an `int`, keep reading the bytes in a loop until you hit a byte that
    is not a digit. In the meantime, keep computing the integer that the string of
    characters represents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'First check whether the buffer is empty; if yes, refill it by reading from
    the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If no more bytes are available in the file, don''t care to flip the buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We read a byte and decrement `readCount` because now the buffer has one less
    byte:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If the character is a digit, keep computing the integer; otherwise, break the
    loop and return the calculated integer value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of this, we will create a program to create a file output, which
    will contain the user IDs provided in `ulist` along with the corresponding friend
    count. The idea is that reading the file is made asynchronous by computing the
    friend count. Since the counting involves a binary search, we want two threads
    doing it instead of one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we simply count the number of lines present in `ulist`. This will let
    us create the correct size array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We create two arrays: one containing the keys and the other containing the
    friend count of each of the keys. The counts are stored in `AtomicInteger` objects
    so that they can be incremented from multiple threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We read `userIDs` from `ulist` in an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we sort the array of `userID` so that we can perform binary search on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The job of our consumers is to search for each user encountered in `com-orkut.ungraph.txt`
    and increment the corresponding count in the array values. Note that creating
    `ProducerConsumerQueue` does not start any processing; only consumer threads are
    created through this. Processing will start only when we produce events, which
    we will do after reading from `com-orkut.ungraph.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the main thread for producing the events. We use the same `FileReader`
    class for reading each user ID separately. This is because both the users in a
    line in `com-orkut.ungraph.txt` have a friend (which is the other one in the same
    line) for each line in the file. So we simply read the users and post them as
    events so that the consumers can process them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we are done processing the entire `com-orkut.ungraph.txt` file, we simply
    mark the queue as completed and wait for the consumer threads to be terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now all the counts must be updated in the values array. So we simply read them
    one by one and output them in the file output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example demonstrates how an actual problem can be solved using
    the reactive technique of **producer-consumer**. Now we will discuss another way
    of implementing our event queue; it does not involve blocking on semaphores.
  prefs: []
  type: TYPE_NORMAL
- en: Spinlock and busy wait
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A semaphore normally blocks a thread before the thread acquires it. This blocking
    is achieved by the operating system by removing the thread from the list of threads
    that are ready to be scheduled for processing time on the CPU. The list of threads
    ready to be scheduled are called running threads. Every semaphore has a list of
    threads waiting on it, and these threads are removed from the list of running
    threads. Once the semaphore is released, threads from the list attached to the
    semaphore are removed and put back on the list of the running threads. This operation
    is somewhat heavyweight and requires processing time. Another way to stop a thread
    from accessing a shared resource is to use a spinlock. A spinlock is generally
    implemented using an atomic variable and compare and set operation. A thread in
    a spinlock simply tries to perform compare and set on a variable in a loop; it
    does so until it succeeds. To the operating system, this thread is as good as
    a running thread and is scheduled just like any other thread. The thread itself,
    however, keeps trying a compare and set operation and consumes processor time.
    This is why it is called a busy wait. The thread can proceed to do something meaningful
    once the compare and set operation is successful. Spinlocks are useful when the
    resource would not be available only for a short period of time. It simply does
    not make sense to do all the heavy lifting of removing the thread from the list
    of running thread and blocking on a semaphore if the resource is unavailable for
    a brief period of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement our thread-safe queue with spinlocks instead of semaphores
    as shown in the following code. Each array location for storing the queue elements
    is protected by two `AtomicBoolean` variables, stored in the `enqueueLocks` and
    `dequeueLocks` arrays. The only thing we want to make sure is that after each
    dequeue, there should only be a single enqueue, and after each enqueue, there
    should only be a single dequeue for a particular array location. Different array
    locations should be independent of one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'When `enqueueLocks[i]` is `false`, it means there is no element being stored
    at the position `i`. When `dequeueLock[i]` is `true`, it means the same thing.
    The reason we need both is for protection when an element is in the process of
    being enqueued or dequeued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the core of the lock. We simply take the next index to enqueue and
    try to get `enqueueLock`. If it is `false`, which means nothing is already enqueued,
    it is atomically set to `true` and it starts the enqueue process; otherwise, we
    keep doing the same thing in a busy loop until the compare and set operation is
    successful. Once the process is complete, we release `dequeueLock` by simply setting
    it to `false`. A compare and set operation is not necessary here because it is
    guaranteed to be `true`. The number of elements are maintained using another atomic
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The dequeue operation is very similar, just that the enqueue and dequeue locks
    have switched places:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the code is self-evident:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can simply replace the queue in the `ProducerConsumerQueue` class to use
    this spinlock-based queue. In the case of our example problem, the spinlock version
    of the queue performs better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s solve another problem using `ProducerConsumerQueue`. Our problem is
    to find all the perfect numbers between 2 and 500,000\. What is a perfect number?
    A perfect number is a number that is the sum of all its divisors, excluding itself.
    The first perfect number is 6\. 6 has three divisors excluding itself, namely
    1, 2, and 3 and 6=1+2+3\. This is what makes 6 a perfect number. To find all the
    perfect numbers between 2 and 500,000, we will check whether each number in the
    range is a perfect number. We can write the following code to figure out whether
    a given number is a perfect number. For every number *div*, we check whether the
    number *x* is divisible by *div*; if so, we add it to the sum. In such a case,
    if we divide *x* by *div*, we will of course get another divisor of *x* as a result
    stored in the variable quotient. This must also be added to the sum, unless it
    is equal to *div*. We stop this process when we pass through the square root of
    *x*, that is, when *div* is bigger than the quotient we get when *x* is divided
    by *div*. Since we, originally, exclude *1* as a divisor to avoid adding the number
    itself, we add *1* to the sum at the end and check whether it is equal to *x*;
    if so, *x* is a perfect number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, checking whether a given number is a perfect number is a computationally
    expensive operation, which makes it desirable to use all the CPUs to compute it.
    We will use our producer-consumer framework to do this. The code is self-explanatory.
    Our consumer code simply checks whether a given number is a perfect number and
    then prints the number if it is so. The producer simply generates and queues all
    the numbers. Since the consumer is run in multiple threads and it is the part
    that is computationally intensive, it should work faster than the single-threaded
    version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Since my computer has four CPU cores, I used four threads to do the heavy lifting.
    On my computer, this program takes 1,596 milliseconds as compared to 4,002 milliseconds
    for the single-threaded program, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Functional way of reactive programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most reactive programming frameworks provide functional APIs for reactive programming,
    which makes it even easier to work with. In this section, we will build a functional
    reactive API and solve a problem with it. The idea is to use the concept of a
    stream. A stream is a data generator or source that can provide input when requested.
    Functional APIs provide map, filter, and consume operations on the stream. The
    map and the filter operations create a new stream, and the consume operation gives
    a `EventConsumer` instance. The idea is that when `EventConsumer` is asked to
    start processing, it would spawn its own producer threads and consumer threads
    and treat each map, filter, or consume operations as a separately scheduled operation
    in a producer-consumer queue. This is just to highlight what we are really trying
    to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, I will put the code to use the functional API to solve the same
    perfect number problem. We will replace the pseudo-method `someWayCreateAStream`
    with the actual code to create a stream later. The point is to show how an event
    stream can be manipulated using the map, filter, and consume method. The processing
    really starts when the process method is called, and in each step of map, filter,
    and consume are decoupled processing steps and are potentially run in different
    threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: When we create an instance of `EventStream` or `EventConsumer`, no processing
    happens; only metadata is created. It is when the method process is invoked that
    the processing starts. This is done by the process method spawning the producer
    and consumer threads. The producer threads create and enqueue events that contain
    the initial value and the processing code (like map, filter, or consume operations).
    A dequeuer runs the first piece of processing and enqueues another event for the
    next level of processing; it does this for map and filter operations. A consume
    operation is the end of the processing chain, and it does not return any value.
    This is when no more events are scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: This requires that a dequeuer thread must also do some enqueue operations. What
    can be a problem with this? There are two kinds of threads that enqueue. One of
    these threads is responsible for dequeueing too. These threads may get blocked
    while trying to perform enqueue operations when the queue is full. But this would
    mean that they would not be able to do any dequeue operation either; this is because
    if they do this, the queue would never have more space again. This situation is
    a deadlock; all the threads are blocked and are expecting other threads to do
    something to unblock them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why this deadlock happens, let''s imagine a queue with length 4\. Suppose
    there are two dequeuing threads that also perform an enqueue operation once in
    some cases. Let''s have another enqueuer thread too. Since the threads can run
    in any order, it is possible that the enqueuer runs first and enqueues four new
    elements to make the queue full. Now say two dequeuers run, each dequeuing one
    element. Before these threads get a chance to enqueue once more, the enqueuer
    thread is run again, and this time it enqueues two new elements to fill the queue.
    Now the dequeuer threads are run, but they are blocked because the queue is full.
    They cannot even dequeue any element because they are themselves blocked from
    enqueuing more elements. This is a deadlock situation. *Figure 2* shows this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Functional way of reactive programming](img/00078.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'What we really want is threads that will not only perform the enqueue operation,
    but also block the queue before it is completely full. This is so that the dequeueing
    threads can use some space to keep dequeuing and enqueuing until they reach a
    point where they will not have to enqueue anymore (because they have reached the
    last step of the processing chain). Eventually, the queue gets empty and the enqueuer
    threads can be unblocked again. To do this, we need to have two different kinds
    of enqueue operations. One that does not block until the queue is full, and another
    that blocks once the queue is half or more full. We can implement the second type
    using the following code in the `ThreadSafeFixedLengthSpinlockQueue` class. The
    `enqueueProducerOnly` method is just like the `enqueue` method, except it performs
    an atomic check of the `currentElementCount` variable instead of just incrementing
    it. If, while enqueueing, it is seen that the queue is already full, we release
    the enqueue lock and restart. The thread that does only enqueue operations and
    no dequeue operation must use this method instead of the regular `enqueue` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this method to implement a corresponding method in the `ProducerConsumerQueue`
    class. This method is exactly the same as the produce method, except that here,
    the call to enqueue has been replaced by a call to the `enqueueProducerOnly` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see the `EventStream` class. The whole point of the `EventStream`
    class is to create metadata in a functional way. It is an abstract class with
    only one abstract method called `read()`. A call to the `read` method should return
    the next object that needs to be processed. The class maintains a pointer to the
    previous `EventStream` on which this `EventStream` will work. This means that
    the operation represented by `EventStream` will work on the data obtained after
    all the previous `EventStream` have been processed. It is really a linked list
    of `EventStream`. Depending on the kind of operation the current `EventStream`
    represents, it either has a mapper, a filter, or nothing. The `read` method is
    applicable only to the first `EventStream` that generates the data. Both the map
    filter methods return another `EventStream` that represents the corresponding
    processing. After all the map and filter calls, the list linked by `EventStream`
    will store all the operations from the last to the first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The consume method, however, returns an instance of `EventConsumer`. This is
    the terminal processing in any chain that does not compute a new value. The `EventConsumer`
    class, as would be shown a little later, contains all of the logic to actually
    start the processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we need to store the details of the processing inside an `EventConsumer`
    instance, we will first make a few classes to store this information. The first
    one is a `Task` interface that represents any of the map, filter, or consume operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This interface is implemented by three classes that represent each kind of
    operation. To store the code, we need two additional functional interfaces that
    represent an expression and a statement that would allow you to throw exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following classes implement the `Task` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Both `MapperTask` and `FilterTask` have a pointer to the next task because
    they are intermediate operations. They also store the piece of code associated
    with the processing. The `ProcessorTask` represents the terminal operation, so
    it does not have a pointer to the next task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now create the `EventConsumer` class that will create a task chain
    and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'A `StreamEvent` is a processing request that is an element of the producer-consumer
    queue. It stores `value` as `Object` and `task`. The `task` can have more tasks
    pointed to by its next reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'An `EventStream` stores its previous operation—that is to say that if we read
    the head of the list, which would be the last operation. Of course, we need to
    arrange the operations in the order of execution and not in reverse order. This
    is what the `eventStreamToTask` method does. A `MapperTask` or `FilterTask` stores
    the next operation, so the head of the list is the first operation to be carried
    out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor is package-accessible; it is intended to be initialized only
    from inside the `consume` method of an `EventStream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the piece of code responsible for actually carrying out the
    operations. The `ConsumerCodeContainer` class implements `Consumer` and acts as
    the consumer of the producer-consumer queue for processing events:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The `onMessage` method is invoked for every event in the producer-consumer
    queue. Based on the actual task, it takes the corresponding action. Notice that
    for `MapperTask` and `FilterTask`, a new event is enqueued with the next operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ProcessorTask` is always the end of a processing chain. The operation
    is simply invoked on the value and no new event is queued:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'For a `FilterTask`, the event with the next task is enqueued only if the condition
    is satisfied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'For a `MapperTask`, the next task is enqueued with the value computed by the
    current map operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The process method is responsible for kicking the actual processing of the
    tasks. It uses a `ProducerConsumerQueue` to schedule events that are processed
    by the consumer previously discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Only the original `EventStream` on which map and filter were called has the
    `read` method implemented. So we simply get a reference to the original `EventStream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The `startingStream` variable points to the original `EventStream`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The producer code also runs in separate threads. The `Runnable producerRunnable`
    contains the producer code. It simply keeps calling the `read` method of the `EventStream`
    until `null` is returned (which marks the end of the stream) and enqueues a `StreamEvent`
    with the value and the task chain we have created with the help of the `eventStreamToTask`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we spawn the `producer` threads and wait for them to finish with the `join`
    calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a method to register a custom error handler and return a new `EventConsumer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Going back to our original problem of perfect numbers, all we have to do now
    is to define an `EventStream` with a read method that generates all the numbers
    and then does maps and filters on them as follows. Notice that the `EventStream.read()`
    method may be invoked by multiple threads simultaneously if we use more than one
    producer thread, so it is better for it to be thread-safe.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `read` method simply increments an `AtomicLong` and returns the previous
    value, unless the previous value is greater than `5_00_000L`; in this case, it
    returns `null`, marking the end of the stream. We have already seen the rest of
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This code runs for almost the same time as the previous reactive version without
    a functional API. I will leave it up to you to use the functional API to implement
    the friend count solution, as it is fairly simple as one gets the hang of it.
    All you need to think about is how to implement the `read` method to return the
    integers from the file.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to do advanced thread synchronization using
    volatile fields, atomic operations, and semaphores. We used these to create our
    own reactive programming framework and also created a functional API for reactive
    programming. We used our frameworks to solve sample problems and saw how multithreaded
    scalable apps can be written easily with a reactive framework.
  prefs: []
  type: TYPE_NORMAL
- en: There are many reactive programming frameworks available, such as RxJava, Akka,
    and many more. They are slightly different in their implementation and features.
    They all provide a lot more features than the one we used. This chapter is just
    an introduction to the topic; interested readers can learn more about reactive
    programming from the books dedicated to this subject.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, I tried to give you a head start in the world of algorithms, with
    implementations in Java. Algorithms are a vast field of study. Every computation
    problem needs to be solved by an algorithm. A further study would include complexity
    classes of algorithms, equivalence of algorithms, and approximate algorithms for
    highly complex problems. A complex problem is a problem that guarantees that any
    algorithm that solves it must have a certain amount of complexity. This gives
    rise to the concept of the complexity classes of problems. There are also formal/mathematical
    ways of proving the correctness of algorithms. All these areas can be pursued
    by you.
  prefs: []
  type: TYPE_NORMAL
- en: The book also covers functional and reactive programming a little bit. This
    should work as a head start in those areas; you can learn more about them in the
    books dedicated to these topics.
  prefs: []
  type: TYPE_NORMAL
