- en: Chapter 3. Working with Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about batch configurations, components,
    and execution modes to match with diverse business needs. Handling the data, which
    includes reading, processing, and writing, is an essential part of any kind of
    application, and batch applications are no exception. Spring Batch provides the
    ability to read different forms of data, process the data in the way business
    expects, and write it back to different systems, which can be easily integrated
    with different frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover three major operations involved in data handling:'
  prefs: []
  type: TYPE_NORMAL
- en: Data reading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data writing![Working with Data](img/3372OS_03_01.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding figure shows the three stages of handling data in a batch application.
    The input (source) can be a database, a filesystem (flat file or XML), or data
    from a web service as well. Applications need to read the data from an input (source),
    process it, and write to the output (destination) system. The output (destination)
    can be a database or a filesystem (a flat file or an XML file). In the processing
    stage, data read in format can be verified and transformed into the desired format
    before writing to the output. Let's examine each of these stages now.
  prefs: []
  type: TYPE_NORMAL
- en: Data reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Batch provides the configuration to read different forms of data from
    different sources, including flat files, XML, and relational databases. It also
    supports the custom reader configurations for the formats that are not available
    with the specification.
  prefs: []
  type: TYPE_NORMAL
- en: ItemReader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spring Batch provides an interface in the form of `ItemReader` to read bulk
    data from different forms, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flat files**: These are typically of two types: fixed width and delimited
    character-based files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XML**: This format is used for different forms of application data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database**: This maintains a set of records of similar or different groups
    of information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the definition of the `ItemReader` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let's examine how `ItemReader` can help us with reading different formats.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from flat files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Flat files are configured in two formats, namely, **fixed width** and **delimited**.
    Fixed width files have each field detail configured with a predefined fixed width,
    whereas the delimited files have fields with a specific character (or tab in general)
    used to delimit them from the other fields.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed width file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A fixed width file generally has a predefined specification of its fields, how
    much length each field should occupy on the file, and from which position to which
    position on a line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is one such specification of the fixed width file we want to
    read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding specification, let''s fill a sample file with employee
    information in a fixed width file, as follows (`employees.txt`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If a Java object is to be generated corresponding to this specification, we
    can create a **Plain Old Java Object** (**POJO**) with the following representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: FlatFileItemReader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`FlatFileItemReader` provides a means of reading different types of flat files
    and parsing them by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`resource`: This represents the file from which the data has to be read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lineMapper`: This represents the mapper that converts the String that is read
    by `ItemReader` to the Java object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linesToSkip`: This is used with files having header content before records.
    It is the number of lines we want to ignore at the top of the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LineMapper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `LineMapper` interface lets us read each line from the file with the line
    number passed in every iteration. It is the Spring Batch standard implementation
    of `LineMapper`. The following is the `LineMapper` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `LineTokenizer` interface converts the line read from `LineMapper` to the
    set of fields (`FieldSet`). `DelimitedLineTokenizer`, `FixedLengthTokenizer`,
    and `PatternMatchingCompositeLineTokenizer` are Spring Batch''s supporting implementations
    of `LineTokenizer`. The following is the `LineTokenizer` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `FieldSetMapper` interface lets us map each field from the String read
    to the `Employee` object. The following is the `FieldSetMapper` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can implement `FieldSetMapper` for our `Employee` data, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The data can be read from the file as part of the batch job, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `setResource()` method sends the flat file resource to `FlatFileItemReader`.
    The `LineTokenizer` interface can be used with field names and ranges with the
    start and end positions on the file set as an array using the `setNames()` and
    `setColumns()` methods respectively. Every time the `read()` method is invoked
    on `itemReader`, it reads a line and moves on to the next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the batch program after reading data from a
    fixed width flat file and capturing them into Java objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `reader`, `linetokenizer`, and `fieldsetmapper` are used in the batch as
    beans, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Delimited file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A delimited flat file contains fields separated by a specific character in each
    line. The following is an example of a delimited file, with each field delimited
    by the character comma. Let's take an example of reading employee details from
    a delimited flat file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the specifications of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: ID
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Department
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Year of joining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each field should be separated from the other with a comma. The following is
    the sample file content (`employees.csv`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Delimited files need to be handled the same way as the fixed width flat files,
    except that `LineTokenizer` used in this case should be `DelimitedLineTokenizer`.
    The following is the Java code realized to read a delimited flat file to be handled
    as part of the batch job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the delimited files, we don''t have to set the column''s property. The
    delimiter used has to be set unless the delimiter is a comma. Executing this program
    should read the delimited flat files into the Java objects and the output is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ItemReader`, `LineTokenizer`, and `FieldSetMapper` in the case of delimited
    files can be configured in the batch as beans and used in the program as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If the lines of file are defined in a different format specific to a business,
    `LineTokenizer` is open for custom implementation and configuration. The `PatternMatchingCompositeLineMapper`
    can be used to read files with complex patterns. For example, if we have multiple
    record types within a single flat file, we can use `PatternMatchingCompositeLineMapper`
    to have tokenizers for each record type, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample flat file with multiple record types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the bean configuration for this multiple record type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `PatternMatchingCompositeLineMapper` identifies each line by its pattern,
    with matching keys to let the corresponding `Tokenizer` and `FieldSetMapper` read
    and match the records.
  prefs: []
  type: TYPE_NORMAL
- en: Exceptions from flat file reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the possible exceptions from flat files, for example when
    in case the file has an incorrect format, a problem in reading the data from file,
    or inconsistent data in the flat file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FlatFileParseException`: This is the exception thrown by `FlatFileItemReader`
    for the errors that occur during file reading'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FlatFileFormatException`: This is the exception thrown by `LineTokenizer`
    for the errors that occur during data tokenizing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IncorrectTokenCountException`: This is thrown if the number of columns specified
    do not match with the number of columns tokenized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IncorrectLineLengthException`: This is thrown during the fixed width flat
    file reading if the line/field lengths do not match with the ones specified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading data from XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Extensible** **Markup Language** (**XML**) is a markup language to define
    documents with data that can be readable by both humans and machines. XML is mainly
    used when multiple systems interact with each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring Batch uses the **Streaming API for XML** (**StAX**) parser. In the StAX
    metaphor, the programmatic entry point is a cursor that represents a point within
    the document. The application moves the cursor forward, ''pulling'' the information
    from the parser as required. Hence, the reading happens in fragments of XML content
    from a file that is represented in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reading data from XML](img/3372OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `StaxItemReader` lets us parse the XML file, considering that the root element
    of each fragment is common (`employee` in the above example). `unmarshaller` converts
    the data into Java objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the `employeeFile` and `employeeFileReader` configuration
    as beans:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use different unmarshalling technologies, including JAXB, XStream binding,
    JiBX, and XML Beans. We used StAX as an engine for marshalling. Let''s consider
    the XStream binding and the following configuration with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If more than one file has the XML details to be read, we can use `MuliResourceItemReader`
    to configure multiple resources to be read in a read operation.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from a database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A database contains information in the form of tables with multiple columns
    to hold each field in it. If a batch job has to read the data from a database,
    it can be performed using the following two types of item reading concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cursor-based** **item reading**: This reads each fragment having a cursor
    pointing to one after the other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Page-based** **item reading**: This reads multiple records together, considering
    them as a page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In comparison, cursor-based item reading works well as it reads little data
    and processes unless their memory leaks with the system.
  prefs: []
  type: TYPE_NORMAL
- en: JdbcCursorItemReader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read the data in a cursor-based technique, we can use `JdbcCursorItemReader`.
    It configures with `RowMapper` (of Spring framework) to match each attribute in
    the database to the Java object attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `RowMapper` for the employee example can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The Java program to read the data from the database with `EmployeeRowMapper`
    can be realized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `JdbcCursorItemReader` and `EmployeeRowMapper` can be configured in the
    batch XML as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`JdbcCursorItemReader` can be customized by setting ignore warnings, fetch
    size, max rows, query timeout, verify cursor position, and such properties with
    the corresponding options for respective items.'
  prefs: []
  type: TYPE_NORMAL
- en: If we want to configure the database read activity using the Hibernate framework,
    we can use `HibernateCursorItemReader`. The stored procedure-based read operation
    can be performed using the `StoredProcedureItemReader`.
  prefs: []
  type: TYPE_NORMAL
- en: JdbcPagingItemReader
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The paging mode read operation on a database can be performed using the `JdbcPagingItemReader`.
    The configuration with the `JdbcPagingItemReader` with the properties of `dataSource`,
    `queryProvider`, and query with different clauses can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Using the `SqlPagingQueryProviderFactoryBean`, we can set the `select`, `from`,
    and `where` clauses separately, along with a sortkey and parameter to be passed.
  prefs: []
  type: TYPE_NORMAL
- en: Spring Batch supports different object relational frameworks as well the corresponding
    item readers such as `JpaPagingItemReader` for JPA and `IbatisPagingItemReader`
    for IBatis.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Batch provides the means to read input data in one form, process it,
    and return it in a desired form of output data. The `ItemProcessor` interface
    is the interface that supports the processing activity.
  prefs: []
  type: TYPE_NORMAL
- en: ItemProcessor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spring Batch provides the simple interface `ItemProcessor` to take the object,
    process it, and transform it to the desired form and return as another object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the definition of the `ItemProcessor` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`ValidatingItemProcessor` is an implementation of the `ItemProcessor` that
    lets us validate the input data before processing. If the data fails to pass the
    validation, it gives `org.springframework.batch.item.validator.ValidationException`.
    Frameworks such as Hibernate have the validation framework (`hibernate-validator`)
    that lets us configure annotation-based validators for the beans.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ItemProcessor` can be implemented for the `Employee` example as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding program takes the `employee` data object, transforms the object,
    and returns an `Associate` data object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `itemProcessor` can be configured as a job chunk in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Chaining the process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The processing activity can be chained by defining more than one item processor
    and calling from one another to make `compositeItemProcessor` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Data writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Batch provides the configuration to write the read and processed data
    to a different output (destination). The writer can integrate easily with different
    relational frameworks. It can also be customized for the different formats.
  prefs: []
  type: TYPE_NORMAL
- en: ItemWriter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spring Batch provides an interface in the form of `ItemWriter` to write bulk
    data. The following is the definition of the `ItemWriter` interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the destination platform onto which we have to write the data, we
    have the following item writers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flat file item writers**: These write the content onto a flat file (fixed
    width and delimited)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XML item writers**: These write the data onto an XML file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database item writers**: These write the data onto a database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flat file item writers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data read from any of the existing formats can be processed to the desired
    format and then be written onto multiple formats, including flat files. The following
    are the APIs that help in flat file item writing.
  prefs: []
  type: TYPE_NORMAL
- en: LineAggregator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `LineAggregator` API concatenates multiple fields into a String to write
    onto the flat file. This works exactly the opposite way of `LineTokenizer` in
    the read operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: PassThroughLineAggregator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`PassThroughLineAggregator` is an implementation of `LineAggreagator` that
    considers the object in use is already aggregated and simply returns the String
    from the object using the `toString()` method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `FlatFileItemWriter` can be configured with the `PassThroughLineAggregator`
    , as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: FieldExtractor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the object writing is more than just writing its String form onto the file,
    `FieldExtractor` needs to be used, wherein each object gets converted to the array
    of fields, aggregated together to form a String to write onto the file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Field extractors are primarily of two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PassThroughFieldExtractor`: For the scenario where the object collection has
    to just be converted to the array and passed to write'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BeanWrapperFieldExtractor`: With a field-level configuration of how each field
    of the object gets placed in the String to write onto the file, this works exactly
    the opposite way of `BeanWrapperFieldSetMapper`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `BeanWrapperFieldSetExtractor` works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Writing delimited files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the Java object can be written onto the flat files in delimited file format,
    we can perform it as shown in the following example. Let's consider the `Employee`
    object defined already.
  prefs: []
  type: TYPE_NORMAL
- en: 'This object can be configured with the `FlatFileItemWriter`, the `DelimitedLineAggregator`,
    and the `BeanWrapperFieldExtractor` to perform the delimited flat file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Writing a fixed width file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spring Batch supports fixed width file writing with the help of `FormatterLineAggregator`.
    Considering the same example data as delimited flat file writing, we can perform
    the fixed width file writing using the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The format value is formed based on the following listed formatter conversions,
    where `arg` represents the argument for conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Conversion | Category | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `b`, `B` | general | This converts Boolean to the String format. The value
    is `false` for `null` |'
  prefs: []
  type: TYPE_TB
- en: '| `h`, `H` | general | This is the `Integer.toHexString(arg.hashCode())` |'
  prefs: []
  type: TYPE_TB
- en: '| `s`, `S` | general | If `arg` implements `Formattable`, then `arg.formatTo()`
    Otherwise, `arg.toString()` |'
  prefs: []
  type: TYPE_TB
- en: '| `c`, `C` | character | This is a Unicode character |'
  prefs: []
  type: TYPE_TB
- en: '| `d` | integral | This is a decimal integer |'
  prefs: []
  type: TYPE_TB
- en: '| `o` | integral | This is an octal integer |'
  prefs: []
  type: TYPE_TB
- en: '| `x`, `X` | integral | This is a hexadecimal integer |'
  prefs: []
  type: TYPE_TB
- en: '| `e`, `E` | floating point | This is a decimal number in computerized scientific
    notation |'
  prefs: []
  type: TYPE_TB
- en: '| `f` | floating point | This is a decimal number |'
  prefs: []
  type: TYPE_TB
- en: '| `g`, `G` | floating point | This is a computerized scientific notation or
    decimal format, depending on the precision and value after rounding |'
  prefs: []
  type: TYPE_TB
- en: '| `a`, `A` | floating point | This is a hexadecimal floating point number with
    a significand and an exponent |'
  prefs: []
  type: TYPE_TB
- en: '| `t`, `T` | date/time | This is the prefix for date and time conversion characters
    |'
  prefs: []
  type: TYPE_TB
- en: '| `%` | percent | This is a literal `%` (`\u0025`) |'
  prefs: []
  type: TYPE_TB
- en: '| `n` | line separator | This is the platform-specific line separator |'
  prefs: []
  type: TYPE_TB
- en: '`FlatFileItemWriter` can be configured with the `shouldDeleteIfExists` option,
    to delete a file if it already exists in the specified location. The header and
    footer can be added to the flat file by implementing `FlatFileHeaderCallBack`
    and `FlatFileFooterCallBack` and including these beans with the `headerCallback`
    and `footerCallback` properties respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: XML item writers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The data can be written to the **Extensible Markup Language** (**XML**) format
    using `StaxEventItemWriter`. The Spring Batch configuration for this activity,
    for the employee example can be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the XStream to do the marshalling activity, the following is the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The Java code for the preceding configuration can be realized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Database item writers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spring Batch supports database item writing with two possible access types:
    JDBC and ORM.'
  prefs: []
  type: TYPE_NORMAL
- en: JDBC-based database writing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spring Batch supports JDBC-based database writing with the help of `JdbcBatchItemWriter`,
    which is an implementation of `ItemWriter`, which executes multiple SQL statements
    in the batch mode. The following is the sample configuration for the employee
    example with the JDBC-based database writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ItemPreparedStatementSetter` can be implemented for our example of `Employee`
    data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: ORM-based database writing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Object** **relational mapping** (**ORM**) is defined as a programming technique
    to convert data between incompatible type systems in object-oriented programming
    languages. ORM takes care of the data persistence from the object oriented program
    to the database. Spring Batch supports multiple ORMs including Hibernate, JPA,
    and iBatis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![ORM-based database writing](img/3372OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In our example, the `Employee` class should be annotated to be used with ORM
    (Hibernate/JPA) for persistence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The annotations specify that the class `Employee` is representing a corresponding
    table in the database with a name as shown with `@Entity`, and each field corresponds
    to a column in the database as shown with the `@ID` and `@Column` annotations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the configuration to be made with Hibernate for the employee
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Similarly, for JPA and iBatis, the configurations can be made with `JpaItemWriter`
    and `IbatisBatchItemWriter` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Custom item readers and writers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Batch supports custom item readers' and writers' configurations. This
    can be done easily by implementing the `ItemReader` and `ItemWriter` interfaces
    for the respective read and write operations with the business logic we want,
    and configuring the `ItemReader` and `ItemWriter` in the XML batch configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Through this chapter we learned the essential data handling mechanisms, including
    reading the data from different sources (such as flat files, XML, and databases),
    processing the data, and writing the data to different destinations including
    flat files, XML, and databases. We also learned about transforming and validating
    the data in the processing data section. We finished this chapter with an understanding
    of the Spring Batch support to custom formats by implementing the interface to
    match the business needs that are different from the default formats. In the next
    chapter, we will learn about managing the transactions with diverse configurations
    and patterns in detail.
  prefs: []
  type: TYPE_NORMAL
