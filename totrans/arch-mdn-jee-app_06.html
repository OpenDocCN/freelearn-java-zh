<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Application Development Workflows</h1>
                </header>
            
            <article>
                
<p>kIn the previous chapter, we saw the necessity for software companies to move fast. This has an impact on the infrastructure and runtime environments and on the way teams of engineers are working together. The motivations behind modern environments are scalability, flexibility and <span>minimizing time and effort</span>.</p>
<p>Development workflows are even more important than the infrastructure alone. The whole process of writing source code until the running application is in production should be specified in a reasonable and productive way. Again, moving fast in a fast-moving world implies that these processes run automated and reliably with as little human intervention as possible.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>The motivations and necessity of Continuous Delivery</li>
<li>The contents of a productive pipeline</li>
<li>How to automate all steps involved</li>
<li>How to sustainably ensure and improve software quality</li>
<li>The required team culture and habits</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation and goals of productive development workflows</h1>
                </header>
            
            <article>
                
<p>Moving fast in terms of development workflows aims to enable fast feedback by fast turnarounds. In order to increase productivity, developers who work on the application's behavior need to verify the implemented features and bug fixes in a timely manner. This includes the time spent on builds, software tests, and deployments.</p>
<p>The key to productive workflows is automation. Software engineers should spend as much time as possible on designing, implementing, and discussing business logic and as little as possible on cross-cutting concerns and repetitive tasks. Computers are designed for quickly and reliably performing deterministic, straightforward tasks. Humans, however, are better at designing, thinking, and brainstorming creative and complex tasks. Simple, straightforward processes that don't require a lot of decision-making should therefore be performed by software.</p>
<p>Build systems are a good start. They automate compiling, resolving dependencies, and packaging of software projects. Continuous Integration servers take this approach further. They orchestrate the whole development workflow from building artifacts to automated testing and deployments. Continuous Integration servers are the <em>golden source of truth</em> of software delivery. They continuously integrate the work of all developers in a central place, making sure the project is in a shippable state.</p>
<p>Continuous Delivery continues the approach of Continuous Integration by automatically shipping the built software to certain environments on each build. Since software changes have to be verified properly before they go to production, applications are first deployed to test and staging environments. All deployment actions have to make sure the environment is prepared and configured and rolled out properly. Automated and manual end-to-end tests make sure the software works as expected. Deployment to production is then done in a <em>half-automated</em> way by triggering the automated deployment manually.</p>
<p>The difference between Continuous Delivery and Continuous Deployment is that the latter automatically deploys each committed software version to production, if the quality requirements are met, of course.</p>
<p>All these approaches minimize the developer intervention required, minimize turnaround times, and improve productivity.</p>
<p>Ideally, the Continuous Delivery approach supports not only rollouts but also reliable rollbacks. Software versions, although verified before, sometimes need to be rolled back for some reason. In such a situation, there is either the way of rolling forward, for example, by committing a new version that will undo the recent changes, or by rolling back to the working state.</p>
<p>As mentioned earlier, software should be built in a reliable way. All versions of used technology, such as build dependencies or application servers, are specified explicitly. Rebuilt applications and containers produce the same result. In the same way, pipeline steps of development workflows should result in the same outcome. It is crucial that the same application artifact that has been verified in test environments is deployed to production later on. Later in this chapter, we cover how to achieve reproducible, repeatable, and independent builds.</p>
<p>In terms of reliability, automated processes are an important aspect as well. Especially, deployments that are executed by software rather than human intervention are far less prone to error. All necessary pipeline steps are well defined and implicitly verified each and every time they are executed. This builds confidence into the automated processes, ultimately more than executing processes manually.</p>
<p>Verification and testing are important prerequisites of Continuous Delivery. Experience shows that the vast majority of software tests can be executed in an automated way. The next chapter will cover this topic in depth. Besides testing, quality assurance also covers the software quality of the project in regard to architecture and code quality.</p>
<p>Continuous Delivery workflows include all steps necessary in order to build, test, ship, and deploy software in a productive and automated way. Let's see how to build effective workflows.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Realizing development workflows</h1>
                </header>
            
            <article>
                
<p>Continuous Delivery pipelines consist of several pipeline build steps that are executed in sequence or in parallel, respectively. All the steps are executed as part of a single build. Builds are usually triggered by committing or rather pushing code changes into version control.</p>
<p>The following examines the aspects of a Continuous Delivery pipeline. These general steps are indifferent to the used technology.</p>
<p>The following diagram shows a high-level overview of a simplified Continuous Delivery pipeline. The steps are executed in a Continuous Integration server and use external repositories such as version control, artifact, and container repositories:</p>
<div class="CDPAlignCenter CDPAlign"><img height="283" width="813" src="assets/b958f901-28cc-4ee0-bbd3-aa877f33d055.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Version control everything</h1>
                </header>
            
            <article>
                
<p>Developers agree that source code should be kept under version control. Distributed version controls such as Git have been widely accepted as state-of-the-art tools. However, as mentioned earlier, besides application source code, there are more assets to track.</p>
<p>The motivation behind infrastructure as code is to keep all artifacts needed to ship the application in one central place. All changes made to the application, configuration, or environment are represented as code and checked in to the repository. Infrastructure as code leverages reproduciblity and automation. Taking this approach further also includes the definition of Continuous Delivery pipelines as code. The <em>Pipeline as code</em> section will cover this approach with the widely used Jenkins server as an example.</p>
<p>As we have seen in the previous chapter, the first principle of 12-factor applications is in fact to keep all files and artifacts needed to build and run the application in one repository.</p>
<p>The first step of the Continuous Delivery pipeline is to check out a specific commit from the version control repository. Teams that use <span>distributed version control systems need to synchronize the</span> desired state to a centralized repository. The Continuous Integration server takes the state of a specific commit in history to start the build process.</p>
<p>The reason behind taking a specific commit version rather than just the latest state is to enable reproducibility. Rebuilding the same build version can only reliably result in the same outcome if the build is based on a specific commit. This is only possible if the build originated from checking-in to version control with a particular commit. Check-in actions usually trigger builds from the corresponding commit version.</p>
<p>Checking out the state of the repository provides all sources and files necessary. The next step is to build the software artifacts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building binaries</h1>
                </header>
            
            <article>
                
<p>As we have seen in the first chapter, the term binaries includes all executable artifacts that run the enterprise application. The project repository only contains source code and files and artifacts required by the infrastructure. The binaries are built by the Continuous Integration server.</p>
<p>A step in the pipeline is responsible for building these binaries and making them accessible in a reliable way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java artifacts</h1>
                </header>
            
            <article>
                
<p>In Java EE, binaries first of all include the packaged enterprise application in form of an archive. Following the approach of zero-dependency applications results in building and packaging the project into a thin WAR file, containing only the application's business logic. This build action includes to resolve required dependencies, compile Java sources, and package the binary classes and other files into the archive. <span>The WAR files are the first produced artifact within the build pipeline.</span></p>
<p>The application artifacts are built using build systems such as Maven or Gradle, which are installed and executed on the CI server. Usually, the project build already executes basic code level tests. Tests that are executed on code level without requiring a container runtime can verify the behavior of classes and components early in the pipeline. The Continuous Delivery approach of failing fast and breaking the build as early as possible minimizes turnaround times.</p>
<p>If required, build systems can publish the artifacts onto an artifact repository. Artifact repositories, such as <strong>Sonatype Nexus</strong> or <strong>JFrog Artifactory</strong>, save the built artifact versions for later retrieval. However, if the application is shipped in Linux containers, the artifact doesn't necessarily have to be deployed onto a repository.</p>
<p>As shown in <a href="">Chapter 2</a>, <em>Designing and Structuring Java Enterprise Applications</em>, a Java project is built using Maven via the command <kbd>mvn package</kbd>. The package phase compiles all Java production sources, compiles and executes the test sources, and packages the application in our case, to a WAR file. The CI server executes a build system command similar to this to build the artifact in its local workspace directory. The artifact can be deployed to an artifact repository<span>,</span> for example, using the <kbd>mvn deploy</kbd> command, <span>to be used</span> <span>in subsequent steps</span>; or it will be taken directly from the workspace directory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Artifact versions</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, the build systems need to produce artifacts in a reliable way. This requires that Java artifacts are built and archived with a distinct version, which is identifiable later on. Software tests verify specific versions of enterprise applications. Later deployments need to refer the identical versions in later build steps as well. Being able to identify and refer to distinct artifact versions is necessary. This is true for all binaries.</p>
<p>One of the 12-factor principles is to explicitly declare dependencies, not only for dependencies being used but also in regard to their versions. As mentioned earlier, the same holds true for container builds. Specified Docker base images as well as installed software should be explicitly, uniquely identified by their versions.</p>
<p>It is quite common, however, to specify Java builds as <em>snapshot</em> versions, for example, <kbd>0.1-SNAPSHOT</kbd>. A snapshot, as opposed to a release version, represents a software state which is currently being developed. Dependency resolution always attempts to include the latest snapshot w<span>hen several snapshot versions are existent</span>, comparable to the Docker <kbd>latest</kbd> tag. The workflow behind snapshots is to release the snapshot version to a uniquely numbered version, once the level of development is sufficient.</p>
<p>However, snapshot versioning contradicts the idea of Continuous Delivery. In CD pipelines every commit is a potential candidate for production deployment. Snapshot versions are naturally not meant to be deployed on production. This implies that the workflow would need to change the snapshot to a release version, once the software version has been sufficiently verified. However, once built, Java artifacts are not meant to be changed. The same artifact that has been verified should be used for deployment. Therefore, snapshot versions do not fit Continuous Delivery pipelines.</p>
<p>Following the widely adopted approach of <strong>semantic versioning</strong>, application developers need to take care of their versions in regard to backward-compatibility. A semantic versioning describes software versions such as <kbd>1.1.0</kbd>, <kbd>1.0.0-beta</kbd>, or <kbd>1.0.1+b102</kbd>. In order to represent versions that are both eligible for Continuous Delivery and provide semantic versioning metadata, properly numbered versions with unique build metadata are a good solution. An example is <kbd>1.0.1+b102</kbd>, for <em>major</em> version <kbd>1</kbd>, <em>minor</em> version <kbd>0</kbd>, <em>patch</em> version <kbd>1</kbd>, and build number <kbd>102</kbd>. The part after the plus sign represents the optional build metadata. Even if the semantic version was not changed in between a number of builds, the produced artifacts are still identifiable. The artifacts can be published to an artifact repository and retrieved via these version numbers later on.</p>
<p>This versioning approach targets enterprise application projects rather than products. Products which have multiple shipped and supported versions at a time, require to have more complex versioning workflows.</p>
<p>At the time of writing, there isn't a de facto standard for versioning containers yet. Some companies follow a semantic versioning approach whereas others exclusively use CI server build numbers or commit hashes. All of these approaches are valid, as long as container images aren't rebuilt or distributed using the same tag twice. A single build must result in a distinct container image version.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building containers</h1>
                </header>
            
            <article>
                
<p>Container images also represent binaries, since they contain the running application, including runtime and operating system binaries. In order to build container images, base images and all artifacts that are added at build time need to be present. If they don't already exist on the build environment, base images are retrieved implicitly.</p>
<p>For each build step defined in the Dockerfile, an image layer is added on top of the previous layer. Last but not least, the application that was built just before is added to the container image build. As shown previously, Java EE application containers consist of an installed and configured application server that auto-deploys the web archive at runtime.</p>
<p>This image build is orchestrated by the CI server as part of the pipeline. One solution is to have the Docker runtime installed, in the same way as the Maven build system. The pipeline step then simply invokes an image build similar to <kbd>docker build -t docker.example.com/hello-cloud:1 .</kbd> in the job workspace directory. The Docker image build, for example, takes the WAR file under Maven's <kbd>target</kbd> directory and adds it into the container.</p>
<p>The built image is tagged with an image name and unique tag, depending on the build number or some other unique information. Docker image names imply the registry they will be pushed to. An image identifier such as <kbd>docker.example.com/hello-cloud:1</kbd> will implicitly be transmitted from and to the host <kbd>docker.example.com</kbd>. The pipeline pushes the image to the Docker registry in most cases, a company-specific registry.</p>
<p>Depending on the company's workflow, Docker images can be re-tagged as part of the pipeline as well. For example, special tags such as the <kbd>latest</kbd> tag can refer to the actual <em>latest</em> built versions and so on. This is accomplished by explicitly re-tagging the image, so that two identifiers point to the same image. Unlike Java archives, Docker images can be re-tagged without changing their contents. The second tag needs to be pushed to the repository, as well. However, the rest of this chapter will show you that it's not required to refer to images using <em>latest</em> versions, such as the Docker <kbd>latest</kbd> tag. In fact, similar to snapshot versioning it's advisable to avoid <em>latest</em> versions. Being explicit in all artifact versions is less prone to error.</p>
<p>Some engineers argue that running Docker builds inside the CI server may not be the best idea if the CI server itself runs as a Docker container. Docker image builds start temporarily running containers. It is certainly possible to either run containers in a container or connect the runtime to another Docker host, without opening the whole platform to potential security concerns. However, some companies choose to build the images outside of the CI server instead. For example, OpenShift, a PaaS built on top of Kubernetes, provides build functionality that comprises a CI server as well as image builds. It is therefore possible to orchestrate image builds from the CI server which are then built in the OpenShift platform. This provides an alternative to building container images directly on the CI server.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quality assurance</h1>
                </header>
            
            <article>
                
<p>The Java artifact build already performs some basic quality assurance. It executes included code level tests, such as unit tests. A reasonable pipeline consists of several test scopes and scenarios, all with slightly different strengths and weaknesses. The included unit tests operate at code level and can be executed without any further running environment. They aim to verify the behavior of the individual classes and components and provide fast feedback in case of test failures. We will see in the next chapter that unit tests need to run self-sufficiently and fast.</p>
<p>Test results are usually recorded from the CI server for visibility and monitoring reasons. Making the outcome of the pipeline steps visible is an important aspect of Continuous Delivery. The CI server can track the number of passed unit tests and show trends over time.</p>
<p>There are build system plugins available that track the code coverage of the executed tests. The coverage shows which parts of the code base have been executed during the test runs. Generally speaking, a greater code coverage is desirable. However, a high percentage of coverage alone tells nothing about the quality of tests and coverage of test assertions. The test results, together with their coverage, are just one of a few quality characteristics.</p>
<p>Source code can already provide a lot of information about the software's quality. So-called <strong>static code analysis</strong> performs certain quality checks on the static source code files of the project without executing them. This analysis gathers information about code statements, class and method sizes, dependencies between classes and packages, and complexity of methods. Static code analysis can already find potential errors in the source code, such as resources that are not properly closed.</p>
<p><strong>SonarQube</strong> is one of the most well-known code quality tools. It provides information about the quality of software projects by correlating the results of different analysis methods, such as static code analysis or test coverage. The merged information is used to provide helpful quality metrics for software engineers and architects. For example, which methods are complex but at the same time sufficiently tested? Which components and classes are the biggest in size and complexity and therefore candidates to be refactored? Which packages have cyclic dependencies and likely contain components that should be merged together? How does the test coverage evolve over time? How many code analysis warnings and errors are there and how does this number evolve over time?</p>
<p>It's advisable to follow some basic guidelines r<span>egarding static code analysis</span>. Some metrics just give insights in terms of rough ideas about the software quality. Test coverage is such an example. A project with high coverage does not necessarily imply well-tested software; the assertion statements could be impractical or insufficient. However, the trend of test coverage does give an idea about the quality, for example, whether software tests are added for new and existing functionality and bug fixes.</p>
<p>There are also metrics that should be strictly followed. Code analysis warnings and errors are one of these. Warnings and errors tell engineers about code style and quality violations. They are indicators about issues that need to be fixed.</p>
<p>First of all, there should be no such things as compilation or analysis warnings. Either the build passes the quality checks sufficiently, a <em>green traffic light</em>; or the quality is not sufficient for deployment, a <em>red traffic light</em>. There is nothing reasonable in between. Software teams need to clarify which issues are plausible and to be resolved and which aren't. Warnings that indicate minor issues in the project therefore are treated as errors; if there is a good reason to resolve them, then the engineers have to, otherwise the build should fail. If the detected error or warning represents a <em>false positive</em>, it won't be resolved; instead, it has to be ignored by the process. In that case, the build is successful.</p>
<p>Following this approach enables a <strong>zero-warning policy</strong>. Project builds and analyses that contain a lot of errors and warnings all the time, even if they are not critical, introduce certain issues. The existing warnings and errors obfuscate the quality view of the project. Engineers won't be able to tell on the first look whether the hundreds of issues are actually issues or not. Besides that, having a lot of issues already demotivates engineers to fix newly introduced warnings at all. For example, imagine a house that is in a terrible condition, with damaged walls and broken windows. Nobody would care if another window gets broken or not. But a recently broken window of an otherwise pristine house that has been taken good care of urges the person in charge to take action. The same is true for software quality checks. If there are hundreds of warnings already, nobody cares about that last commit's newly introduced violation. Therefore, the number of project quality violation should be zero. Errors in builds or code analyses should break the pipeline build. Either the project code needs to be fixed or the quality rules need to be adjusted for the issue to be resolved.</p>
<p>Code quality tools such as SonarQube are integrated in a build pipeline step. Since the quality analysis operates on static input only, the step can easily be parallelized to the next pipeline steps. If the quality gate does not accept the result, the build will fail and the engineers need resolve the issue before continuing development. This is an important aspect to integrate quality into the pipeline. The analysis should not only give insights but also actively prevent the execution to force action.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment</h1>
                </header>
            
            <article>
                
<p>After the binaries have been built and after, or during, the software quality is being verified, the enterprise application will be deployed. There are usually several environments for testing purposes, depending on the project circumstances, such as test or staging and, of course, production. As mentioned earlier, these environments should be as similar as possible. This vastly simplifies the deployment process orchestrated by the CI server.</p>
<p>The process of deploying the application generally takes the binaries in the version that has just been built and deploys them onto the environment. Depending on what the infrastructure looks like, this can take place using plain scripts or more sophisticated technology. The principle should be the same, the binaries as well as the configuration are made available to the environment in an automated and reliable way. Preparation steps that are potentially required by the application or the environment will be executed in this step as well.</p>
<p>Modern environments such as container orchestration frameworks support infrastructure as code. Infrastructure configuration is captured in files in the project's repository and applied to all environments at deployment time. Potential differences, such as Kubernetes config maps contents, are represented as different manifestations in the repository as well.</p>
<p>Using IaC as well as containers provides even more reliability than home-grown shell scripts. T<span>he application should always be rolled out in an idempotent way,</span> independent of which state the environment was in. Since container images contain the whole stack, the outcome is the same as if the software was installed from scratch. Required environment configuration is applied from IaC files as well.</p>
<p>New container image versions can be deployed by orchestration frameworks in many ways. There are certain commands that explicitly set Docker images used in Kubernetes deployments. However, in order to fulfill the requirement of reliability and reproducibility, it makes sense to only edit the infrastructure as code files and apply them on the cluster. This ensures that the configuration files stay the single source of truth. The CI server can edit the image definitions in the IaC files and commit the changes to the VCS repository.</p>
<p>As seen in the previous chapter, Docker images are specified in Kubernetes deployment definitions:</p>
<pre># deployment definition similar to previous chapter
# ...
    spec:
      containers:
      - name: hello-cloud
        <strong>image: docker.example.com/hello-cloud:1</strong>
        imagePullPolicy: IfNotPresent
        livenessProbe:
# ...</pre>
<p>These image definitions are updated within the CI server process and applied to the Kubernetes cluster. The CI server executes Kubernetes commands via the <kbd>kubectl</kbd> CLI. This is the standard way to communicate with Kubernetes clusters. <kbd>kubectl apply -f &lt;file&gt;</kbd> applies the infrastructure as code contents of a file or directory containing YAML or JSON definitions. The pipeline step executes a command similar to this, providing the updated Kubernetes files which were updated in the project repository.</p>
<p>Following this approach enables that infrastructure as code files both contain the current state of the environments as well as changes made by engineers. All updates are rolled out by applying the Kubernetes files in the <span>corresponding version</span> to the cluster. The cluster aims to satisfy the new desired state, containing the new image version, and will therefore perform a rolling update. After triggering this the update, the CI server validates whether the deployment has been executed successfully. Kubernetes rollout actions can be followed by commands similar to <kbd>kubectl rollout status &lt;deployment&gt;</kbd>, which waits until the deployment is either rolled out successfully, or failed.</p>
<p>This procedure is executed on all environments. If single deployment definitions are used for several environments, the image tag definition only has to be updated once, of course.</p>
<p>To give a more concrete example, the following shows a potential configuration file structure of a Maven project:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aa21a40d-2b34-45f4-a5c2-4bfa4b5e9c86.png"/></div>
<p>The <kbd>hello-cloud.yaml</kbd> file contains multiple Kubernetes resource definitions. This is possible by separating each YAML object definitions with a three-dashed line (<kbd>---</kbd>). It's equally doable to provide separate files for each resource type, such as <kbd>deployment.yaml</kbd>, <kbd>service.yaml</kbd>, and so on. Kubernetes can handle both approaches. The <kbd>kind</kbd> type definitions in the YAML objects indicate the type of the resource.</p>
<p>The previous chapter showed how container orchestration frameworks enable zero-downtime deployments out of the box. Applying new image versions to the environments orchestrated by the CI server also accomplishes this goal. The environments will therefore be able to serve traffic with at least one active application at a time. This approach is especially important for production environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuration</h1>
                </header>
            
            <article>
                
<p>Ideally, infrastructure as code covers all aspects required to define the whole environment, including runtimes, networking, and configuration. Using container technologies and container orchestration greatly supports and simplifies this approach. As mentioned earlier, confidential content such as credentials should not be put under version control. This should be configured manually on the environment by an administrator.</p>
<p>Configuration that differs in several environments can be represented using multiple files in the project repository. For example, it makes sense to include subfolders for each environment. The following image shows an example:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90519667-dfc0-4342-9896-3edeb17e8f4f.png"/></div>
<p>The contents of the <kbd>configmap.yaml</kbd> file include the specific config map contents as well as potentially different namespace definitions. As mentioned in the previous chapter, Kubernetes namespaces are a way to differentiate environments. The following code shows an example of a specific production config map:</p>
<pre>---
kind: ConfigMap
apiVersion: v1
metadata:
  name: <strong>hello-cloud-config</strong>
  namespace: <strong>production</strong>
data:
  application.properties: |
    hello.greeting=Hello production
    hello.name=Java EE
---</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Credentials</h1>
                </header>
            
            <article>
                
<p>Due to security reasons, secret content such as credentials is typically not included in the project repository. An administrator usually configures them manually on the specific environments. Similar to other Kubernetes resources, secrets are bound to a specific namespace.</p>
<p>If a project requires multiple secrets, for example, specific credentials for various external systems, manually configuring them can become cumbersome and difficult to keep track of. Configured secrets have to be documented and tracked in a secure form, external to the project repository.</p>
<p>Another approach is to store encrypted credentials that can be decrypted using a single master key <span>in the repository</span>. The repository can therefore safely contain the configured credentials, in encrypted form, and still be safe from disclosing the secrets. The running application will use the dynamically provided master key to decrypt the configured credentials. This approach provides security as well as manageability.</p>
<p>Let's look at a potential solution. Encrypted configuration values can safely be stored in Kubernetes config maps, since the decrypted values will only be visible to the container process. The project can define the encrypted credentials together with other configuration values in the config maps definitions as code. An administrator adds a secret to each environment, containing the master key which was used to symmetrically encrypt the credentials. This master key is provided to the running container, for example, using environment variables as seen earlier. The running application uses this single environment variable to decrypt all encrypted credential values.</p>
<p>Depending on the used technology and algorithm, one solution is to use the Java EE application to decrypt the credentials directly when loading properties files. To provide a secure solution using recent encryption algorithms, the <strong>Java Cryptographic Extensions</strong> (<strong>JCE</strong>) should be installed in the runtime. Another approach is to decrypt the values before the application is being deployed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data migration</h1>
                </header>
            
            <article>
                
<p>Applications that use a database to store their state are bound to a specific database schema. Changes in the schema usually require the application model to change and vice versa. With an application being actively developed and a domain model being continuously refined and refactored, the model will eventually require the database schema to change. New model classes or properties thereof which are added need to be persisted in the database as well. Classes and properties that are refactored or removed should be migrated in the database also, so that the schema doesn't diverge.</p>
<p>However, data migrations are more difficult than code changes. Stateless applications can simply be replaced by new versions thereof, containing the new functionality. A database that contains the application's state, however, needs to carefully migrate the state when the schema changes.</p>
<p>This happens in migration scripts. Relational databases support altering their tables while keeping the data intact. These scripts are executed before the new version of the software is deployed, making sure the database schema matches the application.</p>
<p><span>There is an important aspect to keep in mind when deploying</span> applications using a zero-downtime approach. Rolling updates will leave at least one active instance running in the environment at a time. This results in having both the old and the new software version active for a short period of time. The orchestration should take care that the applications are gracefully started and shut down, respectively, letting in-flight requests finish their work. Applications that connect to a central database instance will result in several versions of the application simultaneously accessing the database. This requires the application to support so-called <strong>N-1 compatibility</strong>. The current application version needs to function with the same database schema version plus and minus one version, respectively.</p>
<p>To support N-1 compatibility, the rolling update approach needs to both deploy a new application version and to updates the database schema, making sure the versions do not differ more than one version. This implies that, <span>the corresponding database migrations are executed</span> just before the application deployment takes place. The database schema, as well as the application, therefore evolves in small migration steps, not in jumps.</p>
<p>This approach, however, is not trivial and involves certain planning and caution. Especially, application version rollbacks require particular attention.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding database structures</h1>
                </header>
            
            <article>
                
<p>Adding tables or table columns to a database schema is comparatively straightforward. The new table or column does not collide with older application versions, since they are unknown to them.</p>
<p>New tables that resulted from new domain entities can simply be added to the schema, resulting in version <em>N+1</em>.</p>
<p>New table columns that define certain constraints, such as <em>not null</em> or <em>unique</em>, need to take care of the current state of the table. The old application version can still write to the table; it will ignore the new column. Therefore, constraints can not necessarily be satisfied. New columns first need to be <em>nullable</em> and without further constraints. The new application version has to deal with empty values in that column, presumably <kbd>null</kbd> values, which originate from the old application version.</p>
<p>Only the next version (<em>N+2</em>) will then, after the current deployment has been completed, contain the correct constraints. This means that adding a column that defines constraints needs at least two separate deployments. The first deployment adds the column and enhances the application's model in a <kbd>null</kbd>-safe way. The second deployment makes sure all contained values fulfill the column constraints, adds the constraints, and removes the <kbd>null</kbd>-safe behavior. These steps are, of course, only required, if the column target state defines constraints.</p>
<p>Rollbacks to the old versions work in a similar way. Rolling back to the intermediate deployment (<em>N+2</em> to <em>N+1</em>) requires the constraints to be removed again.</p>
<p>Rolling back to the original state (<em>N+0</em>) would remove the whole column. However, data migrations should not remove data that is not transferred somewhere else. Rolling back to the state without the column could also simply leave the column untouched so as not to lose data. The question the business experts have to answer is: What happens with the data that was added in the meantime? Intentionally not deleting this data could be a reasonable approach. However, when the column is added again, the rollout script needs to take already existing columns into consideration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing database structures</h1>
                </header>
            
            <article>
                
<p>Changing existing database tables or columns is more complex. Whether columns are renamed or changed in type or constraint, the transitions have to be executed in several steps. Directly renaming or changing columns would lead to incompatibilities with the deployed application instances; changes require intermediate columns.</p>
<p>Let's examine this approach using an example. Assume the car entity has a property <em>color</em>, which must be set, represented in the database column <kbd>color</kbd>. Assuming it will be refactored to the name <em>chassis color</em> or <kbd>chassis_color</kbd> in the database column.</p>
<p>Similar to the previous approach, the change is executed in several deployments. The first deployment adds a nullable column <kbd>chassis_color</kbd>. The application code is enhanced to use the new model property. Since the older application version doesn't know about the property yet, it is not reliably written from all places during the first deployment. Therefore, the first code version still reads the color from the old, <kbd>color</kbd> column, but writes values to both the old and new column.</p>
<p>The migration script on the next deployment updates the missing column values by overwriting the <kbd>chassis_color</kbd> column with the <kbd>color</kbd> column contents. By doing this, it is ensured that the new column is populated consistently. The not null constraint is added to the new column as well. The application code version will then only read from the new, but still write to both, because of the short period when the older version is still active.</p>
<p>The next deployment step removes the not null constraint from the <kbd>color</kbd> column. The application code of this version doesn't use the old column anymore, and both reads and writes to <kbd>chassis_color</kbd>.</p>
<p>The next and final deployment then drops the <kbd>color</kbd> column. Now all data has been gradually transferred to the new <kbd>chassis_color</kbd> column. The application code doesn't include the old model property anymore.</p>
<p>Changing column types or foreign key constraints require similar steps. The only way to gradually migrate databases with zero-downtime is to migrate in small steps, using intermediate columns and properties. It is advisable to perform several commits that only contain these changes to both the migration scripts and application code.</p>
<p>Similar to the previous approach, rollback migrations have to be executed in reverse, for both the database scripts and code changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing database structures</h1>
                </header>
            
            <article>
                
<p>Removing tables or columns is more straightforward than changing them. Once certain properties of the domain model are not required anymore, their usages can be removed from the application.</p>
<p>The first deployment changes the application code to stop reading from the database column but still to write to it. This is required to ensure that the old version can still read values other than <kbd>null</kbd>.</p>
<p>The next deployment will remove a potential not null constraint from the database column. The application code stops writing to column. In this step, occurrences of the model property can already be removed from the code base.</p>
<p>The final deployment step will drop the column. As mentioned before, it highly depends on the business use case whether column data should actually be dropped. Rollback scripts would need to recreate removed columns, which implies that the previous data is gone.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing migration</h1>
                </header>
            
            <article>
                
<p>As we have seen, data migrations have to be executed in several steps. Rollout as well as rollback scripts are executed right before the deployment. This implies that the application supports N-1 compatibility as well as that only one deployment is being executed at a time.</p>
<p>The migration process requires to perform several software releases, each of them consistent in application code and schema migration scripts. Engineers need to plan their commits accordingly. It's advisable to perform the full schema migration in a timely manner, to keep the database schema clean and to ensure that ongoing migrations aren't simply forgotten about.</p>
<p>It's in the nature of the corresponding model refactoring, whether existing data needs to be kept or can be discarded. Generally speaking, it is advisable to not throw away data. This means not to drop structures containing data that doesn't exist somewhere else.</p>
<p>As we have seen in the examples, the migrations will be applied in graceful steps; especially in regard to database constraints, such as <em>not null</em> or referential integrity constraints. Migration scripts should be resilient. For example, the migration should not fail when trying to create already existing columns. They could already exist from previous rollbacks. In general, it makes sense to think through and test different rollout and rollback scenarios upfront.</p>
<p>Engineers need to keep the update time in mind when <span>updating table contents</span>. Updating huge tables at once will take a non-negligible amount of time in which the data is potentially locked. This needs to be considered upfront; ideally, by testing the scripts in a separate database. For huge amount of data involved, the update steps can be executed in shards, for example, by partitioning the data by their IDs.</p>
<p>All rollout and rollback migration scripts should reside in the project repository. The database schema comprises a schema version that corresponds to the numbered migration scripts. This version is stored in the database as metadata together with the current schema state. Before every deployment, the database schema is migrated to its desired version. Right after that, the application with a corresponding version is deployed, making sure that the versions don't differ by more than one.</p>
<p>In a container orchestration framework this means that the database migration needs to be performed right before the new application version is deployed via rolling updates. Since there can be many replicas of pods, this process has to be idempotent. Executing the migration of a database schema to the same version twice, has to result in the same outcome. Kubernetes pods can define so-called <strong>init containers</strong> which execute <em>one-shot</em> processes before the actual containers start. Init containers run mutually exclusive. They have to exit successfully before the actual pod container process can be started.</p>
<p>The following code snippet shows an example of <kbd>initContainer</kbd>:</p>
<pre># ...
    spec:
      containers:
      - name: hello-cloud
        image: .../hello-cloud:1
      <strong>initContainers:</strong>
      - name: <strong>migrate-vehicle-db</strong>
        image: <strong>postgres</strong>
        command: <strong>['/migrate.sh', '$VERSION']</strong>
# ...</pre>
<p>The preceding example implies that the init container image contains the correct tooling to connect to the database instance as well as all recent migration scripts. In order to make this possible, this image is built as part of the pipeline, as well, including all migration scripts from the repository.</p>
<p>There are, however, many solutions to migrate database schemas. The important aspect here is that the idempotent migration needs to be executed upfront, while no second deployment action is being rolled out. The migration scripts of the corresponding versions would be executed in ascending or descending order, depending on whether the database schema version is upgraded or rolled back, until the version matches. After the scripts have been executed, the metadata version is updated in the database, as well.</p>
<p>The correlation between code and database versions can be tracked in the project repository. For example, the most recent rollout script contained in a commit version corresponds to the required database schema. The <em>Build metadata</em> section covers the topic of required metadata and where to store it in more depth.</p>
<p>Since the chosen migration solution highly depends on the project's technology, there is no <em>silver bullet</em> approach that can be shown here. The following example gives one possible solution on migration file structure and execution in <em>pseudo code</em>. It shows migration files for the example of changing the <kbd>color</kbd> column to <kbd>chassis_color</kbd> discussed earlier:</p>
<div class="CDPAlignCenter CDPAlign"><img height="361" width="398" src="assets/63d62980-027e-4ce4-a646-eec8916ec559.png"/></div>
<p>The preceding example shows the rollout and rollback scripts that migrate the database schema version to the desired state. Rollout script <kbd>004_remove_color.sql</kbd> transposes the schema version to version <kbd>4</kbd> by removing the <kbd>color</kbd> column of the example shown earlier. The corresponding rollback script <kbd>003_add_color.sql</kbd> rolls back the schema to version <kbd>3</kbd>, where the <kbd>color</kbd> column still existed; in other words, version <kbd>3</kbd> contains the <kbd>color</kbd> column whereas version <kbd>4</kbd> doesn't, with these two migration files being able to roll back and forth.</p>
<p>The following shows the pseudo code of the script that performs the migrations. The desired version to migrate to is provided as an argument when invoking the script:</p>
<pre><strong>current_version = select the current schema version stored in the database

if current_version == desired_version
    exit, nothing to do

if current_version &lt; desired_version
    folder = /rollouts/
    script_sequence = range from current_version + 1 to desired_version

if current_version &gt; desired_version
    folder = /rollbacks/
    script_sequence = range from current_version - 1 to desired_version

for i in script_sequence
    execute script in folder/i_*.sql
    update schema version to i</strong></pre>
<p>This migration script is executed in the init container before the actual deployment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing</h1>
                </header>
            
            <article>
                
<p>Verifying the output of the pipeline steps is one of the most important aspects in Continuous Delivery. It increases the software quality by detecting potential errors before going live. Proper verification creates reliability in the processes. By writing software tests in general and regression tests in particular, developers become confident in changing and refactoring functionality. Ultimately, software tests enable us to automate development processes.</p>
<p>Building binaries already executes code level tests. Other tests contained in the project may be executed in separate pipeline steps, depending whether they operate at code level or a running container. End-to-end tests, especially, require a running environment.</p>
<p>After the application has been deployed on test environments, end-to-end tests can be executed there. Usually, a project contains several layers of tests, with separate responsibilities, running in separate steps. There can be a great variety of tests, d<span>epending on the project and used technology</span>. The approach is always to execute pipeline steps and sufficiently verify the outcome. By doing so, the risk of breaking new or existing functionality and introducing potential errors is minimized. Especially, container orchestration frameworks with their <em>production-ready</em> nature support companies in the goal to ship scalable, highly available enterprise applications with high quality. <a href="">Chapter 7</a>, <em>Testing</em>, covers all different manifestations of testing, including its execution in Continuous Delivery pipelines.</p>
<p>Failing tests will immediately cause the pipeline to stop and will prevent the corresponding binary from being used further. This is an important aspect to enable fast feedback and also to enforce software quality in the process. Engineers should absolutely avoid to bypass steps of the normal process and other <em>quick fixes</em>. They contradict the idea of continuous improvement and building quality into the Continuous Delivery process and ultimately lead to errors. If a test or quality gate fails, the build has to break and either the application's code or the verification has to change.</p>
<p>Failing tests should not only break the build but also provide insights into why the step failed and record the result. This is part of the build's metadata.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Build metadata</h1>
                </header>
            
            <article>
                
<p>Build metadata records all information that is gathered during the execution of the build. Especially, the specific versions of all assets should be tracked for further reference.</p>
<p>Builds that run from the beginning to the end don't necessarily need further information. The steps are executed in one run until either the build breaks or finishes successfully. If, however, specific steps or artifacts are required to be referenced or re-executed, further information is required.</p>
<p>Artifact versions are the prime example for this necessity. A WAR file and its contents corresponds to a specific version in the VCS commit history. In order to track the originating commit from a deployed application, this information needs to be tracked somewhere. The same is true for container image versions. In order to identify the origin and contents of a container, the versions need to be traceable. Database schema versions are another example. A database schema version matches a specific application version, including the previous and the next version, by following N-1 compatibility. A deployment that migrates the database schema needs to know the schema version to migrate to for the desired application version.</p>
<p>Build metadata is especially required when the process enables rolling out specific application versions. In general, Continuous Delivery deployments roll forward to the current repository version. However, especially with database schemas and migrations involved, the possibility of rolling the environments to an arbitrary state is a huge benefit. The process in theory works like this: <em>take this specific application version and perform everything required in order to run it on this specific environment</em>, no matter whether the rollout is moving forward or backward.</p>
<p>In order to improve traceability and reproducibility, it is advisable to track quality information about the build as well. This includes, for example, results of automated tests, manual tests, or code quality analyses. The deployment steps then are able to verify the existence of specific metadata before deploying.</p>
<p>There are many solutions possible for representing metadata. Some artifact repositories such as JFrog Artifactory provide the possibility of linking built artifacts with custom metadata.</p>
<p>Another approach is to use the CI server to track this information. This sounds like a good fit to store metadata for a build; however, depending on how the CI server is operated and set up, it is not necessarily advisable to use it to store persistent data. Old builds can be discarded and lose information.</p>
<p>In general, the number of <em>points of truth</em>, for example, to store artifacts and information, should be kept low and explicitly defined. Using artifact repositories for metadata therefore certainly makes sense.</p>
<p>Another, more custom solution, is to use company VCS repositories to track certain information. The big benefit of using, for example, Git to store metadata is that it provides full flexibility of the data and structure being persisted. CI servers already contain functionality to access VCS repositories, therefore no vendor-specific tooling is required. Repositories can store all kind of information that are persisted as files, such as recorded test result.</p>
<p>The metadata repository, however implemented, is accessed at various points in the pipeline, for example, when performing deployments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going to production</h1>
                </header>
            
            <article>
                
<p>The last step in the Continuous Delivery pipeline is deploying to production. This deployment is either triggered manually or, when sufficient verification and automated tests are implemented, automatically. The vast majority of companies use a manually triggered deployment. But even if the pipeline does not go <em>all the way</em> from the beginning, Continuous Delivery provides great benefits by automating all steps necessary.</p>
<p>The pipeline then only has two kick-off spots: the initial commit to the repository that triggers the execution, and the final deployment to production after all steps have been verified, manually and automatically.</p>
<p>In a container orchestration environment, deploying to production, that is, either deploying to a separate namespace or a separate cluster, happens in the same way as deploying to test environments. Since the infrastructure as code definitions are similar or ideally identical to the ones executed before, this technology lowers the risk of environment mismatches to production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Branching models</h1>
                </header>
            
            <article>
                
<p>Software development processes can make use of different branching models. Software branches emerge from the same origin and differ in the state of development to make it possible to develop on multiple development stages in parallel.</p>
<p>Especially feature branches are a popular approach. Feature branching creates a separate branch which is used to develop a certain software feature. The branch is merged into the <em>master</em> branch or <em>trunk</em> after the feature is finished. The master branch and other branches remain untouched while the feature is being developed.</p>
<p>Another branching model is to use release branches. Release branches contain single software releases of a specific version. The idea is to have a dedicated point for a released version where bug fixes and features can be added. All changes made to the master branch that apply for the specific release as well are also made in the release branch.</p>
<p>However, branching models like these contradict the idea of Continuous Delivery. Feature branches, for example, postpone the integration of features into the master branch. The longer the integration of new functionality is delayed, the bigger the possibility for potential merge conflicts. Feature branches are therefore only advisable in Continuous Delivery pipelines if they are short-lived and integrated into master in a timely manner.</p>
<p>Release versions and working upon these releases in parallel contradicts the idea of continuously shipping versions as well. Features that are implemented are ideally shipped to production as soon as possible.</p>
<p>This is at least the case for enterprise projects. The continuous life cycle implies that every commit is a potential candidate for production deployment. It makes sense to integrate and apply the work on the master branch, making it possible to integrate and deploy features as early as possible, verified by automated tests. The branching model of Continuous Delivery and Continuous Deployment, respectively, therefore is quite straightforward. Changes are directly applied to the master branch, built, verified, and deployed by the build pipeline.</p>
<p>It's usually not required to manually tag releases. Every commit in the Continuous Delivery pipeline implicitly qualifies for being released and deployed to production, unless the automated verification identifies errors.</p>
<p>The following figure shows the concept of a Continuous Deployment branching model:</p>
<div class="CDPAlignCenter CDPAlign"><img height="220" width="530" src="assets/fb2ffb01-a248-4930-8d3f-226ca071588e.png"/></div>
<p>Individual features branches are kept short-lived and are merged back to <em>master</em> in a timely manner. The releases are implicitly created on successful builds. Broken builds won't result in a deployment to production.</p>
<p>Products, as well as libraries, however, may advisably have different branching models. With multiple supported <em>major</em> and <em>minor</em> versions, and their potential bug fixes, it makes sense to implement branches for separate release versions. The release version branches, such as <kbd>v1.0.2</kbd> can then be used to continue support for bug fixes, for example, into <kbd>v1.0.3</kbd>, while the major development continues on a newer version, such as <kbd>v2.2.0</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technology</h1>
                </header>
            
            <article>
                
<p>When designing Continuous Delivery pipelines, the question remains, which technology to use. This includes not only the CI server itself, but all the tools used within the development workflow such as version control, artifact repositories, build systems, and runtime technologies.</p>
<p>What technology is being used depends on the actual requirements and not least of what the team is familiar with. The following examples will make use of Jenkins, Git, Maven, Docker, and Kubernetes. As of writing this book these are widely-used technologies. However, for engineers it's more important to comprehend the underlying principles and motivations. The technology is quite interchangeable.</p>
<p>No matter what tools are selected, it's advisable to use the tools for what they are meant for. Experience shows that tools are quite often being misused for tasks that would better be executed using different technology. A prime example for this is the build system, for example Maven. Projects often define build processes that have more responsibilities than just building the artifacts.</p>
<p>It makes sense not to mix responsibilities of building containers or deploying software into the artifact build. These concerns are preferably realized directly by the Continuous Integration server. Bringing these steps into the build process unnecessarily couples the build technology to the environment.</p>
<p>It's therefore advisable to use the tools for what they were intended to do, in a straightforward way. For example, Docker containers are advisably built via the corresponding Docker binaries rather than build system plugins. Required abstraction layers are rather added in pipeline as code definitions, as demonstrated in the following examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pipeline-as-code</h1>
                </header>
            
            <article>
                
<p>We previously saw the benefits of representing configuration as code, primarily infrastructure as code files. The same motivations led to pipeline as code definitions, configuration that specifies the CI server pipeline steps.</p>
<p>In the past, many CI servers such as Jenkins required to be configured manually. CI server jobs had to be laboriously <em>clicked together</em> to build up pipelines. Especially, rebuilding pipelines for new applications or feature branches thereof required cumbersome manual work.</p>
<p>Pipeline as code definitions specify the Continuous Delivery pipeline as part of the software project. The CI server builds up and executes the pipeline appropriately, following the script. This vastly simplifies defining and reusing project build pipelines.</p>
<p>There are a lot of CI servers that support pipeline definitions as code. The most important aspect is that engineers understand the motivations and benefits behind this technology. The following shows examples for Jenkins, a widely used CI server in the Java ecosystem.</p>
<p>Users of Jenkins can craft pipelines in a <kbd>Jenkinsfile</kbd>, which is defined using a Groovy DSL. Groovy is an optionally typed, dynamic JVM language, that suits well for DSL and scripts. Gradle build scripts use a Groovy DSL, as well.</p>
<p>The following examples show the steps of a very simple pipeline of a Java enterprise project. The examples are meant to give a rough understanding of the executed process. For full information on Jenkinsfiles, their syntax and semantics, refer to the documentation.</p>
<p>The following shows an example <kbd>Jenkinsfile</kbd>, containing a basic pipeline definition.</p>
<pre>node {
    prepare()

    stage('<strong>build</strong>') {
        build()
    }

    parallel failFast: false,
            'integration-test': {
                stage('<strong>integration-test</strong>') {
                    integrationTest()
                }
            },
            'analysis': {
                stage('<strong>analysis</strong>') {
                    analysis()
                }
            }

    stage('<strong>system-test</strong>') {
        systemTest()
    }

    stage('<strong>performance-test</strong>') {
        performanceTest()
    }

    stage('<strong>deploy</strong>') {
        deployProduction()
    }
}

// method definitions</pre>
<p>The <kbd>stage</kbd> definitions refer to steps in the Jenkins pipeline. Since the Groovy script offers a full-fledged programming language, it is possible and advisable to apply clean code practices that produce readable code. Therefore, the contents of the specific steps are refactored to separate methods, all in the same layer of abstraction.</p>
<p>The <kbd>prepare()</kbd> step, for example, encapsulates several executions to fulfill build prerequisites, such as checking out the build repository. The following code shows its method definition:</p>
<pre>def prepare() {
    deleteCachedDirs()
    checkoutGitRepos()
    prepareMetaInfo()
}</pre>
<p>The build stage also encapsulates several sub-steps, from executing the Maven build, recording metadata and test results, to building the Docker images. The following code shows its method definition:</p>
<pre>def build() {
    buildMaven()
    testReports()
    publishArtifact()
    addBuildMetaInfo()

    buildPushDocker(dockerImage, 'cars')
    buildPushDocker(databaseMigrationDockerImage, 'cars/deployment/database-migration')
    addDockerMetaInfo()
}</pre>
<p>These examples provide insight into how to define and encapsulate specific behavior into steps. Providing detailed Jenkinsfile examples is beyond the scope of this book. I will show you the rough steps necessary to give an idea of what logical executions are required, and how to define them in these pipeline scripts in a readable, productive way. The actual implementations, however, heavily depend on the project.</p>
<p>Jenkins pipeline definitions provide the possibility to include so-called pipeline libraries. These are predefined libraries that contain often-used functionality to simplify usage and reduce duplication beyond several projects. It is advisable to outsource certain functionality, especially in regard to environment specifics, into company-specific library definitions.</p>
<p>The following example shows the deployment of the <em>car manufacture</em> application to a Kubernetes environment. The <kbd>deploy()</kbd> method would be called from within the build pipeline when deploying a specific image and database schema version to a Kubernetes namespace:</p>
<pre>def deploy(String namespace, String dockerImage, String databaseVersion) {
    echo "deploying $dockerImage to Kubernetes $namespace"

    updateDeploymentImages(dockerImage, namespace, databaseVersion)
    applyDeployment(namespace)
    watchRollout(namespace)
}

def updateDeploymentImages(String dockerImage, String namespace, String databaseVersion) {
    updateImage(dockerImage, 'cars/deployment/$namespace/*.yaml')
    updateDatabaseVersion(databaseVersion 'cars/deployment/$namespace/*.yaml')

    dir('cars') {
        commitPush("[jenkins] updated $namespace image to $dockerImage" +
            " and database version $databaseVersion")
    }
}

def applyDeployment(namespace) {
    sh "kubectl apply --namespace=$namespace -f car-manufacture/deployment/$namespace/"
}

def watchRollout(namespace) {
    sh "kubectl rollout status --namespace=$namespace deployments car-manufacture"
}</pre>
<p>This example updates and commits the Kubernetes YAML definitions in the VCS repository. The execution applies the infrastructure as code to the Kubernetes namespace and waits for the deployment to finish.</p>
<p>These examples aim to give the reader an idea of how to integrate Continuous Delivery pipelines as pipeline as code definitions with a container orchestration framework such as Kubernetes. As mentioned earlier, it is also possible to make use of pipeline libraries to encapsulate often-used <kbd>kubectl</kbd> shell commands. Dynamic languages such as Groovy allow engineers to develop pipeline scripts in a readable way, treating them with the same effort as other code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workflows with Java EE</h1>
                </header>
            
            <article>
                
<p>The demonstrated examples cover general Java build piplines which are, of course, applicable to Java EE as well. In fact, using Java Enterprise highly supports productive development pipelines. Fast builds and therefore fast developer feedback is crucial to effective Continuous Delivery workflows.</p>
<p><span>Zero-dependency applications, especially when packaged in containers, leverage these principles a</span>s we have seen in <a href="f0a49441-e411-49c4-a4b6-c6193ba36094.xhtml">Chapter 4</a>, <em>Lightweight Java EE</em>. The enterprise application in the packaged artifact or the container layer, respectively, only contains the business logic that was developed against the API. The application container provides the implementation.</p>
<p>The Continuous Delivery pipeline benefits from zero-dependency applications, since the involved build and distribution steps only require short execution and transmission times, respectively. Artifact builds as well as container builds run as fast as they can get, with only copying what's absolutely necessary. In the same way, publishing and deploying artifacts, as well as container layers, only contain the required business concerns, to minimize transmission time. This leads to fast turnaround and fast feedback.</p>
<p>Having effective pipelines is crucial to implementing a Continuous Delivery culture in the development team. Engineers are motivated to check in early and often, since the pipeline runs fast, provides fast feedback, and increases the confidence that the software quality is met.</p>
<p>As mentioned earlier, build times should not take more than a few seconds. Build pipeline executions, including end-to-end tests, should not take more than a few minutes, ideally even faster.</p>
<p>Putting effort into making builds and pipelines run faster should be a goal of the engineering team. During a workday, developers often build and check in the project. Every check-in results in a Continuous Delivery build that is a potential candidate for production deployment. If this overall process takes just, for example, 1 minute longer, all developers in the team wait 1 minute longer, every time they build the software. One can imagine that this delay adds up to a big number over time. Developers are tempted to check in less often if they have to wait for their result.</p>
<p>Improving the stability and performance of the pipeline, therefore, is a long-term investment in the team's productivity. Tests and steps that provide quick, helpful feedback by breaking the build faster in case of errors should run as early as possible. If some end-to-end tests run inevitably longer in time, due to the nature of the project and the tests, they can be defined in separate downstream pipelines steps, to not delay feedback of earlier verification. Steps that can run in parallel, such as static code analyses, should do so, to speed up the overall execution. Using the modern approaches of Java EE development greatly supports crafting productive build pipelines.</p>
<p>Still, technology is only one aspect of effective Continuous Delivery. Introducing Continuous Delivery has an even bigger impact on the development team's culture. Let's have a closer look into this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous Delivery culture and team habits</h1>
                </header>
            
            <article>
                
<p>Effective Continuous Delivery depends on a healthy team culture. If the team does not live by the principles and recommendations Continuous Delivery makes, the best technology doesn't help much. Pipelines that implement automated deployments have little value if there aren't sufficient software tests verifying the deployed software. The most eager CI server can't help much if developers seldom check in their changes, making integration hard and cumbersome. Full test coverage and code quality checks have no value if the team doesn't react to failing tests or, in the worst case, set the test execution to ignore.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Responsibility</h1>
                </header>
            
            <article>
                
<p>Continuous Delivery starts with being responsible for the software. As mentioned earlier, for the DevOps movement, it is not sufficient for developers to just build their software and let other teams deal with potential errors. The development team that creates and owns the application knows about its responsibilities, used technologies, and troubleshooting in case of potential errors.</p>
<p>Imagine a small startup that has only a single developer who responsible for the application. This person obviously has to deal with all technical issues, such as development, builds, deployment, and troubleshooting the application. He or she will have the best knowledge about the application's internals and will be able to fix potential issues effectively. Obviously, this single point of responsibility approach is the opposite of scalability and only works for tiny teams.</p>
<p>In bigger companies, there are more applications, more developers, and more teams with different responsibilities. The challenge with splitting and shifting responsibilities is to transfer knowledge. The knowledge is ideally spread within a team of engineers who closely work on the same software. Like in small startups, the mantra for developing applications should be: <em>you build it, you run it</em>. For a single team, this is only possible with the support of central, well-defined and automated processes. Implementing Continuous Delivery pipelines implement these processes to reliably ship software.</p>
<p>Managing and refining these processes becomes the responsibility of the whole team of engineers and is no longer an <em>ops problem</em>. All developers are equally responsible for building and shipping working software that provides value to the business. This certainly involves some duties, or team habits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Check in early and often</h1>
                </header>
            
            <article>
                
<p>Continuous Delivery has to be lived by the whole team. Developers who work on features or bug fixes should check in into the master branch early and often. This is crucial to enable Continuous Integration. The more time passes before changes are merged into the master branch, the harder the merging and integration of features becomes. Adding complex functionality in a big bang contradicts the idea of <em>continuous</em> evolution of software. Functionality that should not be visible to users yet can be excluded by feature toggles.</p>
<p>Checking in often encourages developers to write sufficient, automated software tests from the beginning. This is certainly an effort to make during development but will always pay off in the long run. While developing a feature, engineers are aware of its functionality and boundaries. It's far less effort to include not only unit tests but sophisticated end-to-end tests from the beginning then it is after the feature has been written.</p>
<p>Especially for less-experienced developers i<span>t's important to mention</span> that committing early, premature versions of features is nothing to be embarrassed about, but part of the development process. Code which hasn't been refactored yet and doesn't look perfect, but fulfills the requirements and provides business value, can be cleaned up in a second run. It's far more helpful to commit code early in the process than refraining from committing until the very last minute.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Immediately fixing issues</h1>
                </header>
            
            <article>
                
<p>Immediately solving build errors is another important team habit to develop. Tests that fail should not be ignored or postponed but fixed as soon as possible. Builds that fail often and are not taken good care of decrease the productivity of all team members. A failing test that makes the project unable to be built, for example, prevents other developers from integrating and verifying their features. Still, failing builds due to test failures or quality violations is a sign that the validation works and is, obviously, much better than false negatives, that is, mistakenly green builds. It is, however, important to fix project builds as soon as they fail. Developers should execute basic and fast verifications, such as building the Java project and executing code level tests, on their local machines before pushing to the central repository. They should take care not to misuse the pipeline to find careless mistakes which unnecessarily disturb other team members.</p>
<p>As mentioned earlier, compiler or code analysis warnings should be treated as errors that break the build. This introduces a zero-warning policy that urges engineers to either fix the issue or adjust the validation. Build, compilation, or code style warnings are therefore also errors that break the build and need to be fixed as soon as possible.</p>
<p>The team member whose commit caused the build to break should ideally be the first to look into the root cause. It is, however, a responsibility of the whole team to keep the pipeline in a healthy state. This goes back to the whole team being responsible for the whole project. There should not be exclusive <em>code ownership</em>, that is, parts of the projects which are exclusively known to a single team member. It will always be the case that developers who wrote specific functionality have better knowledge about it. Still, in all cases, the team should be able to work on all areas of the project and fix potential issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visibility</h1>
                </header>
            
            <article>
                
<p>The visibility that Continuous Delivery enables is another important aspect. The whole development process, including commits, builds, verifications, and deployments, can be tracked and comprehended in a single place. What visibility aspects are important in a Continuous Delivery pipeline?</p>
<p>First of all, it needs to be represented whether the software is in a shippable state. This includes the build's health in terms of compilation, tests, and code analyses. A dashboard or so-called <strong>extreme feedback</strong> device, such as physical green and red LEDs, provide a quick overview about it.</p>
<p>A reasonable build visibility ideally does not overload with information if the build is green but provides clear and direct insight in case of failing builds. This again follows the principle that there are no such things as warnings in the build; it either passes successfully and there is nothing else to do or it breaks and requires action. Dashboards or other devices that provide this <em>green or red</em> information already provide helpful insights. These visibility instruments should be accessible to all team members to embrace collaboration.</p>
<p>However, in order not to disrupt the day-to-day development too much, it makes sense to notify persons in charge, whose commits caused the build to break, first. They likely have further knowledge how to fix the build again without disturbing the work of their teammates if not necessary. CI servers provide functionality to send emails, use chat communication, or other forms of notification. This both increases the quality of the software as well as the developer's productivity.</p>
<p>The information that is gathered during builds can be used to measure the quality of the software project. This first of all includes build and test results and code quality metrics, such as test coverage. This information can be displayed over time to provide insights and trends about the software quality.</p>
<p>Other very interesting metadata concerns the build pipeline itself. How long does a build usually take? How many builds are there in a day? How often does the build fail? What is the most common failure cause? How long does it take a failing build to be fixed again (<em>time to recover</em>)? The answers to these questions provide helpful insights about the quality of the Continuous Delivery process.</p>
<p>The gathered information serves as good starting points to improve the process further. Visibility of Continuous Delivery not only illuminates the current project status but can also draw the engineers' attention to certain hotspots. The overall goal is to continuously improve the software.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improve continuously</h1>
                </header>
            
            <article>
                
<p>The whole mindset of Continuous Delivery aims to delivery software with consistent quality. Automated processes encourage the usage of quality verifications.</p>
<p>Good software quality, of course, does not come for free. Sufficient test cases as well as code quality analyses require a certain time and effort. Automation and continuously improving the quality, however, will, after an initial threshold, pay off in the long run, and eventually lead to better software.</p>
<p>New features as well as found bugs need to be verified sufficiently during development in order to ensure that functionality works as expected. By automating the tests and keeping them as regression, developers can be sure that no new bugs can disrupt the functionality in the future. The same is true for code quality analyses. Once the analysis is set up with appropriate rules and the found errors are eradicated, it ensures that no new violations can find their way into the software. If new false positive violations emerge, the rules are adjusted and will prevent new false positives in the future.</p>
<p>Introducing new test scenarios, such as end-to-end tests, also highly supports this approach. Regression tests decrease the risk of newly introduced bugs more and more. Again, automation is the key. As we will see in <a href="">Chapter 7</a>, <em>Testing</em>, human intervention is helpful for defining reasonable test scenarios. However, it is crucial to the software quality that these test are then automated made part of the pipeline. By doing so, the quality is improved more and more over time.</p>
<p>This, of course, requires the engineers to put a certain priority into quality improvements. Improving software quality, a<span>s well as refactoring,</span> doesn't provide any immediate benefits for the business. These efforts will, instead, pay off in the long run - by still being able to produce new features with a constant velocity or changing existing behavior with certainty that nothing else breaks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Productive development workflows require fast turnaround times as well as fast feedback. Automating repetitive tasks minimizes the times spent on build, tests and deployments. Zero-dependency Java EE applications supports fast feedback by minimizing build, publish, and deployment times.</p>
<p>It's important to define which category of errors will break the build. Developers should be aware that a build is either broken, due to legitimate errors, or passed, without anything to complain about. Warnings that have no effect on the build outcome have little value.</p>
<p>Data migration is another important topic to consider. Deploying stateless applications is comparably easy; what needs to be taken into account are the database schemas that need to match the application code. Rolling updates together with migration scripts, that rollout modifications in small changes, enable applications to be deployed with zero-downtime. Applications therefore need to support N-1 compatibility.</p>
<p>Continuous Delivery depends on a healthy team culture. It's not sufficient to implement just the technical necessities; all software engineers need to embrace the principles. Potential build issues, test results, software quality, and deployment statuses should be visible to the whole software team.</p>
<p>Continuous Delivery processes support to continuously improve the software. Verification steps that are added, such as automated software tests, run every time the application is built, enabling regression tests and avoiding specific bugs to happen twice. This of course requires developers to put effort into the quality improvement. The effort put into Continuous Delivery will pay off in the long run.</p>
<p>The following chapter stays in the field of software quality and will cover testing enterprise applications.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>