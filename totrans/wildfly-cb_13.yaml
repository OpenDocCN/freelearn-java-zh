- en: Chapter 13. Messaging with WildFly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Running the messaging system using HornetQ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending and receiving messages to/from a JMS queue destination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering HornetQ using a shared store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering HornetQ using message replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to configure HornetQ embedded in our WildFly
    in order to provide JMS capabilities to our applications. As WildFly is a Java
    EE 7-certified application server, it implements the JMS specification version
    2.0.
  prefs: []
  type: TYPE_NORMAL
- en: HornetQ is a **Message oriented middleware** (**MOM**) used to exchange messages
    in clustered and asynchronous systems. It can run standalone (I'm not talking
    about the WildFly standalone mode) or embedded, that is inside an application
    server such as WildFly, by using the **Java EE Connector Architecture** (**JCA**).
  prefs: []
  type: TYPE_NORMAL
- en: Because HornetQ uses the JCA to integrate itself with WildFly, it also provides
    out-of-the-box support for connection pooling, JTA transactions, and container
    managed security, features that simplify a developer's life. Thus, integrating
    your EJB application with the JMS system will not need additional work.
  prefs: []
  type: TYPE_NORMAL
- en: HornetQ uses two fundamental concepts, that is, acceptors and connectors. Acceptors
    determine how the HornetQ server accepts incoming connections, while connectors
    determine how to make connections with other HornetQ servers or JMS clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two types of acceptors and connectors exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '**invm**: The connections within the same JVM (more performance, obviously)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**netty**: The connections to/from remote JVMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each configured connector is used just to reach a server if there is the same
    type of acceptor configured on the other server. In other words, if we are making
    a connection with an `invm` connector, the other server must have an `invm` acceptor
    configured. On the other hand, if we are making a connection with a `netty` connector,
    the other server must have a `netty` acceptor configured.
  prefs: []
  type: TYPE_NORMAL
- en: HornetQ provides HA capabilities using a live server and a backup server. The
    backup server is in idle mode; it does not work until the live server fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, all messages are constantly replicated from the live server to
    the backup server. This synchronization is achieved in two ways: shared store
    and message replication. Shared store essentially means that both the live and
    backup servers share the same filesystem where messages are stored. Message replication,
    on the other hand, works via network traffic. Each server will then persist messages
    in its own local filesystem.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Only persistent messages are replicated; thus they survive a server crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'HornetQ has its own persistence, which is based on a high-performance journal
    filesystem. HornetQ uses three different journals:'
  prefs: []
  type: TYPE_NORMAL
- en: Bindings journal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JMS journal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Message journal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HornetQ supports two different configurations for shared stores:'
  prefs: []
  type: TYPE_NORMAL
- en: GFS2 on a SAN, using the AsyncIO journal type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NFSv4, using either the AsyncIO or NIO journal type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: NFS should only be used in a development environment, because of its performance
    issues.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can find the HornetQ configuration embedded into our WildFly configuration
    files, more precisely in the `standalone-full.xml` and `standalone-full-ha.xml`,
    if we are running in the standalone mode. However, if we are running in the domain
    mode, we can find the proper HornetQ configuration in the WildFly provided profiles
    such as `full` and `full-ha`. The `full` configuration enables the messaging systems,
    while the `full-ha` adds clustering and load-balancing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, it's assumed that you are familiar with JMS concepts such
    as Queue, Topic, MDB, DLQ, ExpiryQueue, point-to-point, and publish/subscribe,
    because we will not go too deep into JMS specification details. If you want to
    start learning about HornetQ, I really suggest you to read the excellent book
    *HornetQ Messaging Developer's Guide*, *Piero Giacomelli*, *Packt Publishing*,
    which goes deep into the development aspects of the framework and the JMS specification.
  prefs: []
  type: TYPE_NORMAL
- en: There are also alternatives to HornetQ, such as ActiveMQ. For a deeper look
    into ActiveMQ, please refer to official documentation at [http://activemq.apache.org](http://activemq.apache.org).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Architecting a messaging system is a hard and complex task; it involves a lot
    of aspects (memory, performance, network, storage, clustering, and so on) and
    there is no one-fits-all configuration. Every messaging system needs to be configured
    adhoc, tested and tuned a lot of times before going into production.
  prefs: []
  type: TYPE_NORMAL
- en: Running the messaging system using HornetQ
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to configure and run the WildFly messaging
    system provided by HornetQ. This is a warm-up recipe, just to get you ready.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, let''s first create an `adhoc` folder to run our WildFly. In
    a terminal window enter the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now it's time to run our WildFly!!!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open a terminal window and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now if we open the Web Console, we should find the **Messaging** subsystem as
    well, as depicted in the following screenshot:![How to do it…](img/3744_13_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Admin Console showing the Messaging subsystem
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Well, that was quite simple! Let's try something more exciting with the following
    recipes of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you should have noticed that we specified the `--server-config`
    directive, which overrides the default configuration filename, that is, `standalone.xml`
    for the standalone mode, and `host.xml` for the domain mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration filename specified is the one which has the messaging system
    pre-configured. If you remember from the earlier recipes of the book, in WildFly
    (as also in JBoss AS, and JBoss EAP) we have the following for different profiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '`default`: This should be used for common web applications (including REST
    backend)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`full`: This should be used for applications with JMS capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ha`: This should be used for common web applications that require clustering
    and balancing capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`full-ha`: This should be used for applications that require JMS, clustering,
    and balancing capabilities all together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the `standalone-full.xml` file gives us the proper configuration to run
    applications with JMS capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we launched the start-up script, the log showed us various entries. Take
    for example the following entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is complaining about the use of the native API to access the filesystem instead
    of using the Java NIO ones. This is because, as mentioned in the introduction,
    HornetQ persists JMS messages on the filesystem, so a better performant API is
    preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other interesting log entries are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding entries are relative to the multi-protocol interoperability that
    HornetQ provides (it doesn't implement just the JMS specification). They imply
    that the server (meaning HornetQ) is now ready to produce and consume messages,
    and all that comes with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last, but not least, the following entries describe the resource that we have
    preconfigured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We have connection factories, which are used to connect and obtain a session
    from the server to send and receive messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have two destinations: `jms.queue.DLQ` and `jms.queue.ExpiryQueue`.'
  prefs: []
  type: TYPE_NORMAL
- en: The first one is the *dead letter queue*, where all messages that couldn't be
    successfully processed (consumed) go to. When something goes wrong while consuming
    a message with a **Message Driven Bean** (**MDB**), the message gets rolled back
    and goes directly to the DLQ. Nevertheless, there are redelivery policies that
    can try to send our message again.
  prefs: []
  type: TYPE_NORMAL
- en: All expired messages go to the queue called `ExpiryQueue`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are running on Mac and using the `full-ha` profile, beware of the following
    network issue you might meet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HQ224033`: Failed to broadcast connector `configs: java.io.IOException`; Can''t
    assign requested address'
  prefs: []
  type: TYPE_NORMAL
- en: The `full-ha` profile, because of its additional clustering and balancing capabilities,
    provides auto-discovery to its HornetQ server, and it does so by using the multicast
    address. Often, UDP traffic gets dropped, so make sure that the environment you
    are in accepts UDP traffic.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in our case, we are probably trying the recipe at home, and UDP
    traffic gets forwarded to the ISP that drops such traffic. What we need to do
    is create a local route to redirect the UDP traffic to our loopback interface,
    which is our localhost, `127.0.0.1` IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add the multicast route to the loopback interface as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sudo route add 224.0.0.0 127.0.0.1 -netmask 240.0.0.0`'
  prefs: []
  type: TYPE_NORMAL
- en: Beware that when running in a production environment, adding this loopback route
    will prevent any and all multicast messages from reaching other servers.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a deeper understanding of the HornetQ server principles, please refer to
    the official documentation at [http://hornetq.jboss.org](http://hornetq.jboss.org).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending and receiving messages to/from a JMS queue destination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to produce and consume a JMS queue message.
    We will use two little applications, one that produces messages, and another one
    that consumes messages.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get started, let''s first create an `adhoc` folder to run our WildFly. In
    a terminal window enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this recipe, we will need an application to test our configuration. For this
    recipe, we will need the applications named `jms-producer` and `jms-consumer`,
    which you can find in my GitHub repository. If you skipped the *Managing applications
    using the deployments folder* recipe of [Chapter 2](ch02.html "Chapter 2. Running
    WildFly in Standalone Mode"), *Running WildFly in Standalone Mode*, please refer
    to it to download all the source code and projects that you will need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To build the application, enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now it's time to run our WildFly!!!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we need to add a user to the `ApplicationRealm` realm, which is
    the one used by the remoting subsystem and hence, the messaging subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a terminal window and execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now it''s time to run our WildFly as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once started, let''s connect to the CLI in a new terminal and create the queue,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In our `server.log`, we should find the following entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Great, we've successfully created our queue destination!!!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now it''s time to run our `jms-producer` and send a message to our `WildFlyCookbookQueue`
    queue. Let''s compile it and run it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of the preceding command is depicted in the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it…](img/3744_13_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Our producer sent ten text messages containing the following text: `A message
    by WildFly Cookbook`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'So if we now connect to the CLI, we can count the number of messages in our
    `WildFlyCookbookQueue` destination, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding CLI command shows, effectively, the number of messages we were
    expecting. Let''s try our consumer by executing the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of the preceding command is depicted in the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How to do it…](img/3744_13_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Great, we successfully consumed the messages that were stored in our queue!!!
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, when consuming a message with an MDB, if your MDB has a bug, or
    if it gets an exception for any reason, its process gets rolled back; thus, the
    message goes to the DLQ.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to understand what we have done and why, we need to look at the
    default messaging subsystem configuration. For the purpose of this recipe, we
    will analyze a specific setting such as the `security-setting`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The above XML code snippet defines that any ("#" symbol is a wildcard to indicate
    any) destination needs to have a specific roles for a specific permission, such
    as sending and consuming a message. Default settings specify the "guest" role
    for sending and consuming a message from any queue.
  prefs: []
  type: TYPE_NORMAL
- en: That's why we need to add a user, using the `add-user` script with the role,
    `guest`. Furthermore, why did we need a user at all?
  prefs: []
  type: TYPE_NORMAL
- en: 'As we are connecting to the destination remotely using the `http-remoting://localhost:8080`
    address, WildFly uses the remoting subsystem to handle remote connections, which
    has the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The remoting subsystem references the `ApplicationRealm` realm, that's why we
    added the `jmsuser` user to that realm.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the Java code side, we added the user''s reference as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If we hadn''t specified a user in the Java code (actually omitting the `SECURITY_PRINCIPAL`
    and `SECURITY_CREDENTIALS` properties), or even added to the realm, we would have
    ended up with the following error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logic seen in the preceding example, also applies to Topic. Within the
    CLI, we have pretty much the same commands for adding a `topic`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Information about the JMS specification can be found at [https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html](https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a deeper understanding of the HornetQ server principles, please refer to
    the official documentation at [http://hornetq.jboss.org](http://hornetq.jboss.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering HornetQ using a shared store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn how to configure HornetQ to provide clustering
    features by configuring to WildFly instances; one acting as a **Live** HornetQ
    server, the other one acting as a **Backup** HornetQ server.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall configuration can be represented as seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering HornetQ using a shared store](img/3744_13_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shared store configuration
  prefs: []
  type: TYPE_NORMAL
- en: Within the recipe, we will use a local shared store by giving each live and
    backup server pair the same `data` directory. This kind of configuration can also
    be very useful in both development and test environments to easily simulate high
    availability and failover capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a production environment, you should provide high availability by having
    more pairs of live and backup servers, and use a proper filesystem as depicted
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering HornetQ using a shared store](img/3744_13_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Shared store configuration with high-availability
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, let's first create two `adhoc` folders to run our WildFly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a terminal window, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this recipe, we will need two applications named `jms-producer` and `jms-consumer`,
    which you can find in my GitHub repository. If you skipped the *Managing applications
    using the deployments folder* recipe of [Chapter 2](ch02.html "Chapter 2. Running
    WildFly in Standalone Mode"), *Running WildFly in Standalone Mode*, please refer
    to it to download all the source code and projects that you will need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To build the application, execute the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, we need to add a user to the `ApplicationRealm` realm, which
    is the one used by the remoting subsystem and hence, the messaging subsystem:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a terminal window and enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now edit the file `standalone-full-ha.xml` of the `hq-live server`, and replace
    the `messaging` subsystem with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now it''s time to run our WildFly `live` instance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While starting the `live` server, edit the file `standalone-full-ha.xml` of
    the `hq-backup` server, and replace the `messaging` subsystem with the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now start the WildFly `backup` instance as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the live server''s log you should find the following entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Whilst within the backup server''s log, you should find the following entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Both the live and backup servers'' log entries confirm our configuration: the
    live server had gained the lock of the data directory, while the backup is waiting
    to acquire the lock and it has announced itself as the backup.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now it's time to do some tests.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To test our configurations, we need to first produce some messages, then check
    if the messages have been stored, and then consume them. In this case, compile
    the `jms-producer` project as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s produce some messages with the `jms-producer` project as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, if you check the message count, you will find ten messages on the live
    server. To check, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you invoke the preceding command on the backup server, you will get the
    following error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you stop the live server, the backup server should become the new live server.
    You should find the following log entries on the backup server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you invoke the count message command on the old backup server again, you
    will see it has all the messages that were produced earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mechanism behind the failover is pretty simple; because HornetQ bases its
    persistence on filesystem, it makes a lock on the data storage folder. Basically,
    for the same folder, the first server to arrive owns the lock and hence, wins.
    The second server then keeps on trying to gain the lock on the same data directory
    until it gets it, which means the live server has shutdown or crashed.
  prefs: []
  type: TYPE_NORMAL
- en: When the backup server gains the lock, it reads all the messages stored in the
    data directory; it is available to store more messages and clients consume them.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if the live server crashes while a client is producing or consuming
    messages?
  prefs: []
  type: TYPE_NORMAL
- en: Because of the failover and high-availability configuration (`<ha>true</ha>`
    in both, `<connection-factory name="RemoteConnectionFactory">` and `<pooled-connection-factory
    name="hornetq-ra">`), the client automatically reconnects to the first available
    live server. The client has the topology on the live and backup servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'While producing the messages, try stopping the live server. You should get
    an output similar to what is depicted in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3744_13_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Client reconnects to the first available live server while producing messages
  prefs: []
  type: TYPE_NORMAL
- en: 'The same applies when consuming messages. Following is an image showing what
    you should get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3744_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Client reconnects to the first available live server while consuming messages
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the introduction, there is a lot to talk about HornetQ, JMS,
    and MOM in general, but it's out of the scope of this book. For this reason, I
    suggest you read the book *HornetQ Messaging Developer's Guide*, *Piero Giacomelli*,
    *Packt Publishing*, which goes deep into the development aspects of the framework
    and the JMS specification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the jboss.org community has done a great job in providing us all with
    information about HornetQ, which is freely available at [http://hornetq.jboss.org](http://hornetq.jboss.org).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last, but not least, the JMS specification is available at [https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html](https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering HornetQ using message replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to configure a cluster with a live and a backup
    HornetQ server.
  prefs: []
  type: TYPE_NORMAL
- en: 'HornetQ cluster environment can be achieved by using a shared store (live and
    backup servers share the same and the entire data directory) as per the previous
    recipe, or via message replication which happens at the network layer—message
    replication mode can be achieved using the following setting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Our final configuration should provide the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering HornetQ using message replication](img/3744_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Message replication configuration
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get started, let's first create an `adhoc` folder to run our WildFly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a terminal window, enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Provide the application user for both the servers to send and receive messages,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this recipe, we will need an application named `cluster-jms-replication`,
    which you can find in my GitHub repository. If you skipped the *Managing applications
    using the deployments folder* recipe of [Chapter 2](ch02.html "Chapter 2. Running
    WildFly in Standalone Mode"), *Running WildFly in Standalone Mode*, please refer
    to it to download all the source code and projects that you will need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To build the application, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now it's time to configure our HornetQ servers in WildFly!!!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To configure our servers, we will rely on the configuration present in my GitHub
    repository, named `wildfly-cookbook`. There you can find a project named `cluster-jms-replication`,
    which has been provided by the reviewer Kylin Soong.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the folder where you download the repository (`~/WFC/github/wildfly-cookbook`),
    there is a project called `cluster-jms-replication`. From within the folder of
    the project, execute the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are ready to start our servers as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the servers are started, go to the `cluster-jms-replication` project folder
    and invoke the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will send ten messages to the HornetQ cluster. In the `node1` server log
    entries, you should find the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the `node2` server log entries, you should find the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will try a failover test by sending the first couple of messages to
    `node1`, and then stop it (regular shutdown or kill is the same). Again go to
    the `cluster-jms-replication` project folder, and invoke the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, as soon as you get the first two messages on server `node1`, stop it.
    At the end of the process, you should get the following log entries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`node1`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '`node2`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are quite a few things that need to be explained. Beware that only the
    persistent messages are replicated to the backup server. Thus, non-persistent
    messages will not fail over. In the live configuration, we added the following
    directives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The first one is quite obvious; it defines that the server will be the live
    one. The `failover-on-shutdown` means that the server will fail over to the backup
    servers, even in case of normal server shutdown, as we did in our test.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `check-for-live-server` directive is used along with the `allow-failback`,
    present in the backup server configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Basically, when the live server comes back up, it checks if there was a failover
    by contacting the other backup servers, and issuing a shutdown on the current
    live one for it to take over.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the introduction, there is a lot to talk about HornetQ, JMS,
    and MOM in general, but it's out of the scope of this book. For this reason, I
    suggest you read the book *HornetQ Messaging Developer's Guide*, *Piero Giacomelli*,
    *Packt Publishing*, which goes deep into the development aspects of the framework
    and the JMS specification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the jboss.org community has done a great job in providing us all with
    information about HornetQ, which is freely available at [http://hornetq.jboss.org](http://hornetq.jboss.org).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last, but not least, the JMS specification is available at [https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html](https://jcp.org/aboutJava/communityprocess/final/jsr343/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
