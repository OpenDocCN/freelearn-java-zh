["```java\nname := \"bitcoin-analyser\"\n\nversion := \"0.1\"\n\nscalaVersion := \"2.11.11\"\nval sparkVersion = \"2.3.1\"\n\nlibraryDependencies ++= Seq(\n  \"org.lz4\" % \"lz4-java\" % \"1.4.0\",\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % Provided,\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % Test classifier \n  \"tests\",\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % Provided,\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion % Test classifier   \"tests\",\n  \"org.apache.spark\" %% \"spark-catalyst\" % sparkVersion % Test     classifier \"tests\",\n  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.9.0\",\n  \"org.scalatest\" %% \"scalatest\" % \"3.0.4\" % \"test\",\n  \"org.typelevel\" %% \"cats-core\" % \"1.1.0\",\n  \"org.typelevel\" %% \"cats-effect\" % \"1.0.0-RC2\",\n  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion % Provided,\n  \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % sparkVersion % \n    Provided exclude (\"net.jpountz.lz4\", \"lz4\"),\n  \"com.pusher\" % \"pusher-java-client\" % \"1.8.0\")\n\n    scalacOptions += \"-Ypartial-unification\"\n\n// Avoids SI-3623\ntarget := file(\"/tmp/sbt/bitcoin-analyser\")\n```", "```java\nimport org.apache.spark.sql.SparkSession\nval spark = SparkSession.builder().master(\"local[*]\").getOrCreate()\nimport spark.implicits._\n```", "```java\nimport org.apache.spark.sql.Dataset\n\nval dsString: Dataset[String] = Seq(\"1\", \"2\", \"3\").toDS()\n// dsString: org.apache.spark.sql.Dataset[String] = [value: string]\n```", "```java\ndsString.show()\n```", "```java\n+-----+\n|value|\n+-----+\n|    1|\n|    2|\n|    3|\n+-----+\n```", "```java\nval dsInt = dsString.map(_.toInt)\n// dsInt: org.apache.spark.sql.Dataset[Int] = [value: int]\ndsInt.explain()\n```", "```java\n== Physical Plan ==\n*(1) SerializeFromObject [input[0, int, false] AS value#96]\n+- *(1) MapElements <function1>, obj#95: int\n   +- *(1) DeserializeToObject value#91.toString, obj#94: \n    java.lang.String\n      +- LocalTableScan [value#91]\n```", "```java\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.types.IntegerType\n\nval df = ds.select($\"value\".cast(IntegerType))\n// df: org.apache.spark.sql.DataFrame = [value: int]\nval dsInt = df.as[Int]\n// dsInt: org.apache.spark.sql.Dataset[Int] = [value: int]\n```", "```java\ndsInt.explain()\n```", "```java\n== Physical Plan ==\nLocalTableScan [value#122]\n```", "```java\n[\n  {\n    \"date\": \"1534582650\",\n    \"tid\": \"72519377\",\n    \"price\": \"6488.27\",\n    \"type\": \"1\",\n    \"amount\": \"0.05000000\"\n  },\n  {\n    \"date\": \"1534582645\",\n    \"tid\": \"72519375\",\n    \"price\": \"6488.27\",\n    \"type\": \"1\",\n    \"amount\": \"0.01263316\"\n  },\n  ...\n]\n```", "```java\nimport java.net.URL\nimport scala.io.Source\n\nval transactions = Source.fromURL(new URL(\"https://www.bitstamp.net/api/v2/transactions/btcusd/?time=hour\")).mkString\n```", "```java\npackage coinyser\n\ncase class HttpTransaction(date: String,\n                           tid: String,\n                           price: String,\n                           `type`: String,\n                           amount: String)\n```", "```java\npackage coinyser\n\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.test.SharedSparkSession\nimport org.scalatest.{Matchers, WordSpec}\n\nclass BatchProducerSpec extends WordSpec with Matchers with SharedSparkSession {\n  val httpTransaction1 = \n    HttpTransaction(\"1532365695\", \"70683282\", \"7740.00\", \"0\", \n    \"0.10041719\")\n  val httpTransaction2 = \n    HttpTransaction(\"1532365693\", \"70683281\", \"7739.99\", \"0\", \n    \"0.00148564\")\n\n  \"BatchProducer.jsonToHttpTransaction\" should {\n    \"create a Dataset[HttpTransaction] from a Json string\" in {\n      val json =\n        \"\"\"[{\"date\": \"1532365695\", \"tid\": \"70683282\", \"price\": \n            \"7740.00\", \"type\": \"0\", \"amount\": \"0.10041719\"},\n          |{\"date\": \"1532365693\", \"tid\": \"70683281\", \"price\": \n            \"7739.99\", \"type\": \"0\", \"amount\": \n            \"0.00148564\"}]\"\"\".stripMargin\n\n      val ds: Dataset[HttpTransaction] = \n        BatchProducer.jsonToHttpTransactions(json)\n      ds.collect() should contain theSameElementsAs \n        Seq(httpTransaction1, httpTransaction2)\n    }\n  }\n}\n```", "```java\npackage coinyser\n\nimport java.time.Instant\nimport java.util.concurrent.TimeUnit\n\nimport cats.Monad\nimport cats.effect.{IO, Timer}\nimport cats.implicits._\nimport org.apache.spark.sql.functions.{explode, from_json, lit}\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.{Dataset, SaveMode, SparkSession}\n\nimport scala.concurrent.duration._\nobject BatchProducer {\n\n  def jsonToHttpTransactions(json: String)(implicit spark: \n  SparkSession): Dataset[HttpTransaction] = {\n    import spark.implicits._\n    val ds: Dataset[String] = Seq(json).toDS()\n    val txSchema: StructType = Seq.empty[HttpTransaction].schema\n    val schema = ArrayType(txSchema)\n    val arrayColumn = from_json($\"value\", schema)\n    ds.select(explode(arrayColumn).alias(\"v\"))\n      .select(\"v.*\")\n      .as[HttpTransaction]\n  }\n}\n```", "```java\ndef jsonToHttpTransactions(json: String)(implicit spark: SparkSession)\n: Dataset[HttpTransaction] =\n```", "```java\nimport spark.implicits.\nval ds: Dataset[String] = Seq(json).toDS()\n```", "```java\n  val txSchema: StructType = Seq.empty[HttpTransaction].toDS().schema\n  val schema = ArrayType(txSchema)\n  val arrayColumn = from_json($\"value\".cast(StringType), schema)\n```", "```java\nds.select(explode(arrayColumn).alias(\"v\"))\n   .select(\"v.*\")\n   .as[HttpTransaction]\n```", "```java\npackage coinyser\n\nimport java.sql.{Date, Timestamp}\nimport java.time.ZoneOffset\n\ncase class Transaction(timestamp: Timestamp,\n                       date: Date,\n                       tid: Int,\n                       price: Double,\n                       sell: Boolean,\n                       amount: Double)\n```", "```java\nobject Transaction {\n  def apply(timestamp: Timestamp,\n            tid: Int,\n            price: Double,\n            sell: Boolean,\n            amount: Double) =\n    new Transaction(\n      timestamp = timestamp,\n      date = Date.valueOf(\n        timestamp.toInstant.atOffset(ZoneOffset.UTC).toLocalDate),\n      tid = tid,\n      price = price,\n      sell = sell,\n      amount = amount)\n}\n```", "```java\n  \"BatchProducer.httpToDomainTransactions\" should {\n    \"transform a Dataset[HttpTransaction] into a Dataset[Transaction]\" \n    in {\n      import testImplicits._\n      val source: Dataset[HttpTransaction] = Seq(httpTransaction1, \n        httpTransaction2).toDS()\n      val target: Dataset[Transaction] = \n        BatchProducer.httpToDomainTransactions(source)\n      val transaction1 = Transaction(timestamp = new \n        Timestamp(1532365695000L), tid = 70683282, price = 7740.00, \n        sell = false, amount = 0.10041719)\n      val transaction2 = Transaction(timestamp = new \n        Timestamp(1532365693000L), tid = 70683281, price = 7739.99, \n        sell = false, amount = 0.00148564)\n\n      target.collect() should contain theSameElementsAs \n        Seq(transaction1, transaction2)\n    }\n```", "```java\n  def httpToDomainTransactions(ds: Dataset[HttpTransaction]): \n    Dataset[Transaction] = {\n    import ds.sparkSession.implicits._\n    ds.select(\n      $\"date\".cast(LongType).cast(TimestampType).as(\"timestamp\"),\n      $\"date\".cast(LongType).cast(TimestampType).\n       cast(DateType).as(\"date\"),\n      $\"tid\".cast(IntegerType),\n      $\"price\".cast(DoubleType),\n      $\"type\".cast(BooleanType).as(\"sell\"),\n      $\"amount\".cast(DoubleType))\n      .as[Transaction]\n  }\n```", "```java\n+-------------------+--------+-------+-----+-------+\n|timestamp          |tid     |price  |sell |amount |\n+-------------------+--------+-------+-----+-------+\n|2018-08-02 07:22:34|       0|7657.58|true |0.1    |\n|2018-08-02 07:22:47|       1|7663.85|false|0.2    |\n|2018-08-02 07:23:09|       2|7663.85|false|0.3    |\n+-------------------+--------+-------+-----+-------+\n```", "```java\n2018-08-02 07:22:34|0|7657.58|1|0.1;2018-08-02 07:22:47|1|7663.85|0|0.2;2018-08-02 07:23:09|2|7663.85|0|0.3  \n```", "```java\n2018-08-02 07:22:34|2018-08-02 07:22:47|2018-08-02 07:23:09;0|1|2;7657.58|7663.85|7663.85;true|false|false;0.1|0.2|0.3\n```", "```java\nimport java.net.URI\ndef unsafeSave(transactions: Dataset[Transaction], path: URI): Unit = ???\n```", "```java\npackage coinyser\n\nimport java.sql.Timestamp\n\nimport cats.effect.{IO, Timer}\nimport org.apache.spark.sql.test.SharedSparkSession\nimport org.scalatest.{Matchers, WordSpec}\n\nclass BatchProducerIT extends WordSpec with Matchers with SharedSparkSession {\n\n  import testImplicits._\n\n  \"BatchProducer.unsafeSave\" should {\n    \"save a Dataset[Transaction] to parquet\" in withTempDir { tmpDir =>\n      val transaction1 = Transaction(timestamp = new \n        Timestamp(1532365695000L), tid = 70683282, price = 7740.00, \n        sell = false, amount = 0.10041719)\n      val transaction2 = Transaction(timestamp = new \n        Timestamp(1532365693000L), tid = 70683281, price = 7739.99, \n        sell = false, amount = 0.00148564)\n      val sourceDS = Seq(transaction1, transaction2).toDS()\n\n      val uri = tmpDir.toURI\n      BatchProducer.unsafeSave(sourceDS, uri)\n      tmpDir.list() should contain(\"date=2018-07-23\")\n      val readDS = spark.read.parquet(uri.toString).as[Transaction]\n          sourceDS.collect() should contain theSameElementsAs \n        readDS.collect()\n    }\n  }\n}\n```", "```java\n  def unsafeSave(transactions: Dataset[Transaction], path: URI): Unit =\n    transactions\n      .write\n      .mode(SaveMode.Append)\n      .partitionBy(\"date\")\n      .parquet(path.toString)\n```", "```java\n      spark.read.parquet(uri + \"/date=2018-07-23\").show()\n```", "```java\n+-------------------+--------+-------+-----+----------+\n|          timestamp|     tid|  price| sell|    amount|\n+-------------------+--------+-------+-----+----------+\n|2018-07-23 18:08:15|70683282| 7740.0|false|0.10041719|\n|2018-07-23 18:08:13|70683281|7739.99|false|0.00148564|\n+-------------------+--------+-------+-----+----------+\n```", "```java\nimport cats.effect.IO\n\nval io = IO{ println(\"Side effect!\"); 1 }\n// io: cats.effect.IO[Int] = …\nio.unsafeRunSync()\n// Side effect!\n// res1: Int = 1\n```", "```java\nval program = for {\n  a <- io\n  b <- io\n} yield a+b\n// program: cats.effect.IO[Int]\nprogram.unsafeRunSync()\n// IO is run!\n// IO is run!\n// res2: Int = 2\n```", "```java\nimport cats.effect.IO\nimport cats.implicits._\nimport scala.concurrent.ExecutionContext.Implicits.global\n\nval io = IO{ Thread.sleep(100); Thread.currentThread().getName }\nval program = (io, io, io).parMapN((a, b, c) => s\"$a\\n$b\\n$c\")\nprogram.unsafeRunSync()\n// res2: String =\n// ForkJoinPool-1-worker-5\n// ForkJoinPool-1-worker-3\n// ForkJoinPool-1-worker-1\n```", "```java\n  def save(transactions: Dataset[Transaction], path: URI): IO[Unit] =\n    IO(unsafeSave(transactions, path))\n```", "```java\n      BatchProducer.save(sourceDS, uri).unsafeRunSync()\n```", "```java\nclass AppContext(val transactionStorePath: URI)\n                (implicit val spark: SparkSession,\n                 implicit val timer: Timer[IO])\n```", "```java\ndef processOneBatch(fetchNextTransactions: IO[Dataset[Transaction]],\n                    transactions: Dataset[Transaction],\n                    saveStart: Instant,\n                    saveEnd: Instant)(implicit appCtx: AppContext)\n: IO[(Dataset[Transaction], Instant, Instant)] = ???\n```", "```java\n\"BatchProducer.processOneBatch\" should {\n  \"filter and save a batch of transaction, wait 59 mn, fetch the next \n    batch\" in withTempDir { tmpDir =>\n    implicit object FakeTimer extends Timer[IO] {\n      private var clockRealTimeInMillis: Long =\n        Instant.parse(\"2018-08-02T01:00:00Z\").toEpochMilli\n\n      def clockRealTime(unit: TimeUnit): IO[Long] =\n        IO(unit.convert(clockRealTimeInMillis, TimeUnit.MILLISECONDS))\n\n      def sleep(duration: FiniteDuration): IO[Unit] = IO {\n        clockRealTimeInMillis = clockRealTimeInMillis + \n        duration.toMillis\n      }\n\n      def shift: IO[Unit] = ???\n      def clockMonotonic(unit: TimeUnit): IO[Long] = ???\n    }\n    implicit val appContext: AppContext = new \n      AppContext(transactionStorePath = tmpDir.toURI)\n```", "```java\n implicit def toTimestamp(str: String): Timestamp = \n   Timestamp.from(Instant.parse(str))\n val tx1 = Transaction(\"2018-08-01T23:00:00Z\", 1, 7657.58, true, \n    0.021762)\n val tx2 = Transaction(\"2018-08-02T01:00:00Z\", 2, 7663.85, false, \n    0.01385517)\n val tx3 = Transaction(\"2018-08-02T01:58:30Z\", 3, 7663.85, false, \n    0.03782426)\n val tx4 = Transaction(\"2018-08-02T01:58:59Z\", 4, 7663.86, false, \n    0.15750809)\n val tx5 = Transaction(\"2018-08-02T02:30:00Z\", 5, 7661.49, true, 0.1)\n\n val txs0 = Seq(tx1)\n val txs1 = Seq(tx2, tx3)\n val txs2 = Seq(tx3, tx4, tx5)\n val txs3 = Seq.empty[Transaction]\n```", "```java\nval start0 = Instant.parse(\"2018-08-02T00:00:00Z\")\nval end0 = Instant.parse(\"2018-08-02T00:59:55Z\")\nval threeBatchesIO =\n  for {\n    tuple1 <- BatchProducer.processOneBatch(IO(txs1.toDS()), \n      txs0.toDS(), start0, end0) \n    (ds1, start1, end1) = tuple1\n\n    tuple2 <- BatchProducer.processOneBatch(IO(txs2.toDS()), ds1, \n      start1, end1)\n    (ds2, start2, end2) = tuple2\n\n    _ <- BatchProducer.processOneBatch(IO(txs3.toDS()), ds2, start2, \n    end2)\n  } yield (ds1, start1, end1, ds2, start2, end2)\nval (ds1, start1, end1, ds2, start2, end2) =   \n  threeBatchesIO.unsafeRunSync()\n```", "```java\nds1.collect() should contain theSameElementsAs txs1\nstart1 should ===(end0)\nend1 should ===(Instant.parse(\"2018-08-02T01:58:55Z\"))\n\nds2.collect() should contain theSameElementsAs txs2\nstart2 should ===(end1)\nend2 should ===(Instant.parse(\"2018-08-02T02:57:55Z\"))\n\nval lastClock = Instant.ofEpochMilli(\n  FakeTimer.clockRealTime(TimeUnit.MILLISECONDS).unsafeRunSync())\nlastClock should === (Instant.parse(\"2018-08-02T03:57:00Z\"))\n```", "```java\nval savedTransactions = spark.read.parquet(tmpDir.toString).as[Transaction].collect()\nval expectedTxs = Seq(tx2, tx3, tx4, tx5)\nsavedTransactions should contain theSameElementsAs expectedTxs\n```", "```java\nval WaitTime: FiniteDuration = 59.minute\nval ApiLag: FiniteDuration = 5.seconds\n\ndef processOneBatch(fetchNextTransactions: IO[Dataset[Transaction]],\n                    transactions: Dataset[Transaction],\n                    saveStart: Instant,\n                    saveEnd: Instant)(implicit appCtx: AppContext)\n: IO[(Dataset[Transaction], Instant, Instant)] = {\n  import appCtx._\n  val transactionsToSave = filterTxs(transactions, saveStart, saveEnd)\n  for {\n    _ <- BatchProducer.save(transactionsToSave, \n    appCtx.transactionStorePath)\n    _ <- IO.sleep(WaitTime)\n    beforeRead <- currentInstant\n    end = beforeRead.minusSeconds(ApiLag.toSeconds)\n    nextTransactions <- fetchNextTransactions\n  } yield (nextTransactions, saveEnd, end)\n}\n```", "```java\ndef filterTxs(transactions: Dataset[Transaction], \n              fromInstant: Instant, untilInstant: Instant): Dataset[Transaction] = {\n  import transactions.sparkSession.implicits._\n  transactions.filter(\n    ($\"timestamp\" >= \n    lit(fromInstant.getEpochSecond).cast(TimestampType)) &&\n      ($\"timestamp\" <     \n    lit(untilInstant.getEpochSecond).cast(TimestampType)))\n}\n```", "```java\ndef currentInstant(implicit timer: Timer[IO]): IO[Instant] =\n  timer.clockRealTime(TimeUnit.SECONDS) map Instant.ofEpochSecond\n```", "```java\ndef processRepeatedly(initialJsonTxs: IO[Dataset[Transaction]], \n                      jsonTxs: IO[Dataset[Transaction]])\n                     (implicit appContext: AppContext): IO[Unit] = {\n  import appContext._\n\n  for {\n    beforeRead <- currentInstant\n    firstEnd = beforeRead.minusSeconds(ApiLag.toSeconds)\n    firstTxs <- initialJsonTxs\n    firstStart = truncateInstant(firstEnd, 1.day)\n    _ <- Monad[IO].tailRecM((firstTxs, firstStart, firstEnd)) {\n      case (txs, start, instant) =>\n        processOneBatch(jsonTxs, txs, start, instant).map(_.asLeft)\n    }\n  } yield ()\n}\n```", "```java\npackage coinyser\n\nimport java.io.{BufferedReader, InputStreamReader}\nimport java.net.{URI, URL}\n\nimport cats.effect.{ExitCode, IO, IOApp}\nimport coinyser.BatchProducer.{httpToDomainTransactions, jsonToHttpTransactions}\nimport com.typesafe.scalalogging.StrictLogging\nimport org.apache.spark.sql.{Dataset, SparkSession}\n\nimport scala.io.Source\n\nclass BatchProducerApp extends IOApp with StrictLogging {\n\n  implicit val spark: SparkSession = \n  SparkSession.builder.master(\"local[*]\").getOrCreate()\n  implicit val appContext: AppContext = new AppContext(new \n    URI(\"./data/transactions\"))\n\n  def bitstampUrl(timeParam: String): URL =\n    new URL(\"https://www.bitstamp.net/api/v2/transactions/btcusd?time=\" \n    + timeParam)\n\n  def transactionsIO(timeParam: String): IO[Dataset[Transaction]] = {\n    val url = bitstampUrl(timeParam)\n    val jsonIO = IO {\n      logger.info(s\"calling $url\")\n      Source.fromURL(url).mkString\n    }\n    jsonIO.map(json => \n    httpToDomainTransactions(jsonToHttpTransactions(json)))\n  }\n\n  val initialJsonTxs: IO[Dataset[Transaction]] = transactionsIO(\"day\")\n  val nextJsonTxs: IO[Dataset[Transaction]] = transactionsIO(\"hour\")\n\n  def run(args: List[String]): IO[ExitCode] =\n    BatchProducer.processRepeatedly(initialJsonTxs, nextJsonTxs).map(_ \n    => ExitCode.Success)\n}\n\nobject BatchProducerAppSpark extends BatchProducerApp\n```", "```java\npackage coinyser\n\nobject BatchProducerAppIntelliJ extends BatchProducerApp\n```", "```java\n(...)\n18/09/02 22:29:08 INFO BatchProducerAppIntelliJ$: calling https://www.bitstamp.net/api/v2/transactions/btcusd?time=day\n18/09/02 22:29:15 WARN TaskSetManager: Stage 0 contains a task of very large size (1225 KB). The maximum recommended task size is 100 KB.\n18/09/02 22:29:15 INFO CodecPool: Got brand-new compressor [.snappy]\n18/09/02 22:29:16 INFO FileOutputCommitter: Saved output of task 'attempt_20180902222915_0000_m_000000_0' to file:/home/mikael/projects/Scala-Programming-Projects/bitcoin-analyser/data/transactions/_temporary/0/task_20180902222915_0000_m_000000\n```", "```java\nimport org.apache.spark.sql.SparkSession\nimplicit val spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\nval ds = spark.read.parquet(\"./data/transactions\")\nds.show()\n```", "```java\ntar xfvz spark-2.3.1-bin-hadoop2.7.tgz ~/\n```", "```java\ncd ~/spark-2.3.1-bin-hadoop2.7/bin\n./spark-shell\n```", "```java\n(...)\nSpark context Web UI available at http://192.168.0.11:4040\nSpark context available as 'sc' (master = local[*], app id = local-1536218093431).\nSpark session available as 'spark'.\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.3.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> \n```", "```java\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.7\"\n```", "```java\nassemblyOption in assembly := (assemblyOption in   \n  assembly).value.copy(includeScala = false)\ntest in assembly := {}\nmainClass in assembly := Some(\"coinyser.BatchProducerAppSpark\")\n```", "```java\n[IJ]sbt:bitcoin-analyser> assembly\n[info] Strategy 'discard' was applied to 3 files (Run the task at debug level to see details)[info] Packaging /tmp/sbt/bitcoin-analyser/scala-2.11/bitcoin-analyser-assembly-0.1.jar \n...\n[info] Done packaging.\n```", "```java\ncd ~/spark-2.3.1-bin-hadoop2.7/bin\n./spark-submit /tmp/sbt/bitcoin-analyser/scala-2.11/bitcoin-analyser-assembly-0.1.jar\n```", "```java\n(...)\n2018-09-07 07:55:27 INFO  SparkContext:54 - Running Spark version 2.3.1\n2018-09-07 07:55:27 INFO  SparkContext:54 - Submitted application: coinyser.BatchProducerAppSpark\n(...)\n2018-09-07 07:55:28 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.\n(...)\n2018-09-07 07:55:28 INFO  SparkContext:54 - Added JAR file:/tmp/sbt/bitcoin-analyser/scala-2.11/bitcoin-analyser-assembly-0.1.jar at spark://192.168.0.11:38371/jars/bitcoin-analyser-assembly-0.1.jar with timestamp 1536303328633\n2018-09-07 07:55:28 INFO  Executor:54 - Starting executor ID driver on host localhost\n(...)\n2018-09-07 07:55:28 INFO  NettyBlockTransferService:54 - Server created on 192.168.0.11:37370\n(...)\n2018-09-07 07:55:29 INFO  BatchProducerApp$:23 - calling https://www.bitstamp.net/api/v2/transactions/btcusd?time=day\n(...)\n2018-09-07 07:55:37 INFO  SparkContext:54 - Starting job: parquet at BatchProducer.scala:115\n(...)\n2018-09-07 07:55:39 INFO  DAGScheduler:54 - Job 0 finished: parquet at BatchProducer.scala:115, took 2.163065 s\n```"]