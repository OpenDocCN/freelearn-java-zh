["```java\ntrait RecWrapper {  }\n```", "```java\nval salesOrderSchema: StructType = StructType(Array(\n  StructField(\"sCustomerId\", IntegerType,false),\n  StructField(\"sCustomerName\", StringType,false),\n  StructField(\"sItemId\", IntegerType,true),\n  StructField(\"sItemName\",  StringType,true),\n  StructField(\"sItemUnitPrice\",DoubleType,true),\n  StructField(\"sOrderSize\", DoubleType,true),\n  StructField(\"sAmountPaid\",  DoubleType,true)\n))\n```", "```java\nval salesLeadSchema: StructType = StructType(Array(\n  StructField(\"sCustomerId\", IntegerType,false),\n  StructField(\"sCustomerName\", StringType,false),\n  StructField(\"sItemId\", IntegerType,true),\n  StructField(\"sItemName\",  StringType,true)\n))\n```", "```java\ndef buildSalesOrders(dataSet: String): DataFrame = {\n  session.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", true).schema(salesOrderSchema).option(\"nullValue\", \"\")\n    .option(\"treatEmptyValuesAsNulls\", \"true\")\n    .load(dataSet).cache()\n}\n```", "```java\ndef buildSalesLeads(dataSet: String): DataFrame = {\n  session.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", true).schema(salesLeadSchema).option(\"nullValue\", \"\")\n    .option(\"treatEmptyValuesAsNulls\", \"true\")\n    .load(dataSet).cache()\n}\n```", "```java\ntrait RecWrapper {\n\n   1) Create a lazy SparkSession instance and call it session.\n   2) Create a schema for the past sales orders dataset\n   3) Create a schema for sales lead dataset\n   4) Write a method to create a dataframe that holds past sales order            \n      data. This method takes in sales order dataset and \n      returns a dataframe\n   5) Write a method to create a dataframe that holds lead sales data\n\n}\n```", "```java\nimport org.apache.spark.mllib.recommendation.{ALS, Rating}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n```", "```java\nobject RecSystem extends App with RecWrapper {   }\n```", "```java\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.DataFrame\n```", "```java\nval salesOrdersDf = buildSalesOrders(\"sales\\\\PastWeaponSalesOrders.csv\")\n```", "```java\nsalesOrdersDf.printSchema()\nroot\n |-- sCustomerId: integer (nullable = true)\n |-- sCustomerName: string (nullable = true)\n |-- sItemId: integer (nullable = true)\n |-- sItemName: string (nullable = true)\n |-- sItemUnitPrice: double (nullable = true)\n |-- sOrderSize: double (nullable = true)\n |-- sAmountPaid: double (nullable = true)\n```", "```java\n val ratingsDf: DataFrame = salesOrdersDf.map( salesOrder =>\n Rating( salesOrder.getInt(0),\n salesOrder.getInt(2),\n salesOrder.getDouble(6)\n ) ).toDF(\"user\", \"item\", \"rating\")\n```", "```java\nC:\\Path\\To\\Your\\Project\\Chapter7>sbt compile\n```", "```java\n[error] C:\\Path\\To\\Your\\Project\\Chapter7\\src\\main\\scala\\com\\packt\\modern\\chapter7\\RecSystem.scala:50:50: Unable to find encoder for type stored in a Dataset. Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._ Support for serializing other types will be added in future releases.\n[error] val ratingsDf: DataFrame = salesOrdersDf.map( salesOrder =>\n[error] ^\n[error] two errors found\n[error] (compile:compileIncremental) Compilation failed\n```", "```java\nimport session.implicits._\n val ratingsDf: DataFrame = salesOrdersDf.map( salesOrder => UserRating( salesOrder.getInt(0), salesOrder.getInt(2), salesOrder.getDouble(6) ) ).toDF(\"user\", \"item\", \"rating\")\n```", "```java\nval ratings: RDD[Rating] = ratingsDf.rdd.map( row => Rating( row.getInt(0), row.getInt(1), row.getDouble(2) ) )\n println(\"Ratings RDD is: \" + ratings.take(10).mkString(\" \") )\n```", "```java\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n```", "```java\nval ratingsModel: MatrixFactorizationModel = ALS.train(ratings, \n   6, /* THE RANK */ \n  10, /* Number of iterations */\n  15.0 /* Lambda, or regularization parameter */\n )\n```", "```java\n val weaponSalesLeadDf = buildSalesLeads(\"sales\\\\ItemSalesLeads.csv\") \n```", "```java\nprintln(\"Weapons Sales Lead dataframe is: \")\nweaponSalesLeadDf.show \n```", "```java\nval customerWeaponsSystemPairDf: DataFrame = weaponSalesLeadDf.map(salesLead => ( salesLead.getInt(0), salesLead.getInt(2) )).toDF(\"user\",\"item\")\n```", "```java\nprintln(\"The Customer-Weapons System dataframe as tuple pairs looks like: \")\ncustomerWeaponsSystemPairDf.show \n```", "```java\nval customerWeaponsSystemPairRDD: RDD[(Int, Int)] = customerWeaponsSystemDf.rdd.map(row => \n                                                               (row.getInt(0),          \n                                                                row.getInt(1)) \n                                                       )\n/*\nNotes: As far as the algorithm is concerned, customer corresponds to \"user\" and \"product\" or item corresponds to a \"weapons system\"\n*/\n```", "```java\nval weaponRecs: RDD[Rating] = ratingsModel.predict(customerWeaponsSystemPairRDD).distinct() \n```", "```java\nprintln(\"Future ratings are: \" + weaponRecs.foreach(rating => { println( \"Customer: \" + rating.user + \" Product:  \" + rating.product + \" Rating: \" + rating.rating ) } ) )\n```", "```java\nspark-submit --class \"com.packt.modern.chapter7.RecSystem\" --master local[2] --deploy-mode client --driver-memory 16g -num-executors 2 --executor-memory 2g --executor-cores 2  <path-to-jar>\n```"]