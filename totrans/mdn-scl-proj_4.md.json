["```java\ntrait SpamWrapper {  }\n```", "```java\nobject SpamClassifierPipeline extends App with SpamWrapper {  }\n```", "```java\nval punctRegex = raw\"[^A-Za-z0-9]+\"\n```", "```java\n//matches whitespaces and punctuation marks\nval regex2 = raw\"[^A-Za-z0-9\\s]+\" \n```", "```java\nval hamRDD2 = hamRDD.map(_.replaceAll(regex2, \"\").trim)\nhamRDD2: org.apache.spark.rdd.RDD[String] = inbox.txt MapPartitionsRDD[1] at textFile at <console>:23\n```", "```java\nhamRDD3.take(10)\nprintln(\"The HAM RDD looks like: \" + hamRDD3.collect())\n```", "```java\ncase class LabeledHamSpam(label: Double, mailSentence: String)\n```", "```java\nval hamRDD3: RDD[LabeledHamSpam] = hamRDD2.repartition(4).map(w => LabeledHamSpam(0.0,w))\n```", "```java\nhamRDD3.take(10)\nprintln(\"The HAM RDD looks like: \" + hamRDD3.collect())\n```", "```java\nval spamRDD = session.sparkContext.textFile(spamFileName)\nspamRDisDset: org.apache.spark.rdd.RDD[String] = junk2.txt MapPartitionsRDD[3] at textFile at <console>:23\n```", "```java\n/*\nReplace all occurrences of punctuation and whitespace\n*/\nval spamRDD2 = spamRDD.map(_.replaceAll(regex2, \"\").trim.toLowerCase)\n/*\nRepartition the above RDD and transform it into a labeled RDD\n*/\nval spamRDD3 = spamRDD2.repartition(4).map(w => LabeledHamSpam(0.0,w))\n```", "```java\nval hamAndSpamNoCache: org.apache.spark.rdd.RDD[LabeledHamSpam] = (hamRDD3 ++ spamRDD3)\nhamAndSpam: org.apache.spark.rdd.RDD[LabeledHamSpam] = UnionRDD[20] at\n$plus$plus at <console>:34\n```", "```java\nval hamAndSpamDFrame\" = hamAndSpam.select(hamAndSpam(\"punctLessSentences\"), hamAndSpam(\"label\"))\ndataFrame2: org.apache.spark.sql.DataFrame = [features: string, label: double]\n```", "```java\nhamAndSpamDFrame.show\n+--------------------+-----+\n| lowerCasedSentences|label|\n+--------------------+-----+\n|this coming tuesd...| 0.0|\n|pin free dialing ...| 0.0|\n|regards support team| 0.0|\n| thankskat| 0.0|\n|speed dialing let...| 0.0|\n|keep your user in...| 0.0|\n| user name ilangostl| 0.0|\n|now your family m...| 0.0|\n\n```", "```java\nhamAndSpamDFrame.printSchema\n root\n  |-- features: string (nullable = true)\n  |-- label: double (nullable = false)\n```", "```java\nhamAndSpamDFrame.columns\nres23: Array[String] = Array(features, label)\n```", "```java\nimport org.apache.spark.sql.DataFrameNaFunctions\n\nval naFunctions: DataFrameNaFunctions = hamAndSpamDFrame.na\n```", "```java\nval nonNullBagOfWordsDataFrame = naFunctions.drop(Array(\"punctFreeSentences\"))\n```", "```java\nprintln(\"Non-Null Bag Of punctuation-free DataFrame looks like this:\")\n```", "```java\nnonNullBagOfWordsDataFrame.show()\n```", "```java\nimport org.apache.spark.ml.feature.Tokenizer\n\nval mailTokenizer = new Tokenizer().setInputCol(\"lowerCasedSentences\").setOutputCol(\"mailFeatureWords\")\nmailTokenizer: org.apache.spark.ml.feature.Tokenizer = tok_0b4186779a55\n\n```", "```java\nval tokenizedBagOfWordsDataFrame: DataFrame = mailTokenizer2.transform(nonNullBagOfWordsDataFrame)\n\n```", "```java\n+--------------------+-----+--------------------+\n| lowerCasedSentences|label| mailFeatureWords|\n+--------------------+-----+--------------------+\n|This coming tuesd...| 0.0|[this, coming, tu...|\n|Pin free dialing ...| 0.0|[pin, free, diali...|\n|Regards support team| 0.0|[regards, support...|\n| Thanks kat| 0.0| [thankskat]|\n|Speed dialing let...| 0.0|[speed, dialing, ...|\n|Keep your user in...| 0.0|[keep, your, user...|\n| User name ilangostl| 0.0|[user, name, ilan...|\n|Now your family m...| 0.0|[now, your, famil...|\n```", "```java\nval stopWordRemover = new StopWordsRemover().setInputCol(\"mailFeatureWords\").setOutputCol(\"noStopWordsMailFeatures\") \n```", "```java\nval noStopWordsDataFrame = stopWordRemover.transform(tokenizedBagOfWordsDataFrame)\n```", "```java\nnoStopWordsDataFrame.show()\n+-----------------------+-----+\n|noStopWordsMailFeatures|label|\n+-----------------------+-----+\n| coming| 0.0|\n| tuesday| 0.0|\n| going| 0.0|\n| take| 0.0|\n| time| 0.0|\n| meeting| 0.0|\n| get| 0.0|\n| everyone| 0.0|\n| running| 0.0|\n| pathways| 0.0|\n\n```", "```java\nimport session.implicits._\n\nval noStopWordsDataFrame2 = noStopWordsDataFrame.select(explode($\"noStopWordsMailFeatures\").alias(\"noStopWordsMailFeatures\"),noStopWordsDataFrame(\"label\"))\n```", "```java\nnoStopWordsDataFrame2.show()\n```", "```java\nimport org.apache.spark.ml.feature.HashingTF\n\nval hashMapper = new HashingTF().setInputCol(\"words\").\nsetOutputCol(\"noStopWordsMailFeatures\").setOutputCol(\"mailFeatureHashes\").setNumFeatures(10000)\nhashFeatures: org.apache.spark.ml.feature.HashingTF = hashingTF_5ff221eac4b4\n```", "```java\nval featurizedDF = hashMapper.transform(noStopWordsDataFrame)\n\n//Display the featurized dataframe\nfeaturizedDF1.show()\n```", "```java\nval splitFeaturizedDF = featurizedDF.randomSplit(Array(0.80, 0.20), 98765L)\nsplitFeaturizedDF1: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([filteredMailFeatures: string, label: double ... 2 more fields],    [filteredMailFeatures: string, label: double ... 2 more fields])\n```", "```java\nval trainFeaturizedDF = splitFeaturizedDF(0)\n```", "```java\nval testFeaturizedDF = splitFeaturizedDF(1)\n```", "```java\nval trainFeaturizedDFNew = trainFeaturizedDF1.drop(\"mailFeatureWords\",\"noStopWordsMailFeatures\",\"mailFeatureHashes\")\n\ntrainFeaturizedDFNew.show()\n```", "```java\nval mailIDF = new IDF().setInputCol(\"mailFeatureHashes\").setOutputCol(\"mailIDF\")\n```", "```java\nval mailIDFFunction = mailIDF.fit(featurizedDF)\n```", "```java\nval normalizer = new Normalizer().setInputCol(\"mailIDF\").setOutputCol(\"features\")\n```", "```java\nval naiveBayes = new NaiveBayes().setFeaturesCol(\"features\").setPredictionCol(\"prediction\")\n```", "```java\nval spamPipeline = new Pipeline().setStages(Array[PipelineStage](mailTokenizer2) ++\n                                                                                            Array[PipelineStage](stopWordRemover) ++\n                                                                                           Array[PipelineStage](hashMapper) ++\n                                                                                          Array[PipelineStage](mailIDF) ++\n                                                                                          Array[PipelineStage](normalizer) ++\n                                                                                          Array[PipelineStage](naiveBayes)\n```", "```java\nval mailModel1 = spamPipeline1.fit(trainFeaturizedDFNew)\n```", "```java\nval rawPredictions = mailModel1.transform(testFeaturizedDF.drop(\"mailFeatureWords\",\"noStopWordsMailFeatures\",\"mailFeatureHashes\"))\n\n```", "```java\nrawPredictions.show(20))\n```", "```java\nval predictions = rawPredictions.select($\"lowerCasedSentences\", $\"prediction\").cache\n```", "```java\npredictions.show(50)\n```", "```java\nsession.stop()\n```"]