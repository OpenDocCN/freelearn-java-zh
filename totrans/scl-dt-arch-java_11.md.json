["```java\n    -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:<filename>\n    ```", "```java\n-Xms<heap size>[unit] // for min heap size\n-Xmx<heap size>[unit] //for max heap size\n//unit can be g(GB),m(MB) or k(KB)\n```", "```java\n        <property>\n          <name>hive.execution.engine</name>\n          <value>spark</value>\n        </property>\n        ```", "```java\n    5. So, in a node, the maximum number of executors will be 3 (16 cores / 5 executor-cores per executor). So, for three nodes, we will have a total of 9 executors (3 nodes x 3 executors/nodes). However, according to the best practice documentation of Spark, the executor cores per executor should be the same as the driver core. Hence, we would require 1 worker to work out of the 9 workers as the application driver. Hence, the number of executors will become 8 (9 workers – 1 driver worker). As a general formula, you can use the following: \n    ```"]