- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Java and Big Data – a Collaborative Odyssey
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java与大数据——协作之旅
- en: Embark on a transformative journey as we harness the power of Java to navigate
    the vast landscape of big data. In this chapter, we’ll explore how Java’s proficiency
    in distributed computing, coupled with its robust ecosystem of tools and frameworks,
    empowers you to tackle the complexities of processing, storing, and extracting
    insights from massive datasets. As we delve into the world of big data, we’ll
    showcase how Apache Hadoop and Apache Spark seamlessly integrate with Java to
    overcome the limitations of conventional methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们利用Java的力量探索大数据的广阔领域，开始一段变革之旅。在本章中，我们将探讨Java在分布式计算方面的专长，以及其强大工具和框架生态系统如何赋予你处理、存储和从海量数据集中提取洞察力的能力。随着我们深入大数据的世界，我们将展示Apache
    Hadoop和Apache Spark如何与Java无缝集成，克服传统方法的局限性。
- en: Throughout this chapter, you’ll gain hands-on experience in building scalable
    data processing pipelines, using Java alongside the Hadoop and Spark frameworks.
    We’ll explore Hadoop’s core components, such as **Hadoop Distributed File System**
    (**HDFS**) and MapReduce, and dive deep into Apache Spark, focusing on its primary
    abstractions, including **Resilient Distributed Datasets** (**RDDs**) and DataFrames.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将获得构建可扩展数据处理管道的实践经验，使用Java配合Hadoop和Spark框架。我们将探讨Hadoop的核心组件，例如**Hadoop分布式文件系统**（**HDFS**）和MapReduce，并深入探讨Apache
    Spark，重点关注其主要抽象，包括**弹性分布式数据集**（**RDDs**）和DataFrames。
- en: We’ll place a strong emphasis on the DataFrame API, which has become the de
    facto standard for data processing in Spark. You’ll discover how DataFrames provide
    a more efficient, optimized, and user-friendly way to work with structured and
    semi-structured data. We’ll cover essential concepts such as transformations,
    actions, and SQL-like querying using DataFrames, enabling you to perform complex
    data manipulations and aggregations with ease.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍DataFrame API，它已成为Spark中数据处理的事实标准。你将发现DataFrame如何提供一种更高效、优化和用户友好的方式来处理结构化和半结构化数据。我们将涵盖诸如转换、操作和SQL-like查询等基本概念，使用DataFrame轻松执行复杂的数据操作和聚合。
- en: To ensure a comprehensive understanding of Spark’s capabilities, we’ll explore
    advanced topics such as the Catalyst optimizer, the execution **Directed Acyclic
    Graph** (**DAG**), caching and persistence techniques, and strategies to handle
    data skew and minimize data shuffling. We’ll also introduce you to the equivalent
    managed services offered by major cloud platforms, enabling you to harness the
    power of big data within the cloud environment.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保对Spark功能的全面理解，我们将探讨高级主题，如Catalyst优化器、执行**有向无环图**（**DAG**）、缓存和持久化技术，以及处理数据倾斜和最小化数据洗牌的策略。我们还将介绍主要云平台提供的等效托管服务，使你能够在云环境中利用大数据的力量。
- en: As we progress, you’ll have the opportunity to apply your newfound skills to
    real-world big data challenges, such as log analysis, recommendation systems,
    and fraud detection. We’ll provide detailed code examples and explanations, emphasizing
    the use of DataFrames and demonstrating how to leverage Spark’s powerful APIs
    to solve complex data processing tasks.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断前进，你将有机会将新获得的技术应用到现实世界的复杂大数据挑战中，例如日志分析、推荐系统和欺诈检测。我们将提供详细的代码示例和解释，强调使用DataFrames，并展示如何利用Spark强大的API解决复杂的数据处理任务。
- en: By the end of this chapter, you’ll be equipped with the knowledge and tools
    to conquer the realm of big data using Java. You’ll understand the core characteristics
    of big data, the limitations of traditional approaches, and how Java’s concurrency
    features and big data frameworks enable you to overcome these challenges. Moreover,
    you’ll have gained practical experience in building real-world applications that
    leverage the power of Java and big data technologies, with a focus on utilizing
    the DataFrame API for optimal performance and productivity.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备使用Java征服大数据领域的知识和工具。你将了解大数据的核心特征、传统方法的局限性，以及Java的并发特性和大数据框架如何帮助你克服这些挑战。此外，你将获得构建利用Java和大数据技术力量的实际应用的经验，重点关注利用DataFrame
    API以实现最佳性能和生产力。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: '**Set up the Hadoop/Spark environment**: Setting up a small Hadoop and Spark
    environment can be a crucial step for hands-on practice and deepening your understanding
    of big data processing. Here’s a simplified guide to get you started in creating
    your own *sandbox* environment:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置 Hadoop/Spark 环境**：设置一个小的 Hadoop 和 Spark 环境可能是动手实践和深化你对大数据处理理解的关键步骤。以下是一个简化的指南，帮助你开始创建自己的
    *沙盒* 环境：'
- en: '`64-bit` OS, at least 8 GB of RAM, and a multi-core processor'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`64位` 操作系统，至少8GB的RAM，以及多核处理器'
- en: '`8` or `11`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`8` 或 `11`'
- en: '`core-site.xml`, `hdfs-site.xml`, and `mapred-site.xml` in the `etc`/`hadoop`
    directory. Refer to the official documentation for detailed configuration steps
    ([https://hadoop.apache.org/docs/stable/](https://hadoop.apache.org/docs/stable/)).*   `bin`
    and `sbin` to your `PATH`, and set `JAVA_HOME` to your `JDK` path.*   `hdfs namenode`
    format, and then start HDFS and `start-yarn.sh`.*   `conf/spark-env.sh` to set
    `JAVA_HOME` and `HADOOP_CONF_DIR` as required*   `./bin/spark-shell` or submit
    a job using `./``bin/spark-submit`*   `hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar
    pi` `4 100`)*   `./``bin/run-example SparkPi`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`etc/hadoop` 目录下的 `core-site.xml`、`hdfs-site.xml` 和 `mapred-site.xml`。请参阅官方文档以获取详细的配置步骤
    ([https://hadoop.apache.org/docs/stable/](https://hadoop.apache.org/docs/stable/))。*   将
    `bin` 和 `sbin` 添加到你的 `PATH` 中，并将 `JAVA_HOME` 设置为你的 `JDK` 路径。*   执行 `hdfs namenode`
    格式化，然后启动 HDFS 和 `start-yarn.sh`。*   在 `conf/spark-env.sh` 中设置所需的 `JAVA_HOME` 和
    `HADOOP_CONF_DIR`。*   使用 `./bin/spark-shell` 或通过 `./bin/spark-submit` 提交作业。*   使用
    `hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar pi 4 100`。*   使用
    `./bin/run-example SparkPi`。'
- en: This streamlined guide provides the essentials to get a Hadoop and Spark environment
    running. Detailed configurations might vary, so refer to the official documentation
    for in-depth instructions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这份简化的指南提供了启动 Hadoop 和 Spark 环境的基本要素。详细的配置可能有所不同，因此请参阅官方文档以获取深入指导。
- en: Download Visual Studio Code (VS Code) from [https://code.visualstudio.com/download](https://code.visualstudio.com/download).
    VS Code offers a lightweight and customizable alternative to the other options
    on this list. It’s a great choice for developers who prefer a less resource-intensive
    IDE and want the flexibility to install extensions tailored to their specific
    needs. However, it may not have all the features out of the box compared to the
    more established Java IDEs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [https://code.visualstudio.com/download](https://code.visualstudio.com/download)
    下载 Visual Studio Code (VS Code)。VS Code 提供了一个轻量级且可定制的替代方案，是那些更喜欢资源消耗较少的 IDE 并希望安装针对其特定需求定制的扩展的开发者的绝佳选择。然而，与更成熟的
    Java IDE 相比，它可能没有所有开箱即用的功能。
- en: 'The code in this chapter can be found on GitHub:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Java-Concurrency-and-Parallelism](https://github.com/PacktPublishing/Java-Concurrency-and-Parallelism)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Java-Concurrency-and-Parallelism](https://github.com/PacktPublishing/Java-Concurrency-and-Parallelism)'
- en: The big data landscape – the evolution and need for concurrent processing
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据领域——并发处理的发展和需求
- en: Within this torrent of data lies a wealth of potential – insights that can drive
    better decision-making, unlock innovation, and transform entire industries. To
    seize this opportunity, however, we need specialized tools and a new approach
    to data processing. Let’s begin by understanding the defining characteristics
    of big data and why it demands a shift in our thinking.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这股数据洪流中蕴藏着丰富的潜力——可以推动更好的决策、解锁创新并改变整个行业。然而，为了抓住这个机会，我们需要专门的工具和一种新的数据处理方法。让我们首先了解大数据的定义特征以及为什么它要求我们转变思维方式。
- en: Navigating the big data landscape
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在大数据领域中导航
- en: Big data isn’t merely about the sheer amount of information. It’s a phenomenon
    driven by the explosion in the volume, speed, and diversity of data being generated
    every second.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据不仅仅是关于信息量的多少。它是一种现象，由每秒产生的数据量、速度和多样性的爆炸性增长所驱动。
- en: 'The core characteristics of big data are volume, velocity, and variety:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据的核心特征是数据量、速度和多样性：
- en: '**Volume**: The massive scale of datasets, often reaching petabytes (millions
    of GB) or even exabytes (billions of GB).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据量**：数据集的巨大规模，通常达到PB（数百万GB）甚至EB（数十亿GB）。'
- en: '**Velocity**: The unprecedented speed at which data is created and needs to
    be collected – think social media feeds, sensor streams, and financial transactions.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：数据创建和收集前所未有的速度——想想社交媒体动态、传感器流和金融交易。'
- en: '**Variety**: Data no longer fits neatly into structured rows and columns. We
    now have images, videos, text, sensor data, and so on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：数据不再整齐地适合结构化的行和列。我们现在有图像、视频、文本、传感器数据等等。'
- en: Imagine a self-driving car navigating the streets. Its cameras, lidar sensors,
    and onboard computers constantly collect a torrent of data to map the environment,
    recognize objects, and make real-time driving decisions. This relentless stream
    of information can easily amount to terabytes of data each day – that’s more storage
    than many personal laptops even hold.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一辆自动驾驶汽车在街道上导航。它的摄像头、激光雷达传感器和车载计算机不断收集大量数据以绘制环境地图、识别物体并做出实时驾驶决策。这种不间断的信息流每天可以轻易达到数以千计的数据量——这比许多个人笔记本电脑的存储空间还要多。
- en: Now, think of a massive online retailer. Every time you search for a product,
    click on an item, or add something to your cart, your actions are tracked. Multiply
    this by millions of shoppers daily, and you can see how an e-commerce platform
    captures a colossal dataset of user behavior.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想想一个大型在线零售商。每次你搜索产品、点击商品或添加到购物车，你的行为都会被追踪。每天数百万购物者的行为累积起来，你可以看到电子商务平台如何捕捉到庞大的用户行为数据集。
- en: Finally, picture the constant flow of posts, tweets, photos, and videos flooding
    social networks every second. This vast and ever-changing collection of text,
    images, and videos embodies the diversity and speed inherent in big data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，想象每秒钟都有大量帖子、推文、照片和视频涌入社交网络。这个庞大且不断变化的文本、图像和视频集合体现了大数据固有的多样性和速度。
- en: 'The tools and techniques that served us well in the past simply can’t keep
    pace with the explosive growth and complexity of big data. Here’s how traditional
    data processing struggles:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 过去为我们服务良好的工具和技术根本无法跟上大数据爆炸式增长和复杂性的步伐。以下是传统数据处理如何挣扎的情况：
- en: '**Scalability roadblocks**: Relational databases, optimized for structured
    data, buckle under massive datasets. Adding more data often translates to sluggish
    performance and skyrocketing hardware and maintenance costs.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性障碍**: 优化于结构化数据的关系数据库在处理大量数据时会崩溃。添加更多数据通常会导致性能缓慢和硬件及维护成本激增。'
- en: '**Data diversity dilemma**: Traditional systems expect data in neat rows and
    columns, while big data embraces unstructured and semi-structured formats such
    as text, images, and sensor data.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据多样性困境**: 传统系统期望数据以整洁的行和列的形式存在，而大数据则拥抱非结构化和半结构化格式，如文本、图像和传感器数据。'
- en: '**Batch processing bottlenecks**: Conventional methods rely on batch processing,
    analyzing data in large chunks, which is slow and inefficient for real-time insights
    that are crucial in the big data world.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量处理瓶颈**: 传统方法依赖于批量处理，分析大量数据，这在大数据世界中对于至关重要的实时洞察来说既慢又低效。'
- en: '**Centralized architecture woes**: Centralized storage solutions become overloaded
    and bottleneck when handling vast amounts of data flowing from multiple sources.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中式架构的烦恼**: 当处理来自多个来源的大量数据流时，集中式存储解决方案会过载并形成瓶颈。'
- en: 'The limitations of relational databases become even clearer when considering
    specific aspects of big data:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到大数据的特定方面，关系数据库的局限性变得更加明显：
- en: '**Volume**: Distributing relational databases is difficult, and single nodes
    can’t handle the sheer volume of big data.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体积**: 分布式关系数据库的分布很困难，单个节点无法处理大数据的巨大体积。'
- en: '**Velocity**: **Atomicity, consistency, isolation, and durability** (**ACID**)
    transactions in relational databases slow down writes, making them unsuitable
    for the high velocity of incoming big data. Batching writes offers a partial solution,
    but it locks tables for other operations.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**: 关系数据库中的**原子性、一致性、隔离性和持久性**（**ACID**）事务会减慢写入速度，使其不适合高速进入的大数据。批量写入提供了一种部分解决方案，但它会锁定其他操作。'
- en: '**Variety**: Storing unstructured data (images, binary, etc.) is cumbersome
    due to size limitations and other challenges. While some semi-structured support
    exists (XML/JSON), it depends on the database implementation and doesn’t fit well
    with the relational model.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**: 由于大小限制和其他挑战，存储非结构化数据（图像、二进制等）很麻烦。虽然存在一些半结构化支持（XML/JSON），但它依赖于数据库实现，并且与关系模型不太匹配。'
- en: These limitations underscore the immense potential hidden within big data but
    also reveal the inadequacy of traditional methods. To unlock this potential, we
    need a new paradigm – one built on distributed systems and the power of Java.
    Frameworks such as Hadoop and Spark represent this paradigm shift, offering the
    tools and techniques to effectively navigate the big data deluge.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制突显了大数据中隐藏的巨大潜力，但也揭示了传统方法的不足。为了释放这种潜力，我们需要一个新的范式——一个建立在分布式系统和Java的力量之上的范式。Hadoop和Spark等框架代表了这种范式转变，提供了有效导航大数据洪流的工具和技术。
- en: Concurrency to the rescue
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发来拯救
- en: At its heart, concurrency is about managing multiple tasks that seem to happen
    at the same time. In big data, this translates to breaking down large datasets
    into smaller, more manageable chunks for processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 并发本质上关于管理看似同时发生的多个任务。在大数据中，这转化为将大型数据集分解成更小、更易于管理的块进行处理。
- en: 'Imagine you have a monumental task – sorting through a vast, dusty library
    filled with thousands of unorganized books. Doing this alone would take months!
    Thankfully, you’re not limited to working solo. Java provides you with a team
    of helpers – threads and processes – to tackle this challenge:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有一个巨大的任务——整理一个庞大、尘土飞扬的图书馆，里面装满了成千上万没有组织的书籍。单独完成这项工作可能需要几个月！幸运的是，你不必独自工作。Java为你提供了一支助手团队——线程和进程——来应对这个挑战：
- en: '**Divide and conquer**: Think of threads and processes as your librarian assistants.
    Threads are lightweight helpers, perfect for tackling smaller tasks within the
    library, such as sorting bookshelves or searching through specific sections. Processes
    are your heavy-lifters, capable of taking on major sections of the library independently.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分而治之**：将线程和进程视为你的图书管理员助手。线程是轻量级的助手，非常适合处理图书馆内的较小任务，例如整理书架或搜索特定部分。进程是你的重型助手，能够独立处理图书馆的主要部分。'
- en: '**The coordination challenge**: Since your assistants work simultaneously,
    imagine the potential chaos without careful planning. Books could end up in the
    wrong places or go missing entirely! This is where synchronization comes in. It’s
    like having a master catalog to track where books belong, ensuring that everything
    stays consistent even amid the whirlwind of activity.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协调挑战**：由于你的助手是同时工作的，想象一下没有周密计划可能带来的混乱。书籍可能会被放在错误的位置，甚至完全丢失！这就是同步的作用所在。它就像有一个主目录来跟踪书籍的位置，确保即使在活动繁忙的情况下，一切都能保持一致。'
- en: '**Maximizing resource utilization**: Your library of computing power isn’t
    just about how many helpers you have; it’s also about using them wisely. Efficient
    resource utilization means spreading a workload evenly. Picture making sure each
    bookshelf in our metaphorical library gets attention and that assistants don’t
    sit idle while others are overloaded.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大化资源利用率**：你的计算能力图书馆不仅关乎你有多少助手，还关乎如何明智地使用他们。高效资源利用率意味着均匀分配工作量。想象一下确保我们比喻中的图书馆中的每个书架都得到关注，助手们不会在其他人超负荷工作时闲置。'
- en: 'Let’s bring this story to life! Say you need to analyze that massive log dataset.
    A concurrent approach is like splitting the library into sections and assigning
    teams of assistants:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把这个故事生动起来！假设你需要分析那个庞大的日志数据集。并发方法就像将图书馆分成几个部分，并分配助手团队：
- en: '**Filtering**: Assistants sift through log files for relevant entries, much
    like sorting through bookshelves to find those on specific topics'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤**：助手会筛选日志文件中的相关条目，就像在书架上寻找特定主题的书籍一样。'
- en: '**Transforming**: Other assistants clean and format the data for consistency
    – it’s like standardizing book titles for a catalog'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：其他助手会清理和格式化数据以确保一致性——就像为目录标准化书籍标题一样。'
- en: '**Aggregating**: Finally, some assistants compile statistics and insights from
    the data, just as you might summarize books on a particular subject'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：最后，一些助手会从数据中编译统计数据和见解，就像你可能对特定主题的书籍进行总结一样。'
- en: By dividing the work and coordinating these efforts, this huge task becomes
    not only manageable but amazingly fast!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分工和协调这些努力，这个巨大的任务不仅变得可管理，而且速度惊人！
- en: Now that we’ve harnessed the power of concurrency and parallelism, let’s explore
    how frameworks such as Hadoop leverage these principles to build a robust foundation
    for distributed big data processing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经利用了并发和并行处理的力量，让我们来探讨框架如Hadoop如何利用这些原则来构建分布式大数据处理的坚实基础。
- en: Hadoop – the foundation for distributed data processing
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hadoop——分布式数据处理的基础
- en: As a Java developer, you’re in the perfect position to harness this power. Hadoop
    is built with Java, offering a rich set of tools and APIs to craft scalable big
    data solutions. Let’s dive into the core components of Hadoop’s HDFS and MapReduce.
    Here’s a detailed explanation of each component.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Java开发者，你处于利用这种力量的最佳位置。Hadoop是用Java构建的，提供了一套丰富的工具和API来构建可扩展的大数据解决方案。让我们深入了解Hadoop的HDFS和MapReduce的核心组件。以下是每个组件的详细解释。
- en: Hadoop distributed file system
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统
- en: '**Hadoop distributed file system** or **HDFS** is the primary storage system
    used by Hadoop applications. It is designed to store massive amounts of data across
    multiple commodity hardware nodes, providing scalability and fault tolerance.
    The key characteristics of HDFS include the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**或**HDFS**是Hadoop应用程序使用的首选存储系统。它被设计用于在多个商用硬件节点上存储大量数据，提供可扩展性和容错性。HDFS的关键特性包括以下内容：'
- en: '**Scaling out, not up**: HDFS splits large files into smaller blocks (typically,
    128 MB) and distributes them across multiple nodes in a cluster. This allows for
    parallel processing and enables a system to handle files that are larger than
    the capacity of a single node.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**横向扩展而非纵向扩展**：HDFS将大文件分割成较小的块（通常是128MB），并将它们分布到集群中的多个节点上。这允许并行处理，并使系统能够处理单个节点容量之外的文件。'
- en: '**Resilience through replication**: HDFS ensures data durability and fault
    tolerance by replicating each block across multiple nodes (the default replication
    factor is 3). If a node fails, the data can still be accessed from the replicated
    copies on other nodes.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过复制实现弹性**：HDFS通过在多个节点上复制每个块（默认复制因子为3）来确保数据的持久性和容错性。如果一个节点失败，数据仍然可以从其他节点上的复制副本中访问。'
- en: '**Scalability**: HDFS is designed to scale horizontally by adding more nodes
    to the cluster. As the data size grows, the system can accommodate the increased
    storage requirements by simply adding more commodity hardware.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：HDFS通过向集群添加更多节点来水平扩展。随着数据量的增长，系统可以通过简单地添加更多商用硬件来满足增加的存储需求。'
- en: '**Namenode and datanodes**: HDFS follows a master-slave architecture. The *Namenode*
    acts as the master, managing the filesystem namespace and regulating client access
    to files. *Datanodes* are slave nodes that store the actual data blocks and serve
    read/write requests from clients.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Namenode和datanodes**：HDFS遵循主从架构。*Namenode*作为主节点，管理文件系统命名空间并调节客户端对文件的访问。*Datanodes*是存储实际数据块的从节点，并从客户端处理读写请求。'
- en: MapReduce – the processing framework
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce – 处理框架
- en: '**MapReduce** is a distributed processing framework that allows developers
    to write programs that process large datasets in parallel across a cluster of
    nodes. It consists of the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**MapReduce**是一个分布式处理框架，允许开发者编写程序，在节点集群上并行处理大型数据集。它由以下部分组成：'
- en: '**Simplified parallelism**: MapReduce simplifies the complexities of distributed
    processing. At its core, it consists of two primary phases:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化并行性**：MapReduce简化了分布式处理的复杂性。在其核心，它由两个主要阶段组成：'
- en: '**Map**: Input data is divided, and *mapper* tasks process these smaller chunks
    simultaneously'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Map**：输入数据被分割，*mapper*任务同时处理这些较小的数据块。'
- en: '**Reduce**: Results from the mappers are aggregated by *reducer* tasks, producing
    the final output'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Reduce**：由*mapper*任务产生的结果由*reducer*任务聚合，生成最终输出。'
- en: '**A data-centric approach**: MapReduce moves code to where data resides on
    the cluster, rather than the traditional approach of moving data to code. This
    optimizes data flow and makes processing highly efficient.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以数据为中心的方法**：MapReduce将代码移动到集群中数据所在的位置，而不是传统的将数据移动到代码的方法。这优化了数据流，并使处理高度高效。'
- en: HDFS and MapReduce form the core of Hadoop’s distributed computing ecosystem.
    HDFS provides the distributed storage infrastructure, while MapReduce enables
    the distributed processing of large datasets. Developers can write MapReduce jobs
    in Java to process data stored in HDFS, leveraging the power of parallel computing
    to achieve scalable and fault-tolerant data processing.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS和MapReduce构成了Hadoop分布式计算生态系统的核心。HDFS提供了分布式存储基础设施，而MapReduce实现了大型数据集的分布式处理。开发者可以用Java编写MapReduce作业来处理存储在HDFS中的数据，利用并行计算的力量实现可扩展和容错的数据处理。
- en: In the next section, we will explore how Java and Hadoop work hand in hand,
    and we will also provide a basic MapReduce code example to demonstrate the data
    processing logic.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨Java和Hadoop如何携手合作，并提供一个基本的MapReduce代码示例来展示数据处理逻辑。
- en: Java and Hadoop – a perfect match
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java和Hadoop – 完美匹配
- en: Apache Hadoop revolutionized big data storage and processing. At its core lies
    a strong connection with Java, the widely used programming language known for
    its versatility and robustness. This section explores how Java and Hadoop work
    together, providing the necessary tools for effective Hadoop application development.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop彻底改变了大数据存储和处理。其核心与Java紧密相连，Java是一种广泛使用的编程语言，以其灵活性和健壮性而闻名。本节探讨了Java和Hadoop如何协同工作，为有效的Hadoop应用程序开发提供必要的工具。
- en: Why Java? A perfect match for Hadoop development
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择Java？完美匹配Hadoop开发
- en: 'Several factors make Java the go-to language for Hadoop development:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 几个因素使Java成为Hadoop开发的首选语言：
- en: '**Java as the foundation** **of Hadoop**:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java作为Hadoop的基础**：'
- en: Hadoop is written in Java, making it the native language for Hadoop development
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop是用Java编写的，使其成为Hadoop开发的本地语言
- en: Java’s object-oriented programming paradigm aligns perfectly with Hadoop’s distributed
    computing model
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java的面向对象编程范式与Hadoop的分布式计算模型完美契合
- en: Java’s platform independence allows Hadoop applications to run seamlessly across
    different hardware and operating systems
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java的平台独立性允许Hadoop应用程序在不同硬件和操作系统上无缝运行
- en: '**Seamless integration with the** **Hadoop ecosystem**:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与Hadoop生态系统的无缝集成**：'
- en: The Hadoop ecosystem encompasses a wide range of tools and frameworks, many
    of which are built on top of Java
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop生态系统包含了一系列的工具和框架，其中许多都是基于Java构建的
- en: Key components such as HDFS and MapReduce heavily rely on Java for their functionality
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如HDFS和MapReduce等关键组件严重依赖Java来实现其功能
- en: Java’s compatibility ensures smooth integration between Hadoop and other Java-based
    big data tools, such as *Apache Hive*, *Apache HBase*, and *Apache Spark*
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java的兼容性确保了Hadoop与其他基于Java的大数据工具（如Apache Hive、Apache HBase和Apache Spark）之间的顺利集成
- en: '**Rich API support for** **Hadoop development**:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为Hadoop开发提供丰富的API支持**：'
- en: Hadoop provides comprehensive Java APIs that enable developers to interact with
    its core components effectively
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop提供了全面的Java API，使开发者能够有效地与其核心组件交互
- en: The Java API for HDFS allows programmatic access and manipulation of data stored
    in the distributed filesystem
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS的Java API允许以编程方式访问和操作存储在分布式文件系统中的数据
- en: MapReduce, the heart of Hadoop’s data processing engine, exposes Java APIs to
    write and manage MapReduce jobs
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop数据处理引擎的核心MapReduce暴露了Java API以编写和管理MapReduce作业
- en: These APIs empower developers to leverage Hadoop’s functionalities and build
    powerful data-processing applications
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些API使开发者能够利用Hadoop的功能并构建强大的数据处理应用程序
- en: As the Hadoop ecosystem continues to evolve, Java remains the foundation upon
    which new tools and frameworks are built, cementing its position as the perfect
    match for Hadoop development.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Hadoop生态系统的不断发展，Java仍然是构建新工具和框架的基础，巩固了其在Hadoop开发中的完美匹配地位。
- en: Now that we understand the strengths of Java in the Hadoop ecosystem, let’s
    delve into the heart of Hadoop data processing. In the next section, we’ll explore
    how to write MapReduce jobs using Java, with a basic code example to solidify
    these concepts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了Java在Hadoop生态系统中的优势，让我们深入探讨Hadoop数据处理的核心。在下一节中，我们将探讨如何使用Java编写MapReduce作业，并附上一个基本的代码示例来巩固这些概念。
- en: MapReduce in action
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce的实际应用
- en: The following example demonstrates how MapReduce in Java can be used to analyze
    website clickstream data and identify user browsing patterns.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了如何使用Java中的MapReduce分析网站点击流数据并识别用户浏览模式。
- en: 'We have a large dataset containing clickstream logs, where each log entry records
    details such as the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含点击流日志的大数据集，其中每个日志条目记录了以下详细信息：
- en: A user ID
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户ID
- en: A timestamp
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳
- en: A visited web page URL
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问过的网页URL
- en: We will analyze user clickstream data to understand user browsing behavior and
    identify popular navigation patterns, which incorporates a custom grouping logic
    within the reducer to group user sessions based on a time window (e.g., 15 minutes),
    and then we will analyze web page visit sequences within each session.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析用户点击流数据以了解用户浏览行为并识别流行的导航模式，这涉及到在reducer中实现自定义分组逻辑，根据时间窗口（例如，15分钟）对用户会话进行分组，然后我们将分析每个会话内的网页访问序列。
- en: 'Here is the Mapper code snippet:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是Mapper代码片段：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code defines a `Mapper` class, a core component in MapReduce responsible
    for processing individual input data records. The key points in this class are
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`Mapper`类，MapReduce中的核心组件，负责处理单个输入数据记录。此类中的关键点如下：
- en: '`Mapper<LongWritable, Text, Text, Text>` class declaration specifies its input
    and output key-value pairs:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mapper<LongWritable, Text, Text, Text>`类声明指定了其输入和输出键值对：'
- en: '`LongWritable key` for line numbers and, `Text value` for text lines'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LongWritable key`用于行号，`Text value`用于文本行'
- en: '`Text` key for session keys and `Text value` for URLs'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Text`键用于会话键和`Text value`用于URL'
- en: '`map` function receives a `key-value pair`, representing a line from the input
    data. The line is split into an array using a comma (`,`) delimiter, assuming
    a comma-separated log format.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`函数接收一个`键值对`，表示输入数据中的一行。该行使用逗号（`,`）分隔符拆分为一个数组，假设日志格式为逗号分隔。'
- en: '`userId`: The user ID from the first element of the array'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`userId`：来自数组第一个元素的用户ID'
- en: '`timestamp`: The long value parsed from the second element'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timestamp`：从第二个元素解析的长值'
- en: '`url`: The web page URL from the third element'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`url`：来自数组的第三个元素的网页URL'
- en: '`15 * 60 * 1000` (15 minutes) to group events within 15-minute intervals, usually
    for session-based analysis*   `key-value pair` for downstream processing:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`15 * 60 * 1000`（15分钟）用于将事件分组到15分钟间隔内，通常用于基于会话的分析*   `键值对`用于下游处理：'
- en: '**Key**: The text representing the generated session key'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键**：表示生成的会话键的文本'
- en: '**Value**: The text representing the extracted URL*   **Purpose and context**:
    This mapper functions within a larger MapReduce job, designed for session-based
    analysis of user activity logs. It groups events belonging to each user into 15-minute
    sessions. The emitted key-value pairs (session key and URL) will undergo shuffling
    and sorting before being processed by the reducers. These reducers will perform
    further aggregation or analysis based on the session keys.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**：表示提取的URL的文本*   **目的和上下文**：此映射器在更大的MapReduce作业中运行，该作业用于基于会话的用户活动日志分析。它将属于每个用户的每个事件分组到15分钟的会话中。在由reducer处理之前，发出的键值对（会话键和URL）将进行洗牌和排序。这些reducer将根据会话键进行进一步的聚合或分析。'
- en: 'Here is the Reducer code snippet:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是Reducer代码片段：
- en: '[PRE1]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This code defines a `Reducer` class, responsible for aggregating and summarizing
    data grouped by a common key after the map phase. The key points in this class
    are as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个`Reducer`类，该类在map阶段之后负责对按公共键分组的数据进行聚合和汇总。此类中的关键点如下：
- en: '`reduce()` function operates on key-value pairs:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduce()`函数在键值对上操作：'
- en: '`Iterable<Text> values`, containing a collection of URLs associated with that
    session'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Iterable<Text> values`，包含与该会话关联的URL集合'
- en: '**Output**: The text key for the session key and the text value for the constructed
    URL sequence'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：会话键的文本键和构建的URL序列的文本值'
- en: '`sessionSequence` to accumulate the URL sequence for the current session.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sessionSequence`用于累积当前会话的URL序列。'
- en: '`sessionSequence`, followed by `->` to maintain order.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sessionSequence`，然后跟`->`以保持顺序。'
- en: '`->` at the end of the constructed sequence for cleaner output.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`->`在构建的序列末尾，以获得更清晰的输出。'
- en: '**Key-value emission**: Emits a new key-value pair:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键值对发射**：发射一个新的键值对：'
- en: '**Key**: The input session key (unchanged)'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键**：未更改的输入会话键'
- en: '**Value**: The text representation of the constructed URL sequence, representing
    the ordered sequence of URLs visited within that session'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值**：构建的URL序列的文本表示，表示该会话中访问的URL的有序序列'
- en: '**Purpose and context**: This reducer works alongside the mapper code to facilitate
    session-based analysis of user activity logs. It aggregates URLs associated with
    each session key, effectively reconstructing the order of web page visits within
    those user sessions. The final output of this MapReduce job takes the form of
    key-value pairs. These keys represent unique user-session combinations, while
    the values hold the corresponding sequences of visited URLs. This valuable output
    enables various analyses, such as understanding user navigation patterns, identifying
    common paths taken during sessions, and uncovering frequently visited page transitions.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目的和上下文**：此reducer与mapper代码协同工作，以促进用户活动日志的基于会话的分析。它聚合与每个会话键关联的URL，有效地重建了用户会话中网页访问的顺序。此MapReduce作业的最终输出形式为键值对。这些键代表唯一的用户会话组合，而值则包含相应的访问URL序列。这种有价值的输出使得各种分析成为可能，例如理解用户导航模式、识别会话期间采取的常见路径以及揭示频繁访问的页面转换。'
- en: For Hadoop developers, *writing MapReduce jobs in Java is essential*. Java’s
    object-oriented features and the Hadoop API empower developers to distribute complex
    data processing tasks across a cluster. The `Mapper` and `Reducer` classes, the
    heart of a MapReduce job, handle the core logic. Java’s rich ecosystem and tooling
    support streamline the writing and debugging of these jobs. As you progress, mastering
    efficient MapReduce development in Java unlocks the full potential of big data
    processing with Hadoop.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Hadoop 开发者来说，*在 Java 中编写 MapReduce 作业是必不可少的*。Java 的面向对象特性和 Hadoop API 使开发者能够将复杂的数据处理任务分布到集群中。MapReduce
    作业的核心是 `Mapper` 和 `Reducer` 类，它们处理核心逻辑。Java 丰富的生态系统和工具支持简化了这些作业的编写和调试。随着您的进步，掌握高效的
    Java MapReduce 开发将释放 Hadoop 大数据处理的全潜能。
- en: Beyond the basics – advanced Hadoop concepts for Java developers and architects
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越基础——Java 开发者和架构师的高级 Hadoop 概念
- en: While understanding the core concepts of Hadoop, such as HDFS and MapReduce,
    is essential, there are several advanced Hadoop components and technologies that
    Java developers and architects should be familiar with. In this section, we’ll
    explore YARN and HBase, two important components of the Hadoop ecosystem, focusing
    on their practical applications and how they can be leveraged in real-world projects.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理解 Hadoop 的核心概念，如 HDFS 和 MapReduce，是必不可少的，但 Java 开发者和架构师还应熟悉几个高级 Hadoop 组件和技术。在本节中，我们将探讨
    YARN 和 HBase，这两个 Hadoop 生态系统的重要组件，重点关注它们的实际应用和如何在实际项目中利用它们。
- en: Yet another resource negotiator
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一个资源协商者
- en: '**Yet Another Resource Negotiator** (**YARN**) is a resource management and
    job scheduling framework in Hadoop. It separates resource management and processing
    components, allowing multiple data processing engines to run on Hadoop. Its key
    concepts are as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**YARN（Yet Another Resource Negotiator**）是 Hadoop 中的一个资源管理和作业调度框架。它将资源管理和处理组件分离，允许多个数据处理引擎在
    Hadoop 上运行。其关键概念如下：'
- en: '**ResourceManager**: Manages the global assignment of resources to applications'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ResourceManager**：管理资源对应用程序的全局分配'
- en: '**NodeManager**: Monitors and manages resources on individual nodes in a cluster'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NodeManager**：监控和管理集群中各个节点上的资源'
- en: '**ApplicationMaster**: Negotiates resources and manages the life cycle of an
    application'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ApplicationMaster**：协商资源并管理应用程序的生命周期'
- en: 'Its benefits for Java developers and architects are as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 它对 Java 开发者和架构师的好处如下：
- en: YARN enables running various data processing frameworks, such as Spark and Flink,
    on the same Hadoop cluster, providing flexibility and efficiency
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN 允许在同一个 Hadoop 集群上运行各种数据处理框架，如 Spark 和 Flink，提供灵活性和效率
- en: It allows better resource utilization and multitenancy, enabling multiple applications
    to share the same cluster resources
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许更好的资源利用和多租户，使多个应用程序能够共享相同的集群资源
- en: Java developers can leverage YARN’s APIs to develop and deploy custom applications
    on Hadoop
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 开发者可以利用 YARN 的 API 在 Hadoop 上开发和部署自定义应用程序
- en: HBase
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HBase
- en: '**HBase** is a column-oriented, NoSQL database built on top of Hadoop. It provides
    real-time, random read/write access to large datasets. Its key concepts are as
    follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**HBase** 是建立在 Hadoop 之上的列式、NoSQL 数据库。它提供对大数据集的实时、随机读写访问。其关键概念如下：'
- en: '**Table:** Consists of rows and columns, similar to a traditional database
    table'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表**：由行和列组成，类似于传统的数据库表'
- en: '**Row key**: Uniquely identifies a row in an HBase table'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行键**：唯一标识 HBase 表中的一行'
- en: '**Column family**: Groups related columns together for better data locality
    and performance'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**列族**：将相关列组合在一起以实现更好的数据局部性和性能'
- en: 'Its benefits for Java developers and architects are as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 它对 Java 开发者和架构师的好处如下：
- en: HBase is ideal for applications that require low-latency and random access to
    large datasets, such as real-time web applications or sensor data storage
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase 适用于需要低延迟和随机访问大数据集的应用程序，如实时 Web 应用程序或传感器数据存储
- en: It integrates seamlessly with Hadoop and allows you to run MapReduce jobs on
    HBase data
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与 Hadoop 无缝集成，允许您在 HBase 数据上运行 MapReduce 作业
- en: Java developers can use the HBase Java API to interact with HBase tables, perform
    **Create, Read, Update, Delete** (**CRUD**) operations, and execute scans and
    filters
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 开发者可以使用 HBase Java API 与 HBase 表交互，执行 **创建、读取、更新、删除** （**CRUD**）操作，并执行扫描和过滤
- en: HBase supports high write throughput and scales horizontally, making it suitable
    to handle large-scale, write-heavy workloads
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBase 支持高写入吞吐量和水平扩展，使其适合处理大规模、写入密集型的工作负载
- en: Integration with the Java ecosystem
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与 Java 生态系统的集成
- en: 'Hadoop integrates well with various Java-based tools and frameworks commonly
    used in enterprise environments. Some notable integrations are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 与企业环境中常用的基于 Java 的工具和框架集成良好。以下是一些显著的集成：
- en: '**Apache Hive**: A data warehousing and SQL-like querying framework built on
    top of Hadoop. Java developers can use Hive to query and analyze large datasets
    using familiar SQL syntax.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Hive**：一个基于 Hadoop 的数据仓库和类似 SQL 的查询框架。Java 开发者可以使用 Hive 使用熟悉的 SQL
    语法查询和分析大型数据集。'
- en: '**Apache Kafka**: A distributed streaming platform that integrates with Hadoop
    for real-time data ingestion and processing. Java developers can use Kafka’s Java
    API to publish and consume data streams.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Kafka**：一个分布式流平台，与 Hadoop 集成以进行实时数据摄取和处理。Java 开发者可以使用 Kafka 的 Java
    API 发布和消费数据流。'
- en: '**Apache Oozie**: A workflow scheduler for Hadoop jobs. Java developers can
    define and manage complex workflows using Oozie’s XML-based configuration or Java
    API.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Oozie**：一个用于 Hadoop 作业的工作流调度器。Java 开发者可以使用 Oozie 的基于 XML 的配置或 Java
    API 定义和管理复杂的工作流。'
- en: For Java developers and architects, Hadoop’s power extends beyond core components.
    Advanced features such as YARN (resource management) and HBase (real-time data
    access) enable flexible, scalable big data solutions. Seamless integration with
    other Java-based tools, such as Hive and Kafka, expands Hadoop’s capabilities.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Java 开发者和架构师来说，Hadoop 的力量不仅限于核心组件。高级功能，如 YARN（资源管理）和 HBase（实时数据访问），使灵活、可扩展的大数据解决方案成为可能。与其他基于
    Java 的工具，如 Hive 和 Kafka 的无缝集成扩展了 Hadoop 的功能。
- en: One real-life system where Hadoop’s capabilities have been expanded through
    integration with Hive and Kafka is LinkedIn’s data processing and analytics infrastructure.
    LinkedIn has built a robust data handling infrastructure, leveraging Hadoop for
    large-scale data storage and processing, complemented by Kafka for real-time data
    streaming and Hive for SQL-like data querying and analysis. Kafka channels vast
    streams of user activity data into the Hadoop ecosystem, where it’s stored and
    processed. Hive then enables detailed data analysis and insight generation. This
    integrated system supports LinkedIn’s diverse analytical needs, from operational
    optimization to personalized recommendations, showcasing the synergy between Hadoop,
    Hive, and Kafka in managing and analyzing big data.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通过集成 Hive 和 Kafka 扩展了 Hadoop 功能的实际系统是 LinkedIn 的数据处理和分析基础设施。LinkedIn 建立了一个强大的数据处理基础设施，利用
    Hadoop 进行大规模数据存储和处理，辅以 Kafka 进行实时数据流和 Hive 进行类似 SQL 的数据查询和分析。Kafka 将大量用户活动数据流引入
    Hadoop 生态系统，在那里进行存储和处理。然后 Hive 使详细的数据分析和洞察生成成为可能。这个集成系统支持 LinkedIn 多样化的分析需求，从运营优化到个性化推荐，展示了
    Hadoop、Hive 和 Kafka 在管理和分析大数据方面的协同作用。
- en: Mastering these concepts empowers architects to build robust big data applications
    for modern businesses. As processing needs evolve, frameworks such as Spark offer
    even faster in-memory computations, complementing Hadoop for complex data pipelines.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 掌握这些概念使架构师能够为现代企业构建强大的大数据应用程序。随着处理需求的发展，如 Spark 这样的框架提供了更快的内存计算，补充了 Hadoop 的复杂数据管道。
- en: Understanding DataFrames and RDDs in Apache Spark
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 Apache Spark 中的 DataFrame 和 RDD
- en: Apache Spark provides two primary abstractions to work with distributed data
    – **Resilient Distributed Datasets** (**RDDs**) and **DataFrames**. Each offers
    unique features and benefits tailored to different types of data processing tasks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 提供了两种主要的抽象来处理分布式数据 – **弹性分布式数据集**（**RDDs**）和 **DataFrame**。每种都提供了针对不同类型数据处理任务的独特功能和优势。
- en: RDDs
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RDDs
- en: RDDs are the fundamental data structure of Spark, providing an immutable distributed
    collection of objects that allows data to be processed in parallel across a distributed
    environment. Each dataset in RDD is divided into logical partitions, which can
    be computed on different nodes of the cluster.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 是 Spark 的基本数据结构，提供了一组不可变的分布式对象集合，允许在分布式环境中并行处理数据。RDD 中的每个数据集都被划分为逻辑分区，这些分区可以在集群的不同节点上计算。
- en: RDDs are well-suited for low-level transformations and actions that require
    fine-grained control over physical data distribution and transformations, such
    as custom partitioning schemes or when performing complex algorithms that involve
    iterative data processing over a network.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 非常适合需要精细控制物理数据分布和转换的低级转换和操作，例如自定义分区方案或执行涉及网络迭代数据处理的复杂算法。
- en: RDDs support two types of operations – transformations, which create a new RDD
    from an existing one, and actions, which return a value to the driver program
    after running a computation on the dataset.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: RDD支持两种类型的操作——转换，它从一个现有的RDD创建一个新的RDD，以及动作，它在数据集上运行计算后向驱动程序返回一个值。
- en: DataFrames
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DataFrame
- en: Introduced as an abstraction on top of RDDs, DataFrames are a distributed collection
    of data organized into named columns, similar to a table in a relational database
    but with richer optimizations under the hood.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 作为RDD之上的一个抽象，DataFrame是一个组织成命名列的分布式数据集合，类似于关系数据库中的表，但在底层有更丰富的优化。
- en: 'Here are the advantages of DataFrames:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是DataFrame的优势：
- en: '**Optimized execution**: Spark SQL’s Catalyst optimizer compiles DataFrame
    operations into highly efficient physical execution plans. This optimization allows
    for faster processing compared to RDDs, which do not benefit from such optimization.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化执行**：Spark SQL的Catalyst优化器将DataFrame操作编译成高度高效的物理执行计划。这种优化使得处理速度比没有这种优化的RDD更快。'
- en: '**Ease of use**: The DataFrame API provides a more declarative programming
    style, making complex data manipulations and aggregations easier to express and
    understand.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**易用性**：DataFrame API提供了一种更声明式的编程风格，使得复杂的数据操作和聚合更容易表达和理解。'
- en: '**Interoperability**: DataFrames support various data formats and sources,
    including Parquet, CSV, JSON, and JDBC, making data integration and processing
    simpler and more robust.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互操作性**：DataFrame支持多种数据格式和来源，包括Parquet、CSV、JSON和JDBC，这使得数据集成和处理更加简单和健壮。'
- en: DataFrames are ideal for handling structured and semi-structured data. They
    are preferred for data exploration, transformation, and aggregation tasks, especially
    when ease of use and performance optimization are priorities.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame非常适合处理结构化和半结构化数据。在易用性和性能优化是优先考虑的情况下，DataFrame是数据探索、转换和聚合任务的优先选择。
- en: Emphasizing DataFrames over RDDs
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 强调DataFrame优于RDD
- en: Since the introduction of Spark `2.0`, DataFrames have been recommended as the
    standard abstraction for data processing tasks due to their significant advantages
    in terms of optimization and usability. While RDDs remain useful for specific
    scenarios requiring detailed control over data operations, DataFrames provide
    a powerful, flexible, and efficient way to work with large-scale data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Spark `2.0`的引入以来，由于DataFrame在优化和易用性方面的显著优势，DataFrame已被推荐为数据处理任务的标准抽象。虽然RDD在需要详细控制数据操作的具体场景中仍然有用，但DataFrame提供了一种强大、灵活且高效的方式，用于处理大规模数据。
- en: RDDs are the foundation of Spark’s distributed data processing capabilities.
    This section dives into how to create and manipulate RDDs to efficiently analyze
    large-scale datasets across a cluster.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: RDD是Spark分布式数据处理能力的基础。本节深入探讨了如何创建和操作RDD，以高效地分析集群中的大规模数据集。
- en: 'RDDs can be created in several ways, including the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: RDD可以通过以下几种方式创建：
- en: 'Parallelizing an existing collection:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行化现有集合：
- en: '[PRE2]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Reading from external datasets (e.g., text files, CSV files, or databases):'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从外部数据集（例如，文本文件、CSV文件或数据库）读取：
- en: '[PRE3]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Transforming existing RDDs:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换现有RDD：
- en: '[PRE4]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: RDDs support two types of operations – transformations and actions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: RDD支持两种类型的操作——转换和动作。
- en: '`map()`, `filter()`, `flatMap()`, and `reduceByKey()`. Transformations are
    lazily evaluated.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map()`、`filter()`、`flatMap()`和`reduceByKey()`。转换是惰性评估的。'
- en: '`collect()`, `count()`, `first()`, and `saveAsTextFile()`.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`collect()`、`count()`、`first()`和`saveAsTextFile()`。'
- en: By leveraging RDDs and their distributed nature, Spark enables developers to
    process and analyze large-scale datasets efficiently across a cluster of machines.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用RDD及其分布式特性，Spark使开发者能够高效地在机器集群中处理和分析大规模数据集。
- en: 'Let’s look at the following code snippet:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下代码片段：
- en: '[PRE5]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code demonstrates the usage of RDDs. It performs the following steps:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码演示了RDD的使用。它执行以下步骤：
- en: It creates an RDD called `lines` by reading a text file located at `"path/to/data.txt"`,
    using `spark.sparkContext().textFile()`. The second argument, `1`, specifies the
    minimum number of partitions for the RDD.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过使用`spark.sparkContext().textFile()`读取位于`"path/to/data.txt"`的文本文件，创建了一个名为`lines`的RDD。第二个参数`1`指定了RDD的最小分区数。
- en: It applies a map transformation to the lines RDD using `Integer::parseInt`.
    This transformation converts each line of text into an integer, resulting in a
    new RDD called `numbers`.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用`Integer::parseInt`对lines RDD应用了一个map转换。这个转换将每一行文本转换成一个整数，从而产生一个新的名为`numbers`的RDD。
- en: It applies a filter transformation to the numbers RDD using `n -> n % 2 == 0`.
    This transformation keeps only the even numbers in the RDD, creating a new RDD
    called `evenNumbers`.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用 `n -> n % 2 == 0` 对数字 RDD 应用一个过滤转换。这个转换只保留 RDD 中的偶数，创建一个新的 RDD，称为 `evenNumbers`。
- en: It performs an action on the `evenNumbers` RDD using `count()`, which returns
    the number of elements in the RDD. The result is stored in the `count` variable.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用 `count()` 在 `evenNumbers` RDD 上执行一个动作，该动作返回 RDD 中的元素数量。结果存储在 `count` 变量中。
- en: Finally, it prints the count of even numbers using `System.out.println()`.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它使用 `System.out.println()` 打印偶数的数量。
- en: This code showcases the basic usage of RDDs in Spark, demonstrating how to create
    an RDD from a text file, apply transformations (map and filter) to the RDD, and
    perform an action (count) to retrieve a result. The transformations are lazily
    evaluated, meaning they are not executed until an action is triggered.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了 Spark 中 RDD 的基本用法，演示了如何从一个文本文件创建 RDD，对 RDD 应用转换（map 和 filter），以及执行一个动作（count）以检索结果。转换是惰性评估的，这意味着它们在触发动作之前不会执行。
- en: Spark programming with Java – unleashing the power of DataFrames and RDDs
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Java 编程 Spark – 激发 DataFrame 和 RDD 的力量
- en: In this section, we’ll explore the commonly used *transformations* and *actions*
    within Spark’s Java API, focusing on both DataFrames and RDDs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索 Spark Java API 中常用的 *转换* 和 *动作*，重点关注 DataFrame 和 RDD。
- en: Spark’s DataFrame API – a comprehensive guide
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Spark 的 DataFrame API – 一份全面的指南
- en: '*DataFrames* have become the primary data abstraction in Spark 2.0 and above,
    offering a more efficient and user-friendly way to work with structured and semi-structured
    data. Let’s explore the *DataFrame API* in detail, including how to create DataFrames,
    perform transformations and actions, and leverage SQL-like querying.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*DataFrame* 已成为 Spark 2.0 及以上版本中的主要数据抽象，提供了一种更高效、更友好的方式来处理结构化和半结构化数据。让我们详细探索
    *DataFrame API*，包括如何创建 DataFrame、执行转换和动作，以及利用类似 SQL 的查询。'
- en: First up is creating DataFrames.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是创建 DataFrame。
- en: 'There are several ways to create DataFrames in Spark; here is an example to
    create one from an existing RDD:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中创建 DataFrame 有几种方法；以下是一个从现有 RDD 创建 DataFrame 的示例：
- en: '[PRE6]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code creates a DataFrame from an existing RDD. It starts by creating an
    RDD of strings (`textRDD`) from a text file. Then, it converts `textRDD` to an
    RDD of row objects (`rowRDD`), using `map()`. The schema for the DataFrame is
    defined using `StructType` and `StructField`. Finally, the DataFrame is created
    using `spark.createDataFrame()`, passing `rowRDD` and the schema.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码从一个现有的 RDD 创建一个 DataFrame。它首先从一个文本文件创建一个字符串 RDD (`textRDD`)。然后，使用 `map()`
    将 `textRDD` 转换为行对象的 RDD (`rowRDD`)。DataFrame 的模式使用 `StructType` 和 `StructField`
    定义。最后，使用 `spark.createDataFrame()` 创建 DataFrame，传递 `rowRDD` 和模式。
- en: Next, we’ll encounter DataFrame transformations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将遇到 DataFrame 转换。
- en: 'DataFrames provide a wide range of transformations for data manipulation and
    processing. Some common transformations include the following:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 提供了广泛的数据操作和处理转换。一些常见的转换包括以下内容：
- en: '**Filtering rows**:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤行**：'
- en: '[PRE7]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Selecting columns**:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择列**：'
- en: '[PRE8]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Adding or** **modifying columns**:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加或修改列**：'
- en: '[PRE9]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Aggregating data**:'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合数据**：'
- en: '[PRE10]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we will move on to DataFrame actions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续到 DataFrame 动作。
- en: 'Actions trigger the computation on DataFrames and return the results to the
    driver program. Some common actions include the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 动作触发了 DataFrame 上的计算，并将结果返回给驱动程序。一些常见的动作包括以下内容：
- en: '**Collecting data in** **the driver**:'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在驱动程序中收集数据**：'
- en: '[PRE11]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Counting rows**:'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数行**：'
- en: '[PRE12]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Saving data to a file or** **data source**:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将数据保存到文件或数据源**：'
- en: '[PRE13]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**SQL-like querying with DataFrames**: One of the powerful features of DataFrames
    is the ability to use SQL-like queries for data analysis and manipulation. Spark
    SQL provides a SQL interface to query structured data stored as DataFrames:'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 DataFrame 进行类似 SQL 的查询**：DataFrame 的一个强大功能是能够使用类似 SQL 的查询进行数据分析和操作。Spark
    SQL 提供了一个 SQL 接口来查询存储为 DataFrame 的结构化数据：'
- en: '**Registering a DataFrame as a** **temporary view**:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将 DataFrame 注册为临时视图**：'
- en: '[PRE14]'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Executing** **SQL queries**:'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行 SQL 查询**：'
- en: '[PRE15]'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Joining DataFrames**:'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接 DataFrame**：'
- en: '[PRE16]'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: These examples demonstrate the expressiveness and flexibility of the DataFrame
    API in Spark. By leveraging DataFrames, developers can perform complex data manipulations,
    transformations, and aggregations efficiently, while also benefiting from the
    optimizations provided by the Spark SQL engine.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例展示了 Spark DataFrame API 的表达性和灵活性。通过利用 DataFrame，开发者可以高效地执行复杂的数据操作、转换和聚合，同时也能受益于
    Spark SQL 引擎提供的优化。
- en: By mastering these operations and understanding when to use DataFrames versus
    RDDs, developers can build efficient and powerful data processing pipelines in
    Spark. The Java API’s evolution continues to empower developers to tackle big
    data challenges effectively with both structured and unstructured data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握这些操作并理解何时使用 DataFrame 而不是 RDD，开发者可以在 Spark 中构建高效且强大的数据处理管道。Java API 的演变继续赋予开发者有效地处理结构化和非结构化大数据挑战的能力。
- en: Performance optimization in Apache Spark
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Apache Spark 的性能优化
- en: 'Optimizing performance in Spark applications involves understanding and mitigating
    several key issues that can affect scalability and efficiency. This section covers
    strategies to handle data shuffling, manage data skew, and optimize data collection
    in the driver, providing a holistic approach to performance tuning:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 应用程序中优化性能涉及理解和缓解几个关键问题，这些问题可能影响可扩展性和效率。本节涵盖了处理数据洗牌、管理数据偏斜和在驱动程序中优化数据收集的策略，提供了一个全面的性能调优方法：
- en: '`groupBy()`, `join()`, or `reduceByKey()` require data to be redistributed
    across partitions. Shuffling involves disk I/O and network I/O and can lead to
    substantial resource consumption.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupBy()`、`join()` 或 `reduceByKey()` 需要数据在分区间重新分配。洗牌涉及磁盘 I/O 和网络 I/O，可能导致大量资源消耗。'
- en: '`map()` before `reduceByKey()` can reduce the amount of data shuffled.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `reduceByKey()` 之前使用 `map()` 可以减少需要洗牌的数据量。
- en: '`repartition()` or `coalesce()` to optimize the number of partitions and distribute
    data more evenly across the cluster.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `repartition()` 或 `coalesce()` 来优化分区数量，并在集群中更均匀地分配数据。
- en: '**Handling data skew**: Data skew occurs when one or more partitions receive
    significantly more data than others, leading to uneven workloads and potential
    bottlenecks.*   **Strategies to handle** **data skew**:'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理数据偏斜**：当一个或多个分区接收的数据明显多于其他分区时，会发生数据偏斜，导致工作负载不均和潜在的瓶颈。*   **处理数据偏斜的策略**：'
- en: '**Salting keys**: Modify the keys that cause skew by adding a random prefix
    or suffix to distribute the load more evenly'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**盐值键**：通过添加随机前缀或后缀来修改导致偏斜的键，从而更均匀地分配负载'
- en: '**Custom partitioning**: Implement a custom partitioner that distributes data
    more evenly, based on your application’s specific characteristics'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义分区**：根据应用程序的特定特征实现一个自定义分区器，以更均匀地分配数据。'
- en: '**Filter and split**: Identify skewed data, process it separately, and then
    merge the results to prevent overloaded partitions*   `collect()` can lead to
    out-of-memory errors and degrade overall performance.*   `take()`, `first()`,
    or `show()` to retrieve only necessary data samples instead of an entire dataset*   `foreachPartition()`
    to apply operations, such as database writes or API calls, directly within each
    partition'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤和拆分**：识别偏斜数据，单独处理，然后合并结果以防止过载的分区*   `collect()` 可能会导致内存不足错误并降低整体性能.*   使用
    `take()`、`first()` 或 `show()` 来检索仅必要的数据样本，而不是整个数据集*   使用 `foreachPartition()`
    在每个分区中直接应用操作，例如数据库写入或 API 调用'
- en: 'Here is an example of efficient data handling:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个高效处理数据的示例：
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This example showcases two techniques to manage data skew and optimize data
    collection in Apache Spark:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本例展示了两种在 Apache Spark 中管理数据偏斜和优化数据收集的技术：
- en: '`CustomPartitioner`) to distribute skewed data more evenly across the cluster.
    By calling `partitionBy()` with the custom partitioner on the skewed data, it
    creates a new RDD (`partitionedData`) with a more balanced data distribution,
    mitigating the impact of skew.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `CustomPartitioner`) 来在集群中更均匀地分配偏斜数据。通过在偏斜数据上调用 `partitionBy()` 并使用自定义分区器，它创建了一个具有更平衡数据分布的新
    RDD (`partitionedData`)，减轻了偏斜的影响。
- en: '`map()` to extract the values and `reduce` to sum them up across partitions.
    By aggregating the data before `collect()`, it reduces the amount of data sent
    to the driver, optimizing data collection and minimizing network overhead.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `map()` 提取值，使用 `reduce` 在分区间求和。通过在 `collect()` 之前聚合数据，可以减少发送到驱动程序的数据量，优化数据收集并最小化网络开销。
- en: These techniques help improve the performance and scalability of Spark applications
    when dealing with skewed data distributions and large result sets.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术有助于提高Spark应用程序在处理倾斜数据分布和大型结果集时的性能和可扩展性。
- en: Spark optimization and fault tolerance – advanced concepts
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark优化和容错 - 高级概念
- en: Understanding some advanced concepts such as the execution **Directed Acyclic
    Graph** (**DAG**), *caching*, and *retry* mechanisms is essential for a deeper
    understanding of Spark’s optimization and fault tolerance capabilities. Integrating
    these topics can enhance the effectiveness of Spark application development. Let’s
    break down these concepts and how they relate to the DataFrame API.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 理解一些高级概念，例如执行**有向无环图**（**DAG**）、*缓存*和*重试*机制，对于深入理解Spark的优化和容错能力至关重要。整合这些主题可以增强Spark应用程序开发的有效性。让我们分解这些概念以及它们与DataFrame
    API的关系。
- en: Execution DAG in Spark
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Spark中的执行DAG
- en: The DAG in Spark is a fundamental concept that underpins how Spark executes
    workflows across a distributed cluster. When you perform operations on a DataFrame,
    Spark constructs a DAG of stages, with each stage consisting of tasks based on
    transformations applied to the data. This DAG outlines the steps that Spark will
    execute across the cluster.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的DAG是一个基本概念，它支撑着Spark如何在分布式集群中执行工作流。当你对一个DataFrame执行操作时，Spark构建了一个由阶段组成的DAG，每个阶段由基于数据转换的任务组成。这个DAG概述了Spark将在集群中执行的步骤。
- en: 'The following are the key points:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关键点：
- en: '`groupBy()`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupBy()`.'
- en: '`show()`, `count()`, or `save()`) is triggered. This allows Spark to optimize
    the entire data processing pipeline, consolidating tasks and stages efficiently.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当触发`show()`、`count()`或`save()`操作时，这允许Spark优化整个数据处理管道，有效地合并任务和阶段。
- en: '**Optimization**: Through the Catalyst optimizer, Spark converts this logical
    execution plan (the DAG) into a physical plan that optimizes the execution, by
    rearranging operations and combining tasks.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**：通过Catalyst优化器，Spark将这个逻辑执行计划（DAG）转换为物理计划，以优化执行，通过重新排列操作和合并任务。'
- en: Caching and persistence
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缓存和持久化
- en: 'Caching in Spark is critical for optimizing the performance of iterative algorithms
    and interactive data analysis, where the same dataset is queried repeatedly. Caching
    can be used as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中进行缓存对于优化迭代算法和交互式数据分析的性能至关重要，在这些情况下，相同的数据集会被反复查询。缓存可以使用如下方式：
- en: '`cache()` or `persist()` methods. This is particularly useful when data is
    accessed repeatedly, such as when tuning machine learning models or running multiple
    queries on the same subset of data.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cache()`或`persist()`方法。这在数据被反复访问时特别有用，例如在调整机器学习模型或对相同数据子集运行多个查询时。'
- en: '`persist()` method can take a storage level parameter (`MEMORY_ONLY`, `MEMORY_AND_DISK`,
    etc.), allowing you finer control over how your data is stored.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`persist()`方法可以接受一个存储级别参数（`MEMORY_ONLY`、`MEMORY_AND_DISK`等），这允许你更精细地控制数据存储的方式。'
- en: Retry mechanisms and fault tolerance
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重试机制和容错
- en: 'Spark provides robust fault tolerance through its distributed architecture
    and by rebuilding lost data, using the lineage of transformations (DAG):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Spark通过其分布式架构和重建丢失数据，使用转换的谱系（DAG）提供了强大的容错能力：
- en: '**Task retries**: If a task fails, Spark automatically retries it. The number
    of retries and the conditions for a retry can be configured in Spark’s settings.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务重试**：如果一个任务失败，Spark会自动重试它。重试的次数和重试的条件可以在Spark的设置中进行配置。'
- en: '**Node failure**: In case of node failures, Spark can recompute lost partitions
    of data from the original source, as long as the source data is still accessible
    and the lineage is intact.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点故障**：在节点故障的情况下，只要源数据仍然可访问且谱系完整，Spark可以从原始源重新计算丢失的数据分区。'
- en: '**Checkpointing**: For long-running and complex DAGs, checkpointing can be
    used to truncate the RDD lineage and save the intermediate state to a reliable
    storage system, such as HDFS. This reduces recovery time if there are failures.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查点**：对于长时间运行和复杂的DAG，可以使用检查点来截断RDD谱系并保存中间状态到可靠的存储系统，如HDFS。这减少了失败时的恢复时间。'
- en: 'Here’s an example demonstrating these concepts in action:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个演示这些概念如何应用的示例：
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This code demonstrates how Spark’s advanced features can be used to optimize
    complex data processing tasks:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码演示了如何使用Spark的高级功能来优化复杂的数据处理任务：
- en: '`spark.read().json(...)`, Spark builds an `execution DAG` that represents the
    data processing pipeline. This DAG outlines the stages of operations on the data.
    Spark utilizes *lazy evaluation*, delaying computations until an action such as
    `show()` is triggered. This allows Spark to analyze the entire DAG and optimize
    the execution plan.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.read().json(...)`, Spark构建一个表示数据处理管道的`执行DAG`。这个DAG概述了数据操作的阶段。Spark利用*延迟评估*，将计算延迟到触发`show()`等操作时。这允许Spark分析整个DAG并优化执行计划。'
- en: '`data (df)` is *cached* using `cache()`. This stores the data in memory, allowing
    faster access for subsequent transformations. Additionally, the transformed data
    (`processedDf`) is *persisted* with `persist(StorageLevel.MEMORY_AND_DISK())`.
    This ensures the that processed data remains available even after the triggering
    action, `(show())`, potentially improving performance for future operations that
    rely on it. Specifying the `MEMORY_AND_DISK` storage level keeps the data in memory
    for faster access, while also persisting it to disk for fault tolerance.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`cache()`将`data (df)`*缓存*。这将在内存中存储数据，允许后续转换更快地访问。此外，转换后的数据（`processedDf`）使用`persist(StorageLevel.MEMORY_AND_DISK())`进行*持久化*。这确保了处理后的数据在触发操作（`show()`）后仍然可用，可能提高依赖于它的未来操作的性能。指定`MEMORY_AND_DISK`存储级别将数据保留在内存中以便快速访问，同时也将其持久化到磁盘以提高容错性。
- en: '`processedDf` fails (due to a potentially non-existent column), Spark can still
    complete the operation by recomputing the required data from the already cached
    `processedDf`. This highlights Spark’s ability to handle failures and ensure successful
    completion of tasks.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`processedDf`失败（由于可能不存在的列），Spark仍然可以通过从已缓存的`processedDf`重新计算所需数据来完成操作。这突出了Spark处理失败并确保任务成功完成的能力。
- en: By effectively utilizing execution DAGs, caching, persistence, and retry mechanisms,
    this code exemplifies how Spark can optimize performance, improve data processing
    efficiency, and ensure robust execution of complex workflows even in the face
    of potential failures.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有效地利用执行DAG、缓存、持久化和重试机制，此代码示例展示了Spark如何优化性能、提高数据处理效率，并在面对潜在失败的情况下确保复杂工作流的稳健执行。
- en: Spark versus Hadoop – choosing the right framework for the job
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark与Hadoop的比较——选择适合工作的正确框架
- en: Spark and Hadoop are two powerful big data processing frameworks that have gained
    widespread adoption in the industry. While both frameworks are designed to handle
    large-scale data processing, they have distinct characteristics and excel in different
    scenarios. In this section, we’ll explore the strengths of Spark and Hadoop and
    discuss situations where each framework is best suited.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Spark和Hadoop是两个强大的大数据处理框架，在业界得到了广泛的应用。虽然这两个框架都旨在处理大规模数据处理，但它们具有不同的特性，在不同的场景中表现出色。在本节中，我们将探讨Spark和Hadoop的优势，并讨论每个框架最适合的情况。
- en: 'Scenarios where Hadoop’s MapReduce excels include the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的MapReduce在以下场景中表现出色：
- en: '**Batch processing**: MapReduce is highly efficient for large-scale batch processing
    tasks where data can be processed in a linear, map-then-reduce manner.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量处理**：MapReduce对于大规模批量处理任务非常高效，其中数据可以以线性、先映射后归约的方式处理。'
- en: '**Data warehousing and archiving**: Hadoop is often used to store and archive
    large datasets, thanks to its cost-effective storage solution, HDFS. It’s suitable
    for scenarios where data doesn’t need to be accessed in real-time.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据仓库和归档**：Hadoop通常用于存储和归档大型数据集，这得益于其成本效益的存储解决方案HDFS。它适用于数据不需要实时访问的场景。'
- en: '**Highly scalable processing**: For tasks that are not time-sensitive and can
    benefit from linear scalability, MapReduce can efficiently process petabytes of
    data across thousands of machines.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高度可扩展的处理**：对于非时间敏感且可从线性可扩展性中受益的任务，MapReduce可以高效地在数千台机器上处理PB级的数据。'
- en: '**Fault tolerance on commodity hardware**: Hadoop’s infrastructure is designed
    to reliably store and process data across potentially unreliable commodity hardware,
    making it a cost-effective solution for massive data storage and processing.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在通用硬件上的容错性**：Hadoop的基础设施旨在在可能不可靠的通用硬件上可靠地存储和处理数据，使其成为大规模数据存储和处理的成本效益解决方案。'
- en: 'Scenarios where Apache Spark excels include the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark在以下场景中表现出色：
- en: '**Iterative algorithms in machine learning and data mining**: Spark’s in-memory
    data processing capabilities make it significantly faster than MapReduce for iterative
    algorithms, which are common in machine learning and data mining tasks.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习和数据挖掘中的迭代算法**：Spark 的内存数据处理能力使其在迭代算法方面比 MapReduce 快得多，而迭代算法在机器学习和数据挖掘任务中很常见。'
- en: '**Real-time stream processing**: Spark Streaming allows you to process real-time
    data streams. It’s ideal for scenarios where data needs to be processed immediately
    as it arrives, such as in log file analysis and real-time fraud detection systems.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时流处理**：Spark Streaming 允许您处理实时数据流。它非常适合需要立即处理到达的数据的场景，例如日志文件分析和实时欺诈检测系统。'
- en: '**Interactive data analysis and processing**: Spark’s ability to cache data
    in memory across operations makes it an excellent choice for interactive data
    exploration, analysis, and processing tasks. Tools such as Apache Zeppelin and
    Jupyter integrate well with Spark for interactive data science work.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交互式数据分析和处理**：Spark 能够在操作间缓存数据于内存中，这使得它非常适合交互式数据探索、分析和处理任务。Apache Zeppelin
    和 Jupyter 等工具与 Spark 集成良好，适用于交互式数据科学工作。'
- en: '**Graph processing**: GraphX, a component of Spark, enables graph processing
    and computation directly within the Spark ecosystem, making it suitable for social
    network analysis, recommendation systems, and other applications that involve
    complex relationships between data points.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图处理**：GraphX 是 Spark 的一个组件，它可以在 Spark 生态系统中直接进行图处理和计算，这使得它非常适合社交网络分析、推荐系统以及其他涉及数据点之间复杂关系的应用。'
- en: In practice, Spark and Hadoop are not mutually exclusive and often used together.
    Spark can run on top of HDFS and even integrate with Hadoop’s ecosystem, including
    YARN for resource management. This integration leverages Hadoop’s storage capabilities
    while benefiting from Spark’s processing speed and versatility, providing a comprehensive
    solution for big data challenges.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，Spark 和 Hadoop 并非互斥，通常会被一起使用。Spark 可以在 HDFS 上运行，甚至可以与 Hadoop 的生态系统集成，包括
    YARN 用于资源管理。这种集成利用了 Hadoop 的存储能力，同时得益于 Spark 的处理速度和灵活性，为大数据挑战提供了全面的解决方案。
- en: Hadoop and Spark equivalents in major cloud platforms
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要云平台上的 Hadoop 和 Spark 等效服务
- en: 'While Apache Hadoop and Apache Spark are widely used in on-premises big data
    processing, major cloud platforms offer managed services that provide similar
    capabilities without the need to set up and maintain the underlying infrastructure.
    In this section, we’ll explore the equivalent services to Hadoop and Spark in
    AWS, Azure, and GCP:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Apache Hadoop 和 Apache Spark 在本地大数据处理中得到了广泛应用，但主要的云平台提供了类似的管理服务，无需设置和维护底层基础设施。在本节中，我们将探讨
    AWS、Azure 和 GCP 中 Hadoop 和 Spark 的等效服务：
- en: '**Amazon Web** **Services (AWS)**:'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Web Services (AWS)**:'
- en: '**Amazon Elastic MapReduce**: Amazon **Elastic MapReduce** (**EMR**) is a managed
    cluster platform that simplifies running big data frameworks, including Apache
    Hadoop and Apache Spark. It provides a scalable and cost-effective way to process
    and analyze large volumes of data. EMR supports various Hadoop ecosystem tools
    such as Hive, Pig, and HBase. It also integrates with other AWS services such
    as Amazon S3 for data storage and Amazon Kinesis for real-time data streaming.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic MapReduce**：Amazon Elastic MapReduce (EMR) 是一个管理集群平台，简化了运行大数据框架（包括
    Apache Hadoop 和 Apache Spark）的过程。它提供了一种可扩展且成本效益高的方式来处理和分析大量数据。EMR 支持各种 Hadoop
    生态系统工具，如 Hive、Pig 和 HBase。它还与其他 AWS 服务集成，例如 Amazon S3 用于数据存储和 Amazon Kinesis 用于实时数据流。'
- en: '**Amazon Simple Storage Service**: Amazon **Simple Storage Service** (**S3**)
    is an object storage service that provides scalable and durable storage for big
    data workflows. It can be used as a data lake to store and retrieve large datasets,
    serving as an alternative to HDFS. S3 integrates seamlessly with Amazon EMR and
    other big data processing services.'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Simple Storage Service**：Amazon Simple Storage Service (S3) 是一种对象存储服务，为大数据工作流程提供可扩展且持久的存储。它可以作为一个数据湖来存储和检索大型数据集，作为
    HDFS 的替代品。S3 与 Amazon EMR 和其他大数据处理服务无缝集成。'
- en: '**Microsoft Azure**:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Microsoft Azure**:'
- en: '**Azure HDInsight**: Azure HDInsight is a managed Apache Hadoop, Spark, and
    Kafka service in the cloud. It allows you to easily provision and manage Hadoop
    and Spark clusters on Azure. HDInsight supports a wide range of Hadoop ecosystem
    components, including Hive, Pig, and Oozie. It integrates with Azure Blob Storage
    and Azure Data Lake Storage to store and access big data.'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure HDInsight**：Azure HDInsight 是一个云中的托管 Apache Hadoop、Spark 和 Kafka 服务。它允许您在
    Azure 上轻松配置和管理 Hadoop 和 Spark 集群。HDInsight 支持广泛的 Hadoop 生态系统组件，包括 Hive、Pig 和 Oozie。它与
    Azure Blob Storage 和 Azure Data Lake Storage 集成，用于存储和访问大数据。'
- en: '**Azure Databricks**: Azure Databricks is a fully managed Apache Spark platform
    optimized for the Microsoft Azure cloud. It provides a collaborative and interactive
    environment to run Spark workloads. Databricks offers seamless integration with
    other Azure services and supports various programming languages, such as Python,
    R, and SQL.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Databricks**：Azure Databricks 是一个针对 Microsoft Azure 云优化的完全管理的 Apache
    Spark 平台。它提供了一个协作和交互式环境来运行 Spark 工作负载。Databricks 提供与其他 Azure 服务的无缝集成，并支持各种编程语言，如
    Python、R 和 SQL。'
- en: '**Google Cloud** **Platform (GCP)**:'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Platform (GCP)**：'
- en: '**Google Cloud Dataproc**: Google Cloud Dataproc is a fully managed Spark and
    Hadoop service. It allows you to quickly create and manage Spark and Hadoop clusters
    on GCP. Dataproc integrates with other GCP services such as Google Cloud Storage
    and BigQuery. It supports various Hadoop ecosystem tools and provides a familiar
    environment to run Spark and Hadoop jobs.'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Dataproc**：Google Cloud Dataproc 是一个完全管理的 Spark 和 Hadoop 服务。它允许您在
    GCP 上快速创建和管理 Spark 和 Hadoop 集群。Dataproc 与其他 GCP 服务（如 Google Cloud Storage 和 BigQuery）集成。它支持各种
    Hadoop 生态系统工具，并提供一个熟悉的环境来运行 Spark 和 Hadoop 作业。'
- en: '**Google Cloud Storage**: Google Cloud Storage is a scalable and durable object
    storage service. It serves as a data lake to store and retrieve large datasets,
    similar to Amazon S3\. Cloud Storage integrates with Google Cloud Dataproc and
    other GCP big data services.'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Cloud Storage**：Google Cloud Storage 是一个可扩展且耐用的对象存储服务。它作为数据湖存储和检索大型数据集，类似于
    Amazon S3。Cloud Storage 与 Google Cloud Dataproc 和其他 GCP 大数据服务集成。'
- en: Major cloud platforms offer managed services that provide equivalent functionality
    to Apache Hadoop and Apache Spark, simplifying the provisioning and management
    of big data processing clusters. These services integrate with their respective
    cloud storage solutions for seamless data storage and access.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 主要云平台提供托管服务，提供与 Apache Hadoop 和 Apache Spark 相当的功能，简化了大数据处理集群的配置和管理。这些服务与其各自的云存储解决方案集成，以实现无缝的数据存储和访问。
- en: By leveraging these managed services, organizations can focus on data processing
    and analysis without the overhead of managing the underlying infrastructure. Developers
    and architects can utilize their existing skills and knowledge while benefiting
    from the scalability, flexibility, and cost-effectiveness of cloud-based big data
    solutions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些托管服务，组织可以专注于数据处理和分析，而无需管理底层基础设施的开销。开发人员和架构师可以利用他们现有的技能和知识，同时从基于云的大数据解决方案的可扩展性、灵活性和成本效益中受益。
- en: Now that we’ve covered the fundamentals, let’s see how Java and big data technologies
    work together to solve real-world problems.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了基础知识，让我们看看 Java 和大数据技术是如何一起解决现实世界问题的。
- en: Real-world Java and big data in action
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界的 Java 和大数据应用
- en: Delving beyond the theoretical, we’ll delve into three practical use cases that
    showcase the power of this combination.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在理论之外，我们将深入探讨三个实际用例，展示这种组合的强大功能。
- en: Use case 1 – log analysis with Spark
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 1 – 使用 Spark 进行日志分析
- en: 'Let’s consider a scenario where an e-commerce company wants to analyze its
    web server logs to extract valuable insights. The logs contain information about
    user requests, including timestamps, requested URLs, and response status codes.
    The goal is to process the logs, extract relevant information, and derive meaningful
    metrics. We will explore log analysis using Spark’s DataFrame API, demonstrating
    efficient data filtering, aggregation, and joining techniques. By leveraging DataFrames,
    we can easily parse, transform, and summarize log data from CSV files:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，一个电子商务公司想要分析其网络服务器的日志以提取有价值的信息。日志包含有关用户请求的信息，包括时间戳、请求的 URL 和响应状态码。目标是处理日志，提取相关信息，并得出有意义的指标。我们将探索使用
    Spark 的 DataFrame API 进行日志分析，展示高效的数据过滤、聚合和连接技术。通过利用 DataFrame，我们可以轻松解析、转换和总结来自
    CSV 文件的日志数据：
- en: '[PRE19]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This Spark code snippet is designed for log analysis, using Apache Spark’s
    *DataFrame API*, an effective tool for handling structured data processing. The
    code performs several operations on server log data, which is assumed to be stored
    in the CSV format:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 此 Spark 代码片段旨在进行日志分析，使用 Apache Spark 的 *DataFrame API*，这是一个有效的处理结构化数据处理工具。代码对服务器日志数据执行了多项操作，假设这些数据以
    CSV 格式存储：
- en: '`spark.read()` function is used to load log data from a CSV file into a DataFrame,
    with `header` set to `true` to use the first line of the file as column names,
    and `inferSchema` set to `true` to automatically deduce the data types of each
    column.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.read()` 函数用于将日志数据从 CSV 文件加载到 DataFrame 中，其中 `header` 设置为 `true` 以使用文件的第一行作为列名，`inferSchema`
    设置为 `true` 以自动推断每列的数据类型。'
- en: '`400` or higher, typically indicating client errors (such as `404 Not Found)`
    or server errors (such as `500 Internal` `Server Error`).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`400` 或更高，通常表示客户端错误（例如 `404 Not Found`）或服务器错误（例如 `500 Internal Server Error`）。'
- en: '**Aggregation**: The filtered logs are grouped by URL, and the occurrences
    of each URL are counted. This step helps to identify which URLs are frequently
    associated with errors.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：将过滤后的日志按 URL 分组，并计算每个 URL 的出现次数。这一步有助于识别哪些 URL 经常与错误相关联。'
- en: '**Average calculation**: A separate aggregation calculates the average response
    time for each URL across all logs, not just those with errors. This provides insights
    into the performance characteristics of each endpoint.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均计算**：一个单独的聚合计算所有日志中每个 URL 的平均响应时间，而不仅仅是错误日志。这提供了对每个端点的性能特征的洞察。'
- en: '**Join operation**: The URL counts from the error logs, and the average response
    times are joined on the URL field, merging the error frequency with performance
    metrics into a single dataset.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接操作**：将错误日志中的 URL 计数和平均响应时间连接到 URL 字段，将错误频率与性能指标合并到单个数据集中。'
- en: '**Result display**: Finally, the combined results are displayed, showing each
    URL along with its count of error occurrences and average response time. This
    output is useful for diagnosing issues and optimizing server performance.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果显示**：最后，显示合并的结果，显示每个 URL 及其错误发生次数和平均响应时间。此输出对于诊断问题和优化服务器性能很有用。'
- en: This example demonstrates how to use Spark to efficiently process and analyze
    large datasets, leveraging its capabilities for filtering, aggregation, and joining
    data to extract meaningful insights from web server logs.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了如何使用 Spark 高效地处理和分析大数据集，利用其过滤、聚合和连接数据的能力，从网络服务器日志中提取有意义的见解。
- en: Use case 2 – a recommendation engine
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 2 – 推荐引擎
- en: 'This code snippet demonstrates how to build and evaluate a recommendation system
    using Apache Spark’s **Machine Learning Library** (**MLlib**). Specifically, it
    utilizes the **Alternating Least Squares** (**ALS**) algorithm, which is popular
    for collaborative filtering tasks such as movie recommendations:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段演示了如何使用 Apache Spark 的 **机器学习库**（**MLlib**）构建和评估推荐系统。具体来说，它利用了 **交替最小二乘法**（**ALS**）算法，该算法在协同过滤任务（如电影推荐）中很受欢迎：
- en: '[PRE20]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This code reads the rating data from a CSV file into a DataFrame called `ratings`.
    The `spark.read()` method is used to read the data, and the `option` method is
    used to specify the following options:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码从 CSV 文件读取评分数据到名为 `ratings` 的 DataFrame 中。使用 `spark.read()` 方法读取数据，并使用 `option`
    方法指定以下选项：
- en: '`"header", "true"`: Indicates that the first line of the CSV file contains
    the column names'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"header", "true"`：表示 CSV 文件的第一行包含列名。'
- en: '`"inferSchema", "true"`: Instructs Spark to infer the data types of the columns
    based on the data'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"inferSchema", "true"`：指示 Spark 根据数据推断列的数据类型'
- en: 'The `csv()` method specifies the path to the CSV file containing the rating
    data:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`csv()` 方法指定包含评分数据的 CSV 文件的路径：'
- en: '[PRE21]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This code splits the ratings DataFrame into training and testing datasets,
    using the `randomSplit()` method. The new `double[]{0.8, 0.2}` argument specifies
    the proportions of the split, with 80% of the data going into the training set
    and 20% into the testing set. The resulting datasets are stored in the `trainingData`
    and `testingData` variables, respectively:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将评分 DataFrame 分割为训练集和测试集，使用 `randomSplit()` 方法。新的 `double[]{0.8, 0.2}` 参数指定分割的比例，其中
    80% 的数据进入训练集，20% 进入测试集。生成的数据集分别存储在 `trainingData` 和 `testingData` 变量中：
- en: '[PRE22]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This code creates an instance of the ALS model using the `ALS` class. The model
    is configured with the following parameters:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用 `ALS` 类创建 ALS 模型的一个实例。模型配置了以下参数：
- en: '`setMaxIter(10)`: Sets the maximum number of iterations to 10'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setMaxIter(10)`: 将最大迭代次数设置为 10'
- en: '`setRegParam(0.01)`: Sets the regularization parameter to 0.01'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setRegParam(0.01)`: 将正则化参数设置为 0.01'
- en: '`setUserCol("userId")`: Specifies the column name for user IDs'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setUserCol("userId")`: 指定用户 ID 的列名'
- en: '`setItemCol("itemId")`: Specifies the column name for item IDs'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setItemCol("itemId")`: 指定项目 ID 的列名'
- en: '`setRatingCol("rating")`: Specifies the column name for ratings'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setRatingCol("rating")`: 指定评分的列名'
- en: '[PRE23]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding code trains the ALS model using the `fit()` method, passing the
    `trainingData DataFrame` as the input. The trained model is stored in the `model`
    variable.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用 `fit()` 方法训练 ALS 模型，将 `trainingData DataFrame` 作为输入。训练好的模型存储在 `model`
    变量中。
- en: '[PRE24]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code generates predictions on the `testingData` DataFrame using
    the trained model. The `transform()` method applies the model to the testing data
    and returns a new DataFrame, called `predictions`, which contains the predicted
    ratings.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用训练好的模型在 `testingData` DataFrame 上生成预测。`transform()` 方法将模型应用于测试数据，并返回一个新的
    DataFrame，称为 `predictions`，其中包含预测评分。
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The preceding code evaluates the performance of the trained model using the
    `RegressionEvaluator` class. `evaluator` is configured to use the `"rating"` column
    and the predicted ratings stored in the `"prediction"` column. The `evaluate()`
    method calculates the RMSE on the `predictions` DataFrame, and the result is printed
    to the console.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用 `RegressionEvaluator` 类评估训练模型的性能。`evaluator` 配置为使用 `"rating"` 列和存储在
    `"prediction"` 列中的预测评分。`evaluate()` 方法在 `predictions` DataFrame 上计算 RMSE，并将结果打印到控制台。
- en: '[PRE26]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The preceding code generates the top 10 movie recommendations for each user
    using the trained model. The `recommendForAllUsers()` method is called with an
    argument of 10, specifying the number of recommendations to generate per user.
    The resulting recommendations are stored in the `userRecs` DataFrame, and the
    `show` method is used to display the recommendations.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用训练好的模型为每个用户生成前 10 部电影的推荐。通过将 10 作为参数调用 `recommendForAllUsers()` 方法，指定每个用户要生成的推荐数量。生成的推荐存储在
    `userRecs` DataFrame 中，并使用 `show` 方法显示推荐。
- en: This example is typical for scenarios where businesses need to recommend products
    or content to users based on their past interactions. It demonstrates the process
    of building a movie recommendation engine using Apache Spark’s DataFrame API and
    the ALS algorithm. The ALS algorithm is particularly well-suited for this purpose,
    due to its scalability and effectiveness in handling sparse datasets that are
    typical of user-item interactions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例适用于企业需要根据用户过去的互动推荐产品或内容的情况。它展示了使用 Apache Spark 的 DataFrame API 和 ALS 算法构建电影推荐引擎的过程。ALS
    算法特别适合此目的，因为它具有可扩展性和处理用户-项目交互中典型的稀疏数据集的有效性。
- en: Use case 3 – real-time fraud detection
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用例 3 - 实时欺诈检测
- en: Fraud detection involves analyzing transactions, user behavior, and other relevant
    data to identify anomalies that could signify fraud. The complexity and evolving
    nature of fraudulent activities necessitates the use of advanced analytics and
    machine learning. Our objective is to monitor transactions in real-time and flag
    those with a high likelihood of being fraudulent, based on historical data and
    `patterns.models`, and massive data processing capabilities.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测涉及分析交易、用户行为和其他相关数据，以识别可能表示欺诈的异常。欺诈活动的复杂性和演变性质需要使用高级分析和机器学习。我们的目标是实时监控交易，并根据历史数据和
    `patterns.models` 以及大规模数据处理能力，标记那些有很高欺诈可能性的交易。
- en: 'This code demonstrates a real-time fraud detection system using Apache Spark
    Streaming. It reads transaction data from a `.CSV` file, applies a pre-trained
    machine learning model to predict the likelihood of fraud for each transaction,
    and outputs the prediction results to the console. Here is a sample code snippet:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码演示了使用 Apache Spark Streaming 的实时欺诈检测系统。它从 `.CSV` 文件中读取交易数据，将预训练的机器学习模型应用于预测每个交易的欺诈可能性，并将预测结果输出到控制台。以下是一个示例代码片段：
- en: '[PRE27]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the code explanation:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码说明：
- en: The `main()` method is defined, which is the entry point of the application.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了 `main()` 方法，它是应用程序的入口点。
- en: A `SparkSession` is created with the application name `FraudDetectionStreaming`.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建了一个名为 `FraudDetectionStreaming` 的 `SparkSession`。
- en: A pre-trained machine learning model is loaded, using `PipelineModel.load()`.
    The path to the trained model is specified as `"path/to/trained/model"`.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`PipelineModel.load()`加载了一个预训练的机器学习模型。指定训练模型的路径为`"path/to/trained/model"`。
- en: The schema for the transaction data is defined using `StructType`. It includes
    fields such as `transactionId`, `amount`, `accountNumber`, `transaction Time`,
    and `merchantId`.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`StructType`定义了交易数据的模式。它包括`transactionId`、`amount`、`accountNumber`、`transaction
    Time`和`merchantId`等字段。
- en: A streaming `DataFrame transactionsStream` is created, using `spark.readStream()`
    to read data from a CSV file. The file path is specified as `"path/to/transaction/data"`.
    The header option is set to `"true"` to indicate that the CSV file has a header
    row, and the schema is provided using the `schema()` method.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`spark.readStream()`从CSV文件读取数据创建了一个流式`DataFrame transactionsStream`。文件路径指定为`"path/to/transaction/data"`。将标题选项设置为`"true"`以指示CSV文件包含标题行，并使用`schema()`方法提供模式。
- en: The pre-trained model is applied to `transactionsStream` using `model.transform()`,
    resulting in a new `DataFrame predictionStream` that includes the predicted fraud
    probabilities.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型被应用于`transactionsStream`，通过`model.transform()`操作，生成一个新的包含预测欺诈概率的`DataFrame
    predictionStream`。
- en: The relevant columns are selected from `predictionStream` using `select()`,
    including `transactionId`, `amount`, `accountNumber`, `transactionTime`, `merchantId`,
    `prediction`, and `probability`.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`select()`从`predictionStream`中选择相关列，包括`transactionId`、`amount`、`accountNumber`、`transactionTime`、`merchantId`、`prediction`和`probability`。
- en: A `StreamingQuery` is created, using `predictionStream.writeStream()` to write
    the prediction results to the console. The output mode is set to `"append"`, and
    the format is set to `"console"`.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`predictionStream.writeStream()`创建了一个`StreamingQuery`，将预测结果写入控制台。输出模式设置为`"append"`，格式设置为`"console"`。
- en: The streaming query starts using `query.start()`, and the application waits
    for the query to terminate using `query.awaitTermination()`.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式查询从`query.start()`开始，应用程序等待查询终止使用`query.awaitTermination()`。
- en: This code demonstrates the basic structure of real-time fraud detection using
    Spark Streaming. You can further enhance it by incorporating additional data preprocessing,
    handling more complex schemas, and integrating with other systems to alert or
    take actions, based on the detected fraudulent transactions.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码演示了使用Spark Streaming进行实时欺诈检测的基本结构。您可以通过添加额外的数据预处理、处理更复杂的模式以及与其他系统集成以根据检测到的欺诈交易发出警报或采取行动来进一步改进它。
- en: Having explored the potential of Java and big data technologies in real-world
    scenarios, such as log analysis, recommendation engines, and fraud detection,
    this chapter showcased the versatility and power of this combination to tackle
    a wide range of data-driven challenges.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了Java和大数据技术在现实场景中的潜力之后，例如日志分析、推荐引擎和欺诈检测，本章展示了这种组合的灵活性和强大功能，以应对各种数据驱动挑战。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we embarked on an exhilarating journey, exploring the realm
    of big data and how Java’s prowess in concurrency and parallel processing empowers
    us to conquer its challenges. We began by unraveling the essence of big data,
    characterized by its immense volume, rapid velocity, and diverse variety – a domain
    where traditional tools often fall short.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们踏上了一段令人兴奋的旅程，探索大数据领域以及Java在并发和并行处理方面的强大能力如何帮助我们克服其挑战。我们首先揭示了大数据的本质，其特征是庞大的数据量、快速的速度和多样的种类——一个传统工具往往难以胜任的领域。
- en: As we ventured further, we discovered the power of Apache Hadoop and Apache
    Spark, two formidable allies in the world of distributed computing. These frameworks
    seamlessly integrate with Java, enabling us to harness the true potential of big
    data. We delved into the intricacies of this integration, learning how Java’s
    concurrency features optimize big data workloads, resulting in unparalleled scalability
    and efficiency.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进一步探索，我们发现Apache Hadoop和Apache Spark在分布式计算领域的强大力量，这两者是强大的盟友。这些框架与Java无缝集成，使我们能够充分利用大数据的潜力。我们深入研究了这种集成的复杂性，学习了Java的并发特性如何优化大数据工作负载，从而实现无与伦比的可扩展性和效率。
- en: Throughout our journey, we placed a strong emphasis on the DataFrame API, which
    has become the de facto standard for data processing in Spark. We explored how
    DataFrames provide a more efficient, optimized, and user-friendly way to work
    with structured and semi-structured data compared to RDDs. We covered essential
    concepts such as transformatieons, actions, and SQL-like querying using DataFrames,
    enabling us to perform complex data manipulations and aggregations with ease.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的旅程中，我们高度重视DataFrame API，它已成为Spark中数据处理的事实标准。我们探讨了DataFrame如何提供比RDD更高效、优化和用户友好的方式来处理结构化和半结构化数据。我们涵盖了诸如转换、操作和SQL-like查询等基本概念，使我们能够轻松地进行复杂的数据操作和聚合。
- en: To ensure a comprehensive understanding of Spark’s capabilities, we delved into
    advanced topics such as the Catalyst optimizer, execution DAG, caching, and persistence
    techniques. We also discussed strategies to handle data skew and minimize data
    shuffling, which are critical for optimizing Spark’s performance in real-world
    scenarios.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保对Spark功能的全面理解，我们深入探讨了高级主题，如Catalyst优化器、执行DAG、缓存和持久化技术。我们还讨论了处理数据倾斜和最小化数据洗牌的策略，这对于优化Spark在实际场景中的性能至关重要。
- en: Our adventure led us through three captivating real-world scenarios – log analysis,
    recommendation systems, and fraud detection. In each of these scenarios, we showcased
    the immense potential of Java and big data technologies, leveraging the DataFrame
    API to solve complex data processing tasks efficiently.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的冒险之旅带我们穿越了三个引人入胜的真实世界场景——日志分析、推荐系统和欺诈检测。在这些场景中，我们展示了Java和大数据技术的巨大潜力，利用DataFrame
    API高效地解决复杂的数据处理任务。
- en: Armed with the knowledge and tools acquired in this chapter, we stand ready
    to build robust and scalable big data applications using Java. We have gained
    a deep understanding of the core characteristics of big data, the limitations
    of traditional data processing approaches, and how Java’s concurrency features
    and big data frameworks such as Hadoop and Spark enable us to overcome these challenges.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们获得了知识和工具，我们准备好使用Java构建健壮和可扩展的大数据应用程序。我们深入理解了大数据的核心特征、传统数据处理方法的局限性，以及Java的并发特性和大数据框架（如Hadoop和Spark）如何帮助我们克服这些挑战。
- en: We are now equipped with the skills and confidence to tackle the ever-expanding
    world of big data. Our journey will continue in the next chapter, as we explore
    how Java’s concurrency features can be harnessed for efficient and powerful machine
    learning tasks.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了处理不断扩大的大数据世界的技能和信心。我们的旅程将在下一章继续，我们将探讨如何利用Java的并发特性来高效且强大地进行机器学习任务。
- en: Questions
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the core characteristics of big data?
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大数据的核心特征是什么？
- en: Speed, accuracy, and format
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 速度、准确性和格式
- en: Volume, velocity, and variety
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 体积、速度和多样性
- en: Complexity, consistency, and currency
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复杂性、一致性和时效性
- en: Density, diversity, and durability
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 密度、多样性和持久性
- en: Which component of Hadoop is primarily designed for storage?
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hadoop的哪个组件主要是为存储而设计的？
- en: '**Hadoop Distributed File** **System** (**HDFS**)'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**（**HDFS**）'
- en: '**Yet Another Resource** **Negotiator** (**YARN**)'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**另一种资源** **协商者**（**YARN**）'
- en: MapReduce
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: MapReduce
- en: HBase
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: HBase
- en: What is the primary advantage of using Spark over Hadoop for certain big data
    tasks?
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Spark而不是Hadoop进行某些大数据任务的主要优势是什么？
- en: Spark is more cost-effective than Hadoop.
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark比Hadoop更具成本效益。
- en: Spark provides better data security than Hadoop.
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark提供比Hadoop更好的数据安全性。
- en: Spark offers faster in-memory data processing capabilities.
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark提供了更快的内存数据处理能力。
- en: Spark supports a wider variety of data formats than Hadoop.
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark支持比Hadoop更广泛的数据格式。
- en: Which of the following is NOT a true statement about Apache Spark?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪项关于Apache Spark的说法是不正确的？
- en: Spark can only process structured data.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark只能处理结构化数据。
- en: Spark allows for in-memory data processing.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark允许内存中数据处理。
- en: Spark supports real-time stream processing.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark支持实时流处理。
- en: Spark uses **Resilient Distributed Datasets** (**RDDs**) for fault-tolerant
    storage.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark使用**弹性分布式数据集**（**RDDs**）进行容错存储。
- en: What is a key benefit of applying concurrency to big data tasks?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用并发到大数据任务中的关键好处是什么？
- en: It simplifies the code base for big data applications.
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它简化了大数据应用程序的代码库。
- en: It ensures data processing tasks are executed sequentially.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确保数据处理任务按顺序执行。
- en: It helps to break down large datasets into smaller, manageable chunks for processing.
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它有助于将大型数据集分解成更小、更易于管理的块进行处理。
- en: It reduces the storage requirements for big data.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它降低了大数据的存储需求。
