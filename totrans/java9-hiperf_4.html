<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 4. Microservices"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 4. Microservices</h1></div></div></div><p class="calibre11">As long as we kept talking about the designing, implementation, and tuning of one process, we were able to keep illustrating it with vivid images (albeit in our imagination only) of pyramid building. Multiple thread management, based on the democratic principle of equality between thread pool members, had also a sense of centralized planning and supervision. Different priorities were assigned to threads programmatically, hardcoded (for most cases) after thoughtful consideration by the programmer in accordance with the expected load, and adjusted after monitoring. The upper limits of the available resources were fixed, although they could be increased after, again, a relatively big centralized decision.</p><p class="calibre11">Such systems had great success and still constitute the majority of the web applications currently deployed to production. Many of them are monoliths, sealed inside a single <code class="literal">.ear</code> or <code class="literal">.war</code> file. This works fine for relatively small applications and a corresponding team size that supports them. They are easy (if the code is well structured) to maintain, build, and if the production load is not very high, they can be easily deployed. If the business does not grow or has little impact on the company's internet presence, they continue to do the job and will do so probably for the foreseeable future. Many service providers are eager to host such websites by charging a small fee and relieving the website owner of the technical worries of production maintenance not directly related to the business. But that is not the case for everybody.</p><p class="calibre11">The higher the load, the more difficult and expensive the scaling becomes unless the code and the overall architecture is restructured in order to become more flexible and resilient to the growing load. This lesson describes the solution many leaders of the industry have adopted while addressing the issue and the motivation behind it. </p><p class="calibre11">The particular aspects of the microservices we are going to discuss in this lesson include the following:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The motivation for the microservices rising</li><li class="listitem">The frameworks that were developed recently in support of microservices</li><li class="listitem">The process of microservices development with practical examples, including the considerations and decision-making process during microservices building</li><li class="listitem">Pros and cons of the three main deployment methods such as container-less, self-contained, and in-container</li></ul></div><div class="calibre2" title="Why Microservices?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Why Microservices?</h1></div></div></div><p class="calibre11">Some businesses have a higher demand for the deployment plan because of the need to keep up with the bigger volume of traffic. The natural answer to this challenge would be and was to add servers with the same <code class="literal">.ear</code> or <code class="literal">.war</code> file deployed and join all the servers into a cluster. So, one failed server could be automatically replaced with another one from the cluster, and the site user would never experience disconnect of the service. The database that backed all the clustered servers could be clustered too. A connection to each of the clusters went through a load balancer, making sure that none of the cluster members worked more than the others.</p><p class="calibre11">The web server and database clustering help but only to a degree, because as the code base grows, its structure can create one or several bottlenecks unless such and similar issues are addressed with a scalable design. One of the ways to do it is to split the code into tiers: front end (or web tier), middle tier (or app tier) and back end (or backend tier). Then, again, each tier can be deployed independently (if the protocol between tiers has not changed) and in its own cluster of servers, as each tier can grow horizontally as needed independently of other tiers. Such a solution provides more flexibility for scaling up, but makes the deployment plan more complex, especially if the new code introduces breaking changes. One of the approaches is to create a second cluster that will host a new code, then take the servers one by one from the old cluster, deploy the new code, and put them in the new cluster. The new cluster would be turned on as soon as at least one server in each tier has the new code. This approach worked fine for the web and app tiers but was more complex for the backend, which once in a while required data migration and similar joyful exercises. Add to it unexpected outages in the middle of the deployment caused by human errors, defects in the code, pure accidents, or some combination of all the earlier mentioned (one time, for example, an electric power cable was cut by an excavator in the nearby construction site), and it is easy to understand why very few people love a deployment of a major release to production.</p><p class="calibre11">Programmers, being by nature problem solvers, tried to prevent the earlier scenario as best as they could by writing defensive code, deprecating instead of changing, testing, and so on. One of the approaches was to break the application into more independently deployable parts with the hope of avoiding deploying everything at the same time. They called these independent units <span class="emphasis"><strong class="calibre13">services</strong></span>, and <span class="emphasis"><strong class="calibre13">Service-Oriented Architecture</strong></span> (<span class="emphasis"><strong class="calibre13">SOA</strong></span>) was born.</p><p class="calibre11">Unfortunately, in many companies, the natural growth of the code base was not adjusted to the new challenges in a timely manner. Like the frog that was eventually boiled in a slowly heated pot of water, they never had time to jump out of the hot spot by changing the design. It was always cheaper to add another feature to the blob of the existing functionality than redesign the whole app. Business metrics of the time-to-market and keeping the bottom line in the black always were and will remain the main criterion for the decision making, until the poorly structured source code eventually stops working, pulling down all the business transactions with it or, if the company is lucky, allows them to weather the storm and shows the importance of the investment in the redesign.</p><p class="calibre11">As a result of all that, some lucky companies remained in the business with their monolithic application still running as expected (maybe not for long, but who knows), some went out of business, some learned from their mistakes and progressed into the brave world of the new challenges, and others learned from their mistakes and designed their systems to be SOA upfront.</p><p class="calibre11">It is interesting to observe similar tendencies in the social sphere. Society moved from the strong centralized governments to more loosely coupled confederations of semi-independent states tied together by the mutually beneficial economic and cultural exchange.</p><p class="calibre11">Unfortunately, maintaining such a loose structure comes with a price. Each participant has to be more responsible in maintaining the contract (social, in the case of a society, and API, in the case of the software) not only formally but also in spirit. Otherwise, for example, the data flowing from a new version of one component, although correct by type, might be unacceptable to another component by value (too big or too small). Maintaining a cross-team understanding and overlapping of responsibility requires constant vigilance in keeping the culture alive and enlightening. Encouraging innovation and risk taking, which can lead to a business breakthrough, contradict the protecting tendencies for stability and risk aversion coming from the same business people.</p><p class="calibre11">Moving from monolithic single-team development to multiple teams and an independent components-based system requires an effort on all levels of the enterprise. What do you mean by <span class="emphasis"><strong class="calibre13">No more Quality Assurance Department</strong></span>? Who then will care about the professional growth of the testers? And what about the IT group? What do you mean by <span class="emphasis"><strong class="calibre13">The developers are going to support production</strong></span>? Such changes affect human lives and are not easy to implement. That's why SOA architecture is not just a software principle. It affects everybody in the company.</p><p class="calibre11">Meanwhile, the industry leaders, who have managed to grow beyond anything we could imagine just a decade ago, were forced to solve even more daunting problems and came back to the software community with their solutions. And that is where our analogy with the pyramid building does not work anymore. Because the new challenge is not just to build something so big that was never built before but also to do it quickly not in a matter of years, but in a few weeks and even days. And the result has to last not for a thousand years but has to be able to evolve constantly and be flexible enough to adapt to new, unexpected requirements in real time. If only one aspect of the functionality has changed, we should be able to redeploy only this one service. If the demand for any service grows, we should be able to scale only along this one service and release resources when the demand drops.</p><p class="calibre11">To avoid big deployments with all hands on deck and to come closer to the continuous deployment (which decreases time-to-market and is thus supported by business), the functionality continued to split into smaller chunks of services. In response to the demand, more sophisticated and robust cloud environments, deployment tools (including containers and container orchestration), and monitoring systems supported this move. The reactive streams, described in the previous lesson, started to develop even before the Reactive Manifesto came out and plugged a snag into the stack of modern frameworks.</p><p class="calibre11">Splitting an application into independent deployment units brought several not quite expected benefits that have increased the motivation for plowing ahead. The physical isolation of services allows more flexibility in choosing a programming language and platform of implementation. It helps not only to select technology that is the best for the job but also to hire people able to implement it, not being bound by a certain technological stack of the company. It also helped the recruiters to spread the net wider and use smaller cells for bringing in new talent, which is not a small advantage with a limited number of available specialists and the unlimited demand of the fast-growing data processing industry.</p><p class="calibre11">Also, such architecture enforced a discussion and explicit definition of the interfaces between smaller parts of the complex system, thus creating a solid foundation for further growth and tuning of the processing sophistication.</p><p class="calibre11">And that is how microservices came into the picture and were put to work by giants of traffic such as Netflix, Google, Twitter, eBay, Amazon, and Uber. Now, let's talk about the results of this effort and the lessons learned.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Building Microservices"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Building Microservices</h1></div></div></div><p class="calibre11">Before diving into the building process, let's revisit the characteristics a chunk of code has to possess in order to be qualified as a microservice. We will do it in no particular order:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The size of the source code of one microservice should be smaller to that of an SOA, and one development team should be able to support several of them.</li><li class="listitem">It has to be deployed independently of other services.</li><li class="listitem">Each has to have its own database (or schema or set of tables), although this statement is still under debate, especially in cases when several services modify the same data set or the inter-dependent data sets; if the same team owns all of the related services, it is easier to accomplish. Otherwise, there are several possible strategies we will discuss later.</li><li class="listitem">It has to be stateless and idempotent. If one instance of the service has failed, another should be able to accomplish what was expected from the service.</li><li class="listitem">It should provide a way to check its <span class="emphasis"><strong class="calibre13">health</strong></span>, meaning that the service is up and running and ready to do the job.</li></ul></div><p class="calibre11">Sharing resources has to be considered during the design, development, and, after deployment, monitored for validation of the assumptions. In the previous lesson, we talked about threads synchronization. You could see that this problem was not easy to solve, and we have presented several possible ways to do it. Similar approaches can be applied toward microservices. Although they are run in different processes, they can communicate to each other if need be, so they can coordinate and synchronize their actions.</p><p class="calibre11">Special care has to be taken during modification of the same persistent data whether shared across databases, schemas, or tables within the same schema. If an eventual consistency is acceptable (which is often the case for larger sets of data, used for statistical purposes, for example) then no special measures are necessary. However, the need for transactional integrity poses a more difficult problem.</p><p class="calibre11">One way to support a transaction across several microservices is to create a service that would play the role of a <span class="emphasis"><strong class="calibre13">Distributed Transaction Manager</strong></span> (<span class="emphasis"><strong class="calibre13">DTM</strong></span>). Other services that need coordination would pass to it the new modified values. The DTM service could keep the concurrently modified data temporarily in a database table and would move it into the main table(s) in one transaction after all the data is ready (and consistent).</p><p class="calibre11">If the time to access the data is an issue or you need to protect the database from an excessive number of concurrent connections, dedicating a database to some services may be an answer. Alternatively, if you would like to try another option, memory cache could be the way to go. Adding a service that provides access to the cache (and updates it as needed) increases isolation from the services that use it, but requires (sometimes difficult) synchronization between the peers that are managing the same cache too.</p><p class="calibre11">After considering all the options and possible solutions for data sharing, it is often helpful to revisit the idea of creating its own database (or schema) for each microservice. One may discover that the effort of the data isolation (and subsequent synchronization on the database level) does not look as daunting as before if compared with the effort to synchronize the data dynamically.</p><p class="calibre11">That said, let's look over the field of the frameworks for microservices implementation. One can definitely write the microservices from scratch, but before doing that, it is always worth looking at what is out there already, even if to find eventually that nothing fits your particular needs.</p><p class="calibre11">There are more than a dozen frameworks that are currently used for building microservices. Two most popular are Spring Boot (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://projects.spring.io/spring-boot/">https://projects.spring.io/spring-boot/</a>) and raw J2EE. The J2EE community founded the initiative MicroProfile (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://microprofile.io/">https://microprofile.io/</a>) with a declared goal of <span class="emphasis"><strong class="calibre13">Optimizing Enterprise Java</strong></span> for a microservices architecture. KumuluzEE (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://ee.kumuluz.com/">https://ee.kumuluz.com/</a>) is a lightweight open-source microservice framework coplined with MicroProfile.</p><p class="calibre11">The list of some other frameworks include the following (in alphabetical order):</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="emphasis"><strong class="calibre13">Akka</strong></span>: This is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://akka.io/">akka.io</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Bootique</strong></span>: This is a minimally opinionated framework for runnable Java apps (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://bootique.io">bootique.io</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Dropwizard</strong></span>: This is a Java framework for developing ops-friendly, high-performance, RESTful web services (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.dropwizard.io">www.dropwizard.io</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Jodd</strong></span>: This is a set of Java microframeworks, tools, and utilities, under 1.7 MB (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://jodd.org">jodd.org</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Lightbend Lagom</strong></span>: This is an opinionated microservice framework built on Akka and Play (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.lightbend.com">www.lightbend.com</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Ninja</strong></span>: This is a full stack web framework for Java (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.ninjaframework.org">www.ninjaframework.org</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Spotify Apollo</strong></span>: This is a set of Java libraries used at Spotify for writing microservices (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spotify.github.io/apollo">spotify.github.io/apollo</a>)</li><li class="listitem"><span class="emphasis"><strong class="calibre13">Vert.x</strong></span>: This is a toolkit for building reactive applications on the JVM (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://vertx.io">vertx.io</a>)</li></ul></div><p class="calibre11">All frameworks support HTTP/JSON communication between microservices; some of them also have an additional way to send messages. If not the latter, any lightweight messaging system can be used. We mentioned it here because, as you may recall, message-driven asynchronous processing is a foundation for elasticity, responsiveness, and resilience of a reactive system composed of microservices.</p><p class="calibre11">To demonstrate the process of microservices building, we will use Vert.x, an event-driven, non-blocking, lightweight, and polyglot toolkit (components can be written in Java, JavaScript, Groovy, Ruby, Scala, Kotlin, and Ceylon). It supports an asynchronous programming model and a distributed event bus that reaches even into in-browser JavaScript (thus allowing the creation of real-time web applications).</p><p class="calibre11">One starts using Vert.x by creating a <code class="literal">Verticle</code> class that implements the interface <code class="literal">io.vertx.core.Verticle</code>:</p><div class="calibre2"><pre class="programlisting">package io.vertx.core;
public interface Verticle {
  Vertx getVertx();
  void init(Vertx vertx, Context context);
  void start(Future&lt;Void&gt; future) throws Exception;
  void stop(Future&lt;Void&gt; future) throws Exception;
}</pre></div><p class="calibre11">The method names previously mentioned are self-explanatory. The method <code class="literal">getVertex()</code> provides access to the <code class="literal">Vertx</code> object the entry point into the Vert.x Core API. It provides access to the following functionality necessary for the microservices building:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Creating TCP and HTTP clients and servers</li><li class="listitem">Creating DNS clients</li><li class="listitem">Creating Datagram sockets</li><li class="listitem">Creating periodic services</li><li class="listitem">Providing access to the event bus and file system API</li><li class="listitem">Providing access to the shared data API</li><li class="listitem">Deploying and undeploying verticles</li></ul></div><p class="calibre11">Using this Vertx object, various verticles can be deployed, which talk to each other, receive an external request, and process and store data as any other Java application, thus forming a system of microservices. Using RxJava implementation from the package <code class="literal">io.vertx.rxjava</code>, we will show how one can create a reactive system of microservices.</p><p class="calibre11">A verticle is a building block in Vert.<code class="literal">x</code> world. It can easily be created by extending the <code class="literal">io.vertx.rxjava.core.AbstractVerticle</code> class:</p><div class="calibre2"><pre class="programlisting">package io.vertx.rxjava.core;
import io.vertx.core.Context;
import io.vertx.core.Vertx;
public class AbstractVerticle 
               extends io.vertx.core.AbstractVerticle {
  protected io.vertx.rxjava.core.Vertx vertx;
  public void init(Vertx vertx, Context context) {
     super.init(vertx, context);
     this.vertx = new io.vertx.rxjava.core.Vertx(vertx);
  }
}</pre></div><p class="calibre11">The earlier mentioned class, in turn, extends <code class="literal">io.vertx.core.AbstractVerticle</code>:</p><div class="calibre2"><pre class="programlisting">package io.vertx.core;
import io.vertx.core.json.JsonObject;
import java.util.List;
public abstract class AbstractVerticle 
                               implements Verticle {
    protected Vertx vertx;
    protected Context context;
    public Vertx getVertx() { return vertx; }
    public void init(Vertx vertx, Context context) {
        this.vertx = vertx;
        this.context = context;
    }
    public String deploymentID() {
        return context.deploymentID();
    }
    public JsonObject config() {
        return context.config();
    }
    public List&lt;String&gt; processArgs() {
        return context.processArgs();
    }
    public void start(Future&lt;Void&gt; startFuture) 
                                throws Exception {
        start();
        startFuture.complete();
    }
    public void stop(Future&lt;Void&gt; stopFuture) 
                                throws Exception {
        stop();
        stopFuture.complete();
    }
    public void start() throws Exception {}
    public void stop() throws Exception {}

}</pre></div><p class="calibre11">A verticle can be created by extending the class <code class="literal">io.vertx.core.AbstractVerticle</code>, too. However, we will write reactive microservices, so we will extend its rx-fied version, <code class="literal">io.vertx.rxjava.core.AbstractVerticle</code>.</p><p class="calibre11">To use Vert.x and run the provided example, all you need to do is to add the following dependencies:</p><div class="calibre2"><pre class="programlisting">&lt;dependency&gt;
    &lt;groupId&gt;io.vertx&lt;/groupId&gt;
    &lt;artifactId&gt;vertx-web&lt;/artifactId&gt;
    &lt;version&gt;${vertx.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;io.vertx&lt;/groupId&gt;
    &lt;artifactId&gt;vertx-rx-java&lt;/artifactId&gt;
    &lt;version&gt;${vertx.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p class="calibre11">Other Vert.x functionality can be added as needed by including other Maven dependencies.</p><p class="calibre11">What makes <code class="literal">Vert.x</code> <code class="literal">Verticle</code> reactive is the underlying implementation of an event loop (a thread) that receives an event and delivers it a <code class="literal">Handler</code> (we will show how to write the code for it). When a <code class="literal">Handler</code> gets the result, the event loop invokes the callback.</p><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="note03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre18">As you see, it is important not to write a code that blocks the event loop, thus the Vert.x golden rule: don't block the event loop.</p></div></div><p class="calibre11">If not blocked, the event loop works very quickly and delivers a huge number of events in a short period of time. This is called the reactor pattern (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Reactor_pattern">https://en.wikipedia.org/wiki/Reactor_pattern</a>). Such an event-driven non-blocking programming model is a very good fit for reactive microservices. For certain types of code that are blocking by nature (JDBC calls and long computations are good examples) a worker verticle can be executed asynchronously (not by the event loop, but by a separate thread using the method <code class="literal">vertx.executeBlocking()</code>), which keeps the golden rule intact.</p><p class="calibre11">Let's look at a few examples. Here is a <code class="literal">Verticle</code> class that works as an HTTP server:</p><div class="calibre2"><pre class="programlisting">import io.vertx.rxjava.core.http.HttpServer;
import io.vertx.rxjava.core.AbstractVerticle;

public class Server extends AbstractVerticle{
  private int port;
  public Server(int port) {
    this.port = port;
  }
  public void start() throws Exception {
    HttpServer server = vertx.createHttpServer();
    server.requestStream().toObservable()
       .subscribe(request -&gt; request.response()
       .end("Hello from " + 
          Thread.currentThread().getName() + 
                       " on port " + port + "!\n\n")
       );
    server.rxListen(port).subscribe();
    System.out.println(Thread.currentThread().getName()
               + " is waiting on port " + port + "...");
  }
}</pre></div><p class="calibre11">In the previous code, the server is created, and the stream of data from a possible request is wrapped into an <code class="literal">Observable</code>. We then subscribed to the data coming from the <code class="literal">Observable</code> and passed in a function (a request handler) that will process the request and generate a necessary response. We also told the server which port to listen. Using this <code class="literal">Verticle</code>, we can deploy several instances of an HTTP server listening on different ports. Here is an example:</p><div class="calibre2"><pre class="programlisting">import io.vertx.rxjava.core.RxHelper;
import static io.vertx.rxjava.core.Vertx.vertx;
public class Demo01Microservices {
  public static void main(String... args) {
    RxHelper.deployVerticle(vertx(), new Server(8082));
    RxHelper.deployVerticle(vertx(), new Server(8083));
  }
}</pre></div><p class="calibre11">If we run this application, the output would be as follows:</p><div class="mediaobject"><img src="Images/04_01.jpg" alt="Building Microservices" class="calibre83"/></div><p class="calibre11">As you can see, the same thread is listening on both ports. If we now place a request to each of the running servers, we will get the response we have hardcoded:</p><div class="mediaobject"><img src="Images/04_02.jpg" alt="Building Microservices" class="calibre84"/></div><p class="calibre11">We ran our examples from the <code class="literal">main()</code> method. A plugin <code class="literal">maven-shade-plugin</code> allows you to specify which verticle you would like to be the starting point of your application. Here is an example from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://vertx.io/blog/my-first-vert-x-3-application">http://vertx.io/blog/my-first-vert-x-3-application</a>:</p><div class="calibre2"><pre class="programlisting">&lt;plugin&gt;
  &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
  &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;
  &lt;version&gt;2.3&lt;/version&gt;
  &lt;executions&gt;
    &lt;execution&gt;
      &lt;phase&gt;package&lt;/phase&gt;
      &lt;goals&gt;
        &lt;goal&gt;shade&lt;/goal&gt;
      &lt;/goals&gt;
      &lt;configuration&gt;
        &lt;transformers&gt;
          &lt;transformer
            implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt;
            &lt;manifestEntries&gt;
              &lt;Main-Class&gt;io.vertx.core.Starter&lt;/Main-Class&gt;
              &lt;Main-Verticle&gt;io.vertx.blog.first.MyFirstVerticle&lt;/Main-Verticle&gt;
            &lt;/manifestEntries&gt;
          &lt;/transformer&gt;
        &lt;/transformers&gt;
        &lt;artifactSet/&gt;
        &lt;outputFile&gt;${project.build.directory}/${project.artifactId}-${project.version}-fat.jar&lt;/outputFile&gt;
      &lt;/configuration&gt;
    &lt;/execution&gt;
  &lt;/executions&gt;
&lt;/plugin&gt;</pre></div><p class="calibre11">Now, run the following command:</p><div class="calibre2"><pre class="programlisting">mvn package
</pre></div><p class="calibre11">It will generate a specified JAR file (called <code class="literal">target/my-first-app-1.0-SNAPSHOT-fat.jar</code>, in this example). It is called <code class="literal">fat</code> because it contains all the necessary dependencies. This file will also contain <code class="literal">MANIFEST.MF</code> with the following entries in it:</p><div class="calibre2"><pre class="programlisting">Main-Class: io.vertx.core.Starter
Main-Verticle: io.vertx.blog.first.MyFirstVerticle</pre></div><p class="calibre11">You can use any verticle instead of <code class="literal">io.vertx.blog.first.MyFirstVerticle</code>, used in this example, but <code class="literal">io.vertx.core.Starter</code> has to be there because that is the name of the <code class="literal">Vert.x</code> class that knows how to read the manifest and execute the method <code class="literal">start()</code> of the specified verticle. Now, you can run the following command:</p><div class="calibre2"><pre class="programlisting">java -jar target/my-first-app-1.0-SNAPSHOT-fat.jar
</pre></div><p class="calibre11">This command will execute the <code class="literal">start()</code> method of the <code class="literal">MyFirstVerticle</code> class the same way the <code class="literal">main()</code> method is executed in our example, which we will continue to use for the simplicity of demonstration.</p><p class="calibre11">To compliment the HTTP server, we can create an HTTP client too. However, first, we will modify the method <code class="literal">start()</code> in the <code class="literal">server</code> verticle to accept the parameter <code class="literal">name</code>:</p><div class="calibre2"><pre class="programlisting">public void start() throws Exception {
    HttpServer server = vertx.createHttpServer();
    server.requestStream().toObservable()
       .subscribe(request -&gt; request.response()
       .end("Hi, " + request.getParam("name") + 
             "! Hello from " + 
             Thread.currentThread().getName() + 
                       " on port " + port + "!\n\n")
       );
    server.rxListen(port).subscribe();
    System.out.println(Thread.currentThread().getName()
               + " is waiting on port " + port + "...");
}</pre></div><p class="calibre11">Now, we can create an HTTP <code class="literal">client</code> verticle that sends a request and prints out the response every second for 3 seconds, then stops:</p><div class="calibre2"><pre class="programlisting">import io.vertx.rxjava.core.AbstractVerticle;
import io.vertx.rxjava.core.http.HttpClient;
import java.time.LocalTime;
import java.time.temporal.ChronoUnit;

public class Client extends AbstractVerticle {
  private int port;
  public Client(int port) {
    this.port = port;
  }
  public void start() throws Exception {
    HttpClient client = vertx.createHttpClient();
    LocalTime start = LocalTime.now();
    vertx.setPeriodic(1000, v -&gt; {
       client.getNow(port, "localhost", "?name=Nick",
         r -&gt; r.bodyHandler(System.out::println));
         if(ChronoUnit.SECONDS.between(start, 
                             LocalTime.now()) &gt; 3 ){
           vertx.undeploy(deploymentID());
       }
    });
  }
}</pre></div><p class="calibre11">Let's assume we deploy both verticles as follows:</p><div class="calibre2"><pre class="programlisting">RxHelper.deployVerticle(vertx(), new Server2(8082));
RxHelper.deployVerticle(vertx(), new Client(8082));</pre></div><p class="calibre11">The output will be as follows:</p><div class="mediaobject"><img src="Images/04_03.jpg" alt="Building Microservices" class="calibre85"/></div><p class="calibre11">In this last example, we demonstrated how to create an HTTP client and periodic service. Now, let's add more functionality to our system. For example, let's add another verticle that will interact with the database and use it via the HTTP server we have already created.</p><p class="calibre11">First, we need to add this dependency:</p><div class="calibre2"><pre class="programlisting">&lt;dependency&gt;
    &lt;groupId&gt;io.vertx&lt;/groupId&gt;
    &lt;artifactId&gt;vertx-jdbc-client&lt;/artifactId&gt;
    &lt;version&gt;${vertx.version}&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p class="calibre11">The newly added JAR file allows us to create an in-memory database and a handler to access it:</p><div class="calibre2"><pre class="programlisting">public class DbHandler {
  private JDBCClient dbClient;
  private static String SQL_CREATE_WHO_CALLED = 
    "CREATE TABLE IF NOT EXISTS " +
          "who_called ( name VARCHAR(10), " +
          "create_ts TIMESTAMP(6) DEFAULT now() )";
  private static String SQL_CREATE_PROCESSED = 
    "CREATE TABLE IF NOT EXISTS " +
         "processed ( name VARCHAR(10), " +
         "length INTEGER, " +
         "create_ts TIMESTAMP(6) DEFAULT now() )";

  public DbHandler(Vertx vertx){
    JsonObject config = new JsonObject()
      .put("driver_class", "org.hsqldb.jdbcDriver")
      .put("url", "jdbc:hsqldb:mem:test?shutdown=true");
    dbClient = JDBCClient.createShared(vertx, config);
    dbClient.rxGetConnection()
      .flatMap(conn -&gt; 
                 conn.rxUpdate(SQL_CREATE_WHO_CALLED)
                       .doAfterTerminate(conn::close) )
      .subscribe(r -&gt; 
        System.out.println("Table who_called created"),
                           Throwable::printStackTrace);
    dbClient.rxGetConnection()
      .flatMap(conn -&gt; 
                 conn.rxUpdate(SQL_CREATE_PROCESSED)
                      .doAfterTerminate(conn::close) )
      .subscribe(r -&gt; 
        System.out.println("Table processed created"),
                          Throwable::printStackTrace);

  }
}</pre></div><p class="calibre11">Those familiar with RxJava can see that Vert.x code closely follows the style and naming convention of RxJava. Nevertheless, we encourage you to go through Vert.x documentation, because it has a very rich API that covers many more cases than just demonstrated. In the previous code, the operation <code class="literal">flatMap()</code> receives the function that runs the script and then closes the connection. The operation <code class="literal">doAfterTerminate()</code> in this case acts as if it was placed inside a finally block in a traditional code and closes the connection either in case of success or if an exception is generated. The <code class="literal">subscribe()</code> method has several overloaded versions. For our code, we have selected the one that takes two functions one is going to be executed in the case of success (we print a message about the table being created) and another in the case of an exception (we just print the stack trace then).</p><p class="calibre11">To use the created database, we can add to <code class="literal">DbHandler</code> methods <code class="literal">insert()</code>, <code class="literal">process()</code>, and <code class="literal">readProcessed()</code> that will allow us to demonstrate how to build a reactive system. The code for the method <code class="literal">insert()</code> can look like this:</p><div class="calibre2"><pre class="programlisting">private static String SQL_INSERT_WHO_CALLED = 
             "INSERT INTO who_called(name) VALUES (?)";
public void insert(String name, Action1&lt;UpdateResult&gt; 
                onSuccess, Action1&lt;Throwable&gt; onError){
  printAction("inserts " + name);
  dbClient.rxGetConnection()
    .flatMap(conn -&gt; 
        conn.rxUpdateWithParams(SQL_INSERT_WHO_CALLED, 
                            new JsonArray().add(name))
                       .doAfterTerminate(conn::close) )
    .subscribe(onSuccess, onError);
}</pre></div><p class="calibre11">The <code class="literal">insert()</code> method, as well as other methods we are going to write, takes full advantage of Java functional interfaces. It creates a record in the table <code class="literal">who_called</code> (using the passed in parameter <code class="literal">name</code>). Then, the operation <code class="literal">subscribe()</code> executes one of the two functions passed in by the code that calls this method. We use the method <code class="literal">printAction()</code> only for better traceability:</p><div class="calibre2"><pre class="programlisting">private void printAction(String action) {  
  System.out.println(this.getClass().getSimpleName() 
                                     + " " + action);
}</pre></div><p class="calibre11">The method <code class="literal">process()</code> also accepts two functions but does not need other parameters. It processes all the records from the table <code class="literal">who_called</code> that are not processed yet (not listed in the table <code class="literal">processed</code>):</p><div class="calibre2"><pre class="programlisting">private static String SQL_SELECT_TO_PROCESS = 
  "SELECT name FROM who_called w where name not in " +
  "(select name from processed) order by w.create_ts " +
  "for update";
private static String SQL_INSERT_PROCESSED = 
     "INSERT INTO processed(name, length) values(?, ?)";
public void process(Func1&lt;JsonArray, Observable&lt;JsonArray&gt;&gt; 
                     process, Action1&lt;Throwable&gt; onError) {
  printAction("process all records not processed yet");
  dbClient.rxGetConnection()
    .flatMapObservable(conn -&gt; 
       conn.rxQueryStream(SQL_SELECT_TO_PROCESS)
           .flatMapObservable(SQLRowStream::toObservable)
           .flatMap(process)
           .flatMap(js -&gt; 
              conn.rxUpdateWithParams(SQL_INSERT_PROCESSED, js)
                  .flatMapObservable(ur-&gt;Observable.just(js)))
           .doAfterTerminate(conn::close))
    .subscribe(js -&gt; printAction("processed " + js), onError);
}</pre></div><p class="calibre11">If two threads are reading the table <code class="literal">who_called</code> for the purpose of selecting records not processed yet, the clause <code class="literal">for update</code> in the SQL query makes sure that only one gets each record, so they are not going to be processed twice. The significant advantage of the method <code class="literal">process()</code> code is its usage of the <code class="literal">rxQUeryStream()</code> operation that emits the found records one at a time so that they are processed independently of each other. In the case of a big number of not processed records, such a solution guarantees a smooth delivery of the results without the spiking of the resources consumption. The following <code class="literal">flatMap()</code> operation does processing using the function passed in. The only requirement for that function is that it must return one integer value (in <code class="literal">JsonArray</code>) that is going to be used as a parameter for the <code class="literal">SQL_INSERT_PROCESSED</code> statement. So, it is up to the code that calls this method to decide the nature of the processing. The rest of the code is similar to the method <code class="literal">insert()</code>. The code indentation helps to follow the nesting of the operations.</p><p class="calibre11">The method <code class="literal">readProcessed()</code> has code that looks very similar to the code of the method <code class="literal">insert()</code>:</p><div class="calibre2"><pre class="programlisting">private static String SQL_READ_PROCESSED = 
  "SELECT name, length, create_ts FROM processed 
                       order by create_ts desc limit ?";
public void readProcessed(String count, Action1&lt;ResultSet&gt; 
                  onSuccess, Action1&lt;Throwable&gt; onError) {
  printAction("reads " + count + 
                            " last processed records");
  dbClient.rxGetConnection()
   .flatMap(conn -&gt; 
      conn.rxQueryWithParams(SQL_READ_PROCESSED, 
                          new JsonArray().add(count))
                      .doAfterTerminate(conn::close) )
   .subscribe(onSuccess, onError);
}</pre></div><p class="calibre11">The preceding code reads the specified number of the latest processed records. The difference from the method <code class="literal">process()</code> is that the method <code class="literal">readProcessed()</code> returns all the read records in one result set, so it is up to the user of this method to decide how to process the result in bulk or one at a time. We show all these possibilities just to demonstrate the variety of the possible options. With the <code class="literal">DbHandler</code> class in place, we are ready to use it and create the <code class="literal">DbServiceHttp</code> microservice, which allows a remote access to the <code class="literal">DbHandler</code> capabilities by wrapping around it an HTTP server. Here is the constructor of the new microservice:</p><div class="calibre2"><pre class="programlisting">public class DbServiceHttp extends AbstractVerticle {
  private int port;
  private DbHandler dbHandler;
  public DbServiceHttp(int port) {
    this.port = port;
  }
  public void start() throws Exception {
    System.out.println(this.getClass().getSimpleName() + 
                            "(" + port + ") starts...");
    dbHandler = new DbHandler(vertx);
    Router router = Router.router(vertx);
    router.put("/insert/:name").handler(this::insert);
    router.get("/process").handler(this::process);
    router.get("/readProcessed")
                         .handler(this::readProcessed);
    vertx.createHttpServer()
          .requestHandler(router::accept).listen(port);
  }
}</pre></div><p class="calibre11">In the earlier mentioned code, you can see how the URL mapping is done in Vert.x. For each possible route, a corresponding <code class="literal">Verticle</code> method is assigned, each accepting the <code class="literal">RoutingContext</code> object that contains all the data of HTTP context, including the <code class="literal">HttpServerRequest</code> and <code class="literal">HttpServerResponse</code> objects. A variety of convenience methods allows us to easily access the URL parameters and other data necessary to process the request. Here is the method <code class="literal">insert()</code> referred in the <code class="literal">start()</code> method:</p><div class="calibre2"><pre class="programlisting">private void insert(RoutingContext routingContext) {
  HttpServerResponse response = routingContext.response();
  String name = routingContext.request().getParam("name");
  printAction("insert " + name);
  Action1&lt;UpdateResult&gt; onSuccess = 
    ur -&gt; response.setStatusCode(200).end(ur.getUpdated() + 
                 " record for " + name + " is inserted");
  Action1&lt;Throwable&gt; onError = ex -&gt; {
    printStackTrace("process", ex);
    response.setStatusCode(400)
        .end("No record inserted due to backend error");
  };
  dbHandler.insert(name, onSuccess, onError);
}</pre></div><p class="calibre11">All it does is extracts the parameter <code class="literal">name</code> from the request and constructs the two functions necessary to call method <code class="literal">insert()</code> of <code class="literal">DbHandler</code> we discussed earlier. The method <code class="literal">process()</code> looks similar to the previous method <code class="literal">insert()</code>:</p><div class="calibre2"><pre class="programlisting">private void process(RoutingContext routingContext) {
  HttpServerResponse response = routingContext.response();
  printAction("process all");
  response.setStatusCode(200).end("Processing...");
  Func1&lt;JsonArray, Observable&lt;JsonArray&gt;&gt; process = 
    jsonArray -&amp;gt; { 
      String name = jsonArray.getString(0);
      JsonArray js = 
            new JsonArray().add(name).add(name.length());
       return Observable.just(js);
  };
  Action1&lt;Throwable&gt; onError = ex -&gt; {
     printStackTrace("process", ex);
     response.setStatusCode(400).end("Backend error");
  };
  dbHandler.process(process, onError);
}</pre></div><p class="calibre11">The function <code class="literal">process</code> mentioned earlier defines what should be done with the records coming from the <code class="literal">SQL_SELECT_TO_PROCESS</code> statement inside the method <code class="literal">process()</code> in <code class="literal">DbHandler</code>. In our case, it calculates the length of the caller's name and passes it as a parameter along with the name itself (as a return value) to the next SQL statement that inserts the result into the table <code class="literal">processed</code>.</p><p class="calibre11">Here is the method <code class="literal">readProcessed()</code>:</p><div class="calibre2"><pre class="programlisting">private void readProcessed(RoutingContext routingContext) {
  HttpServerResponse response = routingContext.response();
  String count = routingContext.request().getParam("count");
  printAction("readProcessed " + count + " entries");
  Action1&lt;ResultSet&gt; onSuccess = rs -&gt; {
     Observable.just(rs.getResults().size() &gt; 0 ? 
       rs.getResults().stream().map(Object::toString)
                   .collect(Collectors.joining("\n")) : "")
       .subscribe(s -&gt; response.setStatusCode(200).end(s) );
  };
  Action1&lt;Throwable&gt; onError = ex -&gt; {
      printStackTrace("readProcessed", ex);
      response.setStatusCode(400).end("Backend error");
  };
  dbHandler.readProcessed(count, onSuccess, onError);
}</pre></div><p class="calibre11">That is where (in the previous code in the function <code class="literal">onSuccess()</code>) the result set from the query <code class="literal">SQL_READ_PROCESSED</code> is read and used to construct the response. Notice that we do it by creating an <code class="literal">Observable</code> first, then subscribing to it and passing the result of the subscription as the response into method <code class="literal">end()</code>. Otherwise, the response can be returned without waiting for the response to be constructed.</p><p class="calibre11">Now, we can launch our reactive system by deploying the <code class="literal">DbServiceHttp</code> verticle:</p><div class="calibre2"><pre class="programlisting">RxHelper.deployVerticle(vertx(), new DbServiceHttp(8082));</pre></div><p class="calibre11">If we do that, in the output we will see the following lines of code:</p><div class="calibre2"><pre class="programlisting">DbServiceHttp(8082) starts...
Table processed created
Table who_called created</pre></div><p class="calibre11">In another window, we can issue the command that generates an HTTP request:</p><div class="mediaobject"><img src="Images/04_04.jpg" alt="Building Microservices" class="calibre86"/></div><p class="calibre11">If we read the processed records now, there should be none:</p><div class="mediaobject"><img src="Images/04_05.jpg" alt="Building Microservices" class="calibre87"/></div><p class="calibre11">The log messages show the following:</p><div class="mediaobject"><img src="Images/4_06.jpg" alt="Building Microservices" class="calibre88"/></div><p class="calibre11">Now, we can request processing of the existing records and then read the results again:</p><div class="mediaobject"><img src="Images/04_07.jpg" alt="Building Microservices" class="calibre89"/></div><p class="calibre11">In principle, it is enough already to build a reactive system. We can deploy many <code class="literal">DbServiceHttp</code> microservices on different ports or cluster them to increase processing capacity, resilience, and responsiveness. We can wrap other services inside an HTTP client or an HTTP server and let them talk to each other, processing the input and passing the results along the processing pipeline.</p><p class="calibre11">However, Vert.x also has a feature that even better suits the message-driven architecture (without using HTTP). It is called an event bus. Any verticle has access to the event bus and can send any message to any address (which is just a string) using either method <code class="literal">send()</code> (<code class="literal">rxSend()</code> in the case of reactive programming) or method <code class="literal">publish()</code>. One or many verticles can register themselves as a consumer for a certain address.</p><p class="calibre11">If many verticles are consumers for the same address, then the method <code class="literal">send()</code> (<code class="literal">rxSend()</code>) delivers the message only to one of them (using a round-robin algorithm to pick the next consumer). The method <code class="literal">publish()</code>, as you would expect, delivers the message to all consumers with the same address. Let's see an example, using the already familiar <code class="literal">DbHandler</code> as the main working horse.</p><p class="calibre11">A microservice, based on an event bus, looks very similar to the one based on the HTTP protocol we discussed already:</p><div class="calibre2"><pre class="programlisting">public class DbServiceBus extends AbstractVerticle {
  private int id;
  private String instanceId;
  private DbHandler dbHandler;
  public static final String INSERT = "INSERT";
  public static final String PROCESS = "PROCESS";
  public static final String READ_PROCESSED 
                              = "READ_PROCESSED";
  public DbServiceBus(int id) { this.id = id; }
  public void start() throws Exception {
    this.instanceId = this.getClass().getSimpleName()
                                     + "(" + id + ")";
    System.out.println(instanceId + " starts...");
    this.dbHandler = new DbHandler(vertx);
    vertx.eventBus().consumer(INSERT).toObservable()
      .subscribe(msg -&gt; {
         printRequest(INSERT, msg.body().toString());
         Action1&lt;UpdateResult&gt; onSuccess 
                               = ur -&gt; msg.reply(...);
         Action1&lt;Throwable&gt; onError 
                   = ex -&gt; msg.reply("Backend error");
         dbHandler.insert(msg.body().toString(), 
                                 onSuccess, onError);
    });

    vertx.eventBus().consumer(PROCESS).toObservable()
        .subscribe(msg -&gt; {
                  .....
                 dbHandler.process(process, onError);
        });

    vertx.eventBus().consumer(READ_PROCESSED).toObservable()
        .subscribe(msg -&gt; {
                 ...
            dbHandler.readProcessed(msg.body().toString(), 
                                        onSuccess, onError);
        });
    }</pre></div><p class="calibre11">We simplified the preceding code by skipping some sections (that are very similar to the <code class="literal">DbServiceHttp</code> class) and trying to highlight the code structure. For demo purposes, we will deploy two instances of this class and send three messages to each of the addresses <code class="literal">INSERT</code>, <code class="literal">PROCESS</code>, and <code class="literal">READ_PROCESSED</code>:</p><div class="calibre2"><pre class="programlisting">void demo_DbServiceBusSend() {
  Vertx vertx = vertx();
  RxHelper.deployVerticle(vertx, new DbServiceBus(1));
  RxHelper.deployVerticle(vertx, new DbServiceBus(2));
  delayMs(200);
  String[] msg1 = {"Mayur", "Rohit", "Nick" };
  RxHelper.deployVerticle(vertx, 
    new PeriodicServiceBusSend(DbServiceBus.INSERT, msg1, 1));
  String[] msg2 = {"all", "all", "all" };
  RxHelper.deployVerticle(vertx, 
    new PeriodicServiceBusSend(DbServiceBus.PROCESS, msg2, 1));
  String[] msg3 = {"1", "1", "2", "3" };
  RxHelper.deployVerticle(vertx, 
     new PeriodicServiceBusSend(DbServiceBus.READ_PROCESSED, 
                                                     msg3, 1));
}</pre></div><p class="calibre11">Notice the delay for 200 ms we inserted using the method <code class="literal">delayMs()</code>:</p><div class="calibre2"><pre class="programlisting">void delayMs(int ms){
    try {
        TimeUnit.MILLISECONDS.sleep(ms);
    } catch (InterruptedException e) {}
}</pre></div><p class="calibre11">The delay is necessary to let the <code class="literal">DbServiceBus</code> verticle to be deployed and started (and the consumers registered with the address). Otherwise, an attempt to send a message may fail because the consumer is not registered with the address yet. The <code class="literal">PeriodicServiceBusSend()</code> verticle code is as follows:</p><div class="calibre2"><pre class="programlisting">public class PeriodicServiceBusSend 
                           extends AbstractVerticle {
  private EventBus eb;
  private LocalTime start;
  private String address;
  private String[] caller;
  private int delaySec;
  public PeriodicServiceBusSend(String address, 
                     String[] caller, int delaySec) {
        this.address = address;
        this.caller = caller;
        this.delaySec = delaySec;
  }
  public void start() throws Exception {
    System.out.println(this.getClass().getSimpleName() 
      + "(" + address + ", " + delaySec + ") starts...");
    this.eb = vertx.eventBus();
    this.start  = LocalTime.now();
    vertx.setPeriodic(delaySec * 1000, v -&gt; {
       int i = (int)ChronoUnit.SECONDS.between(start,
                                    LocalTime.now()) - 1;
       System.out.println(this.getClass().getSimpleName()
          + " to address " + address + ": " + caller[i]);
       eb.rxSend(address, caller[i]).subscribe(reply -&gt; {
         System.out.println(this.getClass().getSimpleName() 
                    + " got reply from address " + address 
                               + ":\n    " + reply.body());
          if(i + 1 &gt;= caller.length ){
               vertx.undeploy(deploymentID());
          }
       }, Throwable::printStackTrace);
    });
  }
}</pre></div><p class="calibre11">The previous code sends a message to an address every <code class="literal">delaySec</code> seconds as many times as the length of the array <code class="literal">caller[]</code>, and then undeploys the verticle (itself). If we run the demo, the beginning of the output will be as follows:</p><div class="mediaobject"><img src="Images/04_08.jpg" alt="Building Microservices" class="calibre90"/></div><p class="calibre11">As you can see, for each address, only <code class="literal">DbServiceBus(1)</code> was a receiver of the first message. The second message to the same address was received by <code class="literal">DbServiceBus(2)</code>. That was the round-robin algorithm (which we mentioned earlier) in action. The final section of the output looks like this:</p><div class="mediaobject"><img src="Images/04_09.jpg" alt="Building Microservices" class="calibre91"/></div><p class="calibre11">We can deploy as many verticles of the same type as needed. For example, let's deploy four verticles that send messages to the address <code class="literal">INSERT</code>:</p><div class="calibre2"><pre class="programlisting">String[] msg1 = {"Mayur", "Rohit", "Nick" };
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.INSERT, msg1, 1));
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.INSERT, msg1, 1));
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.INSERT, msg1, 1));
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.INSERT, msg1, 1));</pre></div><p class="calibre11">To see the results, we will also ask the reading Verticle to read the last eight records:</p><div class="calibre2"><pre class="programlisting">String[] msg3 = {"1", "1", "2", "8" };
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.READ_PROCESSED, 
                                               msg3, 1));</pre></div><p class="calibre11">The result (the final section of the output) then will be as expected:</p><div class="mediaobject"><img src="Images/04_10.jpg" alt="Building Microservices" class="calibre92"/></div><p class="calibre11">Four verticles have sent the same messages, so each name was sent four times and processed that is what we see in the previous output.</p><p class="calibre11">We will now return to one inserting periodic verticle but will change it from using the method <code class="literal">rxSend()</code> to the method <code class="literal">publish()</code>:</p><div class="calibre2"><pre class="programlisting">PeriodicServiceBusPublish(String address, String[] caller, int delaySec) {
  ...
  vertx.setPeriodic(delaySec * 1000, v -&gt; {
    int i = (int)ChronoUnit.SECONDS.between(start, 
                                      LocalTime.now()) - 1;
    System.out.println(this.getClass().getSimpleName()
            + " to address " + address + ": " + caller[i]);
    eb.publish(address, caller[i]);
    if(i + 1 == caller.length ){
        vertx.undeploy(deploymentID());
    }
  });
}</pre></div><p class="calibre11">This change would mean that the message has to be sent to all verticles that are registered as the consumers at that address. Now, let's run the following code:</p><div class="calibre2"><pre class="programlisting">Vertx vertx = vertx();
RxHelper.deployVerticle(vertx, new DbServiceBus(1));
RxHelper.deployVerticle(vertx, new DbServiceBus(2));
delayMs(200);
String[] msg1 = {"Mayur", "Rohit", "Nick" };
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusPublish(DbServiceBus.INSERT, 
                                               msg1, 1));
delayMs(200);
String[] msg2 = {"all", "all", "all" };
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.PROCESS, 
                                               msg2, 1));
String[] msg3 = {"1", "1", "2", "8" };
RxHelper.deployVerticle(vertx, 
  new PeriodicServiceBusSend(DbServiceBus.READ_PROCESSED, 
                                               msg3, 1));</pre></div><p class="calibre11">We have included another delay for 200 ms to give the publishing verticle time to send the message. The output (in the final section) now shows that each message was processed twice:</p><div class="mediaobject"><img src="Images/04_11.jpg" alt="Building Microservices" class="calibre93"/></div><p class="calibre11">That is because two consumers <code class="literal">DbServiceBus(1)</code> and <code class="literal">DbServiceBus(2)</code> were deployed, and each received a message to the address <code class="literal">INSERT</code> and inserted it in the table <code class="literal">who_called</code>.</p><p class="calibre11">All the previous examples we have run in one JVM process. If necessary, Vert.x instances can be deployed in different JVM processes and clustered by adding the <code class="literal">-cluster</code> option to the run command. Therefore, they share the event bus and the addresses are visible to all Vert.x instances. This way, the resources can be added to each address as needed. For example, we can increase the number of processing microservices only and compensate the load's increase.</p><p class="calibre11">Other frameworks we mentioned earlier have similar capabilities. They make microservices creation easy and may encourage breaking the application into tiny single-method operations with an expectation of assembling a very resilient and responsive system. </p><p class="calibre11">However, these are not the only criteria of good quality. System decomposition increases the complexity of its deployment. Also, if one development team is responsible for many microservices, the complexity of versioning so many pieces in different stages (development, test, integration test, certification, staging, production) may lead to confusion and a very challenging deployment process, which, in turn, may slow down the rate of changes necessary to keep the system in sync with the market requirements.</p><p class="calibre11">In addition to the developing of the microservices, many other aspects have to be addressed to support the reactive system:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">A monitoring system has to be designed to provide an insight into the state of the application, but it should not be so complex as to pull the development resources away from the main application.</li><li class="listitem">Alerts have to be installed to warn the team about possible and actual issues in a timely manner, so they can be addressed before affecting the business.</li><li class="listitem">If possible, self-correcting automated processes have to be implemented. For example, the system should be able to add and release resources in accordance with the current load; the retry logic has to be implemented with a reasonable upper limit of a attempts before declaring the failure.</li><li class="listitem">A layer of circuit breakers has to protect the system from the domino effect when failure of one component deprives other components of the necessary resources.</li><li class="listitem">An embedded testing system should be able to introduce disruptions and simulate processing load to ensure that the application resilience and responsiveness do not degrade over time. For example, the Netflix team has introduced a <span class="emphasis"><strong class="calibre13">chaos monkey</strong></span> a system that is able to shut down various parts of the production system to test the ability to recover. They use it even in production because a production environment has a specific configuration, and no test in another environment can guarantee that all possible issues are found.</li></ul></div><p class="calibre11">One of the main considerations of a reactive system design is the selection of the deployment methodology that can be either container-less, self-contained, or in-container. We will look into the pros and cons of each of these approaches in the following sections of this lesson.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Container-Less Deployment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Container-Less Deployment</h1></div></div></div><p class="calibre11">People use the term <span class="emphasis"><strong class="calibre13">container</strong></span> to refer to very different things. In the original usage, a container was something that carried its content from one location to another without changing anything inside. However, when servers were introduced, only one aspect was emphasized the ability to hold an application to contain it. Also, another meaning was added to provide life-supportive infrastructure so that the container's content (an application) can not only survive but also be active and respond to the external requests. Such a redefined notion of a container was applied to web servers (servlet container), application servers (an application container with or without an EJB container), and other software facilities that provided the supportive environment for applications. Sometimes, even the JVM itself was called a container, but this association did not survive, probably, because the ability to actively engage (execute) the content does not align well with the original meaning of a container.</p><p class="calibre11">That is why, later, when people started talking about container-less deployment, they typically meant the ability to deploy an application into a JVM directly, without first installing WebSphere, WebLogic, JBoss, or any other mediating software that provides the runtime environment for the application.</p><p class="calibre11">In the previous sections, we described many frameworks that allow us to build and deploy an application (or rather a reactive system of microservices) without the need for any other container beyond the JVM itself. All you need to do is to build a fat JAR file that includes all the dependencies (except those that come from the JVM itself) and then run it as a standalone Java process:</p><div class="calibre2"><pre class="programlisting">
<span class="emphasis"><strong class="calibre13">$ java -jar myfatjar.jar</strong></span>
</pre></div><p class="calibre11">Well, you also need to make sure that <code class="literal">MANIFEST.MF</code> in your JAR file has an entry <code class="literal">main</code> class that points to the fully qualified class name that has the <code class="literal">main()</code> method and will be run at the startup. We have described how to do it in the previous section, <span class="emphasis"><em class="calibre12">Building Microservices</em></span>.</p><p class="calibre11">That is the promised compile-once-run-everywhere of Java, everywhere meaning everywhere where JVM of a certain version or higher is installed. There are several advantages and disadvantages of this approach. We will discuss them not relative to the traditional deployment in a server container. The advantages of deployment without using the traditional containers are quite obvious, starting with much fewer (if any) licensing costs and ending up with much a lighter deployment and scalability process, not even mentioning much less consumption of resources. Instead, we will compare container-less deployment not with the traditional one, but with a self-contained and an in-container in a new generation of containers that have been developed a few years ago.</p><p class="calibre11">They allow the ability not only to contain and execute the contained code, which the traditional containers did too, but also to move it to a different location without any change to the contained code. From now on, by a container, we are going to mean only the new ones.</p><p class="calibre11">The advantages of container-less deployment are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">It is easy to add more Java processes either inside the same physical (or virtual or in the cloud) machine or on new hardware</li><li class="listitem">An isolation level between processes is high, which is especially important in the shared environment when you have no control over other co-deployed applications, and it is possible that a rogue application would try to penetrate the neighboring execution environment</li><li class="listitem">It has a small footprint since it does not include anything else beyond the application itself or a group of microservices</li></ul></div><p class="calibre11">The disadvantages of container-less deployment are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Each JAR file requires the JVM of a certain version or higher, which may force you to bring up a new physical or virtual machine just for this reason, to deploy one particular JAR file</li><li class="listitem">In the case of an environment you do not control, your code might be deployed with a wrong version of JVM, which could lead to unpredictable results</li><li class="listitem">Processes in the same JVM compete for resources, which are especially hard to manage in the case of the environments shared by different teams or different companies</li><li class="listitem">When several microservices are bundled into the same JAR file, they might require different versions of a third-party library or even incompatible libraries</li></ul></div><p class="calibre11">Microservices can be deployed one per JAR or bundled together by a team, by related services, by the unit of scale, or using another criterion. Not the least important consideration is the total number of such JAR files. As this number grows (Google today deals with hundreds of thousands of deployment units at a time), it may become impossible to handle deployment via simple bash script and require a complex process that allows account ability for possible incompatibilities. If that is the case, then it is reasonable to consider using virtual machines or containers (in their new incarnation, see the following section) for better isolation and management.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Self-Contained Microservices"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Self-Contained Microservices</h1></div></div></div><p class="calibre11">Self-contained microservices look much similar to container-less. The only difference is that the JVM (or JRE, actually) or any other external frameworks and servers necessary for the application to run are included in the fat JAR file too. There are many ways to build such an all-inclusive JAR file.</p><p class="calibre11">Spring Boot, for example, provides a convenient GUI with checkbox list that allows you to select which parts of your Spring Boot application and the external tools you would like to package. Similarly, WildFly Swarm allows you to choose which parts of the Java EE components you would like to bundle along with your application. Alternatively, you can do it yourself using the <code class="literal">javapackager</code> tool. It compiles and packages the application and JRE in the same JAR file (it can also be <code class="literal">.exe</code> or <code class="literal">.dmg</code>) for distribution. You can read about the tool on the Oracle website <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://docs.oracle.com/javase/9/tools/javapackager.htm">https://docs.oracle.com/javase/9/tools/javapackager.htm</a> or you can just run the command <code class="literal">javapackager</code> on a computer where JDK is installed (it comes with Java 8 too) you will get the list of tool options and their brief description.</p><p class="calibre11">Basically, to use the <code class="literal">javapackager</code> tool, all you need to do is to prepare a project with everything you would like to package together, including all the dependencies (packaged in JAR files), and run the <code class="literal">javapackager</code> command with the necessary options that allow you to specify the type of output you would like to have (<code class="literal">.exe</code> or <code class="literal">.dmg</code>, for example), the JRE location you would like to bundle together, the icon to use, the <code class="literal">main</code> class entry for <code class="literal">MANIFEST.MF</code>, and so on. There are also Maven plugins that make the packaging command simpler because much of the setup has to be configured in <code class="literal">pom.xml</code>.</p><p class="calibre11">The advantages of self-contained deployment are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">It is one file (with all the microservices that compose the reactive system or some part of it) to handle, which is simpler for a user and for a distributor</li><li class="listitem">There is no need to pre-install JRE and no risk of mismatching the required version</li><li class="listitem">The isolation level is high because your application has a dedicated JRE, so the risk of an intrusion from a co-deployed application is minimal</li><li class="listitem">You have full control over the dependencies included in the bundle</li></ul></div><p class="calibre11">The disadvantages are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The size of the file is bigger, which might be an impediment if it has to be downloaded</li><li class="listitem">The configuration is more complex than in the case of a container-less JAR file</li><li class="listitem">The bundle has to be generated on a platform that matches the target one, which might lead to mismatch if you have no control over the installation process</li><li class="listitem">Other processes deployed on the same hardware or virtual machine can hog the resources critical for your application needs, which are especially hard to manage if your application is downloaded and run not by the team that has developed it</li></ul></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="In-Container Deployment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec32" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>In-Container Deployment</h1></div></div></div><p class="calibre11">Those who are familiar with <span class="emphasis"><strong class="calibre13">Virtual Machine</strong></span> (<span class="emphasis"><strong class="calibre13">VM</strong></span>) and not familiar with modern containers (such as Docker, Rocket by CoreOS, VMware Photon, or similar) could get the impression that we were talking about VM while saying that a container could not only contain and execute the contained code, but also to move it to a different location without any change to the contained code. If so, that would be quite an apt assumption. VM does allow all of that, and a modern container can be considered a lightweight VM as it also allows the allocation of resources and provides the feeling of a separate machine. Yet, a container is not a full-blown isolated virtual computer.</p><p class="calibre11">The key difference is that the bundle that can be passed around as a VM includes an entire operating system (with the application deployed). So, it is quite possible that a physical server running two VMs would have two different operating systems running on it. By contrast, a physical server (or a VM) running three containerized applications has only one operating system running, and the two containers share (read-only) the operating system kernel, each having its own access (mount) for writing to the resources they do not share. This means, for example, a much shorter start time, because starting a container does not require us to boot the operating system (as in the case of a VM).</p><p class="calibre11">For an example, let's take a closer look at Docker the community leader in container. In 2015, an initiative called <span class="emphasis"><strong class="calibre13">Open Container Project</strong></span> was announced, later renamed the <span class="emphasis"><strong class="calibre13">Open Container Initiative</strong></span> (<span class="emphasis"><strong class="calibre13">OCI</strong></span>), which was supported by Google, IBM, Amazon, Microsoft, Red Hat, Oracle, VMware, HP, Twitter, and many other companies. Its purpose was to develop industry standards for a container format and container runtime software for all platforms. Docker has donated about 5 percent of its code base to the project because its solution was chosen as the starting point.</p><p class="calibre11">There is an extensive Docker documentation at: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://docs.docker.com">https://docs.docker.com</a>. Using Docker, one can include in the package all the Java EE Container and the application as a Docker image, achieving essentially the same result as with a self-contained deployment. Then, you can launch your application by starting the Docker image in the Docker engine using this command:</p><div class="calibre2"><pre class="programlisting">
<span class="emphasis"><strong class="calibre13">$ docker run mygreatapplication</strong></span>
</pre></div><p class="calibre11">It starts a process that looks like running an OS on a physical computer, although it can also be happening in a cloud inside a VM that is running on the physical Linux server shared by many different companies and individuals. That is why an isolation level (which, in the case of containers, is almost as high as in a VM) may be critical in choosing between different deployment models.</p><p class="calibre11">A typical recommendation would be to put one microservice in each container, but nothing prevents you from putting several microservices in one Docker image (or any other container for that matter). However, there are already mature systems of container management (in the world of containers called <span class="emphasis"><strong class="calibre13">orchestration</strong></span>) that can help you with deployment, so the complexity of having many containers, although a valid consideration, should not be a big obstacle if resilience and responsiveness are at stake. One of the popular orchestrations called <span class="emphasis"><strong class="calibre13">Kubernetes</strong></span> supports microservice registry, discovery, and load balancing. Kubernetes can be used in any cloud or in a private infrastructure.</p><p class="calibre11">Containers allow a fast, reliable, and consistent deployment in practically any of the current deployment environments, whether it is your own infrastructure or a cloud at Amazon, Google, or Microsoft. They also allow the easy movement of an application through the development, testing, and production stages. Such infrastructure independence allows you, if necessary, to use a public cloud for development and testing and your own computers for production.</p><p class="calibre11">Once a base operating image is created, each development team can then build their application on top, thus avoiding the complexities of environment configuration. The versions of a container can also be tracked in a version control system.</p><p class="calibre11">The advantages of using containers are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The level of isolation is the highest if compared with container-less and self-contained deployment. In addition, more efforts were put recently into adding security to containers.</li><li class="listitem">Each container is managed, distributed, deployed, started, and stopped by the same set of commands.</li><li class="listitem">There is no need to pre-install JRE and risk of mismatching the required version.</li><li class="listitem">You have full control over the dependencies included in the container.</li><li class="listitem">It is straightforward to scale up/down each microservice by adding/removing container instances.</li></ul></div><p class="calibre11">The disadvantages of using containers are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">You and your team have to learn a whole new set of tools and become involved more heavily in the production stage. On the other hand, that seems to be the general tendency in recent years.</li></ul></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">Microservices is a new architectural and design solution for highly loaded processing systems that became popular after being successfully used in production by such giants as Amazon, Google, Twitter, Microsoft, IBM, and others. It does not mean though that you must adopt it too, but you can consider the new approach and see if some or any of it can help your applications to be more resilient and responsive.</p><p class="calibre11">Using microservices can provide a substantial value, but it is not free. It comes with increased complexity of the need to manage many more units through all the lifecycle from requirements and development through testing to production. Before committing to the full-scale microservice architecture, give it a shot by implementing just a few microservices and move them all the way to production. Then, let it run for some time and gauge the experience. It will be very specific to your organization. Any successful solution must not be blindly copied but adopted as fit for your particular needs and abilities.</p><p class="calibre11">Better performance and overall efficiency often can be achieved by gradual improvements of what is already in place than by radical redesign and re-architecture.</p><p class="calibre11">In the next lesson, we will discuss and demonstrate new API that can improve your code by making it more readable and faster performing.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Assessments"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch04lvl1sec34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Assessments</h1></div></div></div><div class="calibre2"><ol class="orderedlist"><li class="listitem1">Using the _________ object, various verticles can be deployed, which talk to each other, receive an external request, and process and store data as any other Java application, thus forming a system of microservices.</li><li class="listitem1">Which of the following is advantage of container-less deployment?<div class="calibre2"><ol class="orderedlist1"><li class="listitem1">Each JAR file requires the JVM of a certain version or higher, which may force you to bring up a new physical or virtual machine just for this reason, to deploy one particular JAR file</li><li class="listitem1">In the case of an environment you do not control, your code might be deployed with a right version of JVM, which could lead to unpredictable results</li><li class="listitem1">Processes in the same JVM compete for resources, which are especially hard to manage in the case of the environments shared by different teams or different companies</li><li class="listitem1">It has a small footprint since it does not include anything else beyond the application itself or a group of microservices</li></ol></div></li><li class="listitem1">State whether True or False: One way to support a transaction across several microservices is to create a service that would play the role of a Parallel Transaction Manager.</li><li class="listitem1">Which of the following are the Java frameworks that are included in Java 9?<div class="calibre2"><ol class="orderedlist1"><li class="listitem1">Akka</li><li class="listitem1">Ninja</li><li class="listitem1">Orange</li><li class="listitem1">Selenium</li></ol></div></li><li class="listitem1">State whether True or False: The level of isolation in a container is the highest if compared with container-less and self-contained deployment.</li></ol></div></div></div>



  </body></html>