- en: Chapter 9. Providing a Big Data Integration Layer with Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Hadoop client bundles in Apache Karaf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing Apache Hadoop from Karaf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding commands that talk to HDFS for deployment in Karaf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To continue building on data storage models that aren't as inflexible as traditional
    RDBMS structures, we will look at Apache Hadoop. Hadoop was created by Doug Cutting
    and Mike Cafarella in 2005\. Cutting, who was working at Yahoo! at the time, named
    it after his son's toy elephant. It was originally developed to support distribution
    for the Nutch search engine project.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop followed the ideas published by Google in the papers pertaining to Google
    File System and Google MapReduce. With over a decade of use, Hadoop has grown
    to a very large and complex ecosystem with a projected revenue of around $23 billion
    in 2016\. Hadoop drives everything from repackaged distributions to full database
    implementations, analytics packages, and management solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop has also started changing the way startups look at their data models,
    allowing new companies to make Big Data part of their overall strategy.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of Hadoop, you have **Hadoop Distributed File System** (**HDFS**).
    This mechanism is what allows the distribution of data. It evolved from a potential
    single point of failure scenario to having competing implementations from companies
    like DataStax and RedHat with Cassandra FS and RedHat Cluster File System, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the arena of complete product offerings, you'll find MapR Cloudera, Hortonworks,
    Intel, and IBM, to name a few as alternatives to the Apache Hadoop distribution.
  prefs: []
  type: TYPE_NORMAL
- en: If you ponder the future, it does seem that the marriage of SQL-like techniques
    with a distributed store is where the majority of use cases are headed. This allows
    users to, at the very least, leverage the RDBMS ideas that are already tried in
    combination with practically unlimited storage for data mining, social networking
    aspects, monitoring, and decision making.
  prefs: []
  type: TYPE_NORMAL
- en: With YARN (Cluster Resource Management) becoming part of the Hadoop infrastructure,
    HDFS is taken from just storage and MapReduce to an environment that can handle
    batch, interactive, and streaming as well as application deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a standalone Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start this all off, we'll begin by setting up a simple standalone cluster.
    We'll need to download and configure an Apache Hadoop release and make sure that
    we can use it. Then, we will move on to configuring the same configurations and
    access methods in an Apache Karaf container. We will utilize an external cluster
    to show how you can utilize Apache Karaf to spin up a new job engine against a
    large existing cluster. With the features we have and will deploy, you can also
    embed an HDFS filesystem from a Karaf container.
  prefs: []
  type: TYPE_NORMAL
- en: Download a Hadoop release from one of the Apache mirrors at [http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download).
    At the time of writing this book, the latest release is 2.4.0\. A full walkthrough
    of setting up a cluster can be found at [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html).
  prefs: []
  type: TYPE_NORMAL
- en: The following are the changes you need to make to get HDFS up and running and
    talking to a locally installed node, replication handler, and job tracker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expand the downloaded archive and modify the files in the `etc/hadoop/*` folder.
    The `core-site.xml` file needs to be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hdfs-site.xml` file needs to be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mapred-site.xml` file needs to be modified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is accomplished and you can run SSH to your localhost without a password,
    you can start your daemons. If you cannot run SSH, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands will create an empty SSH key for remote login. If you
    already have an existing SSH key associated with your account, you only need to
    run the second command to make sure that you can remotely log in to your localhost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s verify that the existing installation is accessible and we can
    operate against it. Once this is done, we can start all the daemons in one fell
    swoop using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that this command will probably go away as you configure more YARN
    options in the future.
  prefs: []
  type: TYPE_NORMAL
- en: You should see several daemons starting and hopefully no error messages other
    than possible warnings pertaining to your missing native libraries. (This isn't
    covered in this little tutorial but pertains to IO libraries and optimized access
    for your particular platform.)
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a running HDFS system, we''ll first create a directory using the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we verify that we can read using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We've created a directory in the simple single node cluster we started (we are
    calling this a cluster as all the components necessary to cluster are running
    and we are simply running them all on one node). We've also ensured that we can
    list the contents of the said directory. This tells us that HDFS is accessible
    and active.
  prefs: []
  type: TYPE_NORMAL
- en: The following is what we have accomplished so far and what we will be covering
    next.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a Karaf external HDFS system running. This could have been an existing
    deployment, an Amazon job, or a set of virtual servers. Basically, we have the
    fundamentals of a cluster and we know we can access it and create content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is going to be a deep dive into transforming an existing model
    of deployment into an OSGi-friendly deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Hadoop client bundles in Apache Karaf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Hadoop running, we are ready to start utilizing the resources from Apache
    Karaf.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ingredients of this recipe include the Apache Karaf distribution kit, access
    to JDK, and Internet connectivity. We also assume that an Apache Hadoop distribution
    is downloaded and installed. It can be downloaded from [http://hadoop.apache.org/#Download](http://hadoop.apache.org/#Download).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hadoop's HDFS libraries aren't part of the standard Karaf feature library; so,
    we either have to write our own feature or manually install the necessary bundles
    for the client to run. Apache Camel does have this feature available via Camels
    HDFS2 component. We can either use Camel's existing feature or build the feature
    ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the current version of Snappy Java used in the Camel feature, you will
    run into problems using native libraries with Java 7\. This is a well-known issue
    and is being addressed in the 1.0.5 release of Snappy Java ([https://github.com/xerial/snappy-java](https://github.com/xerial/snappy-java)).
    To resolve this issue for all platforms, we will build a Karaf feature of our
    own where we can utilize all of the bundles that are in the Camel feature as well
    as some additional JAR files that will allow us to run the latest versions. This
    can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We can now utilize this feature's file to deploy a Hadoop client that is OSGi-friendly
    and will allow us to utilize all the functionalities in Hadoop. We do this by
    utilizing another project, Apache ServiceMix. Apache ServiceMix maintains a bundle
    repository where commonly used or sought after resources are repackaged and turned
    into working OSGi bundles if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Apache ServiceMix :: Bundles :: hadoop-client` bundle that we are using
    is an uber bundle containing core, YARN, HDFS, MapReduce, common client JAR files,
    and the Hadoop annotations in one fell swoop.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the installation by executing the `list | grep –i hadoop` command
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Accessing Apache Hadoop from Karaf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Hadoop, the core of a cluster is the distributed and replicated filesystem.
    We have HDFS running and can access it from our command line window as a regular
    user. Actually, getting to it from an OSGi container will prove to be slightly
    more complicated than just writing the Java components.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop requires us to provide configuration metadata for our cluster that can
    be looked up as file or classpath resources. In this recipe, we will simply copy
    the HDFS site-specific files we created earlier in the chapter to our `src/main/resources`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: We will also include the default metadata definitions into our resources by
    copying them from a dependency, and finally, we'll allow our bundle classloader
    to perform fully dynamic class loading. To sum it up, we have to copy the `core-site.xml`,
    `hdfs-site.xml`, and `mapred-site.xml` files into our classpath. These files together
    describe to our client how to access HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: As we get to the code, there is also a step we'll perform to trick the Hadoop
    classloaders into utilizing our bundle classloader as well as respecting the configuration
    data by providing specific implementations.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we'll do is make sure that the necessary defaults are copied
    into our tutorial bundle.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will modify the Felix bundle plugin and add the following segment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add another section for dynamic loading of classes as they are
    needed. This isn''t necessarily best practice in OSGi, but sometimes, it is one
    of the few possible ways of getting bundles and JAR files not intended for OSGi
    to work. We do this by adding another little snippet to the Felix bundle plugin
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With this addition, we tell our bundle that if you need a class at a later time,
    just go look it up when we need it. It is a tad more costly and certainly not
    recommended as a general practice as it forces the bundle to scan classpaths.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we pull two more tricks out of our hat inside our implementation code.
    We do this as the Hadoop code is multithreaded, and by default, the classloader
    for a new thread is the system classloader. In an OSGi context, the system classloader
    is going to have very limited visibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we replace the `ThreadContextClassLoader` class as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Secondly, as we are using Maven and as Hadoop unfortunately reuses the same
    file for SPI depending on your implementation, we cannot simply copy resources.
    Our aggregate JAR file and any dependencies we import will be overwritten with
    the last imported version.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get around this, we explicitly tell our `Configuration` object which implementations
    to use when accessing our cluster. This is shown in the following code:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: With the preceding actions, we have satisfied all the classloading and instantiation
    issues. Now, we are actually ready to access HDFS remotely. However, we still
    haven't forced our bundle to import and export all of Hadoop, but we can fairly
    easily change versions and we can externalize the static XML files that define
    the cluster if needed.
  prefs: []
  type: TYPE_NORMAL
- en: How it works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have basically tricked our bundle into being able to imitate a single monolithic
    classloader in a normal classpath-provided JVM. We did this by creating a bundle
    that dynamically imports resources it needs from the Apache Hadoop bundle, and
    we have ensured that our bundle can access all the necessary configuration resources.
    We also adjusted classloading so that the Hadoop code base uses our bundle classloader
    to instantiate new instances.
  prefs: []
  type: TYPE_NORMAL
- en: Adding commands that talk to HDFS for deployment in Karaf
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As HDFS at its core is a filesystem, let's see how we can access that with the
    standard tools and the bundle we've been building up so far.
  prefs: []
  type: TYPE_NORMAL
- en: What we'll do is store one level of configuration files from our running Karaf
    container into HDFS. Then, we'll provide a second command to read the files back.
  prefs: []
  type: TYPE_NORMAL
- en: We've learned how to build a feature for Hadoop that takes care of all the various
    dependencies needed to talk to HDFS, and we have also jumped a little bit ahead
    and discussed classloading and a few tricks to get the Hadoop libraries we deployed
    to cooperate. We are now at a point where we can start writing code against Hadoop
    using the libraries provided.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ingredients of this recipe include the Apache Karaf distribution kit, access
    to JDK, and Internet connectivity. The sample code for this recipe is available
    at [https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1](https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1).
    Remember, you need both the drivers installed and Apache Hadoop's HDFS running
    for these recipes to work!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building a project that can access Hadoop will require the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step is generating a Maven-based bundle project. Create an empty Maven-based
    project. A `pom.xml` file containing the essential Maven coordinate information
    and bundle packaging directives will suffice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is adding dependencies to the POM file. This can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For Karaf 3.0.0, we use OSGi Version 5.0.0\. The Hadoop libraries require quite
    a few supporting bundles. The existing Camel feature was used as a starting point
    but doesn't actually work on all platforms, so we have to rewrite it to suit our
    needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is adding build plugins. Our recipe requires only one build plugin
    to be configured, which is the bundle. We configure the `maven-bundle-plugin`
    to assemble our project code into an OSGi bundle. We add the following plugin
    configuration to our POM file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Felix and Karaf imports are required by the optional Karaf commands. We
    are starting to get a bit more of a complicated bundle plugin as we are enabling
    dynamic classloading and copying resources around so that they are available to
    our classloader.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is creating a Blueprint descriptor file. Create the `src/main/resources/OSGI-INF/blueprint`
    directory tree in your project. We''ll then create a file named `blueprint.xml`
    in this folder. Consider the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is developing an OSGi service with a new Hadoop backend. We''ve
    created the basic project structure and plumbed in configurations for Blueprint
    descriptors. Now, we''ll focus on the underlying Java code of our Hadoop-backed
    application. We break this process down into two steps: defining a service interface
    and providing a concrete implementation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we define a service interface. The service interface will define the
    user API to our project. In our sample code, we implement the `HdfsConfigService`
    interface, which provides the methods required to store and retrieve the configuration
    files we have in our Karaf instance. This can be done as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: The interface's implementation follows standard Java conventions, requiring
    no special OSGi packages.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we implement our communication with HDFS. Now that we have defined our
    service interface, we''ll provide an implementation as two calls to store and
    retrieve the file data from HDFS. This can be done as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is the optional creation of Karaf commands to directly test the
    persistence service. To simplify manual testing of our `HdfsConfigService` interface,
    we can create a set of custom Karaf commands that will exercise our HDFS storage
    and retrieval operations. The sample implementations of these commands are available
    from the book''s website. Of particular interest is how they obtain a reference
    to the `HdfsConfigService` interface and make calls to the service. We must wire
    the command implementation into Karaf via Blueprint. This can be done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Each of our custom command's implementation classes are wired to our `hdfsConfigService`
    instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step is deploying the project into Karaf.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: This demo will require a running Hadoop cluster!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We make sure that all the Hadoop bundles are installed correctly. This can
    be done using the following commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We install our project bundle by executing the `install` command on its Maven
    coordinates:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last step is testing the project. We can use the following commands for
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a Karaf container now communicating with an external HDFS filesystem.
    We can back up configuration files from Karaf to HDFS and we can read them back.
  prefs: []
  type: TYPE_NORMAL
- en: A new Karaf instance can be started to consume and copy these configurations
    or we can use this recipe as the basis for starting up MapReduce jobs, tasks,
    and batch jobs.
  prefs: []
  type: TYPE_NORMAL
