- en: Chapter 9. Providing a Big Data Integration Layer with Apache Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：使用Apache Hadoop提供大数据集成层
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing Hadoop client bundles in Apache Karaf
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Apache Karaf中安装Hadoop客户端包
- en: Accessing Apache Hadoop from Karaf
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Karaf访问Apache Hadoop
- en: Adding commands that talk to HDFS for deployment in Karaf
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加与HDFS通信的命令以在Karaf中部署
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: To continue building on data storage models that aren't as inflexible as traditional
    RDBMS structures, we will look at Apache Hadoop. Hadoop was created by Doug Cutting
    and Mike Cafarella in 2005\. Cutting, who was working at Yahoo! at the time, named
    it after his son's toy elephant. It was originally developed to support distribution
    for the Nutch search engine project.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续构建比传统RDBMS结构更灵活的数据存储模型，我们将探讨Apache Hadoop。Hadoop是由Doug Cutting和Mike Cafarella于2005年创建的。当时在Yahoo!工作的Cutting以他儿子的玩具大象命名了它。它最初是为了支持Nutch搜索引擎项目的分布式而开发的。
- en: Hadoop followed the ideas published by Google in the papers pertaining to Google
    File System and Google MapReduce. With over a decade of use, Hadoop has grown
    to a very large and complex ecosystem with a projected revenue of around $23 billion
    in 2016\. Hadoop drives everything from repackaged distributions to full database
    implementations, analytics packages, and management solutions.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop遵循了Google在其关于Google文件系统和Google MapReduce的论文中发表的思想。经过十多年的使用，Hadoop已经发展成为一个非常庞大且复杂的生态系统，预计2016年的收入约为230亿美元。Hadoop推动了从重新包装的发行版到完整的数据库实现、分析包和管理解决方案的一切。
- en: Hadoop has also started changing the way startups look at their data models,
    allowing new companies to make Big Data part of their overall strategy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop也开始改变初创公司对数据模型的认识，使新公司能够将大数据作为其整体战略的一部分。
- en: At the core of Hadoop, you have **Hadoop Distributed File System** (**HDFS**).
    This mechanism is what allows the distribution of data. It evolved from a potential
    single point of failure scenario to having competing implementations from companies
    like DataStax and RedHat with Cassandra FS and RedHat Cluster File System, respectively.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop的核心，你有**Hadoop分布式文件系统**（**HDFS**）。这个机制是允许数据分布的。它从潜在的单一故障点场景发展到有来自DataStax和RedHat等公司的竞争性实现，分别是Cassandra
    FS和RedHat Cluster File System。
- en: In the arena of complete product offerings, you'll find MapR Cloudera, Hortonworks,
    Intel, and IBM, to name a few as alternatives to the Apache Hadoop distribution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整产品解决方案的领域，你可以找到MapR、Cloudera、Hortonworks、Intel和IBM等作为Apache Hadoop发行版的替代品。
- en: If you ponder the future, it does seem that the marriage of SQL-like techniques
    with a distributed store is where the majority of use cases are headed. This allows
    users to, at the very least, leverage the RDBMS ideas that are already tried in
    combination with practically unlimited storage for data mining, social networking
    aspects, monitoring, and decision making.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你思考未来，似乎SQL-like技术与分布式存储的结合将是大多数用例的发展方向。这使用户至少能够利用已经在组合中尝试过的RDBMS思想，结合几乎无限的数据挖掘、社交网络、监控和决策制定的数据存储。
- en: With YARN (Cluster Resource Management) becoming part of the Hadoop infrastructure,
    HDFS is taken from just storage and MapReduce to an environment that can handle
    batch, interactive, and streaming as well as application deployment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 YARN（集群资源管理）成为Hadoop基础设施的一部分，HDFS不再仅仅是存储和MapReduce，而是一个可以处理批量、交互式、流式以及应用部署的环境。
- en: Starting a standalone Hadoop cluster
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动一个独立的Hadoop集群
- en: To start this all off, we'll begin by setting up a simple standalone cluster.
    We'll need to download and configure an Apache Hadoop release and make sure that
    we can use it. Then, we will move on to configuring the same configurations and
    access methods in an Apache Karaf container. We will utilize an external cluster
    to show how you can utilize Apache Karaf to spin up a new job engine against a
    large existing cluster. With the features we have and will deploy, you can also
    embed an HDFS filesystem from a Karaf container.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始这一过程，我们将首先设置一个简单的独立集群。我们需要下载和配置一个Apache Hadoop版本，并确保我们可以使用它。然后，我们将继续在Apache
    Karaf容器中配置相同的配置和访问方法。我们将利用外部集群来展示如何使用Apache Karaf启动一个针对大型现有集群的新作业引擎。凭借我们拥有的和将要部署的功能，您还可以从Karaf容器中嵌入HDFS文件系统。
- en: Download a Hadoop release from one of the Apache mirrors at [http://hadoop.apache.org/releases.html#Download](http://hadoop.apache.org/releases.html#Download).
    At the time of writing this book, the latest release is 2.4.0\. A full walkthrough
    of setting up a cluster can be found at [http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html](http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The following are the changes you need to make to get HDFS up and running and
    talking to a locally installed node, replication handler, and job tracker.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'Expand the downloaded archive and modify the files in the `etc/hadoop/*` folder.
    The `core-site.xml` file needs to be modified as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `hdfs-site.xml` file needs to be modified as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `mapred-site.xml` file needs to be modified as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once this is accomplished and you can run SSH to your localhost without a password,
    you can start your daemons. If you cannot run SSH, run the following commands:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding commands will create an empty SSH key for remote login. If you
    already have an existing SSH key associated with your account, you only need to
    run the second command to make sure that you can remotely log in to your localhost.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s verify that the existing installation is accessible and we can
    operate against it. Once this is done, we can start all the daemons in one fell
    swoop using the following command:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keep in mind that this command will probably go away as you configure more YARN
    options in the future.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: You should see several daemons starting and hopefully no error messages other
    than possible warnings pertaining to your missing native libraries. (This isn't
    covered in this little tutorial but pertains to IO libraries and optimized access
    for your particular platform.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a running HDFS system, we''ll first create a directory using the
    following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we verify that we can read using the following command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We've created a directory in the simple single node cluster we started (we are
    calling this a cluster as all the components necessary to cluster are running
    and we are simply running them all on one node). We've also ensured that we can
    list the contents of the said directory. This tells us that HDFS is accessible
    and active.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The following is what we have accomplished so far and what we will be covering
    next.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: We now have a Karaf external HDFS system running. This could have been an existing
    deployment, an Amazon job, or a set of virtual servers. Basically, we have the
    fundamentals of a cluster and we know we can access it and create content.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is going to be a deep dive into transforming an existing model
    of deployment into an OSGi-friendly deployment.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Hadoop client bundles in Apache Karaf
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With Hadoop running, we are ready to start utilizing the resources from Apache
    Karaf.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ingredients of this recipe include the Apache Karaf distribution kit, access
    to JDK, and Internet connectivity. We also assume that an Apache Hadoop distribution
    is downloaded and installed. It can be downloaded from [http://hadoop.apache.org/#Download](http://hadoop.apache.org/#Download).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的成分包括Apache Karaf发行套件、对JDK的访问权限和互联网连接。我们还假设已经下载并安装了Apache Hadoop发行版。可以从[http://hadoop.apache.org/#Download](http://hadoop.apache.org/#Download)下载。
- en: How to do it…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: Hadoop's HDFS libraries aren't part of the standard Karaf feature library; so,
    we either have to write our own feature or manually install the necessary bundles
    for the client to run. Apache Camel does have this feature available via Camels
    HDFS2 component. We can either use Camel's existing feature or build the feature
    ourselves.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop的HDFS库不是标准Karaf功能库的一部分；因此，我们或者需要编写自己的功能，或者手动安装客户端运行所需的必要包。Apache Camel确实通过Camel的HDFS2组件提供了这一功能。我们可以使用Camel现有的功能，或者自己构建这个功能。
- en: 'With the current version of Snappy Java used in the Camel feature, you will
    run into problems using native libraries with Java 7\. This is a well-known issue
    and is being addressed in the 1.0.5 release of Snappy Java ([https://github.com/xerial/snappy-java](https://github.com/xerial/snappy-java)).
    To resolve this issue for all platforms, we will build a Karaf feature of our
    own where we can utilize all of the bundles that are in the Camel feature as well
    as some additional JAR files that will allow us to run the latest versions. This
    can be done as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Camel功能中使用的当前版本的Snappy Java，使用Java 7运行原生库时会出现问题。这是一个已知的问题，并且正在Snappy Java的1.0.5版本中解决([https://github.com/xerial/snappy-java](https://github.com/xerial/snappy-java))。为了解决所有平台上的这个问题，我们将构建自己的Karaf功能，其中我们可以利用Camel功能中的所有包以及一些额外的JAR文件，这将使我们能够运行最新版本。这可以通过以下方式完成：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can now utilize this feature's file to deploy a Hadoop client that is OSGi-friendly
    and will allow us to utilize all the functionalities in Hadoop. We do this by
    utilizing another project, Apache ServiceMix. Apache ServiceMix maintains a bundle
    repository where commonly used or sought after resources are repackaged and turned
    into working OSGi bundles if necessary.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以利用这个功能的文件来部署一个OSGi友好的Hadoop客户端，这将使我们能够利用Hadoop中的所有功能。我们通过利用另一个项目Apache
    ServiceMix来实现这一点。Apache ServiceMix维护一个包仓库，其中常用的或寻求的资源被重新打包，并在必要时转换为工作的OSGi包。
- en: 'The `Apache ServiceMix :: Bundles :: hadoop-client` bundle that we are using
    is an uber bundle containing core, YARN, HDFS, MapReduce, common client JAR files,
    and the Hadoop annotations in one fell swoop.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '我们正在使用的`Apache ServiceMix :: Bundles :: hadoop-client`包是一个包含核心、YARN、HDFS、MapReduce、通用客户端JAR文件和Hadoop注解的uber包。'
- en: 'We can verify the installation by executing the `list | grep –i hadoop` command
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下`list | grep –i hadoop`命令来验证安装：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Accessing Apache Hadoop from Karaf
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Karaf访问Apache Hadoop
- en: In Hadoop, the core of a cluster is the distributed and replicated filesystem.
    We have HDFS running and can access it from our command line window as a regular
    user. Actually, getting to it from an OSGi container will prove to be slightly
    more complicated than just writing the Java components.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop中，集群的核心是分布式和复制的文件系统。我们已经运行了HDFS，并且可以从我们的命令行窗口以普通用户身份访问它。实际上，从OSGi容器访问它将比仅仅编写Java组件要复杂一些。
- en: Hadoop requires us to provide configuration metadata for our cluster that can
    be looked up as file or classpath resources. In this recipe, we will simply copy
    the HDFS site-specific files we created earlier in the chapter to our `src/main/resources`
    folder.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop要求我们为我们的集群提供配置元数据，这些元数据可以作为文件或类路径资源进行查找。在本食谱中，我们将简单地复制我们在本章早期创建的HDFS特定文件到我们的`src/main/resources`文件夹中。
- en: We will also include the default metadata definitions into our resources by
    copying them from a dependency, and finally, we'll allow our bundle classloader
    to perform fully dynamic class loading. To sum it up, we have to copy the `core-site.xml`,
    `hdfs-site.xml`, and `mapred-site.xml` files into our classpath. These files together
    describe to our client how to access HDFS.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过从依赖项中复制默认元数据定义，将它们包含到我们的资源中，最后，我们将允许我们的包类加载器执行完全动态的类加载。总结一下，我们必须将`core-site.xml`、`hdfs-site.xml`和`mapred-site.xml`文件复制到我们的类路径中。这些文件共同描述了我们的客户端如何访问HDFS。
- en: As we get to the code, there is also a step we'll perform to trick the Hadoop
    classloaders into utilizing our bundle classloader as well as respecting the configuration
    data by providing specific implementations.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们到达代码时，我们还将执行一个步骤来欺骗Hadoop类加载器利用我们的包类加载器，并通过提供特定的实现来尊重配置数据。
- en: How to do it…
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: The first thing we'll do is make sure that the necessary defaults are copied
    into our tutorial bundle.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要确保必要的默认值被复制到我们的教程包中。
- en: 'First, we will modify the Felix bundle plugin and add the following segment:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将修改Felix包插件并添加以下段：
- en: '[PRE9]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will add another section for dynamic loading of classes as they are
    needed. This isn''t necessarily best practice in OSGi, but sometimes, it is one
    of the few possible ways of getting bundles and JAR files not intended for OSGi
    to work. We do this by adding another little snippet to the Felix bundle plugin
    as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为按需动态加载类添加另一个部分。这并不一定是OSGi中的最佳实践，但有时，这是实现非OSGi设计意图的包和JAR文件工作的几种可能方法之一。我们通过向Felix包插件添加另一个小片段来实现这一点，如下所示：
- en: '[PRE10]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With this addition, we tell our bundle that if you need a class at a later time,
    just go look it up when we need it. It is a tad more costly and certainly not
    recommended as a general practice as it forces the bundle to scan classpaths.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这个添加，我们告诉我们的包，如果您以后需要某个类，只需在我们需要时查找即可。这稍微有点成本，并且绝对不推荐作为一般做法，因为它迫使包扫描类路径。
- en: Finally, we pull two more tricks out of our hat inside our implementation code.
    We do this as the Hadoop code is multithreaded, and by default, the classloader
    for a new thread is the system classloader. In an OSGi context, the system classloader
    is going to have very limited visibility.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在实现代码中使用了两个额外的技巧。我们这样做是因为Hadoop代码是多线程的，默认情况下，新线程的类加载器是系统类加载器。在OSGi环境中，系统类加载器将具有非常有限的可见性。
- en: 'First, we replace the `ThreadContextClassLoader` class as follows:'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们按照以下方式替换`ThreadContextClassLoader`类：
- en: '[PRE11]'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Secondly, as we are using Maven and as Hadoop unfortunately reuses the same
    file for SPI depending on your implementation, we cannot simply copy resources.
    Our aggregate JAR file and any dependencies we import will be overwritten with
    the last imported version.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，由于我们正在使用Maven，并且不幸的是，Hadoop根据您的实现重复使用相同的文件用于SPI，我们无法简单地复制资源。我们的聚合JAR文件以及我们导入的任何依赖项都将被最后导入的版本覆盖。
- en: 'To get around this, we explicitly tell our `Configuration` object which implementations
    to use when accessing our cluster. This is shown in the following code:'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们明确告诉我们的`Configuration`对象在访问我们的集群时应使用哪些实现。这在下述代码中显示：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With the preceding actions, we have satisfied all the classloading and instantiation
    issues. Now, we are actually ready to access HDFS remotely. However, we still
    haven't forced our bundle to import and export all of Hadoop, but we can fairly
    easily change versions and we can externalize the static XML files that define
    the cluster if needed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面的操作，我们已经满足了所有类加载和实例化问题。现在，我们实际上已经准备好远程访问HDFS。然而，我们还没有强迫我们的包导入和导出所有的Hadoop，但我们可以相当容易地更改版本，如果需要，我们还可以外部化定义集群的静态XML文件。
- en: How it works
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: We have basically tricked our bundle into being able to imitate a single monolithic
    classloader in a normal classpath-provided JVM. We did this by creating a bundle
    that dynamically imports resources it needs from the Apache Hadoop bundle, and
    we have ensured that our bundle can access all the necessary configuration resources.
    We also adjusted classloading so that the Hadoop code base uses our bundle classloader
    to instantiate new instances.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上欺骗了我们的包，使其能够在一个正常的类路径提供的JVM中模仿一个单一的单一类加载器。我们通过创建一个动态从Apache Hadoop包导入所需资源的包来实现这一点，并确保我们的包可以访问所有必要的配置资源。我们还调整了类加载，以便Hadoop代码库使用我们的包类加载器来实例化新实例。
- en: Adding commands that talk to HDFS for deployment in Karaf
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加与HDFS通信的命令以在Karaf中部署
- en: As HDFS at its core is a filesystem, let's see how we can access that with the
    standard tools and the bundle we've been building up so far.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HDFS本质上是一个文件系统，让我们看看我们如何使用标准工具和我们迄今为止构建的包来访问它。
- en: What we'll do is store one level of configuration files from our running Karaf
    container into HDFS. Then, we'll provide a second command to read the files back.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做的是将运行中的Karaf容器中的一级配置文件存储到HDFS中。然后，我们将提供第二个命令来读取这些文件。
- en: We've learned how to build a feature for Hadoop that takes care of all the various
    dependencies needed to talk to HDFS, and we have also jumped a little bit ahead
    and discussed classloading and a few tricks to get the Hadoop libraries we deployed
    to cooperate. We are now at a point where we can start writing code against Hadoop
    using the libraries provided.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何构建一个针对Hadoop的功能，该功能负责处理与HDFS通信所需的所有各种依赖项，我们还稍微提前了一点，讨论了类加载和一些技巧，以便我们部署的Hadoop库能够协作。我们现在可以开始使用提供的库编写针对Hadoop的代码了。
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The ingredients of this recipe include the Apache Karaf distribution kit, access
    to JDK, and Internet connectivity. The sample code for this recipe is available
    at [https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1](https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1).
    Remember, you need both the drivers installed and Apache Hadoop's HDFS running
    for these recipes to work!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方所需的原料包括Apache Karaf发行套件、对JDK的访问和互联网连接。这个配方的示例代码可在[https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1](https://github.com/jgoodyear/ApacheKarafCookbook/tree/master/chapter9/chapter-9-recipe1)找到。记住，你需要安装驱动程序，并且Apache
    Hadoop的HDFS必须运行，这些配方才能工作！
- en: How to do it...
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Building a project that can access Hadoop will require the following steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个可以访问Hadoop的项目需要以下步骤：
- en: The first step is generating a Maven-based bundle project. Create an empty Maven-based
    project. A `pom.xml` file containing the essential Maven coordinate information
    and bundle packaging directives will suffice.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是生成基于Maven的bundle项目。创建一个空的基于Maven的项目。一个包含基本Maven坐标信息和bundle打包指令的`pom.xml`文件就足够了。
- en: 'The next step is adding dependencies to the POM file. This can be done as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将依赖项添加到POM文件中。可以按以下方式完成：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For Karaf 3.0.0, we use OSGi Version 5.0.0\. The Hadoop libraries require quite
    a few supporting bundles. The existing Camel feature was used as a starting point
    but doesn't actually work on all platforms, so we have to rewrite it to suit our
    needs.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于Karaf 3.0.0，我们使用OSGi版本5.0.0。Hadoop库需要相当多的支持包。现有的Camel功能被用作起点，但实际上并不适用于所有平台，因此我们必须重写它以满足我们的需求。
- en: 'The next step is adding build plugins. Our recipe requires only one build plugin
    to be configured, which is the bundle. We configure the `maven-bundle-plugin`
    to assemble our project code into an OSGi bundle. We add the following plugin
    configuration to our POM file:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是添加构建插件。我们的配方只需要配置一个构建插件，即bundle插件。我们配置`maven-bundle-plugin`将我们的项目代码组装成一个OSGi包。我们在POM文件中添加以下插件配置：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The Felix and Karaf imports are required by the optional Karaf commands. We
    are starting to get a bit more of a complicated bundle plugin as we are enabling
    dynamic classloading and copying resources around so that they are available to
    our classloader.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Felix和Karaf导入是可选的Karaf命令所必需的。随着我们启用动态类加载并在类加载器周围复制资源，以便它们可用，我们正在开始得到一个更复杂的bundle插件。
- en: 'The next step is creating a Blueprint descriptor file. Create the `src/main/resources/OSGI-INF/blueprint`
    directory tree in your project. We''ll then create a file named `blueprint.xml`
    in this folder. Consider the following code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建一个Blueprint描述符文件。在项目中创建`src/main/resources/OSGI-INF/blueprint`目录树。然后，我们将在这个文件夹中创建一个名为`blueprint.xml`的文件。考虑以下代码：
- en: '[PRE15]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is developing an OSGi service with a new Hadoop backend. We''ve
    created the basic project structure and plumbed in configurations for Blueprint
    descriptors. Now, we''ll focus on the underlying Java code of our Hadoop-backed
    application. We break this process down into two steps: defining a service interface
    and providing a concrete implementation.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是开发一个具有新Hadoop后端的OSGi服务。我们已经创建了基本的项目结构，并配置了Blueprint描述符的配置。现在，我们将专注于我们Hadoop后端应用程序的底层Java代码。我们将这个过程分解为两个步骤：定义服务接口和提供具体的实现。
- en: 'First, we define a service interface. The service interface will define the
    user API to our project. In our sample code, we implement the `HdfsConfigService`
    interface, which provides the methods required to store and retrieve the configuration
    files we have in our Karaf instance. This can be done as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义一个服务接口。服务接口将定义我们项目的用户API。在我们的示例代码中，我们实现了`HdfsConfigService`接口，它提供了存储和检索我们Karaf实例中配置文件所需的方法。这可以按以下方式完成：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The interface's implementation follows standard Java conventions, requiring
    no special OSGi packages.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接口的实现遵循标准的Java约定，不需要特殊的OSGi包。
- en: 'Next, we implement our communication with HDFS. Now that we have defined our
    service interface, we''ll provide an implementation as two calls to store and
    retrieve the file data from HDFS. This can be done as follows:'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现与HDFS的通信。现在我们已经定义了我们的服务接口，我们将通过两个调用提供实现，以将文件数据存储和检索到HDFS。这可以通过以下方式完成：
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The next step is the optional creation of Karaf commands to directly test the
    persistence service. To simplify manual testing of our `HdfsConfigService` interface,
    we can create a set of custom Karaf commands that will exercise our HDFS storage
    and retrieval operations. The sample implementations of these commands are available
    from the book''s website. Of particular interest is how they obtain a reference
    to the `HdfsConfigService` interface and make calls to the service. We must wire
    the command implementation into Karaf via Blueprint. This can be done as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是可选地创建Karaf命令以直接测试持久化服务。为了简化对`HdfsConfigService`接口的手动测试，我们可以创建一组自定义Karaf命令，这些命令将测试我们的HDFS存储和检索操作。这些命令的示例实现可以在本书的网站上找到。特别值得注意的是，它们如何获取`HdfsConfigService`接口的引用并调用该服务。我们必须通过Blueprint将命令实现连接到Karaf。这可以通过以下方式完成：
- en: '[PRE18]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Each of our custom command's implementation classes are wired to our `hdfsConfigService`
    instance.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们每个自定义命令的实现类都连接到我们的`hdfsConfigService`实例。
- en: The next step is deploying the project into Karaf.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是将项目部署到Karaf中。
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This demo will require a running Hadoop cluster!
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个演示需要一个正在运行的Hadoop集群！
- en: 'We make sure that all the Hadoop bundles are installed correctly. This can
    be done using the following commands:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们确保所有Hadoop捆绑包都已正确安装。这可以通过以下命令完成：
- en: '[PRE19]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We install our project bundle by executing the `install` command on its Maven
    coordinates:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过在其Maven坐标上执行`install`命令来安装我们的项目捆绑包：
- en: '[PRE20]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The last step is testing the project. We can use the following commands for
    this:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是测试项目。我们可以使用以下命令来完成这项工作：
- en: '[PRE21]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works…
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We have a Karaf container now communicating with an external HDFS filesystem.
    We can back up configuration files from Karaf to HDFS and we can read them back.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个与外部HDFS文件系统通信的Karaf容器。我们可以从Karaf备份配置文件到HDFS，并且可以读取它们。
- en: A new Karaf instance can be started to consume and copy these configurations
    or we can use this recipe as the basis for starting up MapReduce jobs, tasks,
    and batch jobs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可以启动一个新的Karaf实例来消费和复制这些配置，或者我们可以使用这个配方作为启动MapReduce作业、任务和批处理作业的基础。
