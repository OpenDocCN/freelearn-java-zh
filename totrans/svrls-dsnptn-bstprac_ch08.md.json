["```java\nlorem = \"\"\"\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam ac pulvinar mi. Proin nec mollis\ntellus. In neque risus, rhoncus nec tellus eu, laoreet faucibus eros. Ut malesuada dui vel ipsum\n...\nvenenatis ullamcorper ex sed eleifend. Nam nec pharetra elit.\n\"\"\"\n\nwords = (w.strip() for w in lorem.split())\n\ndef mapper(word):\n    return (word, 1)\n\ndef reducer(mapper_results):\n    results = {}\n    for (word, count) in mapper_results:\n        if word in results:\n            results[word] += count\n        else:\n            results[word] = count\n    return results\n\nmapper_results = map(mapper, words)\nreducer_results = reducer(mapper_results)\nprint(reducer_results)\n```", "```java\n{\n  'Lorem': 1, \n  'ipsum': 4, \n  'dolor': 3,\n  'sit': 9, \n  'amet,': 1,\n   ...\n}\n```", "```java\ndef mapper(word):\n    if word == 'amet':\n        return (word, 10)\n    else:\n        return (word, 1)\n```", "```java\ndef mapper(word):\n    return len(word)\n\ndef reducer(mapper_results):\n    results = list(mapper_results)\n    total_words = len(results)\n    total_len = sum(results)\n    return total_len / total_words\n```", "```java\nimport time\nimport uuid\n\nfrom .aws import (\n        list_s3_bucket,\n        publish_to_sns,\n)\n\ndef crawl(bucket_name, prefix=''):\n    \"\"\"Entrypoint for a map-reduce job.\n\n    The function is responsible for crawling a particular S3 bucket and \n    publishing map jobs\n    asyncrhonously using SNS where the mapping is 1-to-1, file-to-sns.\n\n    It's presumed that lambda mapper functions are hooked up to the SNS \n    topic. These Lambda\n    mappers will each work on a particular file.\n\n    \"\"\"\n    print('Starting at: %s: %s' % (time.time(), time.asctime(), ))\n\n    # Unique identifer for the entire map-reduce run\n    run_id = str(uuid.uuid4())\n    mapper_data = [\n            {\n                'bucket': bucket,\n                'job_id': str(uuid.uuid4()),\n                'key': key,\n                'run_id': run_id,\n            } for (bucket, key) in list_s3_bucket(bucket_name, prefix)\n    ]\n\n    # Let's add in the total number of jobs which will be kicked off.\n    num_mappers = len(mapper_data)\n\n    for i, mapper_dict in enumerate(mapper_data):\n        mapper_dict['total_jobs'] = num_mappers\n        mapper_dict['job_id'] = i\n        publish_to_sns(mapper_dict)\n```", "```java\nimport csv\nimport itertools\nimport json\nimport os\nimport sys\nimport time\n\nimport email.parser\n\n# Make sure we can read big csv files\ncsv.field_size_limit(sys.maxsize)\n\nfrom .aws import (\n        download_from_s3,\n        write_csv_to_s3,\n)\n\ndef _csv_lines_from_filepath(filepath, delete=True):\n    with open(filepath, 'rt') as fh:\n        reader = csv.DictReader(fh, fieldnames=('file', 'message'))\n        for row in reader:\n            yield row\n\n    if delete:\n        os.remove(filepath)\n\ndef map(event):\n    message = json.loads(event['Records'][0]['Sns']['Message'])\n\n    total_jobs = message['total_jobs']\n    run_id = message['run_id']\n    job_id = message['job_id']\n\n    counts = {}\n\n    bucket = os.environ['REDUCE_RESULTS_BUCKET']\n\n    tmp_file = download_from_s3(message['bucket'], message['key'])\n\n    parser = email.parser.Parser()\n\n    for line in _csv_lines_from_filepath(tmp_file):\n        msg = line['message']\n        eml = parser.parsestr(msg, headersonly=True)\n        _from = eml['From']\n\n        _tos = eml.get('To')\n        if not _tos:\n            continue\n\n        _tos = (t.strip() for t in _tos.split(','))\n\n        for from_to in itertools.product([_from], _tos):\n            if from_to not in counts:\n                counts[from_to] = 1\n            else:\n                counts[from_to] += 1\n\n    if not counts:\n        return\n\n    metadata = {\n            'job_id': str(job_id),\n            'run_id': str(run_id),\n            'total_jobs': str(total_jobs),\n    }\n\n    key = 'run-%s/mapper-%s-done.csv' % (run_id, job_id)\n    write_csv_to_s3(bucket, key, counts, Metadata=metadata)\n```", "```java\nfunctions:\n  Reducer:\n    handler: handler.reducer\n    events:\n      - s3:\n          bucket: brianz-${env:ENV}-mapreduce-results\n          event: s3:ObjectCreated:*\n          rules:\n            - suffix: '-done.csv'\n```", "```java\nimport csv\nimport json\nimport time\nimport os\nimport uuid\nimport io\n\nfrom .aws import (\n        download_from_s3,\n        list_s3_bucket,\n        read_from_s3,\n        s3_file_exists,\n        write_to_s3,\n        write_csv_to_s3,\n)\n\ndef _get_final_results_key(run_id):\n    return 'run-%s/FinalResults.csv' % (run_id, )\n\ndef _get_batch_job_prefix(run_id):\n    return 'run-%s/mapper-' % (run_id, )\n\ndef _get_job_metadata(event):\n    s3_record = event['Records'][0]['s3']\n    bucket = s3_record['bucket']['name']\n    key = s3_record['object']['key']\n\n    s3_obj = read_from_s3(bucket, key)\n    job_metadata = s3_obj['Metadata']\n\n    run_id = job_metadata['run_id']\n    total_jobs = int(job_metadata['total_jobs'])\n    return (bucket, run_id, total_jobs)\n\ndef reduce(event):\n    bucket, run_id, total_jobs = _get_job_metadata(event)\n\n    # count up all of the final done files and make sure they equal the \n    total number of mapper jobs\n    prefix = _get_batch_job_prefix(run_id)\n    final_files = [\n            (bucket, key) for (_, key) in \\\n            list_s3_bucket(bucket, prefix) \\\n            if key.endswith('-done.csv')\n    ]\n    if len(final_files) != total_jobs:\n        print(\n            'Reducers are still running...skipping. Expected %d done \n             files but found %s' % (\n                total_jobs, len(final_files),\n            )\n        )\n        return\n\n    # Let's put a lock file here so we can claim that we're finishing \n    up the final reduce step\n    final_results_key = _get_final_results_key(run_id)\n    if s3_file_exists(bucket, final_results_key):\n        print('Skipping final reduce step')\n        return\n\n    # write blank file to lock the final reduce step\n    write_to_s3(bucket, final_results_key, {})\n\n    print('Starting final reduce phase')\n\n    s3_mapper_files = list_s3_bucket(bucket, prefix)\n\n    final_results = {}\n\n    for (bucket, key) in s3_mapper_files:\n        print('reading', key)\n\n        tmp_fn = download_from_s3(bucket, key)\n\n        with open(tmp_fn, 'r') as csv_fh:\n            reader = csv.DictReader(csv_fh, fieldnames=('key', \n            'count'))\n            for line in reader:\n                key = line['key']\n                count = int(line['count'])\n\n                if key in final_results:\n                    final_results[key] += count\n                else:\n                    final_results[key] = count\n\n    print('Final final_results:', len(final_results))\n    print('Writing fiinal output data')\n    write_csv_to_s3(bucket, final_results_key, final_results)\n```", "```java\n$ head FinalResults.csv \n\"('phillip.allen@enron.com', 'tim.belden@enron.com')\",31\n\"('phillip.allen@enron.com', 'john.lavorato@enron.com')\",63\n\"('phillip.allen@enron.com', 'leah.arsdall@enron.com')\",3\n\"('phillip.allen@enron.com', 'randall.gay@enron.com')\",23\n\"('phillip.allen@enron.com', 'greg.piper@enron.com')\",6\n\"('phillip.allen@enron.com', 'david.l.johnson@enron.com')\",4\n\"('phillip.allen@enron.com', 'john.shafer@enron.com')\",4\n\"('phillip.allen@enron.com', 'joyce.teixeira@enron.com')\",3\n\"('phillip.allen@enron.com', 'mark.scott@enron.com')\",3\n\"('phillip.allen@enron.com', 'zimam@enron.com')\",4\n```", "```java\nresults = {}\n\nfor key, value in input_data.items():\n    if key not in results:\n        results[key] = [value]\n    else:\n        results[key].append(value)\n\nr = redis.StrictRedis()\n\n# Use a pipeline to ensure we don't hit a race condition\np = r.pipeline()\nfor key, values in results.items():\n    p.lpush(key, *values)\np.execute()\n```", "```java\nl = len(r.lrange('height', 0, -1))\navg_height = sum((float(i) for i in r.lrange('height', 0, -1))) / l\n```"]