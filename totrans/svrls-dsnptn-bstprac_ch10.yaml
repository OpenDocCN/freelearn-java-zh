- en: Error Handling and Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless architectures are different enough that techniques and best practices
    need to be thought through and evaluated to be successful. Many traditional methods
    for debugging, application development, monitoring, and so on are still applicable
    in a server-based architecture. However, many tried-and-tested techniques that
    you may rely on when working with virtual machines or real hardware will not necessarily
    work with serverless systems. When building on top of a FaaS platform, then, you
    need to keep these differences in mind and have a plan for monitoring, debugging,
    testing, and developing your serverless application.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll review common best practices that will help you to focus
    on building your application rather than getting stuck in the details of organization
    or deployment. We'll also cover the tools and methods available for keeping your
    serverless application secure, easy to develop locally, and observable. We'll
    also discuss the changes you will need to make to track errors and monitor serverless
    applications reliably.
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting here that one could dedicate an entire book to best practices
    for serverless applications. This chapter is not exhaustive by any means, but
    it will cover many topics which will definitely help to improve your serverless
    development experience and the overall quality of your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you can expect to understand the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to set up your application to track unexpected errors with Sentry and Rollbar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with cold starts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and alerting around errors, exceptions, or performance degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local development and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to manage configuration via environment variables across different stacks
    (development versus production)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to encrypt sensitive environment variables to keep your application secure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practically speaking, all software systems crash at some point. One of the reasons
    I love working with serverless systems so much is that they, by their very nature,
    keep an application relatively small and more akin to a microservice, rather than
    a broad monolithic application. This fact by itself can drastically reduce the
    number of ways an application can fail. However, at some point, it will fail,
    and an exception you didn't expect will occur. What, then, is the best way to
    handle unexpected exceptions in a serverless system?
  prefs: []
  type: TYPE_NORMAL
- en: The good news here is that we have multiple options, and that some systems you
    may already be familiar with can work in the same way as they would in a non-serverless
    system. In the following sections, we'll walk through the steps for integrating
    two popular error tracking services, Sentry and Rollbar. Both services offer similar
    functionality and are equally easy to set up. In the following examples, we'll
    be using Python, but both Sentry and Rollbar support a myriad of languages including
    Node.js, Java, Golang, and C#.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Sentry for error tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have used Sentry for many years and highly recommend it. It has a vast feature
    set and many prominent companies use the service. Sentry's free plan gives you
    10,000 events per month, a single login, and a seven-day history of quickly. Whether
    it's for a hobby project or even a medium-scale production system, this free plan
    works out quite well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate Sentry with your serverless function, you''ll, of course, need
    a Sentry account. Following is a code block for an elementary AWS Lambda function.
    All it will do is calculate the quotient of two numbers. Our goal is to ensure
    that any unhandled exceptions are captured and reported somewhere so that we have
    visibility into what our application is doing and have as much information as
    possible with which to debug it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ve set this up with API Gateway, so we can execute it using `curl` and
    get results for two numbers, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what happens when we divide this by `0`, which we know is undefined
    in mathematics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As an application developer, I have no way of knowing that this error has occurred
    as there is no monitoring in place. The only way I can know that an error has
    happened is if I log into the AWS console and look at the execution metrics for
    my Lambda function, or if I happen to be reading through the CloudWatch Logs.
    Of course, you can''t be manually watching for errors day and night. The following
    screenshot shows Invocation errors from the AWS Lambda monitoring page for the `divide`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9bfc0e4-0e6f-4ec6-9acb-42b4e03bca55.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS CloudWatch chart from the Lambda screen, showing a count of the number of
    errors
  prefs: []
  type: TYPE_NORMAL
- en: 'CloudWatch will capture `stdout` and `stderr` for Lambda functions. Because
    unhandled exceptions are written to `stderr`, we can see the details when looking
    at the CloudWatch logs, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/893e8ebd-3c60-4132-adbc-7478e7454366.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS CloudWatch logs, showing an unhandled exception due to division by zero
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrating Sentry will capture unexpected errors, store them, and notify us
    via various delivery mechanisms. Getting Sentry reporting for our Lambda functions
    is quite easy. For Python, you can use the `raven-python-lambda` ([https://github.com/Netflix-Skunkworks/raven-python-lambda](https://github.com/Netflix-Skunkworks/raven-python-lambda))
    library and add a decorator around handler functions, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The only other bit of configuration we need to take care of here is setting
    the Sentry DSN, which tells the library where to send its payload when an error
    occurs. Doing this is it''s just a matter of passing the values from the host
    system''s environment variables into the Lambda function''s environment variables.
    Using the Serverless Framework, this is quite easy, as you can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now Sentry will capture any unhandled exceptions and, at a minimum, email them
    to us. In the following screenshot, you can see a list of various exceptions.
    What is neat is that some of these errors were not even deliberate. As you can
    see in the last row, I misspelled a variable name which caused my division calculation
    to throw an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eb588c9-6f0c-4ab4-9a3c-9fd7972095f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clicking on any of these errors gives us more context into the state of our
    application when the exception was triggered, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e17540fc-b1e4-4260-8077-10f423973e56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another lovely feature of the `raven-python-lambda` library is that it will
    raise warnings when your function is getting too close to its timeout value or
    its maximum allocated memory. To test this, we need to set the timeout of the
    `divide` function to 4 seconds and put `time.sleep(3)` in the middle of the application
    code. After executing the divide Lambda function, you should get a result as expected.
    You will also receive an email about the slow execution speed and see the same
    warning on the Sentry website, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca4319f3-7be6-4f5f-9b9f-f5cd85f49138.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is much more information included with each exception to help you while
    debugging on the Sentry website. There are also many more features in Sentry that
    we don''t have room for here; however, a few features worth noting are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Chat integration (Slack, IRC, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issue rollup and status tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating Rollbar
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rollbar plays the same role as Sentry. Integration is equally as simple. We
    will still use a decorator for our handler functions, but we'll need a different
    library with Rollbar. Rollbar provides an official library for Python ([https://github.com/rollbar/pyrollbar](https://github.com/rollbar/pyrollbar))
    and many other languages.
  prefs: []
  type: TYPE_NORMAL
- en: The setup changes slightly with `pyrollbar`, but it's nothing too complicated.
    The following code block shows how to set up an AWS Lambda function for error
    tracking with Rollbar.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: When we hit an exception, information is delivered to Rollbar. Again, an email
    notification about the `ZeroDivisionError` should be received. Just like Sentry,
    there are plenty of integrations from Rollbar.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows error details displayed on the Rollbar website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d90ac957-6dfc-43b4-8f62-aeffa0946987.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracking exceptions and problems within your application is critical; however,
    there will inevitably be cases where you wish you had more insight into the state
    of your application when a problem occurs. For this task, you will need to set
    yourself up with a good logging strategy. Log messages are a tool we have used
    for a very long time - and still use often. Very often, log messages are sent
    to files on disk and then shipped off to a log aggregator. Since we don't have
    access to these same types of logging system in a serverless architecture, we'll
    need to come up with something new.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda functions and other FaaS providers offer some mechanisms for keeping
    track of `stdout` and `stderr` streams. In the case of Lambda, any `print` statements
    or other error messages will end up in CloudWatch Logs. This delivery to CloudWatch
    happens automatically, and is especially useful as you'll always know where to
    go to check for errors or debugging statements. While this is a helpful feature,
    there are a few improvements we can make to your logging statements so that they're
    easier to search through, find, and categorize.
  prefs: []
  type: TYPE_NORMAL
- en: Structuring log messages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log messages are often used as plain strings via `console.log` or `print` statements.
    These quick and dirty additions to code can be helpful during development but
    won't suffice in a production-level system. Rather than logging flat strings,
    log statements need to be structured so that you can easily find the bits of information
    you're looking for. JSON is an excellent choice for a format since it's widely
    used among different log aggregator services and easy to implement in practically
    any language.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the simple case of our previous divide function. At some point,
    we may want to understand how people are using our service: specifically, what
    numerator and denominators they''re sending us. To do this, we will need to structure
    some log messages so that we can quickly search through them, pull out the pertinent
    information, and finally analyze it. The following code block shows some Python
    code to bootstrap the `structlog` library. This library will take care of logging
    structured messages, rather than the flat messages we usually get from the standard
    library''s `logging` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In our divide function, we can now log any data we find interesting as key-value
    pairs, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Log messages will now arrive in CloudWatch as JSON-formatted objects rather
    than Python strings, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/423f0ced-92b0-4a3c-aaec-b420d60f7aef.png)'
  prefs: []
  type: TYPE_IMG
- en: This has merely set us up for success; next, we'll work on getting these structured
    log messages to a log aggregator for better discoverability, analysis, and integration
    with other services.
  prefs: []
  type: TYPE_NORMAL
- en: Digesting structured logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, there is a myriad of dedicated logging services, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Loggly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sumo Logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splunk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papertrail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosted ELK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list could go on and on. Many of these hosted services may be more accessible,
    and possibly more powerful, when you send them JSON messages. I have used Loggly
    in different applications and know that this is the case.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to ship logs to Loggly, and likely for other services. However,
    shipping logs can change when the destination is somewhere other than your FaaS
    provider. CloudWatch logging is built-in to AWS Lamba and offers free performance,
    so how can we get these same logs out to an external service?
  prefs: []
  type: TYPE_NORMAL
- en: With AWS CloudWatch, it's possible to trigger another Lambda function when new
    log messages arrive. That may seem a bit odd, but it's a great trick to keep your
    application lean and decoupled from any logging service while also solving the
    problem of getting your log message to a more powerful service. We won't go into
    all the details on how to set this up here, but there is detailed documentation
    available on the Loggly site: [https://www.loggly.com/docs/cloudwatch-logs/](https://www.loggly.com/docs/cloudwatch-logs/).
  prefs: []
  type: TYPE_NORMAL
- en: This pattern is not unique to Loggly in any way. If you are using another logging
    service and wish to follow the same pattern, it's merely a matter of implementing
    a Lambda function, which is then triggered by CloudWatch events and sent away
    to your logging provider of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have JSON messages arriving at your logging provider, you have many
    more options in terms of data analysis and discovery. Being able to quickly and
    easily find information when a problem occurs is critical for any production-level
    system, serverless or not. Regardless of which FaaS or logging service you're
    using, just make sure that you can easily find the data you need when it's time.
  prefs: []
  type: TYPE_NORMAL
- en: Cold starts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One commonality between most, if not all, FaaS providers is the issue of cold
    starts. Cold starts are defined as the behavior where an invocation of a cloud
    function which has not been executed in a while takes a considerable amount of
    time to initialize before fulfilling the request. If you have used Docker, for
    example, you'll know that creating a new container from an existing image takes
    slightly longer than starting up a container you have previously run. This Docker
    container behavior is analogous to the way cloud functions, whether it be AWS
    Lambda, Google Cloud Functions, or Azure Cloud Functions, behave. If you do any
    searching around the internet for serverless cold starts, you'll find several
    blog posts and documentation on the matter.
  prefs: []
  type: TYPE_NORMAL
- en: There isn't a silver bullet for bypassing the cold start issue. However, there
    are several things to be aware of so that you can minimize their impact on your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping cloud functions warm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several tips and tricks you can employ to work around cold starts.
    The most common recommendation is to implement another function on a timer, say
    every 5 minutes, which then invokes your target function. With this pattern, the
    target function is always kept warm, which means it can fulfill a legitimate request
    more quickly. This can be a useful trick; however, it does not always solve the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, cloud functions will scale automatically. Think back to some of the
    patterns in this book, specifically the Fanout and MapReduce patterns. In those
    examples, multiple instances of our functions were being executed concurrently.
    In the case of our Fanout pattern for image resizing, a single invocation of our
    initial Lambda function would result in three concurrent image resizing functions.
    If we had a `ping` function to keep the resizing function active, we would have
    a single function warm and ready to process that resizing task. However, when
    three simultaneous invocations occur, a single `ping` function will not help.
    In this scenario, a single resize function would be warm, but the other two would
    pay the cold start cost. If we changed our application to resize an image into
    five different sizes, we would then have four different functions that would start
    from a *cold* state.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda functions and VPCs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are using AWS, keep in mind that cold starts are much worse when running
    Lambda functions inside a VPC. The reason for this is that Lambda functions in
    a VPC are allocated an **Elastic Network Interface** (**ENI**) to access VPC resources
    such as databases. If you ran the example code [Chapter 2, ](svrls-dsnptn-bstprac_ch02.html)*A
    Three-Tier Web Application Using REST* and [Chapter 3, ](svrls-dsnptn-bstprac_ch03.html)*A
    Three-Tier Web Application Pattern with GraphQL*, you may have noticed that the
    first API call took several seconds. This initial lag is mainly because the Lambda
    functions needed access to the RDS instance inside of a VPC, which means the Lambda
    functions themselves are required to be inside the same VPC.
  prefs: []
  type: TYPE_NORMAL
- en: If at all possible, avoid putting Lambda functions inside a VPC. If your functions
    do not rely on any external resources, or non-VPC resources such as DynamoDB,
    do not put them inside of a VPC. However, if you do need access to VPC resources,
    there aren't many options available. If you are running an API that is talking
    to a VPC resource such as an RDS instance, you could run a `pinger` function,
    but we advise raising the concurrency from 1 to something like 10\. In this case,
    you would then have at least 10 functions always warmed up and ready to serve
    traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Start-up times for different languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each supported language comes with its unique behavior in cloud functions. We
    have not thorough profiled all the different FaaS platforms and languages in this
    book, but we do know that it has been reported that Node.js and Python have lower
    cold start times compared with Java and C# on AWS. However, there are also claims
    that C# functions based on .NET Core 2.0 are significantly faster. AWS recently
    rolled out support for Golang; however, we are currently unclear on its relative
    cold start performance.
  prefs: []
  type: TYPE_NORMAL
- en: I may be a bit biased, but I do believe using a language with a lower cold start
    time is a better choice, at least on AWS, as you can accomplish pretty much anything
    you need between Node.js and Python. Reading some of the tests people have made,
    the difference between Java or C# and other languages is two to three orders of
    magnitude; in other words, cold start times range from 1,000-4,000 ms with Java
    and C#, whereas Node.js and Python score in the range of 1-10 ms.
  prefs: []
  type: TYPE_NORMAL
- en: Allocating more memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At least in the case of AWS Lambda, allocating more memory to your functions
    can result in a faster start-up time. Just as running larger EC2 instances affords
    you more CPU power, so too does allocating more memory to your Lambda functions.
    Allocating more memory to your functions may improve performance, but note that
    this will affect your billing as Lambda functions are billed by the combination
    of execution duration and allocated memory.
  prefs: []
  type: TYPE_NORMAL
- en: You can read about CPU allocation relating to AWS Lambda in the following document: [https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html](https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html).
  prefs: []
  type: TYPE_NORMAL
- en: Local development and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One challenge we face as serverless engineers is that of convenience. To be
    more specific, it's a swift process writing code, deploying it, and beginning
    testing. Testing a live system will often result in some code or configuration
    issue, but it is quickly fixed and redeployed. The problem we face, therefore,
    is that it's so easy to fix issues and then redeploy that we can get into the
    habit of skipping testing or not running our stack locally.
  prefs: []
  type: TYPE_NORMAL
- en: Local development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One question I answer with some regularity is, *How do I run this locally?*
    When writing a server-based application, one of the first tasks to undertake is
    getting the system set up so that it can be run during development. When building
    a serverless-based application, however, there really is no server to run. So,
    how do we develop our application?
  prefs: []
  type: TYPE_NORMAL
- en: The truthful answer is that this is a challenge, and one that has not been solved
    perfectly yet; to be fair, this issue is difficult with any microservice-based
    system. So, how can we run a system and ensure it's fully functional when it's
    composed of multiple disparate pieces? My belief here is that we need to lean
    on the principles and strengths of a component-based architecture and use common
    tools that make local development and testing easier. As you write your serverless
    application, it's best to focus on the service itself and ensure via unit testing
    that it works as expected. Don't expect to run a full serverless map-reduce system
    on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a serverless web API, I rely on unit tests rather than a local
    server during development. After all, we've long been taught that unit tests are
    a better approach to development than the manual testing of an API or UI. Regardless
    of where you stand on that topic, local development of serverless systems can
    move along quite quickly when writing unit tests, and testing in these systems
    is relatively simple, as we'll cover in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read through the repository of community plugins in Serverless Framework
    here: [https://github.com/serverless/plugins](https://github.com/serverless/plugins).'
  prefs: []
  type: TYPE_NORMAL
- en: There *are* options for running a serverless application locally, as mentioned
    previously; however, I have not used these tools myself and cannot speak for the
    ease or difficulty of using them. For Serverless Framework, however, there are
    some plugins with the word offline in the name, where the commonality is that
    they all aim to help you run your application locally. Outside these plugins,
    DynamoDB has, for a long time, offered an offline version that can be run on your
    local system.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless systems are still relatively new, and the landscape is maturing and
    changing quickly. It's almost certain that vendors recognize that there are areas
    for improvements in the software development lifecycle of serverless applications;
    I would not be surprised if local development and testing became more comfortable
    in the coming years.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about testing locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concerning local development, I believe the best strategy is to set up robust
    and thorough unit tests. Unit-testing serverless applications is no different
    from testing traditional server-based applications. As long as you follow the
    mantra of keeping your serverless code separate from your business logic, it's
    quite simple to get to a very high test coverage.
  prefs: []
  type: TYPE_NORMAL
- en: But what do we do when our application relies on backing services such as databases,
    caches, and the like? Additionally, what do we do when our serverless application
    calls other services that don't exist locally, such as AWS, SNS, and so on?
  prefs: []
  type: TYPE_NORMAL
- en: My approach to common systems such as Postgres or Redis is to use Docker. If
    you look back at the unit tests from [Chapter 2](svrls-dsnptn-bstprac_ch02.html),
    [](svrls-dsnptn-bstprac_ch02.html)*A Three-Tier Web Application Using REST* and
    [Chapter 3](svrls-dsnptn-bstprac_ch03.html),  *A Three-Tier Web Application Pattern
    with GraphQL*, you will see that they rely on a PostgreSQL database. When developing
    that application, we ran a Docker image that the unit tests used.
  prefs: []
  type: TYPE_NORMAL
- en: Another method for dealing with services you cannot easily run locally or code
    which *is* focused on serverless-specific logic is the judicious use of mocks.
    Take, for example, our Messaging Pattern, where our handler function sends messages
    to SQS. To test this code, we would not want to invoke SQS as that would make
    our tests slower and they would likely end up brittle. What's better, in this
    case, is to instead mocking out the API call to SQS and simply test whether the
    request to the SQS publish function was made.
  prefs: []
  type: TYPE_NORMAL
- en: 'Likewise, when we want to test some code which is specific to our serverless
    implementation, mock can come in handy. This is best explained with an example;
    the following code block shows a single function from our REST API, at the top-level
    `handler.py` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this code has a bit more going on than delegation. Here, the
    `session_detail` function is catching various errors and setting the HTTP response
    code and message based on those exceptions, if any are raised. Testing the `handle_session_detail`
    function is simple, as it is working solely on our application and doesn't contain
    any reliance on or knowledge of AWS Lambda. However, we do need to test the handling
    of errors in `session_detail`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we use a mock object to patch the `handle_session_detail` method.
    The aim of the following code block is to trigger an `Http404` exception so that
    we can verify that the static code and error message are correct. The following
    code block  shows this unit test, where `mocker` is a fixture which comes from
    the `pytest-mock` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Testing is as much an art as it is a science, and so I cannot overstate the
    importance of testing in serverless applications. As usual, the better your test
    are, the more confident you'll be when it's time to refactor your code or deploy
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Managing different environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With most production-level applications, teams maintain multiple environments
    for different purposes. A `QA` environment may exist for the QA team to run automated
    tests, a `staging` environment may exist for the DevOps team to tests their infrastructure
    changes, and the `production` environment exists to serve live traffic. Very often,
    building and maintaining these different environments can be a full-time job.
  prefs: []
  type: TYPE_NORMAL
- en: With serverless systems, I've found that maintaining different environments
    can be much more straightforward. Some of this may come from the fact that, by
    their nature, serverless applications are inherently smaller. Writing a monolithic
    application in a serverless architecture isn't wise - or even natural. How best,
    then, can we manage and maintain different environments for serverless systems?
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, turning to tenant III of the 12-Factor App Methodology helps. This
    tenant can be found at [https://12factor.net/config](https://12factor.net/config)
    and states:'
  prefs: []
  type: TYPE_NORMAL
- en: Store config in the environment
  prefs: []
  type: TYPE_NORMAL
- en: An app’s config is everything that is likely to vary between deploys (staging,
    production, developer environments, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, I used the Serverless Framework to manage and deploy
    my systems. This framework has built-in support for environment variables, which
    we can use to our advantage to efficiently manage different systems without making
    any code changes. The following code block shows a small snippet from the `serverless.yml`
    file from [Chapter 2](svrls-dsnptn-bstprac_ch02.html), [](svrls-dsnptn-bstprac_ch02.html)*A
    Three-Tier Web Application Using REST* for the coffee cupping REST API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, any reference to `${env:NAME}` will pull the actual value for `NAME`
    from the environment. Additionally, Serverless Framework helps us to keep stacks
    separate by using the `stage` variable to name resources. Whenever deploying code,
    the deployment step includes the stage variable, which we also pull out of the
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: These two techniques combined mean that deploying a `dev` stack or `qa` stack
    is just a matter of loading different environment variables. You can load environment
    variables from a file with tools such as `dotenv`, your shell script, or some
    other tool. My technique uses Docker and a Makefile to load up different variables
    based on the `ENV` I wish to work with. The result is the same, regardless of
    how you solve the problem of variable management. If you can quickly change variables,
    you can easily switch between managing completely different stacks. Remember,
    if you're using the Serverless Framework you will also need to handle the `ENV`
    setting. This variable is a single setting which will control the stack that is
    updated during any deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Securing sensitive configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, and in the previous section about managing environments,
    we've relied heavily on environment variables. One very nice feature of pulling
    a configuration from the environment is that sensitive information never needs
    to be checked into the source control. All of our application code and any framework
    code (such as the Serverless Framework) can look up variable values from the environment
    when needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configuration via environment variables is all well and good, but our usage
    of these variables is not perfect. The problem with our usage of environment variables
    and Lambda is that the data pulled from the deployment environment is uploaded
    and stored in AWS Lambda functions as plain text. For example, take a look at
    `serverless.yml` from the previous section about error handling using either Sentry
    or Rollbar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The keys under the environment key are all set on the AWS Lambda functions.
    While we never check the values of those variables into source control, they persist
    inside AWS. In this case, our `SENTRY_DSN` and `ROLLBAR` values should not be
    shared with anyone. However, if you're working in a team environment, anyone with
    access to the AWS Lambda console can very easily peek inside your Lambda functions
    and see the values for any of these variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9f8b4a3-1619-4fd4-ad8e-b74fe0074b50.png)'
  prefs: []
  type: TYPE_IMG
- en: Encrypting variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fix this, we can leverage another AWS service called Key Management Service
    (KMS). KMS works by encrypting data into a string that can only be decrypted using
    KMS itself. What's nice about using KMS is that you can then store, share, or
    even check into source control your encrypted variables, since nobody can decrypt
    them unless they have access to KMS. Your one attack vector here then becomes
    AWS and KMS itself. If anyone has permission to use your KMS key or can gain access
    to a privileged AWS account, they can decrypt any KMS-managed variable.
  prefs: []
  type: TYPE_NORMAL
- en: Azure has something similar called Key Vault, which is something you should
    look into if building on top of Azure. I'm unaware of a similar service within
    Google Compute or other FaaS providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Encrypting data with KMS is quite simple. First, you''ll need to create a KMS
    key. Once you have a key generated you will need to copy the AWS `arn` for your
    newly created key. From there, you can use a variety of APIs to encrypt a plaintext
    string. Using the previous example, I''m going to encrypt my `DB_PASSWORD` of
    `supersecret`. The following code block shows how to encrypt a password using
    Python and the `boto3` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this code is an encrypted string, which you can share throughout
    your infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can accomplish the same task using `aws-cli`, as shown in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Decrypting variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The question, of course, is how we can use this within our application; the
    answer is the inverse of what we just did. Now that we have an encrypted variable,
    our application code will need to read that value and decrypt it. Nothing changes
    from the standpoint of using environment variables. All that has changed now is
    that our sensitive variables are no longer stored in plain text anywhere within
    AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'After setting the `DB_PASSWORD` environment variable to this new encrypted
    string and redeploying, we can verify that the Lambda console is no longer storing
    the `supersecret` password. The following screenshot shows the value for the `DB_PASSWORD`
    variable from the AWS Lambda page for my function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9b6d563-5b3f-40d6-a048-306caf139aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: In order for our Lambda functions to use KMS to decrypt this data, we need to
    authorize it explicitly. To accomplish this, let's add an IAM permission in `serverless.yml`.
    In the following snippet, `KMs_KEY_ARN` is referencing the KMS `arn` , as explained
    previously. This value can also be stored as an environment variable which, going
    back to the section on managing different environments, lets us quickly switch
    between different stacks where we'd otherwise be using different KMS keys.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that is done, we can get the database password with a few lines of code
    to fetch the value and decrypt it into plaintext. The following code block shows
    how to decrypt the password, where the encrypted value is still being pulled out
    of the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With that, we can now use the `db_password` value to connect to the database
    as usual.
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things to take note of here. First, this adds a small bit of
    latency to your code since each call to `decrypt` is an API call to AWS; you can
    take advantage of the statefulness of warm functions and only perform the decryption
    if it hasn't already been done, using a global variable or some other application-level
    variable that can be initialized on startup. Second, once you have decrypted sensitive
    values like this, the onus is on you to not log them in plain text or otherwise
    advertise or record the plaintext values.
  prefs: []
  type: TYPE_NORMAL
- en: There are many things to consider when dealing with services such as KMS. This section
    is only a very brief introduction, and we've barely scratched the surface. I encourage
    you to read more about the subject and carefully think through how you can make
    your application as secure as you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Trimming AWS Lambda versions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This last tip is specific to AWS Lambda. You may have noticed in other chapters
    that there are the following lines in the `serverless.yml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By default, each time you deploy a new version of an AWS Lambda function, AWS
    will help out by keeping the old version around. In a development system where
    you may be deploying dozens of times a day, this can become quite wasteful and,
    as cheap as storage is, it's not unlimited. Also, in the case of a production
    system that has a lifetime of years, the cost of all the old versions can add
    up.
  prefs: []
  type: TYPE_NORMAL
- en: If you're using the Serverless Framework, there is an easy way around this.
    If you're not using the Serverless Framework, however, it would be no more than
    a day's work to write a small script to do this for you. The `serverless-prune-plugin`
    will keep only a certain number of Lambda versions for you and delete the rest.
    The number of versions to keep is configurable and trimming happens whenever you
    perform a full deployment. Additionally, you are given some nice CLI hooks to
    manually delete old versions. You can read the details about this plugin on its
    GitHub page: [https://github.com/claygregory/serverless-prune-plugin](https://github.com/claygregory/serverless-prune-plugin).
  prefs: []
  type: TYPE_NORMAL
- en: 'If I add this to my previous divide function, configure the plugin to run automatically,
    and only keep two versions around, you can guess what will happen when I redeploy.
    That configuration I just mentioned will go into a `custom` block in my `serverless.yml`,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, I''ll deploy my code to see the plugin prune the old versions for me.
    The following code block shows the output after running a full deployment, with
    some lines taken out for brevity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'I recommend always using this plugin for AWS and Serverless Framework, as versioned
    Lambda functions aren''t very useful. Another option is to simply disable function
    versioning completely. This can be accomplished by adding `versionFunctions: false`
    under the `provider` key in the `serverless.yml` file.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered general best practices when deploying serverless
    applications and error tracking. We looked at examples of how to integrate Rollbar
    and Sentry, two error tracking and reporting services, in AWS Lambda functions
    so that unexpected errors do not go unnoticed. We also discussed some strategies
    regarding application logging and methods to ensure you get the metrics and telemetry
    you need. We also addressed the issue of cold starts in cloud functions, and we
    discussed ways of working around them. From there, we walked through some techniques
    to help you with local testing and setting up serverless functions and systems.
    Finally, we reviewed the management of different environments or stacks using
    environment variables and the encryption of sensitive variables using AWS's Key
    Management Service.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for serverless applications could fill an entire book by themselves.
    We touched on many significant topics in this chapter that put you on the right
    trajectory moving forward. While this chapter cannot solve all of the challenges
    you may face in serverless application development, it does provide solutions
    to some of the most common issues and gives you the background necessary to find
    answers to your unique problems. At this point, readers should feel confident
    setting up and managing their own production-level serverless application.
  prefs: []
  type: TYPE_NORMAL
