["```java\nversion: \"3.2\"services:\n  elasticsearch:\n    container_name: es-container\n    image: docker.elastic.co/elasticsearch/\n        elasticsearch:8.7.0\n    environment:\n      - xpack.security.enabled=false\n      - \"discovery.type=single-node\"\n    networks:\n      - elk-net\n    ports:\n      - 19200:9200\n```", "```java\n  logstash:    container_name: ls-container\n    image: docker.elastic.co/logstash/logstash:8.7.0\n    environment:\n      - xpack.security.enabled=false\n    command: logstash -e 'input { tcp { port => 5001 codec\n     => \"json\" }} output { elasticsearch { hosts =>\n    \"elasticsearch:9200\" index => \"modern-api\" }}'\n    networks:\n      - elk-net\n    depends_on:\n      - elasticsearch\n    ports:\n      - 5002:5001\n```", "```java\n    kibana:      container_name: kb-container\n      image: docker.elastic.co/kibana/kibana:8.7.0\n      environment:\n        - ELASTICSEARCH_HOSTS=http://es-container:9200\n      networks:\n        - elk-net\n      depends_on:\n        - elasticsearch\n     ports:\n        - 5600:5601\nnetworks:\n  elk-net:\n    driver: bridge\n```", "```java\n$ docker-compose up –dCreating network \"chapter12_elk-net\" with driver \"bridge\"\nCreating es-container ... done\nCreating ls-container ... done\nCreating kb-container ... done\n```", "```java\n{  \"name\" : \"1bfa291e20b2\",\n  \"cluster_name\" : \"docker-cluster\",\n  \"cluster_uuid\" : \"Lua_MmozTS-grM0ZeJ5EBA\",\n  \"version\" : {\n    \"number\" : \"8.7.0\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"docker\",\n    \"build_hash\" :\n        \"09520b59b6bc1057340b55750186466ea715e30e\",\n    \"build_date\" : \"2023-03-27T16:31:09.816451435Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"9.5.0\",\n    \"minimum_wire_compatibility_version\" : \"7.17.0\",\n    \"minimum_index_compatibility_version\" : \"7.0.0\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n```", "```java\n// Don't use flag –t as it is a switch that turns on the timestamp$ docker-compose logs --tail=\"10\" elasticsearch\n$ docker-compose logs --tail=\"10\" kibana\n```", "```java\n$ docker-compose downStopping ls-container ... done\nStopping kb-container ... done\nStopping es-container ... done\nRemoving ls-container ... done\nRemoving kb-container ... done\nRemoving es-container ... done\nRemoving network chapter12_elk-net\n```", "```java\n    implementation 'net.logstash.logback:    logstash-logback-encoder:7.3'implementation 'io.micrometer:    micrometer-tracing-bridge-brave'implementation 'org.springframework.boot:    spring-boot-starter-actuator'\n    ```", "```java\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?><configuration>  <springProperty scope=\"context\"     name=\"applicationName\" source=\"spring.application         .name\"/>  <springProperty scope=\"context\"     name=\"logstashDestination\"     source=\"logstash.destination\" />  <property name=\"LOG_PATTERN\"    value=\"%d{yyyy-MM-dd HH:mm:ss.SSS}    %5p [${applicationName},%X{traceId:-},%X{spanId:-}]    ${PID:-} --- [%15.15t] %-40.40logger{39} :       %msg%n\"/>  <property name=\"LOG_FILE\" value=\"${chapter12-grpc-   server.service.logging.file:-chapter12-grpc-server-   logs}\"/>  <property name=\"LOG_DIR\" value=\"${chapter12-grpc-   server.service.logging.path:-chapter12-grpc-server-   logs}\"/>  <property name=\"SERVICE_ENV\" value=\"${service.env:-   dev}\"/>  <property name=\"LOG_BASE_PATH\"   value=\"${LOG_DIR}/${SERVICE_ENV}\"/>  <property name=\"MAX_FILE_SIZE\" value=\"${chapter12.service.   logging.rolling.maxFileSize:-  100MB}\"/><!-- other configuration has been removed for brevity -->\n    ```", "```java\n<appender name=\"STASH\"   class=\"net.logstash.logback.appender\n       .LogstashTcpSocketAppender\">\n  <destination>${logstashDestination}</destination>\n  <encoder\n  class=\"net.logstash.logback.encoder.LogstashEncoder\" />\n</appender>\n<!-- other configuration has been removed for brevity -->\n```", "```java\n<!-- other configuration has been removed for brevity --><root level=\"INFO\">\n  <appender-ref ref=\"STDOUT\"/>\n  <appender-ref ref=\"STASH\"/>\n  <appender-ref ref=\"FILE\"/>\n</root>\n```", "```java\n    spring.application.name=grpc-serverspring.main.web-application-type=nonegrpc.port=8080logstash.destination=localhost:5002management.tracing.sampling.probability=1.0\n    ```", "```java\n@Configurationpublic class Config {\n  @Bean\n  public ObservationGrpcServerInterceptor\n         interceptor(ObservationRegistry registry) {\n    return new\n           ObservationGrpcServerInterceptor(registry);\n  }\n}\n```", "```java\n    @Componentpublic class GrpcServer {  // code truncated for brevity  private final ObservationGrpcServerInterceptor         oInterceptor;  public GrpcServer(SourceService sourceService,      ChargeService chargeService,      ExceptionInterceptor      exceptionInterceptor,      ObservationGrpcServerInterceptor oInterceptor) {    this.sourceService = sourceService;    this.chargeService = chargeService;    this.exceptionInterceptor = exceptionInterceptor;    this.oInterceptor = oInterceptor;  }  public void start()throws IOException,      InterruptedException {    server = ServerBuilder.forPort(port)        .addService(sourceService)        .addService(chargeService)        .intercept(exceptionInterceptor)        .intercept(oInterceptor)        .build().start();  // code truncated for brevity\n    ```", "```java\n// Commands from Chapter12/server directory$ ./gradlew clean build\n// You may want to up docker-compose before running the server\n// to avoid Logstash connect errors.\n$ java -jar build/libs/chapter12-server-0.0.1-SNAPSHOT.jar\n// Logs truncated for brevity\n2023-04-23 21:30:42.120      INFO [grpc-server,,]     49296 --- [           main] com.packt.modern.api.server.GrpcServer   : gRPC server is starting on port: 8080.\n```", "```java\n    implementation 'net.logstash.logback:    logstash-logback-encoder:7.3'implementation 'io.micrometer:micrometer-tracing-bridge-brave'implementation 'org.springframework.boot:spring-boot-starter-actuator'\n    ```", "```java\n    spring.application.name=grpc-clientserver.port=8081grpc.server.host=localhostgrpc.server.port=8080logstash.destination=localhost:5002management.tracing.sampling.probability=1.0\n    ```", "```java\n@Configurationpublic class Config {\n @Bean\n public ObservationGrpcClientInterceptor interceptor\n      (ObservationRegistry registry) {\n   return new ObservationGrpcClientInterceptor\n      (registry);\n }\n}\n```", "```java\n    @Componentpublic class GrpcClient {  @Autowired  private ObservationGrpcClientInterceptor      observationGrpcClientInterceptor;  // code truncated for brevity  public void start() {   channel = ManagedChannelBuilder.forAddress       (host, port)             .intercept(observationGrpcClientInterceptor)    .usePlaintext().build();   sourceServiceStub = SourceServiceGrpc       .newBlockingStub(channel);   chargeServiceStub = ChargeServiceGrpc       .newBlockingStub(channel);  }  // code truncated for brevity\n    ```", "```java\n// Commands from Chapter12/client directory$ ./gradlew clean build\n// You may want to up docker-compose before running the server\n// to avoid Logstash connect errors.\n$ java -jar build/libs/chapter12-client-0.0.1-SNAPSHOT.jar\n// log truncated for brevity\n2023-04-23 23:02:35.297      INFO [grpc-client,,]     51746 --- [           main] com.packt.modern.api.ClientApp           : Started ClientApp in 3.955 seconds (process running for 4.611)\n2023-04-23 23:02:35.674      INFO [grpc-client,,]     51746 --- [           main] com.packt.modern.api.client.GrpcClient   : gRPC client connected to localhost:8080\n```", "```java\n$ curl http://localhost:8081/charges\n```", "```java\n{  \"charge\": [{\n    \"id\": \"aibn4f45m49bojd3u0p16erbi5lnelui\",\n    \"amount\": 1000,\n    \"amountCaptured\": 0,\n    \"amountRefunded\": 0,\n    \"balanceTransactionId\": \"\",\n    \"calculatedStatementDescriptor\": \"\",\n    \"receiptEmail\": \"receipt@email.com\",\noutput truncated for brevity\n    \"refunded\": false,\n    \"refunds\": [],\n    \"statementDescriptor\": \"Statement Descriptor\",\n    \"status\": \"SUCCEEDED\",\n    \"sourceId\": \"inccsjg6gogsvi4rlprdbvvfq2ft2e6c\"\n  }]\n}\n```", "```java\n2023-04-23 23:10:37.882      INFO [grpc-client,64456d940c51e3e2baec07f 7448beee6,baec07f7448beee6]     51746 --- [nio-8081-exec-1] brave.Trac er                             : {\"traceId\":\"64456d940c51e3e2baec07f 7448beee6\",\"parentId\":\"baec07f7448beee6\",\"id\":\"0645e686d86968b6\",\"kind\":\"CLIENT\",\"name\":\"com.packtpub.v1.ChargeService/RetrieveAll\",\"timestamp\":1682271636300866,\"duration\":1578184,\"localEndpoint\":{\"serviceName\":\"unknown\",\"ipv4\":\"192.168.1.2\"},\"tags\":{\"rpc.service\":\"com.packtpub.v1.ChargeService\",\"rpc.method\":\"RetrieveAll\"}}2023-04-23 23:10:37.886      INFO [grpc-client,64456d940c51e3e2baec07f 7448beee6,baec07f7448beee6]     51746 --- [nio-8081-exec-1] c.p.m.api.controller.ChargeController    : Server response received in Json Format: charge {\n  id: \"iivpc3i9el2dso9s2s2rqf9j3s2pomlm\"\n  amount: 1000\n  created: 1682265641\n  currency: \"USD\"\n  customerId: \"ab1ab2ab3ab4ab5\"\n  description: \"Charge Description\"\n  receiptEmail: receipt@email.com\n  statementDescriptor: \"Statement Descriptor\"\n  sourceId: \"6ufgh93stkjod1ih2vhkmamj9l1m0hvv\"\n}\n```", "```java\n2023-04-23 23:10:37.821      INFO [grpc-server), trace ID (64456d940c51e3e2baec07f7448beee6), and span ID (182159d509ce0714). The trace ID is the same as what is displayed in the gRPC client logs. The span IDs are different from the gRPC client service because span IDs belong to their respective individual services. This is how the trace/correlational ID helps you trace the call for requests across different services because it will be propagated to all the services it involves.\nTracing this request was simple as the logs contain just a few lines and are scattered across only two services. What if you have a few gigabytes of logs scattered across various services? Then, you can make use of the ELK stack to search the log index using different query criteria. However, we are going to use the trace ID for this purpose.\nFirst, open the Kibana home page in the browser ([http://localhost:5600/](http://localhost:5600/)). Then, click on the hamburger menu in the top-left corner, as shown in the following screenshot. After that, click on the **Discover** option in the menu that appears:\n![Figure 12.3 – Kibana hamburger menu](img/Figure_12.03_B19349.jpg)\n\nFigure 12.3 – Kibana hamburger menu\nThis should open the **Discover** page, as shown in the following screenshot. If this your first time opening the page, you must create an index pattern that will filter out the indexes available in Elasticsearch:\n![Figure 12.4 – Kibana’s Discover page](img/Figure_12.04_B19349.jpg)\n\nFigure 12.4 – Kibana’s Discover page\nNext, click on the `modern-api`) given in the Logstash configuration in the ELK stack’s Docker Compose file:\n![Figure 12.5 – Kibana’s Create data view page](img/Figure_12.05_B19349.jpg)\n\nFigure 12.5 – Kibana’s Create data view page\nHere, you can also provide a name for the data view and an index pattern. You can keep the default value of `@timestamp` for **Timestamp field**.\nThen, click on the **Save data view to Kibana** button. This action will create the data view and may show the following view (if you have called the client’s REST APIs recently; otherwise, it will show no data view):\n![Figure 12.6 – Kibana’s saved data view page](img/Figure_12.06_B19349.jpg)\n\nFigure 12.6 – Kibana’s saved data view page\nYou can add the filter query to the **Search** textbox and the **Date/Duration** menu at the top right of the **Discover** page.\nQuery criteria can be input using the **Kibana Query Language** (**KQL**), which allows you to add different comparator and logical operators. For more information, refer to [https://www.elastic.co/guide/en/kibana/master/kuery-query.html](https://www.elastic.co/guide/en/kibana/master/kuery-query.html).\n![Figure 12.7 – Kibana’s Discover page – filtering](img/Figure_12.07_B19349.jpg)\n\nFigure 12.7 – Kibana’s Discover page – filtering\nAs shown in the preceding figure, we have entered criteria (`traceId: 64457832c37c7ac9f957869d819bc0c6`) (*1*) and kept the **Duration** field as its default (the last 15 minutes) (*2*). The left-hand side also shows you how to select the Elasticsearch index and all the fields available in it (*3*).\nOnce you press the *Enter* key or click on the refresh button after entering the criteria, the search displays the available logs from all the services. The searched values are highlighted in yellow (*4*).\nYou can also observe that the searched trace ID shows logs from both server and client services (*5*).\nThe searched **Discovery** page also shows the graph that shows the number of calls made during a particular period. You can generate more logs and reveal any errors, and then you can use different criteria to filter the results and explore further.\nYou can also save the searches and perform more operations, such as customizing the dashboard. Please refer to [https://www.elastic.co/guide/en/kibana/master/index.html](https://www.elastic.co/guide/en/kibana/master/index.html) for more information.\nThe ELK stack is good for log aggregation, filtering, and debugging using the trace ID and other fields. However, it can’t check the performance of API calls – the time taken by the call. This is especially important when you have a microservice-based application.\nThis is where **Zipkin** (also known as **OpenZipkin**), along with Micrometer, comes in.\nDistributed tracing with Zipkin and Micrometer\n**Spring Micrometer** is a utility library that collects the metrics generated by the Spring Boot application. It provides vendor-neutral APIs that allow you to export the collected metrics to different systems, such as ELK. It collects different types of metrics. A few of them are the following:\n\n*   Metrics related to the JVM, CPU, and cache\n*   Latencies in Spring MVC, WebFlux, and the REST client\n*   Metrics related to Datasource and HikariCP\n*   Uptime and Tomcat usage\n*   Events logged to Logback\n\nZipkin, along with Micrometer, helps you not only to trace transactions across multiple service invocations but also to capture the response time taken by each service involved in the distributed transaction. Zipkin also shows this information using nice graphs. It helps you to locate the performance bottlenecks and drill down into the specific API call that creates the latency issue. You can find out the total time taken by the main API call as well as its internal API call time.\nServices developed with Spring Boot facilitate their integration with Zipkin. You just need to make two code changes – the addition of the `zipkin-reporter-brave` dependency and the addition of the Zipkin endpoint property.\nYou can make these two changes to both the gRPC server and client, as shown next:\n\n1.  First, add the highlighted dependency to `build.gradle` (both the gRPC server and client projects):\n\n    ```", "```java\n    management.zipkin.tracing.endpoint=                      http://localhost:9411/api/v2/spansmanagement.tracing.sampling.probability=1.0\n    ```", "```java\n\nThe Zipkin `tracing.endpoint` property points to the Zipkin API endpoint.\nYou are done with the changes required in the code for publishing the tracing information to Zipkin. Rebuild both the server and client services after making these changes.\nNow, let’s install and start Zipkin.\nThere are various ways to install and run Zipkin. Please refer to [https://zipkin.io/pages/quickstart](https://zipkin.io/pages/quickstart) to find out about these options. You can add it in `docker-compose` too. However, for development purposes, we are going to fetch the latest release as a self-contained executable JAR from [https://search.maven.org/remote_content?g=io.zipkin&a=zipkin-server&v=LATEST&c=exec](https://search.maven.org/remote_content?g=io.zipkin&a=zipkin-server&v=LATEST&c=exec) and then start it using the following command (make sure to change the version in the `JAR` file based on the downloaded file):\n\n```", "```java\n\n The previous command, when executed, will start Zipkin with an in-memory database. For production purposes, it is recommended to use a persistence store, such as Elasticsearch.\nIt should start with the default port `9411` at `http://127.0.0.1:9411/` if executed on `localhost`.\nOnce the Zipkin server and the ELK stack are up and running, you can start both the gRPC server and client services and execute the following command:\n\n```", "```java\n\n This command should print a log like the following log statement in the gRPC client service:\n\n```", "```java\n\n Keep the trace ID (`64461c16391707ee95478f957f3ccb1d` in the previous output) handy as you are going to use it in the Zipkin UI. Open the Zipkin home page by accessing `http://localhost:9411`. It will look as follows:\n![Figure 12.8 – Zipkin home page](img/Figure_12.08_B19349.jpg)\n\nFigure 12.8 – Zipkin home page\nYou can observe that Zipkin also allows you to run queries. However, we’ll make use of the trace ID. Paste the copied trace ID in the **Search by trace ID** textbox in the top-right corner (highlighted in green) and then press *Enter*:\n![Figure 12.9 – Zipkin search result page](img/Figure_12.09_B19349.jpg)\n\nFigure 12.9 – Zipkin search result page\nIn the preceding figure, the Zipkin trace ID shows complete API call information at the top (*1* and *2*) if the trace ID is available. In the left-hand side section, it shows all the corresponding API calls with a hierarchy, which shows the individual call times in a graphical way (*3*). These API call rows are selectable. Selected call details are displayed on the right-hand side (*4*).\nThe `grpc-server` call:\n![Figure 12.10 – Annotation details](img/Figure_12.10_B19349.jpg)\n\nFigure 12.10 – Annotation details\nTime tracking at a granular level for each distributed API call allows you to identify the latency issues and relative time tracking for performance tuning.\nSummary\nIn this chapter, you learned how the trace/correlation ID is important and how it can be set up using Micrometer with Brave. You can use these generated IDs to find the relevant logs and API call durations. You integrated the Spring Boot services with the ELK stack and Zipkin.\nYou also implemented extra code and configurations, which are required for enabling distributed tracing for gRPC-based services.\nYou acquired log aggregation and distributed tracing skills using Micrometer, Brave, the ELK stack, and Zipkin.\nIn the next chapter, you are going to learn about the fundamentals of GraphQL APIs.\nQuestions\n\n1.  What is the difference between the trace ID and span ID?\n2.  Should you use a broker between services that generate the logs and the ELK stack? If yes, why?\n3.  How does Zipkin work?\n\nAnswers\n\n1.  Trace IDs and span IDs are created when the distributed transaction is initiated. A trace ID is generated for the main API call by the receiving service using Spring Cloud Sleuth. A trace ID is generated only once for each distributed call. Span IDs are generated by all the services participating in the distributed transaction. A trace ID is a correlation ID that will be common across the service for a call that requires a distributed transaction. Each service will have its own span ID for each of the API calls.\n2.  Yes, a broker such as Kafka, RabbitMQ, or Redis allows robust persistence of logs and removes the risk of losing log data in unavoidable circumstances. It also performs better and can handle sudden spikes of data.\n3.  A tracer such as Micrometer with Brave or Spring Cloud Sleuth (*which performs instrumentation*) does two jobs – records the time and metadata of the call being performed, and propagates the trace IDs to other services participating in the distributed transaction. Then, it pushes the tracing information to Zipkin using a *reporter* once the scan completes. The reporter uses *transport* such as HTTP and Kafka to publish the data in Zipkin.\n\nThe *collector* in Zipkin collects the data sent by the transporters from the running services and passes it to the storage layer. The storage persists the data. Persisted data is exposed by the Zipkin APIs. The Zipkin UI calls these APIs to show the information graphically:\n![](img/Figure_12.11_B19349.jpg)\n\nFurther reading\n\n*   Elasticsearch documentation: [https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n*   Kibana documentation: [https://www.elastic.co/guide/en/kibana/master/index.html](https://www.elastic.co/guide/en/kibana/master/index.html)\n*   Kibana Query Language: [https://www.elastic.co/guide/en/kibana/master/kuery-query.html](https://www.elastic.co/guide/en/kibana/master/kuery-query.html)\n*   Logstash documentation: [https://www.elastic.co/guide/en/logstash/master/index.html](https://www.elastic.co/guide/en/logstash/master/index.html)\n*   *Elasticsearch 8.x Cookbook – Fifth* *Edition*: [https://www.packtpub.com/product/elasticsearch-8x-cookbook-fifth-edition/9781801079815](https://www.packtpub.com/product/elasticsearch-8x-cookbook-fifth-edition/9781801079815)\n*   Zipkin documentation: [https://zipkin.io/pages/quickstart](https://zipkin.io/pages/quickstart)\n\n```"]