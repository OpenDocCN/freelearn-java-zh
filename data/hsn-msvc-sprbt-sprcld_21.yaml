- en: Using a Service Mesh to Improve Observability and Management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务网格改善可观测性和管理
- en: In this chapter, you will be introduced to the concept of a service mesh and
    see how its capabilities can be used to handle challenges in a system landscape
    of microservices in areas including security, policy enforcement, resilience,
    and traffic management. A service mesh can also be used to provide observability,
    that is, the capability to visualize how traffic flows between microservices in
    a service mesh.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将介绍服务网格的概念，并了解其功能如何用于处理微服务系统架构在安全性、策略执行、弹性和流量管理方面的挑战。服务网格还可以用于提供可观测性，即可视化服务网格中微服务之间流量流动的能力。
- en: A service mesh overlaps partly with the capabilities of Spring Cloud and Kubernetes
    we learned about earlier in this book. But most of the functionality in a service
    mesh complements Spring Cloud and Kubernetes, as we will see in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格在与本书前面所学的Spring Cloud和Kubernetes的功能有所重叠的同时，也提供了大部分Spring Cloud和Kubernetes所没有的功能，正如我们将在本章所看到的。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An introduction to the service mesh concept and Istio, a popular open source
    implementation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍服务网格概念以及Istio，一个流行的开源实现
- en: 'You will also learn how to do the following:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你还将学习如何进行以下操作：
- en: Deploy Istio in Kubernetes
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中部署Istio
- en: Create a service mesh
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个服务网格
- en: Observe a service mesh
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察服务网格
- en: Secure a service mesh
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护服务网格
- en: Ensure that a service mesh is resilient
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保服务网格具有弹性
- en: Perform zero downtime deployments using a service mesh
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用服务网格执行零停机部署
- en: Test the microservice landscape using Docker Compose to ensure that the source
    code in the microservices is not locked into either Kubernetes or Istio
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker Compose测试微服务架构，以确保微服务中的源代码既不受Kubernetes或Istio的限制。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All commands described in this book are run on a MacBook Pro using macOS Mojave,
    but modifying these commands should be sufficiently straightforward to run them
    on another platform such as Linux or Windows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中描述的所有命令都是在MacBook Pro上使用macOS Mojave运行的，但修改这些命令以在另一个平台（如Linux或Windows）上运行应该是非常直接的。
- en: 'The only new tool required for this chapter is Istio''s command-line tool, `istioctl`.
    This can be installed using Homebrew with the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章唯一需要的新工具是Istio的命令行工具`istioctl`。这可以通过使用Homebrew以下命令进行安装：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The source code for this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在GitHub上找到，地址为[https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter18)。
- en: 'To be able to run the commands as described in the book, you need to download
    the source code to a folder and set up an environment variable, `$BOOK_HOME`,
    that points to that folder. Examples of sample commands include the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够按照书中所述运行命令，你需要把源代码下载到一个文件夹中，并设置一个环境变量`$BOOK_HOME`，使其指向该文件夹。示例命令包括以下内容：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Java source code is written for Java 8 and tested on Java 12\. This chapter
    uses Spring Cloud 2.1, SR2 (also known as the **Greenwich** release), Spring Boot
    2.1.6, and Spring 5.1.8, that is, the latest available version of the Spring components
    at the time of writing this chapter. The source code has been tested using Kubernetes
    V1.15.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Java源代码是为Java 8编写的，并在Java 12上进行了测试。本章使用了Spring Cloud 2.1, SR2（也被称为**Greenwich**版本），Spring
    Boot 2.1.6和Spring 5.1.8，即在撰写本章时可用的Spring组件的最新版本。源代码已经使用Kubernetes V1.15进行了测试。
- en: All source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter18`,
    but are, in several cases, edited to remove non-relevant parts of the source code,
    such as comments, and import and log statements.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有源代码示例均来自`$BOOK_HOME/Chapter18`的源代码，但在许多情况下，为了去除源代码中不相关的内容（如注释、导入和日志语句），对这些示例进行了编辑。
- en: If you want to see the changes applied to the source code in [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, that is, the changes
    required to create a service mesh using Istio, you can compare it with the source
    code for [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Implementing
    Kubernetes Features as an Alternative*. You can use your favorite diff tool and
    compare the two folders, `$BOOK_HOME/Chapter17` and `$BOOK_HOME/Chapter18`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看源代码中[第18章](422649a4-94bc-48ae-b92b-e3894c014962.xhtml)，*使用服务网格提高可观测性和管理能力*的更改，也就是使用Istio创建服务网格所需的更改，你可以将其与[第17章](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)，*作为替代实现Kubernetes特性*的源代码进行比较。你可以使用你喜欢的差异工具，比较这两个文件夹，`$BOOK_HOME/Chapter17`和`$BOOK_HOME/Chapter18`。
- en: Introduction to service mesh using Istio
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Istio的服务网格简介
- en: A service mesh is an infrastructure layer that controls and observes the communication
    between services, for example, microservices. The capabilities in a service mesh,
    for example, observability, security, policy enforcement, resilience, and traffic
    management,are implemented by controlling and monitoring all internal communication
    inside the service mesh, that is, between the microservices in the service mesh.
    One of the core components in a service mesh is a lightweight **proxy **component
    that is injected into all microservices that will be part of the service mesh.
    All traffic in and out of a microservice is configured to go through its proxy
    component. The proxy components are configured in runtime by a **control plane** in
    the service mesh using API's exposed by the proxy. The control plane also collects
    telemetry data through these APIs from the proxies to visualize how the traffic
    flows in the service mesh.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格是一个基础设施层，用于控制和观察服务之间的通信，例如微服务。服务网格的功能，例如可观测性、安全性、策略执行、弹性和流量管理，是通过控制和监控服务网格内的所有内部通信实现的，即服务网格中微服务之间的通信。服务网格中的一个核心组件是一个轻量级的**代理**组件，它被注入到将成为服务网格一部分的所有微服务中。微服务所有进出流量都被配置为通过其代理组件。代理组件在运行时通过服务网格中的**控制平面**使用代理暴露的API进行配置。控制平面还通过这些API从代理收集遥测数据，以可视化服务网格中的流量流动。
- en: 'A service mesh also contains a** data plane**, consisting of the proxy components
    in all microservices in the service mesh together with separate components for
    handling external incoming and outgoing traffic to and from the service mesh.
    This is illustrated by the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格还包括一个**数据平面**，由服务网格中所有微服务中的代理组件以及处理来自和服务网格之间外部传入和传出流量的单独组件组成。以下图表说明了这一点：
- en: '![](img/6f4234c5-40c7-405d-856e-9e11009c242d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f4234c5-40c7-405d-856e-9e11009c242d.png)'
- en: The first publicly available implementation of a service mesh was the open source
    project Linkerd, managed by Buoyant ([https://linkerd.io](https://linkerd.io)),
    having its origins in Twitter's Finagle project ([http://twitter.github.io/finagle](http://twitter.github.io/finagle)).
    It was launched during 2016 and, one year later, in 2017, IBM, Google, and Lyft
    launched the open source project, Istio ([https://istio.io](https://istio.io)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个公开可用的服务网格实现是开源项目Linkerd，由Buoyant管理([https://linkerd.io](https://linkerd.io))，其起源于Twitter的Finagle项目([http://twitter.github.io/finagle](http://twitter.github.io/finagle))。它于2016年推出，一年后的2017年，IBM、Google和Lyft推出了开源项目Istio([https://istio.io](https://istio.io))。
- en: One of the core components in Istio, the proxy component, is based on Lyft's Envoy
    proxy ([https://www.envoyproxy.io](https://www.envoyproxy.io)). Linkerd and Istio
    are, at the time of writing this chapter, the two most popular and widely used
    service mesh implementations. In this chapter, we will use Istio.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Istio中，核心组件之一，代理组件，是基于Lyft的Envoy代理([https://www.envoyproxy.io](https://www.envoyproxy.io))。在撰写本章时，Linkerd和Istio是两个最受欢迎且广泛使用的服务网格实现。在本章中，我们将使用Istio。
- en: Istio can be deployed in various environments, including Kubernetes (see [https://istio.io/docs/setup](https://istio.io/docs/setup)). When
    deploying Istio on Kubernetes, its runtime components are deployed into a separate
    Kubernetes namespace, `istio-system`. Istio also comes with a set of Kubernetes
    **Custom Resources Definitions** (**CRDs**). CRDs are used in Kubernetes to extend
    its API, that is, to add new objects to its API. The Istio objects added are used
    to configure how Istio will be used. Finally, Istio comes with a CLI tool, `istioctl`,
    which will be used to inject Istio proxies into the microservices that participate
    in the service mesh.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 可以在各种环境中部署，包括 Kubernetes（参见 [https://istio.io/docs/setup](https://istio.io/docs/setup)）。在
    Kubernetes 上部署 Istio 时，其运行时组件被部署到一个单独的 Kubernetes 命名空间 `istio-system` 中。Istio
    还提供一套 Kubernetes **自定义资源定义**（**CRD**）。CRD 用于在 Kubernetes 中扩展其 API，即添加新的对象到其 API
    中。添加的 Istio 对象用于配置 Istio 的使用方式。最后，Istio 提供了一个 CLI 工具 `istioctl`，它将用于将 Istio 代理注入到参与服务网格的微服务中。
- en: Istio is, as explained previously, divided into a control plane and a data plane. As
    an operator, we will define a desired state by creating Istio objects in the Kubernetes
    API server, for example, declaring routing rules. The control plane will read
    these objects and send commands to the proxies in the data plane to take actions
    according to the desired state, for example, configuring routing rules. The proxies
    handle the actual communication between the microservices and report back telemetry
    data to the control plane. The telemetry data is used by various components in
    a control plane to visualize what's going on in the service mesh.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Istio 被分为控制平面和数据平面。作为一个操作员，我们将通过在 Kubernetes API 服务器中创建 Istio 对象来定义期望的状态，例如，声明路由规则。控制平面将读取这些对象，并向数据平面中的代理发送命令，以根据期望的状态采取行动，例如，配置路由规则。代理处理微服务之间的实际通信，并向控制平面报告遥测数据。遥测数据被控制平面的各个组件用来可视化服务网格中正在发生的事情。
- en: 'In the following subsections, we will cover the following topics:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将涵盖以下主题：
- en: How Istio proxies are injected into microservices
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将 Istio 代理注入到微服务中
- en: The Istio API objects that we will use in this chapter
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章我们将使用的 Istio API 对象
- en: The runtime components in Istio that constitute the control plane and the data
    plane
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构成 Istio 控制平面和数据平面的运行时组件
- en: Changes in the microservice landscape as a result of the introduction of Istio
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入 Istio 后微服务景观的变化
- en: Injecting Istio proxies into existing microservices
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Istio 代理注入现有微服务
- en: The microservices we have deployed in Kubernetes in the previous chapters run
    as a single container in a Kubernetes pod (refer to the *Introducing Kubernetes
    API objects* section in [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml), *Introduction
    to Kubernetes*, for a recap). To make a microservice join an Istio-based service
    mesh, an Istio proxy is injected into each microservice. This is done by adding
    an extra container to the pod that runs the Istio proxy.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中在 Kubernetes 中部署的微服务作为一个容器在 Kubernetes pod 中运行（回顾[第 15 章](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml)，*Kubernetes
    简介*中的*介绍 Kubernetes API 对象*部分）。要使一个微服务加入基于 Istio 的服务网格，向每个微服务中注入 Istio 代理。这是通过向运行
    Istio 代理的 pod 添加一个额外容器来实现的。
- en: Containers added to a pod with the aim of supporting the main container, such
    as an Istio proxy, are referred to as a *sideca**r*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 被添加到 pod 中以支持主容器（如 Istio 代理）的容器被称为*sidecar*。
- en: 'The following diagram shows how an Istio proxy has been injected into a sample
    pod, **Pod A**, as a sidecar:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了如何将 Istio 代理作为*sidecar*注入到样本 pod **Pod A** 中：
- en: '![](img/a824d18b-8f01-4c08-97ef-d697d7154fc8.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a824d18b-8f01-4c08-97ef-d697d7154fc8.png)'
- en: The main container in the pod, **Container A**, is configured to route all its
    traffic through the Istio proxy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 pod 中的主容器**容器 A**，被配置通过 Istio 代理路由其所有流量。
- en: Istio proxies can be injected either automatically when a deployment object
    is created or manually using the `istioctl` tool.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 代理可以当创建部署对象时自动注入，也可以使用`istioctl`工具手动注入。
- en: 'In this chapter, we will inject the Istio proxies manually. The reason for
    this is that Istio proxies do not support the protocols used by MySQL, MongoDB,
    and RabbitMQ, so we will only inject Istio proxies into pods where the HTTP protocol
    is used. An Istio proxy can be injected into the pods of an existing deployment
    object by means of the following command:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将手动注入Istio代理。原因是Istio代理不支持MySQL、MongoDB和RabbitMQ使用的协议，所以我们只会在使用HTTP协议的pod中注入Istio代理。可以通过以下命令将Istio代理注入现有部署对象的pods：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This command may, at first glance, appear somewhat daunting, but it is actually
    just three separate commands. The previous command sends its output to the next
    command using pipes, that is, the `|` character. Let''s go through each command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令初看起来可能有些令人畏惧，但实际上它只是三个独立的命令。前一个命令将其输出通过管道发送给下一个命令，即`|`字符。让我们逐一介绍每个命令：
- en: The `kubectl get deployment` command gets the current definition of a deployment
    named `sample-deployment` from the Kubernetes API server and returns its definition
    in the YAML format.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl get deployment`命令从Kubernetes API服务器获取名为`sample-deployment`的部署的当前定义，并以YAML格式返回其定义。'
- en: The `istioctl kube-inject` command reads the definition from the `kubectl get
    deployment` command and adds an extra container for an Istio proxy in pods that
    the deployment handles. The configuration for the existing container in the deployment
    object is updated so that incoming and outgoing traffic goes through the Istio
    proxy.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`istioctl kube-inject`命令从`kubectl get deployment`命令中读取定义，并在部署处理的 pod 中添加一个额外的Istio代理容器。部署对象中现有容器的配置更新，以便传入和传出的流量都通过Istio代理。'
- en: The `istioctl` command returns the new definition of the deployment object,
    including a container for the Istio proxy.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`istioctl`命令返回部署对象的新定义，包括一个Istio代理的容器。'
- en: The `kubectl apply` command reads the updated configuration from the `istioctl
    kube-inject` command and applies the updated configuration. A rolling upgrade
    of the pods belonging to the deployment will start up in the same way as we have
    seen before (refer to the *Performing a rolling upgrade* section in [Chapter 16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml), *Deploying
    Our Microservices to Kubernetes)*.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl apply`命令从`istioctl kube-inject`命令中读取更新后的配置，并应用更新后的配置。将启动与之前我们所见过的相同的滚动升级（请参阅[第16章](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)，*将我们的微服务部署到Kubernetes*中的*执行滚动升级*部分），部署的pods将以相同的方式开始。'
- en: The deployment scripts in the `kubernetes/scripts` folder have been extended
    to use `istioctl` to inject the Istio proxies. Refer to the upcoming *Creating
    the service mesh* section for details.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubernetes/scripts`文件夹中的部署脚本已扩展为使用`istioctl`来注入Istio代理。有关详细信息，请参考即将到来的*创建服务网格*部分。'
- en: Introducing Istio API objects
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Istio API对象
- en: 'Istio extends the Kubernetes API with a number of objects using its CRDs. Refer
    to the *Introducing Kubernetes API objects* section in [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml), *Introduction
    to Kubernetes*, for a recap of the Kubernetes API. In this chapter we will use
    the following Istio objects:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Istio通过其CRDs使用许多对象扩展了Kubernetes API。回顾Kubernetes API，请参考[第15章](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml)，*Kubernetes简介*中的*介绍Kubernetes
    API对象*部分。在本章中，我们将使用以下Istio对象：
- en: '`Gateway` is used to configure how to handle incoming traffic to, and outgoing
    traffic from, the service mesh. A gateway depends on a virtual service routing
    the incoming traffic to Kubernetes services. We will use a gateway object to accept
    incoming traffic to the DNS name, `minikube.me`, using HTTPS. Refer to the *Kubernetes
    Ingress resource replaced with Istio Ingress Gateway as an edge server* section
    for details.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gateway`用于配置如何处理服务网格的入站和出站流量。网关依赖于一个虚拟服务，将入站流量路由到Kubernetes服务。我们将使用网关对象接受通过HTTPS到DNS名称`minikube.me`的入站流量。有关详细信息，请参考*用Istio
    Ingress Gateway替换Kubernetes Ingress资源作为边缘服务器*部分。'
- en: '`VirtualService` is used to define routing rules in the service mesh. We will
    use virtual services to describe how to route incoming traffic from an Istio gateway
    to the Kubernetes services and between services. We will also use virtual services to
    inject faults and delays in order to test the reliability and resilience capabilities
    of the service mesh.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VirtualService`用于在服务网格中定义路由规则。我们将使用虚拟服务来描述如何将来自Istio网关的流量路由到Kubernetes服务和服务之间。我们还将使用虚拟服务注入故障和延迟，以测试服务网格的可靠性和弹性能力。'
- en: '`DestinationRule` is used to define policies and rules for traffic that is
    routed (using a virtual service) to a specific service (that is, a destination).
    We will use destination rules to set up encryption policies to encrypt internal
    HTTP traffic and define service subsets that describe available versions of the
    services. We will use service subsets when performing zero downtime (blue/green)
    deployments from an existing version of a microservice to a new version.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DestinationRule`用于定义路由到特定服务（即目的地）的流量的策略和规则（使用虚拟服务）。我们将使用目的地规则来设置加密策略，以加密内部HTTP流量并定义描述服务可用版本的服务子集。在将微服务的现有版本部署到新版本时，我们将使用服务子集执行零停机（蓝/绿）部署。'
- en: '`Policy` is used to define how requests will be authenticated. We will use
    policies to require incoming requests to the service mesh to be authenticated
    using a JWT-based OAuth 2.0/OIDC access token. Refer to the *Authenticating external
    requests using OAuth 2.0/OIDC access tokens* section of this chapter. A policy
    can also be used to define how to secure parts of the internal communication in
    the service mesh. For example, a policy can require that internal requests are
    encrypted using HTTPS or allow plain text requests. Finally, a `MeshPolicy` object
    can be used to define global policies that apply to the whole service mesh.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Policy`用于定义请求如何进行认证。我们将使用策略要求服务网格传入的请求使用基于JWT的OAuth 2.0/OIDC访问令牌进行认证。请参阅本章节中*使用OAuth
    2.0/OIDC访问令牌认证外部请求*部分。策略还可以用于定义服务网格内部通信的安全部分。例如，策略可以要求内部请求使用HTTPS进行加密或允许明文请求。最后，可以使用`MeshPolicy`对象定义适用于整个服务网格的全球策略。'
- en: Introducing runtime components in Istio
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Istio中引入运行时组件
- en: Istio contains a number of runtime components, is highly configurable in terms
    of what components to use, and provides fine-grained control over the configuration
    of each component. Refer to the *Deploying Istio in a Kubernetes cluster* section
    of this chapter for information on the configuration we will use in this chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Istio包含许多运行时组件，在选择使用哪些组件方面具有高度可配置性，并提供了对每个组件配置的细粒度控制。有关我们将在此章节中使用的配置信息，请参阅本章节中*在Kubernetes集群中部署Istio*部分。
- en: 'In the configuration used in this chapter, the Istio control plane consists
    of the following runtime components:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节使用的配置中，Istio控制平面包含以下运行时组件：
- en: '**Pilot**:responsible for supplying all sidecars with updates of the service mesh
    configuration.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pilot**：负责向所有边车提供服务网格配置的更新。'
- en: '**Mixer**: consists of two different runtime components:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mixer**：包含两个不同的运行时组件：'
- en: '**Policy** – enforces network policies such as authentication, authorization,
    rate limits, and quotas.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**–执行网络策略，例如认证、授权、速率限制和配额。'
- en: '**Telemetry***–*collects telemetry information and sends it to Prometheus, for
    example.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遥测**–收集遥测信息并将其发送到Prometheus，例如。'
- en: '**Galley**:responsible for collecting and validating configuration information
    and distribution to the other Istio components in the control plane.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Galley**：负责收集和验证配置信息，并将其分发到控制平面上的其他Istio组件。'
- en: '**Citadel**: responsible for issuing and rotating internally used certificates.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Citadel**：负责发放和轮换内部使用的证书。'
- en: '**Kiali**: provides observability to the service mesh, visualizing what is
    going on in the mesh. Kiali is a separate open source project (see [https://www.kiali.io](https://www.kiali.io))'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kiali**：为服务网格提供可观测性，可视化网格中正在发生的事情。Kiali是一个独立的开源项目（参见[https://www.kiali.io](https://www.kiali.io)）。'
- en: '**Prometheus**: performs data ingestion and storage for time series-based data,
    for example, performance metrics.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Prometheus**：对基于时间序列的数据执行数据摄取和存储，例如，性能指标。'
- en: Prometheus is a separate open source project (refer to [https://prometheus.io](https://prometheus.io)).
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prometheus是一个独立的开源项目（请参阅[https://prometheus.io](https://prometheus.io)）。
- en: '**Grafana**: visualizes performance metrics and other time series-related data
    collected in Prometheus. Grafana is a separate open source project (see [https://grafana.com](https://grafana.com)).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Grafana**：可视化Prometheus收集的性能指标和其他时间序列相关数据。Grafana是一个独立的开源项目（参见[https://grafana.com](https://grafana.com)）。'
- en: '**Tracing**: handles and visualizes distributed tracing information. Based
    on Jaeger, it is an open source project for distributed tracing (refer to [https://www.jaegertracing.io](https://www.jaegertracing.io)).
    Jaeger provides the same type of functionality as Zipkin, which we used in [Chapter 14](42f456c5-d911-494a-a1ba-4631863068b6.xhtml),
    *Understanding Distributed Tracing*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**追踪**：处理并可视化分布式追踪信息。基于 Jaeger，它是一个开源的分布式追踪项目（参考[https://www.jaegertracing.io](https://www.jaegertracing.io)）。Jaeger
    提供与 Zipkin 相同的功能，我们在第 14 章中使用过，*理解分布式追踪*。'
- en: Kiali is accessed using a web browser and integrates Grafana for viewing performance
    metrics and Jaeger for visualizing distributed tracing information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Kiali 通过网页浏览器访问，并集成了 Grafana 以查看性能指标和 Jaeger 以可视化分布式追踪信息。
- en: 'The Istio data plane consists of the following runtime components:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 数据平面包括以下运行时组件：
- en: '**Ingress** **Gateway**: handles incoming traffic to the service mesh'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**入口网关**：处理服务网格的入站流量'
- en: '**E****gress** **Gateway**: handles outgoing traffic from the service mesh'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**出口网关**：处理服务网格的出站流量'
- en: All pods with an Istio proxy are injected as a sidecar
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有带有 Istio 代理的 pod 都作为边车注入。
- en: 'The runtime components in Istio''s control plane and data plane are summarized
    in the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Istio 控制平面和数据平面的运行时组件总结如下：
- en: '![](img/bc1d5d4f-c2d3-4504-8d01-506c8fec94c4.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc1d5d4f-c2d3-4504-8d01-506c8fec94c4.png)'
- en: In the next section, we will go through changes applied to the microservice
    landscape arising from the introduction of Istio.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍由于引入 Istio 而对微服务景观所做的更改。
- en: Changes in the microservice landscape
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务景观的变化
- en: 'As we have seen in the preceding section, Istio comes with components that
    overlap with components currently used in the microservice landscape in terms
    of functionality:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Istio 带有与当前在微服务景观中使用的组件在功能上重叠的组件：
- en: The Istio Ingress Gateway can act as an edge server, an alternative to the Kubernetes
    Ingress resources.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio Ingress Gateway 可以作为边缘服务器，是 Kubernetes Ingress 资源的替代品。
- en: The Jaeger component that comes bundled with Istio can be used for distributed
    tracing instead of Zipkin.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio 自带的 Jaeger 组件可以用于分布式追踪，而不是 Zipkin。
- en: In the following two subsections, we will learn why and how Kubernetes Ingress
    resources are replaced with Istio Ingress Gateway, and how and why Zipkin is replaced
    with Jaeger.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个子节中，我们将学习为什么以及如何用 Istio Ingress Gateway 替换 Kubernetes Ingress 资源，以及为什么用
    Jaeger 替换 Zipkin。
- en: Kubernetes Ingress resources are replaced with Istio Ingress Gateway as an edge
    server
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用 Istio Ingress Gateway 作为边缘服务器替换 Kubernetes Ingress 资源
- en: In the previous chapter, we introduced Kubernetes Ingress resources as edge
    servers (refer to the *Replacing the Spring Cloud Gateway* section in [Chapter
    17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Implementing Kubernetes Features
    as an Alternative)*. Unfortunately, ingress resources cannot be configured to
    handle the fine-grained routing rules that come with Istio. Instead, Istio has
    its own edge server, the Istio ingress Gateway, introduced previously in the *Introducing
    runtime components in Istio* section. The Istio Ingress Gateway is used by creating
    `Gateway` and `VisualService` resources described previously in the *Introducing
    Istio API objects* section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 Kubernetes Ingress 资源作为边缘服务器（参考第 17 章中的*用 Kubernetes 特性作为替代*部分，*Replacing
    the Spring Cloud Gateway*）。不幸的是，ingress 资源无法配置以处理 Istio 带来的细粒度路由规则。相反，Istio 有自己的边缘服务器，即
    Istio ingress Gateway，在前面的*介绍 Istio 运行时组件*一节中已经介绍过。通过创建前面介绍的*引入 Istio API 对象*一节中描述的
    `Gateway` 和 `VisualService` 资源来使用 Istio Ingress Gateway。
- en: The definition files for the following Kubernetes Ingress resources, `kubernetes/services/base/ingress-edge-server.yml`
    and `kubernetes/services/base/ingress-edge-server-ngrok.yml`, have therefore been
    removed. Definition files for Istio `Gateway` and `VirtualService` resources will
    be added in the *Creating the service mesh* section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下 Kubernetes Ingress 资源定义文件`kubernetes/services/base/ingress-edge-server.yml`和`kubernetes/services/base/ingress-edge-server-ngrok.yml`已被移除。在*创建服务网格*一节中将添加
    Istio `Gateway` 和 `VirtualService` 资源的定义文件。
- en: The Istio Ingress Gateway is reached using a different IP address than the IP
    address used to access Kubernetes Ingress resources, so we also need to update
    the IP address mapped to the hostname, `minikube.me`, which we use when running
    tests. This is handled in the *Setting up access to Istio services* section in
    this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Istio Ingress Gateway时使用的IP地址与访问Kubernetes Ingress资源的IP地址不同，因此我们还需要更新映射到主机名`minikube.me`的IP地址，我们在运行测试时使用这个主机名。这在本书的*设置对Istio服务的访问*一节中处理。
- en: Simplifying the system landscape and replacing Zipkin with Jaeger
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化系统架构，用Jaeger替换Zipkin
- en: As mentioned in the *Introducing runtime components in Istio* section, Istio
    comes with built-in support for distributed tracing using Jaeger*.* Using Jaeger,
    we can offload and simplify the microservice landscape by removing the Zipkin
    server we introduced in [Chapter 14](42f456c5-d911-494a-a1ba-4631863068b6.xhtml),
    *Understanding Distributed Tracing*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在*在Istio中引入运行时组件*一节中提到，Istio内置了对分布式追踪的支持，使用Jaeger*.*通过Jaeger，我们可以卸载并简化微服务架构，删除我们在[第14章](42f456c5-d911-494a-a1ba-4631863068b6.xhtml)，*理解分布式追踪*中引入的Zipkin服务器。
- en: 'The following changes have been applied to the source code to remove the Zipkin
    server:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对源代码所做的更改，以移除Zipkin服务器：
- en: The dependency to `org.springframework.cloud:spring-cloud-starter-zipkin` in
    all microservice build files, `build.gradle`, has been removed.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有微服务构建文件`build.gradle`中，已移除了对`org.springframework.cloud:spring-cloud-starter-zipkin`的依赖。
- en: The definition of the Zipkin server in the three Docker Compose files, `docker-compose.yml`, `docker-compose-partitions.yml`,
    and `docker-compose-kafka.yml`, has been removed.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在三个Docker Compose文件`docker-compose.yml`、`docker-compose-partitions.yml`和`docker-compose-kafka.yml`中，已移除了对Zipkin服务器的定义。
- en: 'The following Kubernetes definition files for Zipkin have been removed:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已删除以下Zipkin的Kubernetes定义文件：
- en: '`kubernetes/services/base/zipkin-server.yml`'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes/services/base/zipkin-server.yml`'
- en: '`kubernetes/services/overlays/prod/zipkin-server-prod.yml`'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes/services/overlays/prod/zipkin-server-prod.yml`'
- en: Jaeger will be installed in the *Creating the service mesh* section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在*创建服务网格*一节中，将安装Jaeger。
- en: The changes were made to the microservice landscape due to the introduction
    of Istio. We are now ready to deploy Istio in the Kubernetes cluster.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入了Istio，对微服务架构进行了更改。现在我们准备在Kubernetes集群中部署Istio。
- en: Deploying Istio in a Kubernetes cluster
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中部署Istio
- en: In this section, we will learn how to deploy Istio in a Kubernetes cluster and
    how to access Istio services in it.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在Kubernetes集群中部署Istio以及如何访问其中的Istio服务。
- en: We will use v1.2.4 of Istio, the latest release available when this chapter
    was written.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在本章撰写时可用的最新版本的Istio，即v1.2.4。
- en: We will be using a demo configuration of Istio that is suitable for testing
    Istio in a development environment, that is, with most features enabled but configured
    for minimalistic resource usage.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个适合在开发环境中测试Istio的Istio演示配置，即大多数功能启用但配置为最小化资源使用的配置。
- en: This configuration is unsuitable for production usage and for performance testing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置不适合生产使用和性能测试。
- en: For other installation options, see [https://istio.io/docs/setup/kubernetes/install](https://istio.io/docs/setup/kubernetes/install).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他安装选项，请参阅[https://istio.io/docs/setup/kubernetes/install](https://istio.io/docs/setup/kubernetes/install)。
- en: 'To deploy Istio, perform the following steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署Istio，请执行以下步骤：
- en: 'Download Istio as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式下载Istio：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Ensure that your Minikube instance is up-and-running with the following command:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的Minikube实例正在运行以下命令：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Expect a response along the lines of the following, provided it is up and running:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果运行正常，预期会收到类似以下内容的响应：
- en: '![](img/b73dc4d8-a721-40eb-87be-a3395fc8c84f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b73dc4d8-a721-40eb-87be-a3395fc8c84f.png)'
- en: 'Install Istio-specific custom resource definitions (CRDs) in Kubernetes:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kubernetes中安装Istio特定的自定义资源定义（CRDs）：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Install Istio demo configurations in Kubernetes as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式在Kubernetes中安装Istio演示配置：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Wait for the Istio deployments to become available:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待Istio部署变得可用：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The command will report deployment resources in Istio as available, one after
    another. Expect 12 messages such as `deployment.extensions/NNN condition met` before
    the command ends. It can take a couple of minutes (or more) depending on your
    hardware and internet connectivity.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将会逐一报告Istio中的部署资源为可用。在命令结束前，预期会收到12条类似于`deployment.extensions/NNN condition
    met`的消息。这可能会花费几分钟（或更长时间），具体取决于您的硬件和互联网连接速度。
- en: 'Update Kiali''s config map with URLs to Jaeger and Grafana with the following
    commands:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令更新Kiali的配置文件，为其添加Jaeger和Grafana的URL：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The config map, `kubernetes/istio/setup/kiali-configmap.yml`, contains URLs
    to Jaeger and Grafana that utilize the DNS names set up by the `minikube tunnel`
    command used in the next section.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件`kubernetes/istio/setup/kiali-configmap.yml`包含了利用下一节中使用的`minikube tunnel`命令设置的DNS名称的Jaeger和Grafana的URL。
- en: Istio is now deployed in Kubernetes, but before we move on and create the service
    mesh, we need to learn a bit about how to access Istio services in a Minikube
    environment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Istio现在已部署在Kubernetes中，但在我们继续创建服务网格之前，我们需要了解一些关于如何在Minikube环境中访问Istio服务的内容。
- en: Setting up access to Istio services
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置对Istio服务的访问
- en: The demo configuration used in the previous section to install Istio comes with
    a few connectivity-related issues that we need to resolve. The Istio Ingress Gateway
    is configured as a load-balanced Kubernetes service; that is, its type is `LoadBalancer`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节中用于安装Istio的演示配置包含一些与连通性相关的问题需要我们解决。Istio Ingress Gateway被配置为一个负载均衡的Kubernetes服务；也就是说，它的类型是`LoadBalancer`。
- en: It can also be reached using its node port, in the port range `30000`-`32767`,
    on the IP address of the Minikube instance. Unfortunately, HTTPS-based routing
    in Istio can't include port numbers; that is, Istio's Ingress Gateway must be
    reached over the default port for HTTPS (`443`). Therefore, a node port can't
    be used. Instead, a load balancer must be used to be able to use Istio's routing
    rules with HTTPS.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它也可以通过Minikube实例的IP地址上的节点端口访问，端口范围在`30000`-`32767`之间。不幸的是，Istio中的基于HTTPS的路由不能包括端口号；也就是说，Istio的Ingress
    Gateway必须通过HTTPS的默认端口（`443`）访问。因此，不能使用节点端口。相反，必须使用负载均衡器才能使用Istio的路由规则进行HTTPS访问。
- en: Minikube contains a command that can be used to simulate a local load balancer,
    `minikube tunnel`. This command assigns an external IP address to each load-balanced
    Kubernetes service, including the Istio Ingress Gateway. This gives you what we
    need to update the translation of the hostname `minikube.me`, which we use in
    our tests. The hostname, `minikube.me`, now needs to be translated to the external
    IP address of the Istio Ingress Gateway, instead of to the IP address of the Minikube
    instance that we used in the previous chapters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube包含一个可以用来模拟本地负载均衡器的命令，即`minikube tunnel`。此命令为每个负载均衡的Kubernetes服务分配一个外部IP地址，包括Istio
    Ingress Gateway。这将为我们更新主机名`minikube.me`的翻译提供所需的内容，我们在测试中使用该主机名。现在，主机名`minikube.me`需要被翻译为Istio
    Ingress Gateway的外部IP地址，而不是我们在前几章中使用的Minikube实例的IP地址。
- en: 'The `minikube tunnel` command also makes cluster-local Kubernetes services
    accessible using their DNS name. The DNS name is based on the naming convention:
    `{service-name}.{namespace}.svc.cluster.local`. For example, Istio''s Kiali service
    can be reached from a local web browser using the DNS name, `kiali.istio-system.svc.cluster.local`,
    when the tunnel is up and running.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`minikube tunnel`命令还使使用它们的DNS名称的集群本地Kubernetes服务可用。DNS名称基于命名约定：`{service-name}.{namespace}.svc.cluster.local`。例如，当隧道运行时，可以从本地网页浏览器通过DNS名称`kiali.istio-system.svc.cluster.local`访问Istio的Kiali服务。'
- en: 'The following diagram summarizes how Istio services are accessed:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表总结了如何访问Istio服务：
- en: '![](img/6df8153e-3139-4cc7-9cb2-0f832f9e5129.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6df8153e-3139-4cc7-9cb2-0f832f9e5129.png)'
- en: 'Perform the following steps to set up the Minikube tunnel:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来设置Minikube隧道：
- en: 'Make Kubernetes services available locally. Run the following command in a
    separate terminal window (the command locks the terminal window when the tunnel
    is up and running):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使Kubernetes服务在本地可用。在一个单独的终端窗口中运行以下命令（当隧道运行时，该命令会锁定终端窗口）：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that this command requires that your user has `sudo` privileges and that
    you enter your password during startup and shutdown. It takes a couple of seconds
    before the command asks for the password, so it is easy to miss!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此命令要求您的用户具有`sudo`权限，并且在启动和关闭时输入您的密码。在命令要求输入密码之前，会有一两秒钟的延迟，所以很容易错过！
- en: 'Config `minikube.me` to be resolved to the IP address of the Istio Ingress
    Gateway as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`minikube.me`以解析到Istio Ingress Gateway的IP地址如下：
- en: 'Get the IP address exposed by the `minikube tunnel` command for the Istio Ingress
    Gateway and save it in an environment variable named `INGRESS_HOST`:'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`minikube tunnel`命令为Istio Ingress Gateway暴露的IP地址，并将其保存为名为`INGRESS_HOST`的环境变量：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Update `/etc/hosts` so that `minikube.me` points to the Istio Ingress Gateway:'
  id: totrans-136
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`/etc/hosts`，使`minikube.me`指向Istio Ingress Gateway：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Remove the line in `/etc/hosts` where `minikube.me` that points to the IP address
    of the Minikube instance (`minikube ip`). Verify that `/etc/hosts` only contains
    one line that translates `minikube.me` and that it points to the IP address of
    the Istio Ingress Gateway; for example, the value of `$INGRESS_HOST`:'
  id: totrans-138
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除`/etc/hosts`文件中指向Minikube实例（`minikube ip`）IP地址的`minikube.me`那一行。确认`/etc/hosts`只包含将`minikube.me`翻译为IP地址的一条线，并且它指向Istio
    Ingress Gateway的IP地址；例如，`$INGRESS_HOST`的值：
- en: '![](img/466a71aa-3959-4801-8017-ce9c50266daf.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/466a71aa-3959-4801-8017-ce9c50266daf.png)'
- en: 'Verify that Kiali, Jaeger, and Grafana can be reached through the tunnel with
    the following commands:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认Kiali，Jaeger和Grafana可以通过隧道访问，使用以下命令：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Each command should return `200` (OK).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 每个命令应返回`200`（OK）。
- en: The `minikube tunnel` command can stop running if, for example, your computer
    or the Minikube instance in the virtual machine is paused or restarted. The command needs
    to be restarted manually in these cases. So, if you fail to call API's on the
    `https://minikube.me` URL or if Kiali's web UI can't reach Jaeger to visualize
    distributed tracing, or Grafana to visualize performance metrics, always check
    whether the Minikube tunnel is running and restart it if required.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`minikube tunnel`命令可能会因为例如您的计算机或虚拟机中的Minikube实例被暂停或重新启动而停止运行。在这些情况下，需要手动重新启动该命令。因此，如果您无法调用`https://minikube.me`
    URL上的API，或者Kiali的Web UI无法访问Jaeger以可视化分布式跟踪，或者无法通过Grafana以可视化性能指标，总是要检查Minikube隧道是否正在运行，并在需要时重新启动它。'
- en: An added bonus from using the minikube tunnel command
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`minikube tunnel`命令的额外好处
- en: 'Running the `minikube tunnel` command also makes it possible to access some
    other cluster-internal Kubernetes services that may be of interest. Once the environment
    is up and running as described in the *Running commands to create the service
    mesh* section, the following can be achieved:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`minikube tunnel`命令还使可以访问一些其他可能感兴趣的集群内部Kubernetes服务成为可能。一旦按照*运行创建服务网格的命令*部分描述的方式启动环境，以下是可以实现的：
- en: 'The `health` endpoint of the `product-composite` microservice can be checked
    with the following command:'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`product-composite`微服务的`health`端点可以通过以下命令进行检查：'
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Refer to the *Observing the service mesh* section for an explanation of the
    use of port `4004`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有关端口`4004`的用法的解释，请参阅*观察服务网格*部分。
- en: 'MySQL tables in the review database can be accessed with the following command:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在以下命令中访问审查数据库中的MySQL表：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'MongoDB collections in the `product` and `recommendations` databases can be
    accessed with the following commands:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过以下命令访问`product`和`recommendations`数据库中的MongoDB集合：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: RabbitMQ's web UI can be accessed using the following URL: `http://rabbitmq.hands-on.svc.cluster.local:15672`.
    Log in using the credentials `rabbit-user-dev` and `rabbit-pwd-dev`.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过以下URL访问RabbitMQ的Web UI：`http://rabbitmq.hands-on.svc.cluster.local:15672`。使用`rabbit-user-dev`和`rabbit-pwd-dev`凭据登录。
- en: With the Minikube tunnel in place, we are now ready to create the service mesh.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Minikube隧道就位后，我们现在准备创建服务网格。
- en: Creating the service mesh
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建服务网格
- en: With Istio deployed, we are ready to create the service mesh. We will use the
    `kubernetes/scripts/deploy-dev-env.bash` script to set up an environment for development
    and testing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在Istio部署完成后，我们准备创建服务网格。我们将使用`kubernetes/scripts/deploy-dev-env.bash`脚本来为开发和测试设置环境。
- en: The steps required to create the service mesh are basically the same as those
    we used in [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Implementing
    Kubernetes Features as an Alternative* (refer to the *Testing with Kubernetes
    ConfigMaps, secrets, and ingress* section). Let's first see what additions have
    been made to the Kubernetes definition files to set up the service mesh before
    we run the commands to create the service mesh.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务网格所需的步骤基本上与我们在第[17章](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)中使用的*作为替代实现Kubernetes特性*（请参阅*使用Kubernetes
    ConfigMaps, secrets和ingress进行测试*部分）相同。首先让我们看看已经对Kubernetes定义文件做了哪些添加，以设置服务网格，然后再运行创建服务网格的命令。
- en: Source code changes
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码更改
- en: 'To be able to run the microservices in a service mesh managed by Istio, the
    following changes have been applied to the Kubernetes definition files:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够在由Istio管理的服务网格中运行微服务，已经对Kubernetes定义文件进行了以下更改：
- en: The deployment scripts have been updated to inject Istio proxies
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署脚本已更新以注入Istio代理
- en: The file structure of the Kubernetes definition files has been changed
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes定义文件的文件结构已更改
- en: Kubernetes definition files for Istio have been added
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经添加了Istio的Kubernetes定义文件
- en: Let's go through them one by one.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一进行。
- en: Updating the deployment scripts to inject Istio proxies
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新部署脚本以注入Istio代理
- en: The scripts used to deploy microservices in Kubernetes, `deploy-dev-env.bash` and `deploy-prod-env.bash`,
    both in the `kubernetes/scripts` folder, have been updated to inject Istio proxies
    into the five microservices, that is, the `auth-server`, `product-composite`,
    `product`, `recommendation`, and `review` services.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kubernetes/scripts`文件夹中的用于在Kubernetes中部署微服务的脚本`deploy-dev-env.bash`和`deploy-prod-env.bash`，都已更新以向五个微服务注入Istio代理，即`auth-server`、`product-composite`、`product`、`recommendation`和`review`服务。
- en: The `deploy-prod-env.bash` script will be used in the *Performing zero downtime
    deploys *section*. *
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`deploy-prod-env.bash`脚本将在*执行零停机部署*部分使用。'
- en: 'The `istioctl kube-inject` command previously described in the *Injecting Istio
    proxies in existing microservices* section has been added to both deployment scripts
    as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 之前在*向现有微服务中注入Istio代理*部分描述的`istioctl kube-inject`命令已添加到两个部署脚本中，如下所示：
- en: '[PRE16]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since the `kubectl apply` command will start a rolling upgrade, the following
    command has been added to wait for the upgrade to be completed:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`kubectl apply`命令将启动滚动升级，以下命令已添加以等待升级完成：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'During the rolling upgrade, we will have two pods running for each microservice:
    an old one without an Istio proxy and a new one with the Istio proxy injected.
    The `waitForPods` function will wait until the old pods are terminated; that is,
    the rolling upgrade is complete, and only the five new pods are running. To identify
    what pods to wait for, a label named `version` is used. In a development environment,
    all microservice pods are labeled with `version=latest`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在滚动升级期间，我们将为每个微服务运行两个容器：一个没有Istio代理的老版本容器和一个注入了Istio代理的新版本容器。`waitForPods`函数将等待直到老版本容器终止；也就是说，滚动升级完成，只有五个新容器在运行。为了确定要等待哪个容器，使用了一个名为`version`的标签。在开发环境中，所有微服务容器都标记为`version=latest`。
- en: 'For example, the deployment file for the product microservice, `kubernetes/services/base/deployments/product-deployment.yml`, has
    the following definition of the `version` label:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，产品微服务的部署文件`kubernetes/services/base/deployments/product-deployment.yml`，`version`标签的定义如下：
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the *Performing zero downtime deployments* section, where we will upgrade
    microservices from version `v1` to `v2`, the version label will be set to `v1` and `v2`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在第*执行零停机部署*部分，我们将从版本`v1`升级微服务到`v2`，版本标签将设置为`v1`和`v2`。
- en: 'Finally, the following command has been added to the scripts to make them wait
    until the deployments and their pods are ready:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本中已添加以下命令，以使脚本等待部署及其容器就绪：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After reviewing the updates in the deployment scripts, let's see how the file
    structure of the Kubernetes definition files has been affected as a result of
    the introduction of Istio.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看部署脚本的更新后，让我们看看引入Istio后Kubernetes定义文件的文件结构有何变化。
- en: Changing the file structure of the Kubernetes definition files
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改Kubernetes定义文件的文件结构
- en: 'The file structure for the Kubernetes definition files in `kubernetes/services` has
    been expanded a bit since [Chapter 16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml),
    *Deploying Our Microservices to Kubernetes* (refer to the *Introduction to Kustomize*
    section) and this now appears as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自[第16章](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)《将我们的微服务部署到Kubernetes》以来，`kubernetes/services`中的Kubernetes定义文件结构已经有所扩展，*Introduction
    to Kustomize*部分进行了说明，现在的结构如下所示：
- en: '![](img/cc35c1d8-4834-4161-9759-347150bb845d.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc35c1d8-4834-4161-9759-347150bb845d.png)'
- en: The `base` folder consists of three subfolders. The reason for this is that
    we will run two versions of the microservices concurrently in the *Performing zero
    downtime deploys* section, that is, one pod per microservice version. Since pods
    are managed by the deployment object, we also need two deployment objects per
    microservice. To be able to achieve this, the base version of the deployment objects
    has been placed in a separate folder, `deployments`. The service objects and the
    Istio definitions are placed in their own base folders: `services` and `istio`, respectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`base`文件夹包含三个子文件夹。这是因为在*执行零停机部署*部分，我们将同时运行微服务的两个版本，即每个微服务版本一个容器。由于容器由部署对象管理，我们还需要每个微服务两个部署对象。为了实现这一点，部署对象的基础版本已放在一个单独的文件夹`deployments`中，服务对象和Istio定义分别放在它们自己的基础文件夹：`services`和`istio`。'
- en: 'In the development environment, we will only run one version per microservice. Its
    kustomization file, `kubernetes/services/overlays/dev/kustomization.yml`, has
    been updated to include all three folders as base folders:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发环境中，我们将为每个微服务运行一个版本。其kustomization文件`kubernetes/services/overlays/dev/kustomization.yml`已更新，以将所有三个文件夹作为基本文件夹包括在内：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Refer to the following *Performing zero downtime deploys* section for details
    on how two concurrent versions of the microservices will be deployed using the
    setup for the production environment.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何在生产环境设置中部署两个并发微服务版本的详细信息，请参阅后面的*零停机部署*部分。
- en: For now, let's also go through the new files in the Istio folder.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们也浏览一下Istio文件夹中的新文件。
- en: Adding Kubernetes definition files for Istio
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Istio添加Kubernetes定义文件
- en: Istio definitions have been added to the `istio` folder. The Istio files of
    interest in this section are the gateway definition and its corresponding virtual
    services. The other Istio files will be explained in the *Authenticating external
    requests using OAuth 2.0/OIDC access tokens* and *Protecting internal communication
    using mutual authentication (mTLS)* sections.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在`istio`文件夹中添加了Istio定义。本节中感兴趣的Istio文件是网关定义及其相应的虚拟服务。其他Istio文件将在*使用OAuth 2.0/OIDC访问令牌进行外部请求认证*和*使用相互认证（mTLS）保护内部通信*部分进行解释。
- en: 'The Istio gateway is declared in the `kubernetes/services/base/istio/gateway.yml` file and
    appears as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Istio网关在`kubernetes/services/base/istio/gateway.yml`文件中声明，如下所示：
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following are some explanations of the preceding source code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对前面源代码的一些解释：
- en: The gateway is named `hands-on-gw`; this name is used by the virtual services
    underneath.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网关名为`hands-on-gw`；这个名称被下面的虚拟服务使用。
- en: The `selector` field specifies that the gateway resource will be handled by
    the built-in Istio Ingress Gateway.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`selector`字段指定网关资源将由内置的Istio Ingress Gateway处理。'
- en: The `hosts` and `port` fields specify that the gateway will handle incoming
    requests for the `minikube.me` hostname using HTTPS over port `443`.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hosts`和`port`字段指定网关将使用端口`443`上的HTTPS处理`minikube.me`主机名的传入请求。'
- en: The `tls` field specifies where the Istio Ingress Gateway can find the certificate
    and private key used for HTTPS communication. Refer to the *Protecting external
    endpoints with HTTPS and certificates* section for details on how these certificate
    files are created.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tls`字段指定了Istio Ingress Gateway用于HTTPS通信的证书和私钥的位置。有关这些证书文件如何创建的详细信息，请参阅*使用HTTPS和证书保护外部端点*部分。'
- en: 'The virtual service object for routing requests from the gateway to the `product-composite`
    service, `kubernetes/services/base/istio/product-composite-virtual-service.yml`,
    appears as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 用于将网关请求路由到`product-composite`服务的虚拟服务对象，即`kubernetes/services/base/istio/product-composite-virtual-service.yml`，如下所示：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Explanations for the preceding source code are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前面源代码的解释如下：
- en: The `hosts` field specifies that the virtual service will route requests sent
    to the host, `minikube.me`.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hosts`字段指定虚拟服务将路由发送到主机`minikube.me`的请求。'
- en: The `match` and `route` blocks specify that requests that contain a URI starting
    with `/product-composite` will be forwarded to the Kubernetes service named `product-composite`.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`match`和`route`块指定包含以`/product-composite`开头的URI的请求将被转发到名为`product-composite`的Kubernetes服务。'
- en: In the preceding source code, the destination host is specified using its short
    name, in other words, `product-composite`. This works since the example in this
    chapter keeps all Kubernetes definitions in one and the same namespace, `hands-on`.
    If that is not the case, it is recommended in the Istio documentation using the
    host's **fully qualified domain name **(**FQDN**) instead, that is, `product-composite.hands-on.svc.cluster.local`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的源代码中，目标主机使用其简称指定，也就是说，`product-composite`。由于本章节的示例将所有Kubernetes定义保存在同一个命名空间`hands-on`中，因此这样可以工作。如果不是这种情况，Istio文档建议使用主机的**完全限定域名**（**FQDN**）代替，即`product-composite.hands-on.svc.cluster.local`。
- en: Finally, the virtual service object for routing requests from the gateway to
    the auth server, `kubernetes/services/base/istio/auth-server-virtual-service.yml`,
    looks very similar, the difference being that it routes requests that start with `/oauth`
    to the Kubernetes service, `auth-server`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虚拟服务对象用于将网关请求路由到认证服务器，即`kubernetes/services/base/istio/auth-server-virtual-service.yml`，看起来非常相似，不同之处在于它将带有`/oauth`前缀的请求路由到Kubernetes服务`auth-server`。
- en: With these changes in the source code in place, we are now ready to create the
    service mesh.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在源代码中实施了这些更改后，我们现在准备创建服务网格。
- en: Running commands to create the service mesh
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行创建服务网格的命令
- en: 'Create the service mesh by running the following commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令创建服务网格：
- en: 'Build Docker images from source with the following commands:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从源代码构建Docker镜像：
- en: '[PRE23]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Recreate the `hands-on` namespace, and set it as the default namespace:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建`hands-on`命名空间，并将其设置为默认命名空间：
- en: '[PRE24]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Execute the deployment by running the `deploy-dev-env.bash` script with the
    following command:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令执行部署：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Once the deployment is complete, verify that we have two containers in each
    of the microservice pods:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，验证我们每个微服务pods中都有两个容器：
- en: '[PRE26]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Expect a response along the lines of the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的响应类似于以下内容：
- en: '![](img/f0040d84-3544-45f4-9dd2-fd7a30e5ea31.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0040d84-3544-45f4-9dd2-fd7a30e5ea31.png)'
- en: Note that the pods that run our microservices report two containers per pod;
    that is, they have the Istio proxy injected as a sidecar!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，运行我们微服务的pods每个pods报告两个容器；也就是说，它们有一个作为边车的Istio代理被注入！
- en: 'Run the usual tests with the following command:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行常规测试：
- en: '[PRE27]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The default values for `script test-em-all.bash` have been updated from previous
    chapters to accommodate Kubernetes running in Minikube.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`script test-em-all.bash`的默认值已从之前的章节更新，以适应在Minikube中运行的Kubernetes。'
- en: 'Expect the output to be similar to what we have seen in previous chapters:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 期望输出与我们在前面的章节中看到的内容类似：
- en: '![](img/f18d0079-84bb-4a36-96db-00e71ec593df.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f18d0079-84bb-4a36-96db-00e71ec593df.png)'
- en: 'You can try out the APIs manually by running the following commands:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令手动尝试API。
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Expect the requested product ID, `2`, in the response.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 期望在响应中收到请求的产品ID，`2`。
- en: With the service mesh up and running, let's see how we can observe what's going
    on in the service mesh using Kiali!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格运行起来后，让我们看看如何使用Kiali观察服务网格中正在发生的事情！
- en: Observing the service mesh
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察服务网格
- en: In this section, we will use Kiali together with Jaeger to observe what's going
    on in the service mesh. For performance monitoring using Grafana, refer to [Chapter
    20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml), *Monitoring Microservices*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Kiali和Jaeger一起观察服务网格中的情况。有关使用Grafana进行性能监控的信息，请参阅[第20章](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml)，*监控微服务*。
- en: Before we do that, we need to get rid of some noise created by the health checks
    performed by Kubernetes' liveness and readiness probes. In the previous chapters,
    they have been using the same port as the API requests. This means that Istio
    will collect telematics data for both health checks and requests sent to the API.
    This will cause the graphs shown by Kiali to become unnecessarily cluttered. Kiali
    can filter out traffic that we are not interested in, but a simpler solution is
    to use a different port for the health checks.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样做之前，我们需要消除由Kubernetes的生存和就绪探针执行的健康检查产生的某些噪声。在前面的章节中，它们一直使用与API请求相同的端口。这意味着Istio将收集健康检查和发送到API的请求的遥测数据。这会导致Kiali显示的图表不必要地变得混乱。Kiali可以过滤掉我们不感兴趣的流量，但一个更简单的解决方案是使用不同的端口进行健康检查。
- en: 'Microservices can be configured to use a separate port for requests sent to
    the actuator endpoints, for example, health checks sent to the `/actuator/health` endpoint. The
    following line has been added to the common configuration file for all microservices,
    `config-repo/application.yml`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务可以配置为使用单独的端口来发送到actuator端点的请求，例如，发送到`/actuator/health`端点的健康检查。所有微服务的共同配置文件`config-repo/application.yml`中已添加以下行：
- en: '[PRE29]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This will make all microservices use port `4004` to expose the health endpoints.
    All deployment files in the `kubernetes/services/base/deployments` folder have
    been updated to use port `4004` in their liveness and readiness probes.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使所有微服务使用端口`4004`来暴露健康端点。`kubernetes/services/base/deployments`文件夹中的所有部署文件都已被更新为在其生存和就绪探针中使用端口`4004`。
- en: 'The Spring Cloud Gateway (this is retained so we can run tests in Docker Compose)
    will continue to use the same port for requests to the API and the `health` endpoint.
    In the `config-repo/gateway.yml` configuration file, the management port is reverted
    to the port used for the API:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Gateway（保留此内容，这样我们可以在Docker Compose中运行测试）将继续为API和`health`端点的请求使用相同的端口。在`config-repo/gateway.yml`配置文件中，管理端口已恢复为用于API的端口：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With the requests sent to the health endpoint out of the way, we can start to
    send some requests through the service mesh.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 处理了发送到健康端点的请求后，我们可以开始通过服务网格发送一些请求。
- en: 'We will start a low-volume load test using `siege`, which we got to know in
    [Chapter 16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml), *Deploying Our Microservices
    to Kubernetes* (refer to the *Performing a rolling upgrade* section). After that,
    we will go through some of the most important parts of Kiali to see how Kiali
    can be used to observe a service mesh. We will also explore Kiali''s integration
    with Jaeger and how Jaeger is used for distributed tracing:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`siege`开始一次低流量的负载测试，我们在[第16章](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)中了解过，*将我们的微服务部署到Kubernetes*（参考*执行滚动升级*部分）。之后，我们将浏览Kiali的一些最重要的部分，了解如何使用Kiali观察服务网格。我们还将探索Kiali与Jaeger的集成以及Jaeger如何用于分布式跟踪：
- en: 'Start the test with the following commands:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令开始测试：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The first command will get an OAuth 2.0/OIDC access token that will be used
    in the next command, where `siege` is used to submit one HTTP request per second
    to the product-composite API.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令将获取一个OAuth 2.0/OIDC访问令牌，该令牌将在下一个命令中使用，其中`siege`用于向product-composite API每秒提交一个HTTP请求。
- en: 'Expect output from the `siege` command as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 预计`siege`命令的输出如下：
- en: '![](img/124bc0e4-2fda-4b01-be2f-81c0653eaa0a.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/124bc0e4-2fda-4b01-be2f-81c0653eaa0a.png)'
- en: 'Open Kiali''s web UI using the `http://kiali.istio-system.svc.cluster.local:20001/kiali` URL in
    a web browser and log in with the following username and password: `admin` and
    `admin`. Expect a web page similar to the following:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`http://kiali.istio-system.svc.cluster.local:20001/kiali` URL在Web浏览器中打开Kiali的Web
    UI，并使用以下用户名和密码登录：`admin`和`admin`。预计会出现如下网页：
- en: '![](img/621bc3a8-d0ad-4254-9de3-fea188d12428.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/621bc3a8-d0ad-4254-9de3-fea188d12428.png)'
- en: Click on the Overview tab, if not already active.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击概述标签，如果尚未激活。
- en: 'Click on the graph icon in the hands-on namespace. Expect a graph to be shown,
    representing the current traffic flow through the service mesh, along the lines
    of the following:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在hands-on命名空间中点击图表图标。预计会显示一个图表，代表服务网格当前的流量流向，如下所示：
- en: '![](img/8e6dcb80-2f70-4714-acb9-b8abcf8e49cd.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e6dcb80-2f70-4714-acb9-b8abcf8e49cd.png)'
- en: Click on the Display-button, unselect Service Nodes, and select Traffic Animation.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击显示按钮，取消选择服务节点，选择流量动画。
- en: Kiali displays a graph representing requests that are currently sent through
    the service mesh, where active requests are represented by small moving circles
    along with arrows.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kiali显示一个代表通过服务网格发送的当前请求的图表，其中活动请求由小移动圆圈和箭头表示。
- en: This gives a pretty good initial overview of what's going on in the service
    mesh!
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这为服务网格中正在发生的事情提供了相当不错的初始概览！
- en: 'Let''s now look at some distributed tracing using Jaeger:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们使用Jaeger进行一些分布式跟踪：
- en: '![](img/36f57f31-b217-474d-8bd6-99ddb55b7370.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36f57f31-b217-474d-8bd6-99ddb55b7370.png)'
- en: Click on the product node.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击产品节点。
- en: 'Click on the Service: product link. On the web page for the service, click
    on the Traces tab in the menu and Kiali will use Jaeger to show an embedded view
    of traces that the product service is involved in. Expect a web page such as the
    following:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击服务：product链接。在服务网页上，点击菜单中的跟踪标签，Kiali将使用Jaeger显示产品服务参与的跟踪的内嵌视图。预计会出现如下网页：
- en: '![](img/4e530bf8-fba4-4321-9f45-450dfb3fe9dc.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e530bf8-fba4-4321-9f45-450dfb3fe9dc.png)'
- en: 'Click on one of the traces to examine it. Expect a web page such as the following:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击其中一个跟踪项进行检查。预计会出现如下网页：
- en: '![](img/402e5bb3-c03c-48dc-a5aa-ed14e971ecb0.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/402e5bb3-c03c-48dc-a5aa-ed14e971ecb0.png)'
- en: This is basically the same tracing information as Zipkin, made available in
    [Chapter 14](42f456c5-d911-494a-a1ba-4631863068b6.xhtml), *Understanding Distributed
    Tracing*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上与Zipkin相同跟踪信息，在[第14章](42f456c5-d911-494a-a1ba-4631863068b6.xhtml)中提供，*理解分布式跟踪*。
- en: There is much more to explore, but this is enough by way of an introduction.
    Feel free to explore the web UI in Kiali, Jaeger, and Grafana on your own.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多值得探索的内容，但这已经足够作为一个介绍了。可以自由地探索Kiali、Jaeger和Grafana的Web UI。
- en: In [Chapter 20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml), *Monitoring Microservices*,
    we will explore performance monitoring capabilities further.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第20章](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml)中，*监控微服务*，我们将进一步探索性能监控功能。
- en: Let's move on and learn how Istio can be used to improve security in the service
    mesh!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续学习如何使用Istio改进服务网格的安全性！
- en: Securing a service mesh
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护服务网格
- en: 'In this section, we will learn how to use Istio to improve the security of
    a service mesh. We will cover the following topics:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用Istio提高服务网格的安全性。我们将涵盖以下主题：
- en: How to protect external endpoints with HTTPS and certificates
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用HTTPS和证书保护外部端点
- en: How to require that external requests are authenticated using OAuth 2.0/OIDC
    access tokens
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何要求外部请求使用OAuth 2.0/OIDC访问令牌进行认证
- en: How to protect internal communication using mutual authentication (mTLS)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用相互认证（mTLS）保护内部通信
- en: Let's now understand each of these in the following sections.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在理解这些每个部分。
- en: Protecting external endpoints with HTTPS and certificates
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HTTPS和证书保护外部端点
- en: 'In the *Creating the service mesh* section, we saw how the Istio Ingress Gateway
    is configured to use the following certificate files to protect external requests
    sent to `minikube.me` using HTTPS. The Istio Ingress Gateway is configured as
    follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在*创建服务网格*一节中，我们看到了Istio Ingress Gateway是如何配置使用以下证书文件来保护通过HTTPS发送到`minikube.me`的外部请求的。Istio
    Ingress Gateway的配置如下：
- en: '[PRE32]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: But where did these files come from, you may ask?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些文件是从哪里来的，你可能会问？
- en: 'We can see how the Istio Ingress Gateway is configured by running the following
    command:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下命令来查看Istio Ingress Gateway的配置：
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will find that it is prepared to mount an optional secret named `istio-ingressgateway-certs`
    and that it will be mapped to the folder, `/etc/istio/ingressgateway-certs/`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会发现它准备挂载一个名为`istio-ingressgateway-certs`的可选证书，并将其映射到`/etc/istio/ingressgateway-certs/`文件夹：
- en: '![](img/206f48a3-7bfa-42df-97ad-e1be388adda9.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/206f48a3-7bfa-42df-97ad-e1be388adda9.png)'
- en: This results in certificate files, `tls.crt` and `tls.key`, from a secret named `istio-ingressgateway-certs`
    being made available to the Istio Ingress Gateway on the `/etc/istio/ingressgateway-certs/tls.crt`
    and `/etc/istio/ingressgateway-certs/tls.key` file paths.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致从名为`istio-ingressgateway-certs`的秘密中创建了证书文件`tls.crt`和`tls.key`，使Istio Ingress
    Gateway能够在`/etc/istio/ingressgateway-certs/tls.crt`和`/etc/istio/ingressgateway-certs/tls.key`文件路径上使用。
- en: 'Creating of this secret is handled by means of the `deploy-dev-env.bash` and `deploy-prod-env.bash` deployment
    scripts, found in the `kubernetes/scripts` folder, by means of the following command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个秘密的创建是通过在`kubernetes/scripts`文件夹中使用`deploy-dev-env.bash`和`deploy-prod-env.bash`部署脚本来处理的，使用以下命令：
- en: '[PRE34]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The certificate files were created in [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml),
    *Implementing Kubernetes Features as an Alternative* (refer to the *Testing with
    Kubernetes ConfigMaps, secrets, and ingress* section).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 证书文件是在第17章[实现Kubernetes特性作为替代](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)中创建的，*使用Kubernetes
    ConfigMaps、secrets和ingress进行测试*部分。
- en: 'To verify that it is these certificates that are used by the Istio Ingress
    Gateway, we can run the following command:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证确实是这些证书被Istio Ingress Gateway使用，我们可以运行以下命令：
- en: '[PRE35]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Expect the following output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 期待以下输出：
- en: '![](img/a5d99e95-d5ab-49fc-b2c3-62afb06295d1.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5d99e95-d5ab-49fc-b2c3-62afb06295d1.png)'
- en: The output shows that the certificate is issued for `minikube.se` and that it
    is self-signed; that is, the issuer is also `minikube.me`.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示证书是为`minikube.se`发行的，并且是自签名的，即发行者也是`minikube.me`。
- en: This self-signed certificate can be replaced with a certificate bought by a
    trusted certificate authority (CA) for production use cases. Istio has recently
    added support for the automated provisioning of trusted certificates using, for
    example, the cert manager and Let's Encrypt, as we did in [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml),
    *Implementing Kubernetes Features as an Alternative* (refer to the *Provisioning
    certificates with the cert manager and Let's Encrypt* section). This support is
    currently a bit too complex to fit into this chapter.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自签名的证书可以替换为信任的证书授权机构（CA）为生产使用案例购买的证书。Istio最近添加了对使用例如cert manager和Let's Encrypt的信任证书的自动化支持，正如我们在第17章[实现Kubernetes特性作为替代](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)中一样，*使用cert
    manager和Let's Encrypt提供证书*部分。这种支持目前过于复杂，不适合这一章。
- en: With the certificate configuration verified, let's now move on to see how the
    Istio Ingress Gateway can protect microservices from unauthenticated requests**.**
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 证书配置验证完成后，让我们接下来看看Istio Ingress Gateway如何保护微服务免受未经认证的请求**。
- en: Authenticating external requests using OAuth 2.0/OIDC access tokens
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OAuth 2.0/OIDC访问令牌验证外部请求
- en: Istio Ingress Gateway is capable of requiring and validating JWT-based OAuth
    2.0/OIDC access tokens, in other words, protecting the microservices in the service
    mesh from external unauthenticated requests. For a recap on JWT, OAuth 2.0, and
    OIDC, refer to [Chapter 11](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Secure
    Access to APIs* (see the *Authenticating and authorizing API access using OAuth
    2.0 and OpenID Connect* section).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Istio Ingress Gateway能够要求并验证基于JWT的OAuth 2.0/OIDC访问令牌，换句话说，保护服务网格中的微服务免受外部未认证请求的侵害。回顾JWT，OAuth
    2.0和OIDC，请参阅第11章[《保护API的访问》](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)，*安全的API访问*（参考*使用OAuth
    2.0和OpenID Connect对API访问进行身份验证和授权*部分）。
- en: 'To enable authentication, we need to create an Istio `Policy` object that specifies
    which targets should be protected and which access token issuers, that is, OAuth
    2.0/OIDC providers, should be trusted. This is done in the `kubernetes/services/base/istio/jwt-authentication-policy.yml`
    file and appears as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用身份验证，我们需要创建一个Istio `Policy`对象，指定应受保护的目标和应信任的访问令牌发行商，即OAuth 2.0/OIDC提供程序。这是在`kubernetes/services/base/istio/jwt-authentication-policy.yml`文件中完成的，并如下所示：
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Explanations for the preceding source code are as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下源代码的解释如下：
- en: The `targets` list specifies that the authentication check will be performed
    for requests sent to the `product-composite` microservice.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targets`列表指定了将对发送到`product-composite`微服务的请求执行身份验证检查。'
- en: The `origins` list specifies the OAuth 2.0/OIDC providers we rely on. For each
    issuer, the name of the issuer and the URL for its JSON web key set are specified.
    For a recap, see [Chapter 11](bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml), *Securing
    Access to APIs* (refer to the *Introducing OpenId Connect* section). We have specified
    the local auth server, `http://auth-server.local`.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`origins`列表指定了我们依赖的OAuth 2.0/OIDC提供程序。对于每个发行者，指定了发行者的名称及其JSON Web密钥集的URL。回顾一下，请参阅第11章[《保护API的访问》](bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml)，*介绍OpenId
    Connect*（参考*使用OAuth 2.0和OpenID Connect对API访问进行身份验证和授权*部分）。我们已经指定了本地认证服务器，`http://auth-server.local`。'
- en: The policy file was applied by the `kubernetes/scripts/deploy-dev-env.bash` deployment
    script, when it was used in the *Running commands to create the service mesh* section.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 策略文件是由`kubernetes/scripts/deploy-dev-env.bash`部署脚本在*运行创建服务网格的命令*部分应用的。
- en: The easiest way to verify that an invalid request is rejected by the Istio Ingress
    Gateway and not the `product-composite` microservice is to make a request without
    an access token and observe the error message that is returned. The Istio Ingress
    Gateway returns the following error message, `Origin authentication failed.`,
    in the event of a failed authentication, while the `product-composite` microservice
    returns an empty string. Both return the HTTP code `401` (Unauthorized).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证无效请求是否被Istio Ingress Gateway拒绝而不是`product-composite`微服务，最简单的方法是发送一个没有访问令牌的请求，并观察返回的错误信息。在认证失败的情况下，Istio
    Ingress Gateway会返回以下错误信息，`Origin authentication failed.`，而`product-composite`微服务会返回一个空字符串。两者都返回HTTP代码`401`（未授权）。
- en: 'Try it out with the following commands:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令尝试：
- en: 'Make a request without an access token along the lines of the following:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送类似以下内容的无访问令牌请求：
- en: '[PRE37]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Expect a response saying `Origin authentication failed. HTTP Code: 401`.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '期待一个说`Origin authentication failed. HTTP Code: 401`的响应。'
- en: 'Temporarily delete the policy with the following command:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暂时使用以下命令删除策略：
- en: '[PRE38]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Wait a minute to allow that policy change to be propagated to the Istio Ingress
    Gateway and then retry the request without an access token. The response should
    now only contain the HTTP code: `HTTP Code: 401`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '等待一分钟，让策略更改传播到Istio Ingress Gateway，然后尝试不带访问令牌发送请求。现在响应应仅包含HTTP代码：`HTTP Code:
    401`。'
- en: 'Enable the policy again with the following command:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令再次启用策略：
- en: '[PRE39]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Suggested additional exercise:** Try out the Auth0 OIDC provider, as described
    in [Chapter 11](bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml), *Securing Access
    to APIs* (refer to the T*esting with an OpenID Connect provider, Auth0,* section.
    Add your Auth0 provider to `jwt-authentication-policy.yml`. In my case, it appears
    as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**建议的额外练习：**尝试使用Auth0 OIDC提供程序，如第11章[《保护API的访问》](bcb9bba0-d2fe-4ee8-954b-07a7e38e1115.xhtml)所述，*使用OpenID
    Connect提供程序Auth0进行测试*。将您的Auth0提供程序添加到`jwt-authentication-policy.yml`中。在我的情况下，它如下所示：'
- en: '[PRE40]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, let's move on to the last security mechanism that we will cover in Istio
    – the automatic protection of internal communication in the service mesh using
    mutual authentication, mTLS.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们转向我们将要在Istio中涵盖的最后一种安全机制——使用相互认证（mTLS）保护服务网格内部通信的自动保护。
- en: Protecting internal communication using mutual authentication (mTLS)
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用相互认证（mTLS）保护内部通信
- en: In this section, we will learn how Istio can be configured to automatically
    protect internal communication within the service mesh using *mutual* *authentication*,
    mTLS. When using mutual authentication, not only does the service side prove its
    identity by exposing a certificate, but also the clients prove their identity
    to the servers by exposing a client-side certificate. This provides a higher level
    of security compared to normal TLS/HTTPS usage, where only the identity of the
    server is proven. Setting up and maintaining mutual authentication; that is, the
    provision of new, and the rotating of outdated, certificates, is known to be complex
    and is therefore seldom used. Istio fully automates the provisioning and rotation
    of certificates for mutual authentication used for internal communication inside
    the service mesh. This makes it much easier to use mutual authentication compared
    to setting it up manually.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置Istio以自动保护服务网格内部通信，使用*相互* *认证*，即mTLS。在使用相互认证时，服务端通过暴露证书来证明其身份，客户端也通过暴露客户端证书来向服务器证明其身份。与仅证明服务器身份的正常TLS/HTTPS使用相比，这提供了更高的安全性。设置和维护相互认证——即提供新的证书和轮换过时的证书——被认为很复杂，因此很少使用。Istio完全自动化了用于服务网格内部通信的相互认证的证书提供和轮换。这使得与手动设置相比，使用相互认证变得容易得多。
- en: So, why should we use mutual authentication? Isn't it sufficient to protect
    external APIs with HTTPS and OAuth 2.0/OIDC access tokens?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们为什么应该使用相互认证？仅仅保护外部API的HTTPS和OAuth 2.0/OIDC访问令牌难道还不够吗？
- en: As long as the attacks come through the external API, it might be sufficient.
    But what if a pod inside the Kubernetes cluster becomes compromised? For example,
    if an attacker gains control over a pod, then the attacker can start listening
    to traffic between other pods in the Kubernetes cluster. If the internal communication
    is sent as plain text, it will be very easy for the attacker to gain access to
    sensitive information sent between pods in the cluster. To minimize the damage
    caused by such an intrusion, mutual authentication can be used to prevent an attacker
    from eavesdropping on internal network traffic.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 只要攻击是通过外部API发起的，可能就足够了。但如果Kubernetes集群中的一个pods被攻陷了会怎样呢？例如，如果攻击者控制了一个pods，那么攻击者可以开始监听Kubernetes集群中其他pods之间的通信。如果内部通信以明文形式发送，攻击者将很容易获取集群中pods之间传输的敏感信息。为了尽量减少此类入侵造成的损害，可以使用相互认证来防止攻击者监听内部网络流量。
- en: To enable the use of mutual authentication managed by Istio, Istio needs to
    be configured both on the server side, using a policy, and on the client side,
    using a destination rule.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用由Istio管理的相互认证的使用，Istio需要在服务器端使用策略进行配置，在客户端使用目的地规则进行配置。
- en: 'When using the demo configuration of Istio, as we did in the *Deploying Istio
    in a Kubernetes cluster* section, we got a global mesh policy created that configures
    the server side to use a permissive mode, meaning the Istio proxies will allow
    both plain text and encrypted requests. This can be verified with the following
    command:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Istio的演示配置时，如我们在*在Kubernetes集群中部署Istio*一节中那样，我们创建了一个全局网格策略，配置服务器端使用宽容模式，这意味着Istio代理将允许明文和加密的请求。这可以通过以下命令来验证：
- en: '[PRE41]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Expect a response similar to the following:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的回答与以下类似：
- en: '![](img/adef0bf8-61fd-484e-854c-a07e50609603.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adef0bf8-61fd-484e-854c-a07e50609603.png)'
- en: 'To configure microservices to use mutual authentication when sending requests
    internally to other microservices, a destination rule is created for each microservice.
    This is done in the `kubernetes/services/base/istio/internal_mtls_destination_rules.yml` file. The destination
    rules all look the same; for example, for the `product-composite` service, they
    appear as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使微服务在向其他微服务内部发送请求时使用相互认证，需要为每个微服务创建一个目的地规则。这是在`kubernetes/services/base/istio/internal_mtls_destination_rules.yml`文件中完成的。目的地规则看起来都一样；例如，对于`product-composite`服务，它们如下所示：
- en: '[PRE42]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`trafficPolicy` is set to use `tls` with `ISTIO_MUTUAL`, meaning mutual authentication
    is managed by Istio.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`trafficPolicy` 设置为使用 `tls` 与 `ISTIO_MUTUAL`，这意味着相互认证是由 Istio 管理的。'
- en: The destination rules were applied by the `kubernetes/scripts/deploy-dev-env.bash` deployment
    script, when it was used in the preceding *Running commands to create the service
    mesh* section.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 目的地规则是在前面的*运行创建服务网格的命令*部分中使用 `kubernetes/scripts/deploy-dev-env.bash` 部署脚本时应用的。
- en: 'To verify that the internal communication is protected, perform the following
    steps:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证内部通信是否得到保护，请执行以下步骤：
- en: Ensure that the load tests started in the preceding *Observing the service mesh* section are
    still running.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在前面的*观察服务网格*部分中启动的负载测试仍在运行。
- en: Go to the Kiali graph in a web browser ([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali)).
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在网络浏览器中打开 Kiali 图([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali))。
- en: 'Click on the Display button to enable the Security label. The graph will show
    a padlock on all communication links that are protected by Istio''s automated
    mutual authentication, as follows:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击显示按钮以启用安全标签。图表将在所有由 Istio 自动相互认证保护的通信链路上显示一个锁，如下所示：
- en: '![](img/cc3e1088-449c-4140-8da3-9d90d416f1ab.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc3e1088-449c-4140-8da3-9d90d416f1ab.png)'
- en: Expect a padlock on all links except for those to resource managers – RabbitMQ,
    MySQL, and MongoDB.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 期望所有链接上都有一个锁，除了那些到资源管理器的链接——RabbitMQ、MySQL 和 MongoDB。
- en: Calls to RabbitMQ, MySQL, and MongoDB are not handled by Istio proxies, and
    therefore require manual configuration to be protected using TLS.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 对 RabbitMQ、MySQL 和 MongoDB 的调用不由 Istio 代理处理，因此需要手动配置以使用 TLS 进行保护。
- en: With this, we have seen all three security mechanisms in Istio in action, and
    it's now time to see how Istio can help us to verify that a service mesh is resilient.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经看到了 Istio 中所有三种安全机制的实际应用，现在该看看 Istio 是如何帮助我们验证服务网格具有弹性的。
- en: Ensuring that a service mesh is resilient
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保服务网格具有弹性
- en: In this section, we will learn how to use Istio to ensure that a service mesh
    is resilient; that is, it can handle temporary faults in a service mesh. Istio
    comes with mechanisms similar to what the Spring Framework offers in terms of
    timeouts, retries, and a type of circuit breaker called **outlier detection** to
    handle temporary faults. When it comes to deciding whether language-native mechanisms
    should be used to handle temporary faults, or whether this should be delegated
    to a service mesh such as Istio, I tend to favor using language-native mechanisms,
    as in the examples in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4J*. In many cases, it is important to keep
    the logic for handling errors, for example, handling fallback alternatives for
    a circuit breaker, together with other business logic for a microservice.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 Istio 确保服务网格具有弹性；也就是说，它能够处理服务网格中的临时故障。Istio 提供了类似于 Spring Framework
    在超时、重试以及称为**异常检测**的类型断路器方面的机制来处理临时故障。当涉及到决定是否使用语言原生机制来处理临时故障，或者是否将此任务委托给如 Istio
    的服务网格时，我倾向于使用语言原生机制，如在[第 13 章](23795d34-4068-4961-842d-989cde26b642.xhtml)、*使用
    Resilience4J 提高弹性*中的例子所示。在许多情况下，将处理错误的逻辑与其他微服务业务逻辑保持在一起是很重要的，例如为断路器处理回退选项。
- en: There are cases when the corresponding mechanisms in Istio could be of great
    help. For example, if a microservice is deployed and it is determined that it
    can't handle temporary faults that occur in production from time to time, then
    it can be very convenient to add a timeout or a retry mechanism using Istio instead
    of waiting for a new release of the microservice with corresponding error handling
    features put in place.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Istio 中相应的机制可以提供很大帮助的情况。例如，如果一个微服务被部署，并且确定它不能处理在生产中偶尔发生的临时故障，那么使用 Istio 添加一个超时或重试机制而不是等待具有相应错误处理功能的微服务新版本发布将非常方便。
- en: Another capability in the area of resilience that comes with Istio is the capability
    to inject faults and delays into an existing service mesh. Why would anyone want
    to do that?
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Istio 而来的弹性方面的另一个功能是向现有服务网格注入故障和延迟的能力。为什么有人想这么做呢？
- en: Injecting faults and delays in a controlled way is very useful for verifying
    that the resilient capabilities in the microservices work as expected! We will
    try them out in this section, verifying that the retry, timeout, and circuit breaker
    in the `product-composite` microservice work as expected.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以受控的方式注入故障和延迟非常有用，可以验证微服务中的弹性能力是否如预期工作！我们将在本节中尝试它们，验证 `product-composite` 微服务中的重试、超时和断路器是否如预期工作。
- en: In [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml), *Improving Resilience
    Using Resilience4j* (refer to the *Adding programmable delays and random errors*
    section), we added support for injecting faults and delays in the microservices source
    code. That source code can preferably be replaced by using Istio's capabilities
    for injecting faults and delays at runtime, as demonstrated in the following.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在第13章 *使用 Resilience4j 提高韧性* 中（参考 *在微服务源代码中添加可编程延迟和随机错误* 部分），我们添加了对在微服务源代码中注入故障和延迟的支持。最好使用
    Istio 的功能在运行时注入故障和延迟，如下所示。
- en: We will begin by injecting faults to see whether the retry mechanisms in the
    `product-composite` microservice work as expected. After that, we will delay the
    responses from the product service and verify that the circuit breaker handles
    the delay as expected.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先注入故障，以查看 `product-composite` 微服务中的重试机制是否按预期工作。之后，我们将延迟产品服务的响应，并验证断路器如何按预期处理延迟。
- en: Testing resilience by injecting faults
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过注入故障测试弹性
- en: Let's make the product service throw random errors and verify that the microservice
    landscape handles this correctly. We expect the retry mechanism in the `product-composite`
    microservice to kick in and retry the request until it succeeds or its limit of
    max numbers of retries is reached. This will ensure that a shortlived fault does
    not affect the end user more than the delay introduced by the retry attempts.
    Refer to the *Adding a retry mechanism* section in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4j*, for a recap on the retry mechanism in
    the `product-composite` microservice.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让产品服务抛出随机错误，并验证微服务架构是否正确处理此情况。我们期望 `product-composite` 微服务中的重试机制启动，并重试请求，直到成功或达到最大重试次数限制。这将确保短暂故障不会比重试尝试引入的延迟对最终用户产生更大影响。参考[第13章](23795d34-4068-4961-842d-989cde26b642.xhtml)
    *使用 Resilience4j 提高韧性* 中的 *添加重试机制* 部分，回顾 `product-composite` 微服务中的重试机制。
- en: 'Faults can be injected using `kubernetes/resilience-tests/product-virtual-service-with-faults.yml`.
    This appears as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `kubernetes/resilience-tests/product-virtual-service-with-faults.yml` 注入故障。这如下所示：
- en: '[PRE43]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The definition says that 20% of the requests sent to the product service shall
    be aborted with the HTTP status code 500 (Internal Server Error).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 定义说明，发送到产品服务的20%的请求应使用 HTTP 状态码500（内部服务器错误）终止。
- en: 'Perform the following steps to test this:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以测试此功能：
- en: Ensure that the load tests using `siege`, as started in the *Observing the service
    mesh*section, are running.
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保正在运行使用 `siege` 的负载测试，如在第 *观察服务网格* 节中启动。
- en: 'Apply fault injection with the following command:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令应用故障注入：
- en: '[PRE44]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Monitor the output from the `siege` load tests tool. Expect output similar
    to the following:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控 `siege` 负载测试工具的输出。期望输出类似于以下内容：
- en: '![](img/0bc01f04-88dd-4507-8e09-21be25135194.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bc01f04-88dd-4507-8e09-21be25135194.png)'
- en: From the sample output, we can see that all requests are still successful, in
    other words, status 200 (OK) is returned; however, some of them (20%) take an
    extra second to complete. This indicates that the retry mechanism in the `product-composite`
    microservice has kicked in and has retried a failed request to the product service.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 从样本输出中，我们可以看到所有请求仍然成功，换句话说，返回状态200（OK）；然而，其中一些（20%）需要额外一秒钟才能完成。这表明 `product-composite`
    微服务中的重试机制已经启动，并重试了发送到产品服务的失败请求。
- en: 'Kiali will also indicate that something is wrong with requests sent to the
    product service, as follows:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kiali 也会指示产品服务收到的请求存在问题，如下所示：
- en: Go to the call graph in Kiali's web UI that we used earlier to observe the traffic
    in our namespace, `hands-on`.
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往我们之前用于观察我们命名空间 `hands-on` 中的流量的 Kiali web UI 的调用图。
- en: Click on the Display menu button and select Service Nodes.
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击显示菜单按钮，选择服务节点。
- en: Click on the menu button to the left of the Display button, named No edge labels,
    and select the Response time option.
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击显示按钮左侧的菜单按钮，名为无边缘标签，然后选择响应时间选项。
- en: 'The graph will show something like the following:'
  id: totrans-349
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图表将显示如下内容：
- en: '![](img/ee4ac35f-e8a5-437f-b1de-261228e0ed60.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee4ac35f-e8a5-437f-b1de-261228e0ed60.png)'
- en: The arrow to the Service Node product will be shown in red to indicate that
    failed requests are detected. If we click on the arrow, we can see fault statistics
    to the right.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 服务节点产品的箭头将显示为红色，以指示检测到失败请求。如果我们点击箭头，可以在右侧看到故障统计。
- en: In the preceding sample screenshot, an error rate of 19.4% is reported, which
    corresponds well with the 20% we asked for. Note that the arrow from the Istio
    Gateway to the `product-composite` service is still green. This means that the
    retry mechanism in the `product-composite` service protects the end user; in other
    words, the faults do not propagate to the end user.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例屏幕截图中，报告的错误率为19.4%，与我们要求的20%相符。请注意，从Istio网关到`product-composite`服务的箭头仍然是绿色的。这意味着`product-composite`服务中的重试机制保护了终端用户；换句话说，故障不会传播到终端用户。
- en: 'Conclude the removal of the fault injection with the following command:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令结束故障注入的移除：
- en: '[PRE45]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let's now move on to the next section, where we will inject delays to trigger
    the circuit breaker.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一节，在那里我们将注入延迟以触发电路断路器。
- en: Testing resilience by injecting delays
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过注入延迟来测试弹性
- en: From [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml), *Improving Resilience
    Using Resilience4j* (refer to the *Introducing the circuit breaker* section),
    we know that a circuit breaker can be used to prevent problems due to the slow
    response of services, or the fact that the services do not respond at all.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第13章](23795d34-4068-4961-842d-989cde26b642.xhtml)，*使用Resilience4j提高弹性*（参考*介绍电路断路器*部分），我们知道电路断路器可以用来防止服务响应缓慢或服务根本不响应的问题。
- en: Let's verify that the circuit breaker in the `product-composite` service works
    as expected by injecting a delay into the product service using Istio. A delay
    can be injected using a virtual service.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过向`product-composite`服务中注入延迟来验证电路断路器是否按预期工作。可以使用虚拟服务注入延迟。
- en: 'Refer to `kubernetes/resilience-tests/product-virtual-service-with-delay.yml`.
    Its code appears as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考`kubernetes/resilience-tests/product-virtual-service-with-delay.yml`。其代码如下所示：
- en: '[PRE46]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The preceding definition says that all requests sent to the product service
    shall be delayed by 3 seconds.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的定义说明所有发送到产品服务的请求都应延迟3秒。
- en: Requests sent to the product service from the `product-composite` service are
    configured to timeout after 2 seconds. The circuit breaker is configured to open
    its circuit if 3 consecutive requests fail. When the circuit is open, it will
    fast-fail; in other words, it will immediately throw an exception, not attempting
    to call the underlying service. The business logic in the `product-composite`
    microservice will catch this exception and apply fallback logic. For a recap,
    see  [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml), *Improving Resilience
    Using Resilience4j* (refer to the *Adding a circuit breaker* section).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 从`product-composite`服务发送到产品服务的请求配置为在2秒后超时。如果连续三次请求失败，电路断路器配置为打开电路。当电路打开时，它将快速失败；换句话说，它将立即抛出异常，不尝试调用底层服务。`product-composite`微服务中的业务逻辑将捕获此异常并应用回退逻辑。回顾一下，请参阅[第13章](23795d34-4068-4961-842d-989cde26b642.xhtml)，*使用Resilience4j提高弹性*（参考*添加电路断路器*部分）。
- en: 'Perform the following steps to test the circuit breaker by injecting a delay:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下步骤测试通过注入延迟的电路断路器：
- en: Stop the load test run by means of the `siege` command by pressing *Ctrl + C* in
    the terminal window where `siege` is running.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在运行`siege`的终端窗口中按下`Ctrl + C`命令来停止负载测试运行。
- en: 'Create a temporary delay in the product service with the following command:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在产品服务中创建临时延迟：
- en: '[PRE47]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Acquire an access token as follows:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下获取访问令牌：
- en: '[PRE48]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Send six requests in a row. Expect the circuit to open up after the first three
    failed calls, that the circuit breaker applies fast-fail logic for the three last
    calls, and that a fallback response is returned, as follows:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续发送六个请求。期望在第一次三次失败调用后打开电路，电路断路器对最后三次调用应用快速失败逻辑，并返回回退响应，如下所示：
- en: '[PRE49]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The responses from the first 3 calls are expected to be a timeout-related error
    message, and a response time of 2 seconds, in other words, the timeout time. Expect
    responses for the first 3 calls along the lines of the following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 前三次调用的响应预期会出现与超时相关的错误信息，响应时间为2秒，换句话说，就是超时时间。期望前三次调用的响应类似于以下内容：
- en: '**![](img/c61d561d-e023-488b-9e75-1dbb32bd2f5c.png)**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/c61d561d-e023-488b-9e75-1dbb32bd2f5c.png)**'
- en: 'The responses from the last 3 calls are expected to be a response from the
    fallback logic with a short response time. Expect responses for the last 3 calls
    as follows:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 最后3次调用的响应预计将是回退逻辑的响应，响应时间短。期待最后3次调用的响应如下：
- en: '![](img/468fa43c-0a38-47fa-8baf-80de72b55711.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](img/468fa43c-0a38-47fa-8baf-80de72b55711.png)'
- en: 'Simulate the fact that the delay problem is fixed by removing the temporary
    delay with the following command:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下命令删除临时延迟来模拟延迟问题已解决：
- en: '[PRE50]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Verify that correct answers are returned again and without any delay by sending
    a new request using the preceding command.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用前一个命令发送新请求，验证正确答案再次返回，且没有任何延迟。
- en: 'If you want to check the state of the circuit breaker, you can do it with the
    following command:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想检查熔断器的状态，您可以使用以下命令：
- en: '**`curl product-composite.hands-on.svc.cluster.local:4004/actuator/health -s
    | jq -r .details.productCircuitBreaker.details.state`**'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl product-composite.hands-on.svc.cluster.local:4004/actuator/health -s
    | jq -r .details.productCircuitBreaker.details.state`'
- en: It should report `CLOSED`, `OPEN`, or `HALF_OPEN`, depending on its state.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该报告`CLOSED`，`OPEN`或`HALF_OPEN`，具体取决于其状态。
- en: This proves that the circuit breaker reacts as expected when we inject a delay
    using Istio. This concludes testing features in Istio that can be used to verify
    that the microservice landscape is resilient. The final feature we will explore
    in Istio is its support for traffic management; we will establish how it can be
    used to enable deployments with zero downtime.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了我们使用Istio注入延迟时，熔断器按预期反应。这结束了在Istio中测试可以验证微服务架构具有弹性的功能。我们将在Istio中探索的最后一项功能是其对流量管理的支持；我们将了解如何使用它来实现零停机部署。
- en: Performing zero-downtime deployments
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行零停机部署
- en: As already mentioned in [Chapter 16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml),
    *Deploying Our Microservices to Kubernetes* (refer to the *Performing a rolling
    upgrade* section), being able to deploy an update without downtime becomes crucial
    with a growing number of autonomous microservices that are updated independently
    of one another.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第16章](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)中提到的*将我们的微服务部署到Kubernetes*（请参阅*执行滚动升级*部分），随着越来越多的自治微服务独立于彼此进行更新，能够在不停机的情况下部署更新变得至关重要。
- en: In this section, we will learn about Istio's traffic management and routing
    capabilities and how they can be used to perform deployments of new versions of
    microservices without requiring any downtime. In [Chapter 16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml),
    *Deploying Our Microservices to Kubernetes* (refer to the *Performing a rolling
    upgrade* section), we saw how Kubernetes can be used to perform a rolling upgrade
    without requiring any downtime. Using the Kubernetes rolling upgrade mechanism
    automates the entire process, but unfortunately provides no option to test the
    new version before all users are routed to the new version.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习Istio的流量管理和路由功能，以及如何使用它们来执行微服务的全新版本的部署，而不需要任何停机时间。在第16章中，我们看到了Kubernetes如何用来执行滚动升级而不需要停机。（请参阅*执行滚动升级*部分）。使用Kubernetes的滚动升级机制可以自动化整个过程，但不幸的是，它提供了一个测试新版本的机会，在所有用户都被路由到新版本之前。
- en: 'Using Istio, we can deploy the new version, but initially route all users to
    the existing version (called the old version in this chapter). After that, we
    can use Istio''s fine-grained routing mechanism to control how users are routed
    to the new and the old versions. We will see how two popular upgrade strategies
    can be implemented using Istio:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Istio，我们可以部署新版本，但最初将所有用户路由到现有版本（在本章中称为旧版本）。在那之后，我们可以使用Istio的精细路由机制来控制用户如何被路由到新旧版本。我们将了解如何使用Istio实现两种流行的升级策略：
- en: '**Canary deploys*: ***When using canary deploys, most users are routed to the
    old version, except for a group of selected test users who are routed to the new
    version. When the test users have approved the new version, regular users can
    be routed to the new version using a blue/green deploy.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金丝雀部署*: 当使用金丝雀部署时，大多数用户被路由到旧版本，除了被选中的测试用户被路由到新版本。当测试用户批准了新版本，可以使用蓝/绿部署将普通用户路由到新版本。'
- en: '**Blue/****green** **deploys*****: ***Traditionally, a blue/green deploy means
    that all users are switched to either the blue or the green version, one being
    the new version and the other being the old version. If something goes wrong when
    switching over to the new version, it is very simple to switch back to the old
    version. Using Istio, this strategy can be refined by gradually shifting users
    over to the new version, for example, starting with 20% of the users and then
    slowly increasing the percentage of users who are routed to the new version. At
    all times, it is very easy to route all users back to the old version if a fatal
    error is revealed in the new version.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝/绿** **部署**：传统上，蓝/绿部署意味着所有用户都被切换到蓝色或绿色版本之一，一个是新版本，另一个是旧版本。如果切换到新版本时出现问题，很容易切换回旧版本。使用Istio，可以通过逐渐将用户切换到新版本来优化此策略，例如，从20%的用户开始，然后逐渐增加被路由到新版本的用户百分比。在任何时候，如果新版本中揭示了致命错误，很轻易地将所有用户路由回旧版本。'
- en: As already stated in[ ](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)[Chapter
    16](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml), *Deploying Our Microservices
    to Kubernetes* (refer to the *Performing a rolling upgrade* section), it is important
    to remember that a prerequisite for these types of upgrade strategies is that
    the upgrade is backward-compatible. Such an upgrade is compatible both in terms
    of APIs and message formats, which are used to communicate with other services
    and database structures. If the new version of the microservice requires changes
    to external APIs, message formats, or database structures that the old version
    can't handle, these upgrade strategies can't be applied.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在第[16章](ebeb41b4-eea4-4d73-89ba-6788e2e68bac.xhtml)《将我们的微服务部署到Kubernetes》（参考*执行滚动升级*部分）中提到的，这些升级策略的一个重要前提是升级是向后兼容的。这种升级在API和消息格式上都兼容，这些是与其他服务和数据库结构通信所使用的。如果微服务的新版本需要对外部API、消息格式或数据库结构进行更改，而旧版本无法处理，则无法应用这些升级策略。
- en: 'We will go through the following deploy scenario in the following subsections:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下子节中逐步部署以下部署场景：
- en: We will start by deploying the `v1` and `v2` versions of the microservices,
    with routing configured to send all requests to the `v1` version of the microservices.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先部署`v1`和`v2`版本的微服务，路由配置为将所有请求发送到`v1`版本的微服务。
- en: Next, we will allow a test group to run canary tests; that is, we'll verify
    the new `v2` versions of the microservices. To simplify the tests somewhat, we
    will only route test users to the new versions of the core microservices, that
    is, the `product`, `recommendation`, and `review` microservices.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将允许一个测试组运行金丝雀测试；也就是说，我们将验证微服务的新的`v2`版本。为了简化测试，我们将只将测试用户路由到核心微服务的新版本，即`product`、`recommendation`和`review`微服务。
- en: 'Finally, we will start to move regular users over to the new versions using
    a blue/green deploy: initially, a small percentage of users and then, over time,
    more and more users until, eventually, they are all routed to the new version.
    We will also see how we can quickly switch back to the `v1` versions if a fatal
    error is detected in the new v2 version.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将开始使用蓝/绿部署将普通用户切换到新版本：最初是一小部分用户，然后随着时间的推移，越来越多的用户，直到最终所有用户都被路由到新版本。我们还将了解如何在检测到新v2版本中的致命错误时，快速切换回`v1`版本。
- en: Let's first see what changes have been applied to the source code to deploy
    two concurrent versions, `v1` and `v2`, of the microservices.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看为了部署两个并发版本的微服务（`v1`和`v2`），源代码中已经应用了哪些更改。
- en: Source code changes
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码更改
- en: 'As already mentioned in the *Changing the file structure of the Kubernetes
    definition files *section, the file structure for the Kubernetes definition files
    in `kubernetes/services` has been expanded in this chapter to support the deployment
    of concurrent versions of the microservices in the production environment. The
    expanded file structure appears as follows:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*改变Kubernetes定义文件的文件结构*章节中提到的，为了支持在生产环境中部署微服务的并发版本，在本章中`kubernetes/services`中的Kubernetes定义文件的文件结构进行了扩展。扩展后的文件结构如下所示：
- en: '![](img/26f33b74-c0c1-46ea-80f3-259b536c3bf8.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26f33b74-c0c1-46ea-80f3-259b536c3bf8.png)'
- en: Details regarding the development environment have been removed from the preceding
    diagram.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图表中已经移除了有关开发环境的具体细节。
- en: Let's first see how service and deployment objects for the `v1` and `v2` versions
    of the microservices are configured and created. After that, we will go through
    additional definition files for Istio objects used to control the routing.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们看看微服务`v1`和`v2`版本的服务和部署对象是如何配置和创建的。之后，我们将查看用于控制路由的Istio对象的附加定义文件。
- en: Service and deployment objects for concurrent versions of microservices
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务并发版本的服务和部署对象
- en: To be able to run multiple versions of a microservice concurrently, the deployment
    objects and their corresponding pods must have different names, for example, `product-v1`
    and `product-v2`. There must, however, be only one Kubernetes service object per
    microservice. All traffic to a specific microservice always goes through one and
    the same service object, irrespective of what version of the pod the request will
    be routed to in the end. This is achieved using Kustomize by splitting up deployment
    objects and service objects into different folders.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够同时运行微服务的多个版本，部署对象及其相应的pods必须有不同的名称，例如`product-v1`和`product-v2`。然而，每个微服务只能有一个Kubernetes服务对象。所有特定微服务的流量总是通过一个相同的service对象，无论请求最终会被路由到哪个版本的pods。这是通过将部署对象和服务对象分成不同的文件夹来使用Kustomize实现的。
- en: 'To give deployment objects and their pods version-dependent names, the `kustomization.yml`
    file can use the `nameSuffix` directive to tell Kustomize to add the given suffix
    to all Kubernetes objects it creates. For example, the `kustomization.yml` file
    used for the `v1` version of the microservices in the `kubernetes/services/overlays/prod/v1` folder
    appears as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给部署对象及其pods赋予版本依赖的名称，`kustomization.yml`文件可以使用`nameSuffix`指令告诉Kustomize在它创建的所有Kubernetes对象上添加给定后缀。例如，用于`v1`版本的微服务在`kubernetes/services/overlays/prod/v1`文件夹中的`kustomization.yml`文件如下所示：
- en: '[PRE51]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `nameSuffix: -v1` setting will result in all objects created using this `kustomization.yml` file
    being named with the `-v1` suffix.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`nameSuffix: -v1`设置会导致使用这个`kustomization.yml`文件创建的所有对象都带有`-v1`后缀。'
- en: 'To create the objects without a version suffix, and the deployment objects
    and their pods with the `v1` and `v2` version suffixes, the `kubernetes/scripts/deploy-prod-env.bash` deployment
    script executes separate `kubectl apply` commands as follows:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建没有版本后缀的对象，以及具有`v1`和`v2`版本后缀的部署对象及其pods，`kubernetes/scripts/deploy-prod-env.bash`部署脚本执行单独的`kubectl
    apply`命令，如下所示：
- en: '[PRE52]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Let's also see what Istio definition files we have added to configure routing
    rules.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再看看我们都添加了哪些Istio定义文件来配置路由规则。
- en: Added Kubernetes definition files for Istio
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加了用于Istio的Kubernetes定义文件。
- en: 'To configure routing rules, we will add Istio objects to the `kubernetes/services/overlays/prod/istio` folder.
    Each microservice has a virtual service object that defines the weight distribution
    for the routing between the old and the new versions. Initially, it is set to
    route 100% of the traffic to the old version. For example, the routing rule for
    the product microservice in `product-routing-virtual-service.yml` appears as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置路由规则，我们将向`kubernetes/services/overlays/prod/istio`文件夹添加Istio对象。每个微服务都有一个虚拟服务对象，定义了旧版本和新版本之间的路由权重分布。最初，它被设置为将100%的流量路由到旧版本。例如，产品微服务在`product-routing-virtual-service.yml`中的路由规则如下所示：
- en: '[PRE53]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The virtual service defines subsets for the old and the new versions. To define
    what actual versions the old and new versions are, each microservice also has
    a destination rule defined. The destination rule details how the old subset and
    the new subset shall be identified, for example, in the case of the product microservice
    in `old_new_subsets_destination_rules.yml`:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟服务定义了旧版本和新版本的子集。为了定义旧版本和新版本实际的版本，每个微服务也定义了一个目的地规则。目的地规则详细说明了如何识别旧子集和新子集，例如，在`old_new_subsets_destination_rules.yml`中的产品微服务：
- en: '[PRE54]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The subset named `old` points to product pods that have the `version` label
    set to `v1`, while the subset named `new` points to pods with the `version` label
    set to `v2`.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 名为`old`的子集指向标签`version`设置为`v1`的产品pods，而名为`new`的子集指向标签`version`设置为`v2`的pods。
- en: To route traffic to a specific version, Istio documentation recommends that
    pods are labeled with a label named `version` to identify its version. Refer to [https://istio.io/docs/setup/kubernetes/additional-setup/requirements/](https://istio.io/docs/setup/kubernetes/additional-setup/requirements/) for
    details.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将流量路由到特定版本，Istio 文档建议将 pod 标记为名为 `version` 的标签以标识其版本。 有关详细信息，请参阅 [https://istio.io/docs/setup/kubernetes/additional-setup/requirements/](https://istio.io/docs/setup/kubernetes/additional-setup/requirements/)。
- en: 'Finally, to support canary testers, an extra routing rule has been added to
    the virtual services for the three core microservices: product, recommendation,
    and review. This routing rule states that any incoming request that has an HTTP
    header named `X-group` set to the value `test` will always be routed to the new
    version of the service. This appears as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了支持金丝雀测试人员，在三个核心微服务（`product`、`recommendation` 和 `review`）的虚拟服务中添加了一个额外的路由规则。
    此路由规则说明，任何带有名为 `X-group` 的 HTTP 头的 incoming 请求设置为值 `test` 总是会路由到服务的新版本。 它如下所示：
- en: '[PRE55]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The `match` and `route` sections specify that requests with the HTTP header, `X-group`,
    set to the value, `test`, shall be routed to the subset named `new`.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`match` 和 `route` 部分指定，带有 HTTP 头 `X-group` 设置为 `test` 值的请求将被路由到名为 `new` 的子集中。'
- en: 'To create these Istio objects, the `kubernetes/scripts/deploy-prod-env.bash` deployment
    script executes the following command:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这些 Istio 对象，`kubernetes/scripts/deploy-prod-env.bash` 部署脚本执行了以下命令：
- en: '[PRE56]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Finally, to be able to route canary testers to the new version based on header-based
    routing, the `product-composite` microservice has been updated to forward the
    HTTP header `X-group`. Refer to the `getCompositeProduct()` method in the `se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl` class  for
    details.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了能够基于基于头部的路由将金丝雀测试人员路由到新版本，`product-composite` 微服务已更新以转发 HTTP 头 `X-group`。
    有关详细信息，请参阅 `se.magnus.microservices.composite.product.services.ProductCompositeServiceImpl`
    类中的 `getCompositeProduct()` 方法。
- en: Now, we have seen all the changes to the source code and we are ready to deploy
    v1 and v2 versions of the microservices.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到了源代码的所有更改，我们准备部署微服务的 v1 和 v2 版本。
- en: Deploying v1 and v2 versions of the microservices with routing to the v1 version
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署带有路由到 v1 版本的 v1 和 v2 版本的微服务
- en: To be able to test the `v1` and `v2` versions of the microservices, we need
    to remove the development environment we have been using earlier in this chapter
    and create a production environment where we can deploy the v1 and v2 versions
    of the microservices.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够测试微服务的 `v1` 和 `v2` 版本，我们需要删除本章前面一直在使用的开发环境，并创建一个生产环境，我们可以在其中部署 `v1` 和 `v2`
    版本的微服务。
- en: 'To achieve this, run the following commands:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请运行以下命令：
- en: 'Recreate the `hands-on` namespace:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建 `hands-on` 命名空间：
- en: '[PRE57]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Execute the deployment by running the script with the following command:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令来执行部署脚本：
- en: '[PRE58]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The command takes a couple of minutes and should eventually list all the v1
    or v2 versions of the pods as follows:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令需要几分钟时间，最终应如下列出所有 v1 或 v2 版本的 pod：
- en: '![](img/3ac160f5-b56c-46fd-8d26-d0cad647e4d5.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ac160f5-b56c-46fd-8d26-d0cad647e4d5.png)'
- en: 'Run the usual tests to verify that everything works:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下常规测试以验证一切是否正常工作：
- en: '[PRE59]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: If this command is executed immediately after the `deploy` command, it sometimes
    fails. Simply rerun the command and it should run fine!
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在此命令执行后立即执行 `deploy` 命令，有时会失败。 只需重新运行命令，它应该会正常运行！
- en: Since we now have two pods (version V1 and V2) running for each microservice,
    the circuit breaker tests no longer work. The reason for this is that the test
    script can't control which pod it talks to, through the Kubernetes service. The
    test script asks about the state of the circuit breaker in the `product-composite`
    microservice using the actuator endpoint on port `4004`. This port is not managed
    by Istio, so its routing rules do no apply. The test script will therefore not
    know whether it is checking the state of the circuit breaker in V1 or V2 of the `product-composite`
    microservice. We can skip circuit breaker tests by using the `SKIP_CB_TESTS=true` flag.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在为每个微服务运行了两个 pod（V1 和 V2 版本），因此电路测试不再工作。 这是因为测试脚本无法控制它通过 Kubernetes 服务与哪个
    pod 进行通信。 测试脚本通过 `product-composite` 微服务的 actuator 端口 `4004` 查询电路 breaker 的状态。
    此端口不由 Istio 管理，因此其路由规则不适用。 测试脚本因此不知道它是在检查 `product-composite` 微服务的 V1 或 V2 版本的电路
    breaker 状态。 我们可以通过使用 `SKIP_CB_TESTS=true` 标志来跳过电路测试。
- en: 'Expect output that is similar to what we have seen from the previous chapters,
    but excluding the circuit breaker tests:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 期望输出类似于我们在前几章中看到的内容，但排除熔断器测试：
- en: '![](img/4ebb9d21-ccfc-4210-92a0-8be37b37fd74.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ebb9d21-ccfc-4210-92a0-8be37b37fd74.png)'
- en: We are now ready to run some *zero-downtime deploy* tests. Let's begin by verifying
    that all traffic goes to the v1 version of the microservices!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好运行一些*零停机部署*测试。首先，让我们验证所有流量都路由到了微服务的v1版本！
- en: Verifying that all traffic initially goes to the v1 version of the microservices
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证所有流量最初都路由到v1版本的微服务
- en: To verify that all requests are routed to the v1 version of the microservices,
    we will start up the load test tool, `siege`, and then observe the traffic that
    flows through the service mesh using Kiali.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所有请求都被路由到微服务的v1版本，我们将启动负载测试工具`siege`，然后观察通过服务网格使用Kiali流动的流量。
- en: 'Perform the following steps:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Get a new access token and start the `siege` load test tool, with the following
    commands:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个新的访问令牌并启动`siege`负载测试工具，使用以下命令：
- en: '[PRE60]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Go to the graph view in Kiali''s web UI ([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali)):'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在Kiali的Web UI中转到图表视图([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali)):'
- en: Click on the Display menu button and deselect Service Nodes.
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击显示菜单按钮并取消选择服务节点。
- en: 'After a minute or two, expect only traffic to the v1 version of the microservices as
    follows:'
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一两分钟后，期望只有以下v1版本的微服务流量：
- en: '![](img/e2170279-e949-462e-b545-b0b5c6c69d54.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2170279-e949-462e-b545-b0b5c6c69d54.png)'
- en: Good! This means that, even though the v2 versions of the microservices are
    deployed, they do not get any traffic routed to them. Let's now try out canary
    tests where selected test users are allowed to try out the v2 versions of the
    microservices!
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！这意味着尽管已经部署了微服务的v2版本，但它们并没有获得任何路由到的流量。现在让我们尝试进行金丝雀测试，其中选定的测试用户被允许尝试微服务的v2版本！
- en: Running canary tests
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行金丝雀测试
- en: To run a canary test, in other words, in order to be routed to the new versions
    while all other users are still routed to the old versions of the deployed microservices,
    we need to add the `X-group` HTTP header set to the value `test` in our requests
    sent to the external API.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行金丝雀测试，换句话说，即所有其他用户仍然被路由到部署的微服务的旧版本时，我们需要在我们的对外API请求中添加`X-group`HTTP头，设置为`test`值。
- en: To see which version of a microservice served a request, the `serviceAddresses`
    field in the response can be inspected. The `serviceAddresses` field contains
    the hostname of each service that took part in creating the response. The hostname
    is equal to the name of the pod, so we can find the version in the hostname; for
    example, `product-v1-...` for a product service of version V1, and `product-v2-...` for
    a product service of version V2.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看响应了请求的微服务的哪个版本，可以检查响应中的`serviceAddresses`字段。`serviceAddresses`字段包含每个参与创建响应的服务的主机名。主机名等于pods的名称，因此我们可以通过主机名找到版本；例如，对于版本V1的产品服务，主机名为`product-v1-...`，对于版本V2的产品服务，主机名为`product-v2-...`。
- en: Let's begin by sending a normal request and verify that it is the v1 versions
    of the microservices that respond to our request. Next, send a request with the `X-group` HTTP
    header set to the value `test`, and verify that the new v2 versions are responding.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，发送一个正常请求并验证是微服务的v1版本响应了我们的请求。接下来，发送一个`X-group`HTTP头设置为`test`值的请求，并验证新的v2版本正在响应。
- en: 'To do this, perform the following steps:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，执行以下步骤：
- en: 'Perform a normal request to verify that the request is routed to the v1 version
    of the microservices by using `jq` to filter out the `serviceAddresses` field
    in the response:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送一个正常请求以验证请求被路由到微服务的v1版本，使用`jq`过滤掉响应中的`serviceAddresses`字段：
- en: '[PRE61]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Expect a response along the lines of the following:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 期望得到如下类似的响应：
- en: '![](img/661a5a65-6610-40c6-aaf4-5a55f2e57b81.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![](img/661a5a65-6610-40c6-aaf4-5a55f2e57b81.png)'
- en: As expected, all three core services are v1 versions of the microservices.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，所有三个核心服务都是微服务的v1版本。
- en: 'If we add the `X-group=test` header, we expect the request to be served by
    v2 versions of the core microservices. Run the following command:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们添加了`X-group=test`头，我们期望请求由核心微服务的v2版本处理。运行以下命令：
- en: '[PRE62]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Expect a response similar to the following:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 期望得到如下类似的响应：
- en: '![](img/1f63113a-ae33-446b-bdde-05fa4cf621c6.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f63113a-ae33-446b-bdde-05fa4cf621c6.png)'
- en: As expected, all three core microservices that respond are now v2 versions;
    that is, as a canary tester, we are routed to the new v2 versions!
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期那样，现在所有三个核心微服务都是v2版本；也就是说，作为金丝雀测试员，我们被路由到新的v2版本！
- en: Given that the canary tests returned the expected results, we are ready to allow
    normal users to be routed to the new v2 versions using blue/green deployment.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到金丝雀测试返回了预期结果，我们准备允许普通用户通过蓝/绿部署路由到新的v2版本。
- en: Running blue/green tests
  id: totrans-463
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行蓝/绿测试
- en: To route parts of the normal users to the new v2 versions of the microservices,
    we have to modify the weight distribution in the virtual services. They are currently
    100/0; in other words, all traffic is routed to the old v1 versions. We can achieve
    this as we did before, that is, by editing the definition files of the virtual
    services in the `kubernetes/services/overlays/prod/istio` folder and then running
    a `kubectl apply` command to make the change take effect. As an alternative, we
    can use the `kubectl patch` command to change the weight distribution directly
    on the virtual service objects in the Kubernetes API server.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将部分普通用户路由到微服务的新的v2版本，我们必须修改虚拟服务中的权重分布。目前它们是100/0；也就是说，所有流量都被路由到旧的v1版本。我们可以像以前一样实现这一点，即通过编辑`kubernetes/services/overlays/prod/istio`文件夹中虚拟服务的定义文件，然后运行`kubectl
    apply`命令以使更改生效。作为一种替代方案，我们可以使用`kubectl patch`命令直接在 Kubernetes API 服务器中的虚拟服务对象上更改权重分布。
- en: I find the patch command useful when making a number of changes to the same
    objects to try something out, for example, to change the weight distribution in
    the routing rules. In this section, we will use the `kubectl patch` command to
    quickly change the weight distribution in the routing rules between the v1 and
    v2 versions of the microservices. To get the state of a virtual service after
    a number of `kubectl patch` commands have been executed, a command such as `kubectl
    get vs NNN -o yaml` can be issued. For example, to get the state of the virtual
    service of the product microservice, issue the following command: `kubectl get
    vs product-vs -o yaml`.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 patch 命令在要对同一对象进行许多更改以尝试某些东西时很有用，例如，更改路由规则中的权重分布。在本节中，我们将使用`kubectl patch`命令快速更改微服务v1和v2版本之间的路由规则的权重分布。要在一系列`kubectl
    patch`命令执行后获取虚拟服务的状态，可以发出类似于`kubectl get vs NNN -o yaml`的命令。例如，要获取产品微服务的虚拟服务状态，可以发出以下命令：`kubectl
    get vs product-vs -o yaml`。
- en: Since we haven't used the `kubectl patch` command before and it can be a bit
    involved to start with, let's undertake a short introduction to how it works before
    we perform the green/blue deploy.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们以前没有使用过`kubectl patch`命令，而且一开始可能会有些复杂，因此在执行绿/蓝部署之前，让我们先简要介绍它的工作原理。
- en: A short introduction to the kubectl patch command
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要介绍 kubectl patch 命令
- en: 'The `kubectl patch` command can be used to update specific fields in an existing
    object in the Kubernetes API server. We will try the patch command on the virtual
    service for the review microservice, named `review-vs`. The relevant parts of
    the definition for the virtual service, `review-vs`, appear as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl patch`命令可用于在 Kubernetes API 服务器中更新现有对象的具体字段。我们将尝试在名为`review-vs`的评论微服务的虚拟服务上使用
    patch 命令。虚拟服务`review-vs`的定义的相关部分如下所示：'
- en: '[PRE63]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: For the full source code, refer to `kubernetes/services/overlays/prod/istio/review-routing-virtual-service.yml`.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码请参阅`kubernetes/services/overlays/prod/istio/review-routing-virtual-service.yml`。
- en: 'A sample patch command that changes the weight distribution of the routing
    to the v1 and v2 pods in the `review` microservice appears as follows:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 一个改变`review`微服务中路由到v1和v2容器的权重分布的示例 patch 命令如下所示：
- en: '[PRE64]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The command will configure the routing rules of the review microservice to route
    80% of the requests to the old version, and 20% of the requests to the new version.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将配置评论微服务的路由规则，将80%的请求路由到旧版本，将20%的请求路由到新版本。
- en: To specify that the `weight` value shall be changed in the `review-vs` virtual
    service, the `/spec/http/1/route/0/weight` path is given for the old version and `/spec/http/1/route/1/weight`
    for the new version.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定`weight`值将在`review-vs`虚拟服务中更改，旧版本给出了`/spec/http/1/route/0/weight`路径，新版本给出`/spec/http/1/route/1/weight`。
- en: The `0` and `1` in the path are used to specify the index of array elements
    in the definition of the virtual service. For example, `http/1` means the second
    element in the array under the `http` element. See the definition of the preceding `review-vs`
    virtual service.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 路径中的`0`和`1`用于指定虚拟服务定义中数组元素的索引。例如，`http/1`意味着在`http`元素下的第二个元素。请参阅前面的`review-vs`虚拟服务的定义。
- en: From the preceding definition we can see that the second element is the `route`
    element. The first element with index `0` being the match element.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的定义中我们可以看出，第二个元素是`route`元素。第一个元素，索引为`0`的是匹配元素。
- en: Now that we know a bit more about the `kubectl patch` command, we are ready
    to test a blue/green deployment.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对`kubectl patch`命令有了更多的了解，我们准备测试蓝绿部署。
- en: Performing the blue/green deployment
  id: totrans-478
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行蓝绿部署
- en: 'Now, it is time to gradually move more and more users to the new versions using
    blue/green deployment. To perform the deployment, run the following steps:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是逐渐让更多用户使用蓝绿部署转移到新版本的时候了。要执行部署，请按照以下步骤操作：
- en: Ensure that the load test tool, `Siege`, is still running.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保负载测试工具`Siege`仍在运行。
- en: It was started in the preceding *Verifying that all traffic initially goes to
    the v1 version of the microservices* section.
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它是在前面的*验证所有流量最初都发送到微服务的v1版本*部分启动的。
- en: 'To allow 20% of the users to be routed to the new v2 version of the review
    microservice, we can patch the virtual service and change weights with the following
    command:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将20%的用户路由到新的v2版本的`review`微服务，我们可以修补虚拟服务，并使用以下命令更改权重：
- en: '[PRE65]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: To observe the change in the routing rule, go to the Kiali web UI ([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali))
    and select the graph view.
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了观察路由规则的变化，请前往Kiali web UI([http://kiali.istio-system.svc.cluster.local:20001/kiali](http://kiali.istio-system.svc.cluster.local:20001/kiali))并选择图表视图。
- en: Change the edge label to `Requests percentage`.
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将边缘标签更改为`请求百分比`。
- en: 'Wait for a minute before the statics are updated in Kiali so that we can observe
    the change. Expect the graph in Kiali to show something like the following:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kiali中更新统计数据之前等待一分钟，这样我们就可以观察到变化。期望Kiali中的图表显示如下：
- en: '![](img/a610a4da-ad35-4bbd-ab61-f30f72241355.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a610a4da-ad35-4bbd-ab61-f30f72241355.png)'
- en: Depending on how long you have waited, the graph might look a bit different!
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于你等待了多长时间，图表可能看起来会有点不同！
- en: In the screenshot, we can see that Istio now routes traffic to both the v1 and
    v2 versions of the `review` microservice.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图中，我们可以看到Istio现在将流量路由到`review`微服务的v1和v2版本。
- en: 'Of the 33% of the traffic that is sent to the `review` microservice from the
    `product-composite` microservice, 7% are routed to the new v2 pod, and 26% to
    the old v1 pod. This means that 7/33 (= 21%) of the requests are routed to the
    v2 pod, and 26/33 (= 79%) to the v1 pod. This is in line with the 20/80 distribution
    we have requested:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 从`product-composite`微服务发送到`review`微服务的33%流量中，7%被路由到新的v2 pod，26%到旧的v1 pod。这意味着有7/33（=21%）的请求被路由到v2
    pod，有26/33（=79%）被路由到v1 pod。这与我们请求的20/80分布相符：
- en: 'Please feel free to try out the preceding `kubectl patch` command to affect
    the routing rules for the other core microservices: `product` and `recommendation`.'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请随意尝试前面的`kubectl patch`命令，以影响其他核心微服务（`product`和`recommendation`）的路由规则：
- en: 'If you want to route all traffic to the v2 version of all microservices, you
    can run the following script:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想将所有流量路由到所有微服务的v2版本，你可以运行以下脚本：
- en: '[PRE66]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: You have to give Kiali a minute or two to collect metrics before it can visualize
    the changes in routing between the v1 and v2 versions of the microservices, but
    remember that the change in the actual routing is immediate!
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须给Kiali一分钟左右的时间来收集指标，然后它才能可视化微服务v1和v2版本之间的路由变化，但请记住，实际路由的变化是即时的！
- en: 'Expect only v2 versions of the microservices to show up in the graph after
    a while:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 过了一会儿，期望微服务图中的只有v2版本显示出来：
- en: '![](img/fafbc4a4-4e90-47d5-8a5f-b933a548edf3.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fafbc4a4-4e90-47d5-8a5f-b933a548edf3.png)'
- en: Depending on how long you have waited, the graph might look a bit different!
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 取决于你等待了多长时间，图表可能看起来会有点不同！
- en: 'If something goes terribly wrong after the upgrade to v2, the following command
    can be executed to revert all traffic back to the v1 version of all microservices:'
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果升级到v2后出现严重问题，以下命令可以执行以将所有流量恢复到所有微服务的v1版本：
- en: '[PRE67]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: After a short while, the graph in Kiali should look like the screenshot in the
    previous *Verifying that all traffic initially goes to the v1 version of the microservices* section;
    that is, visualize that all requests go to the v1 version of all microservices
    again.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间后，Kiali中的图表应该看起来像前文*验证微服务初始所有流量都发送到v1版本*部分所示的截图；也就是说，再次可视化所有请求都发送到所有微服务的v1版本。
- en: This concludes the introduction to the service mesh concept and Istio as an
    implementation of the concept.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了服务网格概念和作为概念实现的Istio的介绍。
- en: Before we wrap up the chapter, let's recap how we can run tests in Docker Compose
    to ensure that the source code of our microservices does not rely on deployment
    in Kubernetes.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们回顾一下我们如何使用Docker Compose运行测试，以确保我们的微服务源代码不依赖于在Kubernetes中的部署。
- en: Running tests with Docker Compose
  id: totrans-503
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行测试
- en: As mentioned in [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Implementing
    Kubernetes Features as an Alternative* (refer to the *Verifying that the microservices
    work without Kubernetes* section), it is important to ensure that the source code
    of the microservices doesn't become dependent on a platform such as Kubernetes
    or Istio from a functional perspective.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第17章](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)所述，*实施Kubernetes特性作为替代*（参考*验证微服务在没有Kubernetes的情况下是否工作*部分），从功能角度确保微服务的源代码不依赖于Kubernetes或Istio这样的平台是很重要的。
- en: 'To verify that the microservices work as expected without the presence of Kubernetes
    and Istio, run the tests as described in [Chapter 1*7*](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)*,
    Implementing Kubernetes Features as an Alternative* (refer to the T*esting with
    Docker Compose* section). Since the default values of the test script, `test-em-all.bash`,
    are changed, as described previously in the *Running commands to create the service
    mesh* section, the following parameters must be set when using Docker Compose: `HOST=localhost
    PORT=8443 HEALTH_URL=https://localhost:8443`. For example, to run the tests using
    the default Docker Compose file, `docker-compose.yml`, run the following command:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证在没有Kubernetes和Istio的情况下微服务是否按预期工作，请按照[第1章7节](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)*，实施Kubernetes特性作为替代*（参考*使用Docker
    Compose测试*部分）所述运行测试。由于测试脚本`test-em-all.bash`的默认值已如前所述在*运行创建服务网格的命令*部分更改，因此在使用Docker
    Compose时必须设置以下参数：`HOST=localhost PORT=8443 HEALTH_URL=https://localhost:8443`。例如，要使用默认的Docker
    Compose文件`docker-compose.yml`运行测试，请运行以下命令：
- en: '[PRE68]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The tests should, as before, begin by starting all containers; it should then
    run the tests, and finally stop all containers. For details of the expected output,
    see [Chapter 17](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml), *Implementing Kubernetes
    Features as an Alternative* (refer to the *Verifying that the microservices work
    without Kubernetes* section).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 测试应如之前所述，首先启动所有容器；然后运行测试，最后停止所有容器。关于预期输出的详细信息，请参见[第17章](a9327ecc-49e7-4f72-9221-3321b7158d83.xhtml)，*实施Kubernetes特性作为替代*（参考*验证微服务在没有Kubernetes的情况下是否工作*部分）。
- en: After successfully executing the tests using Docker Compose, we have verified
    that the microservices are dependent neither on Kubernetes nor Istio from a functional
    perspective. These tests conclude the chapter on using Istio as a service mesh.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功使用Docker Compose执行测试后，我们验证了微服务在功能上既不依赖于Kubernetes也不依赖于Istio。这些测试结论了使用Istio作为服务网格的章节。
- en: Summary
  id: totrans-509
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the service mesh concept and Istio, an open
    source project that implements it. A service mesh provides capabilities for handling
    challenges in a system landscape of microservices in areas such as security, policy
    enforcement, resilience, and traffic management. A service mesh can also be used
    to make a system landscape of microservices observable by visualizing the traffic
    that flows through the microservices.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了服务网格概念以及实现该概念的开源项目Istio。服务网格为处理系统微服务景观中的挑战提供了能力，如安全、策略执行、弹性和流量管理。服务网格还可以通过可视化微服务之间的流量来使微服务景观可观测。
- en: For observability, Istio uses Kiali, Jaeger, and Grafana (more on Grafana in
    [Chapter 20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml), *Monitoring Microservices*). When
    it comes to security, Istio can be configured to use a certificate to protect
    external APIs with HTTPS and require that external requests contain valid JWT-based
    OAuth 2.0/OIDC access tokens. Finally, Istio can be configured to automatically
    protect internal communication using **mutual authentication** (**mTLS**).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可观测性，Istio使用了Kiali、Jaeger和Grafana（关于Grafana的更多信息请参见[第20章](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml)，*监控微服务*）。当谈到安全性时，Istio可以配置为使用证书来保护使用HTTPS的外部API，并要求外部请求包含有效的基于JWT的OAuth
    2.0/OIDC访问令牌。最后，Istio可以配置为使用**相互认证**（**mTLS**）来自动保护内部通信。
- en: For resilience and robustness, Istio comes with mechanisms for handling retries,
    timeouts, and an outlier detection mechanism similar to a circuit breaker. In
    many cases, it is preferable to implement these resilience capabilities in the
    source code of the microservices, if possible. The ability in Istio to inject
    faults and delays is very useful for verifying that the microservices in the service
    mesh work together as a resilient and robust system landscape. Istio can also
    be used to handle zero-downtime deployments. Using its fine-grained routing rules,
    both canary and blue/green deployments can be performed.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弹性和健壮性，Istio提供了处理重试、超时和类似于断路器的异常检测机制的机制。在许多情况下，如果可能的话，在微服务的源代码中实现这些弹性能力是更好的。Istio注入故障和延迟的能力对于验证服务网格中的微服务作为一个弹性和健壮的系统景观一起工作非常有用。Istio还可以用于处理零停机部署。使用其细粒度的路由规则，可以执行金丝雀和蓝/绿部署。
- en: One important area that we haven't covered yet is how to collect and analyze
    log files created by all microservice instances. In the next chapter, we will
    see how this can be done using a popular stack of tools, known as the EFK stack,
    based on Elasticsearch, Fluentd, and Kibana.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有涉及的一个重要领域是如何收集和分析由所有微服务实例创建的日志文件。在下一章中，我们将了解如何使用流行的工具堆栈，即基于Elasticsearch、Fluentd和Kibana的EFK堆栈，来完成这项工作。
- en: Questions
  id: totrans-514
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the purpose of a proxy component in a service mesh?
  id: totrans-515
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务网格中的代理组件有什么作用？
- en: What's the difference between a control plane and a data plane in a service
    mesh?
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务网格中的控制平面和数据平面有什么区别？
- en: What is the `istioctl kube-inject` command used for?
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`istioctl kube-inject`命令是用来做什么的？'
- en: What is the `minikube tunnel`  command used for?
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`minikube tunnel`命令是用来做什么的？'
- en: What tools are used in Istio for observability?
  id: totrans-519
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Istio中用于可观测性的工具有哪些？
- en: What configuration is required to make Istio protect communication within the
    service mesh using mutual authentication?
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了让Istio使用相互认证来保护服务网格内的通信，需要进行哪些配置？
- en: What can the `abort` and `delay` elements in a virtual service be used for?
  id: totrans-521
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在虚拟服务中，`abort`和`delay`元素可以用来做什么？
- en: What configuration is required to set up a blue/green deploy scenario?
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置蓝/绿部署场景需要进行哪些配置？
