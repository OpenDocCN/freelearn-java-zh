- en: Troubleshooting Guide
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除指南
- en: We have come so far and I am sure you are enjoying each and every moment of
    this challenging and joyful learning journey. I will not say that this book ends
    after this chapter, but rather you are completing the first milestone. This milestone
    opens the doors for learning and implementing a new paradigm in the cloud with
    microservice-based design. I would like to reaffirm that integration testing is
    an important way to test the interaction between microservices and APIs. While
    working on your sample application **online table reservation system** (**OTRS**),
    I am sure you have faced many challenges, especially while debugging the application.
    Here, we will cover a few of the practices and tools that will help you to troubleshoot
    the deployed application, Docker containers, and host machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经走了这么远，我相信您享受这段具有挑战性和快乐的学习旅程中的每一个时刻。我不会说这本书在此章节后结束，而是您正在完成第一个里程碑。这个里程碑为基于微服务设计的云学习和新范式实施打开了大门。我想再次确认集成测试是测试微服务和API之间交互的重要方法。在您处理在线表格预订系统（OTRS）的示例应用程序时，我相信您遇到了许多挑战，尤其是在调试应用程序时。在这里，我们将介绍一些可以帮助您排除部署应用程序、Docker容器和宿主机的故障的最佳实践和工具。
- en: 'This chapter covers the following three topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下三个主题：
- en: Logging and the ELK stack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录和ELK栈
- en: Use of correlation ID for service calls using Zipkin and Sleuth
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zipkin和Sleuth进行服务调用时使用相关ID
- en: Dependencies and versions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖关系和版本
- en: Logging and the ELK stack
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录和ELK栈
- en: Can you imagine debugging any issue without seeing a log on the production system?
    Simply, no, as it would be difficult to go back in time. Therefore, we need logging.
    Logs also give us warning signals about the system if they are designed and coded
    that way. Logging and log analysis is an important step for troubleshooting any
    issue, and also for throughput, capacity, and monitoring the health of the system.
    Therefore, having a very good logging platform and strategy will enable effective
    debugging. Logging is one of the most important key components of software development
    in the initial days.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您能想象在生产系统上不查看日志的情况下调试任何问题吗？简单地说，不能，因为回到过去将会很困难。因此，我们需要日志记录。日志还为我们提供了关于系统的警告信号，如果它们是这样设计和编码的话。日志记录和日志分析是排除任何问题的重要步骤，也是提高吞吐量、扩展能力和监控系统健康状况的重要步骤。因此，拥有一个非常好的日志平台和策略将使调试变得有效。日志是软件开发初期最重要的关键组成部分之一。
- en: 'Microservices are generally deployed using image containers such as Docker
    that provide the log with commands that help you to read logs of services deployed
    inside the containers. Docker and Docker Compose provide commands to stream the
    log output of running services within the container and in all containers respectively.
    Please refer to the following `logs` command of Docker and Docker Compose:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务通常使用如Docker之类的图像容器进行部署，这些容器提供有助于您读取部署在容器内的服务日志的命令。Docker和Docker Compose提供命令以分别流式传输容器内运行服务和所有容器的日志输出。请参阅以下Docker和Docker
    Compose的`logs`命令：
- en: '**Docker logs command:** **Usage:** `docker logs [OPTIONS] <CONTAINER NAME>`  **Fetch
    the logs of a container:**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker日志命令：** **用法：** `docker logs [OPTIONS] <CONTAINER NAME>`  **获取容器的日志：**'
- en: '`**-f, --follow Follow log output**`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-f, --follow 跟随日志输出**`'
- en: '`**--help Print usage**`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--help 打印用法**`'
- en: '`**--since="" Show logs since timestamp**`'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--since="" 自时间戳以来显示日志**`'
- en: '`**-t, --timestamps Show timestamps**`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-t, --timestamps 显示时间戳**`'
- en: '`**--tail="all" Number of lines to show from the end of the logs**`'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--tail="all" 显示日志末尾的行数**`'
- en: '**Docker Compose logs command:** `**Usage: docker-compose logs [options] [SERVICE...]**`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker Compose日志命令：** `**用法：docker-compose logs [options] [SERVICE...]**`'
- en: '**Options:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**选项：**'
- en: '`**--no-color Produce monochrome output**`'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--no-color 产生单色输出**`'
- en: '`**-f, --follow Follow log output**`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-f, --follow 跟随日志输出**`'
- en: '`**-t, --timestamps Show timestamps**`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`**-t, --timestamps 显示时间戳**`'
- en: '`**--tail Number of lines to show from the end of the logs for each container**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`**--tail 显示每个容器日志末尾的行数**'
- en: '**[SERVICES...] Service representing the container - you can give multiple**`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**[SERVICES...] 代表容器的服务 - 你可以指定多个**`'
- en: These commands help you to explore the logs of microservices and other processes
    running inside the containers. As you can see, using the above commands would
    be a challenging task when you have a higher number of services. For example,
    if you have tens or hundreds of microservices, it would be very difficult to track
    each microservice log. Similarly, you can imagine, even without containers, how
    difficult it would be to monitor logs individually. Therefore, you can assume
    the difficulty of exploring and correlating the logs of tens to hundreds of containers.
    It is time-consuming and adds very little value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令帮助你探索运行在容器中的微服务和其它进程的日志。正如你所看到的，当你有很多服务时，使用上述命令将是一个挑战性的任务。例如，如果你有数十个或数百个微服务，跟踪每个微服务的日志将非常困难。同样，你可以想象，即使没有容器，单独监控日志也会非常困难。因此，你可以想象探索和关联数十到数百个容器的日志有多么困难。这是耗时的，并且几乎没有任何价值。
- en: Therefore, a log aggregator and visualizing tools such as the ELK stack come
    to our rescue. It will be used for centralizing logging. We'll explore this in
    the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像ELK堆栈这样的日志聚合和可视化工具就派上用场了。它将用于集中日志。我们将在下一节中探讨这一点。
- en: A brief overview
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要概述
- en: 'The **Elasticsearch, Logstash, Kibana** (**ELK**) stack is a chain of tools
    that performs log aggregation, analysis, visualization, and monitoring. The ELK
    stack provides a complete logging platform that allows you to analyze, visualize,
    and monitor all of your logs, including all types of product logs and system logs.
    If you already know about the ELK stack, please skip to the next section. Here,
    we''ll provide a brief introduction to each tool in the ELK Stack:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch、Logstash、Kibana**（**ELK**）堆栈是一系列执行日志聚合、分析、可视化和监控的工具。ELK堆栈提供了一个完整的日志平台，允许你分析、可视化和监控所有日志，包括各种产品日志和系统日志。如果你已经了解ELK堆栈，请跳到下一节。在这里，我们将简要介绍ELK堆栈中的每个工具：'
- en: '![](img/87defb1f-0f40-4d90-8f8d-0be85aaf64c6.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87defb1f-0f40-4d90-8f8d-0be85aaf64c6.png)'
- en: 'ELK overview (source: elastic.co)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ELK概览（来源：elastic.co）
- en: Elasticsearch
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: Elasticsearch is one of the most popular enterprise full text search engines.
    It is open source software. It is distributable and supports multi-tenancy. A
    single Elasticsearch server stores multiple indexes (each index represents a database),
    and a single query can search the data of multiple indexes. It is a distributed
    search engine and supports clustering.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch是最受欢迎的企业级全文搜索引擎之一。它是开源软件。它是可分发的，并支持多租户。单个Elasticsearch服务器存储多个索引（每个索引代表一个数据库），单个查询可以搜索多个索引的数据。它是一个分布式搜索引擎，并支持集群。
- en: It is readily scalable and can provide near-real-time searches with a latency
    of 1 second. It is developed in Java using Apache Lucene. Apache Lucene is also
    free and open source, and it provides the core of Elasticsearch, also known as
    the informational retrieval software library.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它易于扩展，可以提供接近实时的搜索，延迟仅为1秒。它使用Java编写，依赖于Apache Lucene。Apache Lucene也是免费和开源的，它为Elasticsearch提供了核心，也被称为信息检索软件库。
- en: Elasticsearch APIs are extensive in nature and very elaborative. Elasticsearch
    provides a JSON-based schema, less storage, and represents data models in JSON.
    Elasticsearch APIs use JSON documents for HTTP requests and responses.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch API广泛且详尽。Elasticsearch提供基于JSON的架构，占用更少的存储，并以JSON的形式表示数据模型。Elasticsearch
    API使用JSON文档进行HTTP请求和响应。
- en: Logstash
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logstash
- en: Logstash is an open source data collection engine with real-time pipeline capabilities.
    In simple words, it collects, parses, processes, and stores the data. Since Logstash
    has data pipeline capabilities, it helps you to process any event data, such as
    logs, from a variety of systems. Logstash runs as an agent that collects the data,
    parses it, filters it, and sends the output to a designated app, such as Elasticsearch,
    or simple standard output on a console.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash是一个具有实时流水线功能的开源数据收集引擎。简单来说，它收集、解析、处理和存储数据。由于Logstash具有数据流水线功能，它帮助你处理来自各种系统的各种事件数据，如日志。Logstash作为一个代理运行，收集数据、解析数据、过滤数据，并将输出发送到指定应用，如Elasticsearch，或简单的控制台标准输出。
- en: 'It also has a very good plugin ecosystem (image sourced from [www.elastic.co](http://www.elastic.co)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 它还拥有一个非常好的插件生态系统（图片来源于[www.elastic.co](http://www.elastic.co)：
- en: '![](img/d8951d1d-10a5-4118-bf8d-6a6174fb9975.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8951d1d-10a5-4118-bf8d-6a6174fb9975.jpg)'
- en: Logstash ecosystem
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash生态系统
- en: Kibana
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kibana
- en: Kibana is an open source analytics and visualization web application. It is
    designed to work with Elasticsearch. You use Kibana to search, view, and interact
    with data stored in Elasticsearch indices.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana是一个开源的分析与可视化网页应用程序。它被设计用来与Elasticsearch协同工作。你使用Kibana来搜索、查看与交互存储在Elasticsearch索引中的数据。
- en: It is a browser-based web application that lets you perform advanced data analysis
    and visualize your data in a variety of charts, tables, and maps. Moreover, it
    is a zero-configuration application. Therefore, it neither needs any coding nor
    additional infrastructure after installation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于浏览器的网络应用程序，让你执行高级数据分析并在各种图表、表格和地图中可视化你的数据。此外，它是一个零配置应用程序。因此，安装后既不需要编写任何代码，也不需要额外的基础设施。
- en: ELK stack setup
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELK栈设置
- en: Generally, these tools are installed individually and then configured to communicate
    with each other. The installation of these components is pretty straightforward.
    Download the installable artifact from the designated location and follow the
    installation steps, as shown in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些工具是单独安装，然后配置成相互通信。这些组件的安装相当直接。从指定位置下载可安装的工件，并按照下一节中的安装步骤进行操作。
- en: The installation steps provided below are part of a basic setup which is required
    for setting up the ELK stack you want to run. Since this installation was done
    on my localhost machine, I have used the host localhost. It can be changed easily
    with any respective hostname that you want.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下面提供的安装步骤是基本设置的一部分，这是你想要运行的ELK栈所必需的。由于这个安装是在我的本地主机上完成的，所以我使用了主机localhost。它可以很容易地用你想要的任何相应的主机名来替换。
- en: Installing Elasticsearch
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Elasticsearch
- en: 'To install Elasticsearch, we can use the Elasticsearch Docker image:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Elasticsearch，我们可以使用Elasticsearch的Docker镜像：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also install Elasticsearch by following these steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以按照以下步骤安装Elasticsearch：
- en: Download the latest Elasticsearch distribution from [https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch)下载最新的Elasticsearch分发版。
- en: Unzip it to the desired location in your system.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它解压到系统中的所需位置。
- en: Make sure the latest Java version is installed and the `JAVA_HOME` environment
    variable is set.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保安装了最新版本的Java，并且`JAVA_HOME`环境变量已设置。
- en: Go to Elasticsearch home and run `bin/elasticsearch` on Unix-based systems and
    `bin/elasticsearch.bat` on Windows.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往Elasticsearch的主页并运行`bin/elasticsearch`，在基于Unix的系统上，以及在Windows上运行`bin/elasticsearch.bat`。
- en: 'Open any browser and hit `http://localhost:9200/`. On successful installation,
    it should provide you with a JSON object similar to the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何浏览器并输入`http://localhost:9200/`。成功安装后，它应该会为你提供一个类似于以下的JSON对象：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By default, the GUI is not installed. You can install one by executing the
    following command from the `bin` directory; make sure the system is connected
    to the internet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GUI并没有安装。你可以通过从`bin`目录执行以下命令来安装，确保系统连接到互联网：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you are using the Elasticsearch image, then run the Docker image (later,
    we'll use `docker-compose` to run the ELK stack together).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你正在使用Elasticsearch镜像，那么就运行Docker镜像（稍后，我们将使用`docker-compose`一起运行ELK栈）。
- en: Now, you can access the GUI interface with the URL `http://localhost:9200/_plugin/head/`.
    You can replace `localhost` and `9200` with your respective hostname and port
    number.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以通过URL`http://localhost:9200/_plugin/head/`访问GUI界面。你可以将`localhost`和`9200`替换为你的主机名和端口号。
- en: Installing Logstash
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Logstash
- en: 'To install Logstash, we can use the Logstash Docker image:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Logstash，我们可以使用Logstash的Docker镜像：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can also install Logstash by performing the following steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过执行以下步骤来安装Logstash：
- en: Download the latest Logstash distribution from [https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/logstash](https://www.elastic.co/downloads/logstash)下载最新的Logstash分发版。
- en: Unzip it to the desired location in your system.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它解压到系统中的所需位置。
- en: 'Prepare a configuration file, as shown. It instructs Logstash to read input
    from given files and passes it to Elasticsearch (see the following `config` file;
    Elasticsearch is represented by localhost and the `9200` port). It is the simplest
    configuration file. To add filters and learn more about Logstash, you can explore
    the Logstash reference documentation available at [https://www.elastic.co/guide/en/logstash/current/index.html](https://www.elastic.co/guide/en/logstash/current/index.html):'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准备一个配置文件，如下所示。它指示Logstash从给定文件中读取输入并将其传递给Elasticsearch（请参阅下面的`config`文件；Elasticsearch由localhost和`9200`端口表示）。这是最简单的配置文件。要添加过滤器并了解更多关于Logstash的信息，你可以探索可用的Logstash参考文档[https://www.elastic.co/guide/en/logstash/current/index.html](https://www.elastic.co/guide/en/logstash/current/index.html)。
- en: As you can see, the OTRS `service` log and `edge-server` log are added as input.
    Similarly, you can also add log files of other microservices.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，OTRS的`service`日志和`edge-server`日志作为输入添加了。同样地，你也可以添加其他微服务的日志文件。
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Go to Logstash home and run `bin/logstash agent -f logstash.conf` on Unix-based
    systems and `bin/logstash.bat agent -f logstash.conf` on Windows. Here, Logstash
    is executed using the `agent` command. The Logstash agent collects data from the
    sources provided in the input field in the configuration file and sends the output
    to Elasticsearch. Here, we have not used the filters, because otherwise it may
    process the input data before providing it to Elasticsearch.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unix-based系统上，前往Logstash主目录并运行`bin/logstash agent -f logstash.conf`，在Windows上，运行`bin/logstash.bat
    agent -f logstash.conf`。在这里，Logstash使用`agent`命令执行。Logstash代理从配置文件中提供的输入字段中的源收集数据，并将输出发送到Elasticsearch。在这里，我们没有使用过滤器，因为否则它可能会在将数据提供给Elasticsearch之前处理输入数据。
- en: Similarly, you can run Logstash using the downloaded Docker image (later, we'll
    use the `docker-compose` to run the ELK stack together).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，你可以使用下载的Docker镜像来运行Logstash（稍后，我们将使用`docker-compose`来一起运行ELK栈）。
- en: Installing Kibana
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kibana
- en: 'To install Kibana, we can use the Kibana Docker image:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装Kibana，我们可以使用Kibana的Docker镜像：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also install the Kibana web application by performing the following
    steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过执行以下步骤来安装Kibana网页应用程序：
- en: 'Download the latest Kibana distribution from: [https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana).'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.elastic.co/downloads/kibana](https://www.elastic.co/downloads/kibana)下载最新的Kibana分发版。
- en: Unzip it to the desired location in your system.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其解压到系统中的所需位置。
- en: 'Open the configuration file `config/kibana.yml` from the Kibana home directory
    and point the `elasticsearch.url` to the previously configured Elasticsearch instance:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Kibana主目录下的配置文件`config/kibana.yml`，并将`elasticsearch.url`指向之前配置的Elasticsearch实例。
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Go to Kibana home and run `bin/kibana agent -f logstash.conf` on Unix-based
    systems and `bin/kibana.bat agent -f logstash.conf` on Windows.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unix-based系统上，前往Kibana主目录并运行`bin/kibana agent -f logstash.conf`，在Windows上，运行`bin/kibana.bat
    agent -f logstash.conf`。
- en: If you are using the Kibana Docker image, then you can run the Docker image
    (later, we'll use docker-compose to run the ELK stack together).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用的是Kibana的Docker镜像，那么你可以运行Docker镜像（稍后，我们将使用docker-compose来一起运行ELK栈）。
- en: Now, you can access the Kibana app from your browser using the URL `http://localhost:5601/`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以通过URL`http://localhost:5601/`从你的浏览器访问Kibana应用。
- en: To learn more about Kibana, explore the Kibana reference documentation at [https://www.elastic.co/guide/en/kibana/current/getting-started.html](https://www.elastic.co/guide/en/kibana/current/getting-started.html).
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解更多关于Kibana的信息，请探索Kibana参考文档[https://www.elastic.co/guide/en/kibana/current/getting-started.html](https://www.elastic.co/guide/en/kibana/current/getting-started.html)。
- en: As we followed the preceding steps, you may have noticed that it requires some
    amount of effort. If you want to avoid a manual setup, you can Dockerize it. If
    you don't want to put effort into creating the Docker container of the ELK stack,
    you can choose one from Docker Hub. On Docker Hub, there are many ready-made ELK
    stack Docker images. You can try different ELK containers and choose the one that
    suits you the most. `willdurand/elk` is the most downloaded container and is easy
    to start, working well with Docker Compose.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们遵循前面的步骤，你可能已经注意到它需要一些努力。如果你想要避免手动设置，你可以Docker化它。如果你不想花精力创建ELK栈的Docker容器，你可以在Docker
    Hub上选择一个。在Docker Hub上，有许多现成的ELK栈Docker镜像。你可以尝试不同的ELK容器，选择最适合你的那个。`willdurand/elk`是最受欢迎的容器，启动简单，与Docker
    Compose配合良好。
- en: Running the ELK stack using Docker Compose
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行ELK栈
- en: 'ELK images available on elastic.co''s own Docker repository have the XPack
    package enabled by default at the time of writing this section. In the future,
    it may be optional. Based on XPack availability in ELK images, you can modify
    the docker-compose file `docker-compose-elk.yml`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本节时，elastic.co自己的Docker仓库中可用的ELK镜像默认启用了XPack包。将来，这可能成为可选的。根据ELK镜像中XPack的可用性，您可以修改`docker-compose-elk.yml`
    `docker-compose`文件：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once you save the ELK Docker Compose file, you can run the ELK stack using
    the following command (the command is run from the directory that contains the
    Docker Compose file):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦保存了ELK Docker Compose文件，您可以使用以下命令运行ELK堆栈（该命令从包含Docker Compose文件的目录运行）：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output for the preceding command is as shown in the following screenshot:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前一条命令的输出如以下截图所示：
- en: '![](img/c32104c2-3d7a-4822-a5f1-15d7e7be0b52.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c32104c2-3d7a-4822-a5f1-15d7e7be0b52.png)'
- en: Running the ELK stack using Docker Compose
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Docker Compose运行ELK堆栈
- en: 'If volume is not used, the environment pipeline does not work. For a Windows
    environment such as Windows 7, where normally volume is hard to configure, you
    can copy the pipeline CONF file inside the container and restart the Logstash
    container:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用卷，环境管道将无法工作。对于像Windows 7这样的Windows环境，通常很难配置卷，您可以将管道CONF文件复制到容器内并重新启动Logstash容器：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Please restart the Logstash container after copying the pipeline CONF file
    `pipeline/logstash.conf`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制`pipeline/logstash.conf`管道CONF文件后，请重新启动Logstash容器：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Pushing logs to the ELK stack
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将日志推送到ELK堆栈
- en: We are done making the ELK stack available for consumption. Now, Logstash just
    needs a log stream that can be indexed by Elasticsearch. Once the Elasticsearch
    index of logs is created, logs can be accessed and processed on the Kibana dashboard.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了使ELK堆栈可供消费的工作。现在，Logstash只需要一个可以被Elasticsearch索引的日志流。一旦创建了日志的Elasticsearch索引，就可以在Kibana仪表板上访问和处理日志。
- en: To push the logs to Logstash, we need to make the following changes in our service
    code. We need to add logback and logstash-logback encoder dependencies in OTRS
    services.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将日志推送到Logstash，我们需要在我们的服务代码中进行以下更改。我们需要在OTRS服务中添加logback和logstash-logback编码器依赖项。
- en: 'Add the following dependencies in the `pom.xml` file:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pom.xml`文件中添加以下依赖项：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We also need to configure the logback by adding `logback.xml` to `src/main/resources`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要通过向`src/main/resources`添加`logback.xml`来配置logback。
- en: 'The `logback.xml` file will look something like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`logback.xml`文件将看起来像这样：'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the destination is `192.168.99.100:5001`, where Logstash is hosted; you
    can change it based on your configuration. For the encoder, the `net.logstash.logback.encoder.LogstashEncoder`
    class is used. The value of the `spring.application.name` property should be set
    to the service for which it is configured. Simiarly, a shutdown hook is added,
    so that once the service is stopped, all resources should be released and cleaned.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，目标是在`192.168.99.100:5001`上，那里托管Logstash；根据您的配置，您可以进行更改。对于编码器，使用了`net.logstash.logback.encoder.LogstashEncoder`类。`spring.application.name`属性的值应设置为配置的服务。同样，添加了一个关闭钩子，以便在服务停止时，应释放和清理所有资源。
- en: You want to start services after the ELK stack is available, so services can
    push the logs to Logstash.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您希望在ELK堆栈可用后启动服务，以便服务可以将日志推送到Logstash。
- en: Once the ELK stack and services are up, you can check the ELK stack to view
    the logs. You want to wait for a few minutes after starting the ELK stack and
    then access the following URLs (replace the IP based on your configuration).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦ELK堆栈和服务启动，您可以检查ELK堆栈以查看日志。您希望在启动ELK堆栈后等待几分钟，然后访问以下URL（根据您的配置替换IP）。
- en: 'To check whether Elasticsearch is up, access the following URL:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查Elasticsearch是否启动，请访问以下URL：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To check whether indexes have been created or not, access either of the following
    URLs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查是否已创建索引，请访问以下任一URL：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once the Logstash index is done (you may have a few service endpoints to generate
    some logs), access Kibana:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成了Logstash索引（您可能有一些服务端点来生成一些日志），请访问Kibana：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tips for ELK stack implementation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ELK堆栈实现的技巧
- en: 'The following are some useful tips for implementing the ELK stack:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些实施ELK堆栈的有用技巧：
- en: To avoid any data loss and handle the sudden spike of input load, using a broker
    such as Redis or RabbitMQ is recommended between Logstash and Elasticsearch.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免任何数据丢失并处理输入负载的突然激增，建议在Logstash和Elasticsearch之间使用如Redis或RabbitMQ之类的代理。
- en: Use an odd number of nodes for Elasticsearch if you are using clustering to
    prevent the split-brain problem.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用集群，为Elasticsearch使用奇数个节点，以防止分脑问题。
- en: In Elasticsearch, always use the appropriate field type for given data. This
    will allow you to perform different checks; for example, the `int` field type
    will allow you to perform `("http_status:<400")` or `("http_status:=200")`. Similarly,
    other field types also allow you to perform similar checks.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Elasticsearch中，总是为给定数据使用适当的字段类型。这将允许您执行不同的检查；例如，`int`字段类型将允许您执行`("http_status:<400")`或`("http_status:=200")`。同样，其他字段类型也允许您执行类似的检查。
- en: Use of correlation ID for service calls
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为服务调用使用关联ID
- en: When you make a call to any REST endpoint and if any issue pops up, it is difficult
    to trace the issue and its root origin because each call is made to a server,
    and this call may call another, and so on and so forth. This makes it very difficult
    to figure out how one particular request was transformed and what it was called.
    Normally, an issue that is caused by one service can have domino effect on other
    services or can fail other service operation. It is very difficult to track and
    may require an enormous amount of effort. If it is monolithic, you know that you
    are looking in the right direction, but microservices make it difficult to understand
    what the source of the issue is and where you should get your data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用任何REST端点时，如果出现任何问题，很难追踪问题和其根本原因，因为每个调用都是对服务器的调用，这个调用可能调用另一个，依此类推。这使得很难弄清楚特定请求是如何转换的以及它调用了什么。通常，由一个服务引起的问题可能会在其他服务上产生连锁反应，或者可能导致其他服务操作失败。这很难追踪，可能需要巨大的努力。如果是单体结构，你知道你在正确的方向上，但是微服务使得难以理解问题的来源以及你应该获取数据的位置。
- en: Let's see how we can tackle this problem
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们看看我们如何解决这个问题
- en: By using a correlation ID that is passed across all calls, it allows you to
    track each request and track the route easily. Each request will have its unique
    correlation ID. Therefore, when we debug any issue, the correlation ID is our
    starting point. We can follow it and, along the way, we can find out what went
    wrong.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在所有调用中传递关联ID，它允许您轻松跟踪每个请求和跟踪路由。每个请求都将有其唯一的关联ID。因此，当我们调试任何问题时，关联ID是我们的起点。我们可以跟随它，在这个过程中，我们可以找出哪里出了问题。
- en: The correlation ID requires some extra development effort, but it's effort well
    spent as it helps a lot in the long run. When a request travels between different
    microservices, you will be able to see all interactions and which service has
    problems.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关联ID需要一些额外的开发工作，但这是值得的努力，因为它在长远中帮助很大。当请求在不同微服务之间传递时，你将能够看到所有交互以及哪个服务存在问题。
- en: This is not something new or invented for microservices. This pattern is already
    being used by many popular products such as Microsoft SharePoint.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是为微服务发明的新东西。这个模式已经被许多流行产品使用，例如微软SharePoint。
- en: Use of Zipkin and Sleuth for tracking
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zipkin和Sleuth进行跟踪
- en: For the OTRS application, we'll make use of Zipkin and Sleuth for tracking.
    It provides trace IDs and span IDs and a nice UI to trace the requests. More importantly,
    you can find out the time taken by each request in Zipkin and it allows you to
    drill down to find out the request that makes maximum time for serving the request.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于OTRS应用程序，我们将利用Zipkin和Sleuth进行跟踪。它提供了跟踪ID和跨度ID以及一个漂亮的UI来跟踪请求。更重要的是，您可以在Zipkin中找到每个请求所花费的时间，并允许您深入挖掘以找出响应请求耗时最长的请求。
- en: 'In the following screenshot, you can see the time taken by the `findById` API
    call of the restaurant as well as the trace ID of the same request. It also shows
    the span ID:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，你可以看到餐厅`findById` API调用所花费的时间以及同一请求的跟踪ID。它还显示了跨度ID：
- en: '![](img/b2422251-7e3e-4319-947a-3d747347f8b8.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2422251-7e3e-4319-947a-3d747347f8b8.png)'
- en: Total time taken and trace ID of restaurant `findById` API call
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 餐厅`findById` API调用的总时间和跟踪ID
- en: We'll stick to the following steps to configure the Zipkin and Sleuth in OTRS
    services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循以下步骤来配置OTRS服务中的Zipkin和Sleuth。
- en: 'You just need to add Sleuth and Sleuth-Zipkin dependencies to enable the tracking
    and request tracing:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要向跟踪和请求跟踪中添加Sleuth和Sleuth-Zipkin依赖项：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Access the Zipkin dashboard and find out the time taken by different requests.
    Replace the port if the default port is changed. Please make sure that services
    are up before making use of Zipkin:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Zipkin仪表板，找出不同请求所花费的时间。如果默认端口已更改，请替换端口。请确保在使用Zipkin之前服务已经启动：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, if the ELK stack is configured and up, then you can use this trace ID
    to find the appropriate logs in Kibana, as shown in following screenshot. The
    X-B3-TraceId field is available in Kibana, which is used to filter the logs based
    on trace ID:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果ELK栈已经配置并运行，那么你可以使用这个跟踪ID在Kibana中找到相应的日志，如下面的屏幕截图所示。Kibana中有一个X-B3-TraceId字段，用于根据跟踪ID过滤日志：
- en: '![](img/04a4d229-71fa-4d6d-97b5-0289e3e650cd.png)Kibana dashboard - search
    based on request trace ID'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/04a4d229-71fa-4d6d-97b5-0289e3e650cd.png)Kibana仪表板 - 根据请求跟踪ID搜索'
- en: Dependencies and versions
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖关系和版本
- en: Two common problems that we face in product development are cyclic dependencies
    and API versions. We'll discuss them in terms of microservice-based architecture.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在产品开发中，我们面临的两个常见问题是循环依赖和API版本。我们将讨论它们在微服务架构中的情况。
- en: Cyclic dependencies and their impact
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环依赖及其影响
- en: Generally, monolithic architecture has a typical layer model, whereas microservices
    carry the graph model. Therefore, microservices may have cyclic dependencies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，单体架构有一个典型的层次模型，而微服务携带图模型。因此，微服务可能会有循环依赖。
- en: Therefore, it is necessary to keep a dependency check on microservice relationships.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对微服务关系进行依赖检查是必要的。
- en: 'Let us have a look at the following two cases:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看以下两个案例：
- en: If you have a cycle of dependencies between your microservices, you are vulnerable
    to distributed stack overflow errors when a certain transaction might be stuck
    in a loop. For example, when a restaurant table is being reserved by a person.
    In this case, the restaurant needs to know the person (`findBookedUser`), and
    the person needs to know the restaurant at a given time (`findBookedRestaurant`).
    If it is not designed well, these services may call each other in a loop. The
    result may be a stack overflow generated by JVM.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你在你的微服务之间有一个依赖循环，当某个事务可能卡在循环中时，你的分布式栈可能会遇到溢出错误。例如，当一个人在预订餐厅的桌子时。在这种情况下，餐厅需要知道这个人（`findBookedUser`），而这个人需要知道在某个时间点的餐厅（`findBookedRestaurant`）。如果设计不当，这些服务可能会相互调用形成循环。结果可能是由JVM产生的栈溢出。
- en: If two services share a dependency and you update that other service's API in
    a way that could affect them, you'll need to update all three at once. This brings
    up questions such as, which should you update first? In addition, how do you make
    this a safe transition?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个服务共享一个依赖项，而你以可能影响它们的方式更新那个其他服务的API，你需要一次性更新所有三个。这会引发一些问题，比如你应该先更新哪一个？此外，你如何使这个过渡变得安全？
- en: Analyzing dependencies while designing the system
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设计系统时分析依赖关系
- en: Therefore, it is important while designing the microservices to establish the
    proper relationship between different services internally to avoid any cyclic
    dependencies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在设计微服务时，确立不同服务之间的适当关系以避免任何循环依赖是非常重要的。
- en: It is a design issue and must be addressed, even if it requires a refactoring
    of the code.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个设计问题，必须加以解决，即使这需要对代码进行重构。
- en: Maintaining different versions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维护不同版本
- en: When you have more services, it means different release cycles for each of them,
    which adds to this complexity by introducing different versions of services, in
    that there will be different versions of the same REST services. Reproducing the
    solution to a problem will prove to be very difficult when it has gone in one
    version and returns in a newer one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拥有更多服务时，这意味着每个服务都有不同的发布周期，这通过引入不同版本的服务增加了这种复杂性，因为同样的REST服务会有不同的版本。当解决方案在一个版本中消失，在更高版本中回归时，重现问题将变得非常困难。
- en: Let's explore more
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们进一步探索
- en: The versioning of APIs is important because, over time, APIs change. Your knowledge
    and experience improves with time, and that leads to changes in APIs. Changing
    APIs may break existing client integrations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: API的版本化很重要，因为随着时间的推移，API会发生变化。你的知识和经验会随着时间而提高，这会导致API的变化。改变API可能会破坏现有的客户端集成。
- en: 'Therefore, there are various ways to manage the API versions. One of these
    is using the version in the path that we have used in this book; some also use
    the HTTP header. The HTTP header could be a custom request header or you could
    use `Accept Header` for representing the calling API version. For more information
    on how versions are handled using HTTP headers, please refer to *RESTful Java
    Patterns and Best Practices* by Bhakti Mehta, Packt Publishing: [https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices](https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有许多方法可以管理API版本。其中一种方法是使用我们在本书中使用的路径版本；还有一些人使用HTTP头。HTTP头可能是一个自定义请求头，或者您可以使用`Accept
    Header`来表示调用API的版本。有关如何使用HTTP头处理版本的信息，请参阅Bhakti Mehta著，Packt出版社出版的*RESTful Java
    Patterns and Best Practices*： [https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices](https://www.packtpub.com/application-development/restful-java-patterns-and-best-practices)。
- en: It is very important while troubleshooting any issue that your microservices
    are implemented to produce the version numbers in logs. In addition, ideally,
    you should avoid any instance where you have too many versions of any microservice.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在排查任何问题时，让微服务在日志中产生版本号非常重要。此外，理想情况下，您应该避免任何微服务版本过多的实例。
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'This following links will have more information:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接将提供更多信息：
- en: 'Elasticsearch: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Elasticsearch: [https://www.elastic.co/products/elasticsearch](https://www.elastic.co/products/elasticsearch)'
- en: 'Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Logstash: [https://www.elastic.co/products/logstash](https://www.elastic.co/products/logstash)'
- en: 'Kibana: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kibana: [https://www.elastic.co/products/kibana](https://www.elastic.co/products/kibana)'
- en: '`willdurand/elk`: ELK Docker image'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`willdurand/elk`：ELK Docker镜像'
- en: '*Mastering Elasticsearch - Second* *Edition*: [https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition](https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*精通Elasticsearch - 第二版*: [https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition](https://www.packtpub.com/web-development/mastering-elasticsearch-second-edition)'
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have explored the ELK stack overview and installation. In
    the ELK stack, Elasticsearch is used for storing the logs and service queries
    from Kibana. Logstash is an agent that runs on each server that you wish to collect
    logs from. Logstash reads the logs, filters/transforms them, and provides them
    to Elasticsearch. Kibana reads/queries the data from Elasticsearch and presents
    it in tabular or graphical visualizations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了ELK堆栈的概述和安装。在ELK堆栈中，Elasticsearch用于存储来自Kibana的日志和服务查询。Logstash是运行在您希望收集日志的每个服务器上的代理。Logstash读取日志，过滤/转换它们，并将它们提供给Elasticsearch。Kibana从Elasticsearch中读取/查询数据，并以表格或图形可视化的形式呈现它们。
- en: We also understand the utility of having the correlation ID while debugging
    issues. At the end of this chapter, we also discovered the shortcomings of a few
    microservice designs. It was a challenging task to cover all of the topics relating
    to microservices in this book, so I tried to include as much relevant information
    as possible with precise sections with references, which allow you to explore
    more. Now, I would like to let you start implementing the concepts we have learned
    in this chapter in your workplace or in your personal projects. This will not
    only give you hands-on experience, but may also allow you to master microservices.
    In addition, you will also be able to participate in local meetups and conferences.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也非常理解在排查问题时拥有关联ID的实用性。在本章末尾，我们也发现了某些微服务设计的一些不足。由于在本书中涵盖所有与微服务相关的主题具有挑战性，因此我尽可能地包含尽可能多的相关信息，并配有精确的章节和参考文献，这使您能够进一步探索。现在，我希望让您开始在您的工作场所或个人项目中实施本章学到的概念。这不仅能为您提供实践经验，还可能使您掌握微服务。此外，您还将能够参加当地的技术聚会和会议。
