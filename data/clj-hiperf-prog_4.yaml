- en: Chapter 4. Host Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章：主机性能
- en: 'In the previous chapters, we noted how Clojure interoperates with Java. In
    this chapter we will go a bit deeper to understand the internals better. We will
    touch upon several layers of the entire stack, but our major focus will be the
    JVM, in particular the Oracle HotSpot JVM, though there are several JVM vendors
    to choose from ([http://en.wikipedia.org/wiki/List_of_Java_virtual_machines](http://en.wikipedia.org/wiki/List_of_Java_virtual_machines)).
    At the time of writing this, Oracle JDK 1.8 is the latest stable release and early
    OpenJDK 1.9 builds are available. In this chapter we will discuss:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们提到了Clojure如何与Java交互。在本章中，我们将更深入地了解内部结构。我们将触及整个堆栈的几个层次，但我们的主要重点是JVM，特别是Oracle
    HotSpot JVM，尽管有多个JVM供应商可供选择([http://en.wikipedia.org/wiki/List_of_Java_virtual_machines](http://en.wikipedia.org/wiki/List_of_Java_virtual_machines))。在撰写本文时，Oracle
    JDK 1.8是最新的稳定版本，并且有早期的OpenJDK 1.9构建可用。在本章中，我们将讨论：
- en: How the hardware subsystems function from the performance viewpoint
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从性能角度来看，硬件子系统是如何工作的
- en: Organization of the JVM internals and how that is related to performance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM内部结构的组织以及它与性能的关系
- en: How to measure the amount of space occupied by various objects in the heap
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何测量堆中各种对象占用的空间量
- en: Profile Clojure code for latency using Criterium
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Criterium对Clojure代码进行延迟分析
- en: The hardware
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件
- en: There are various hardware components that may impact the performance of software
    in different ways. The processors, caches, memory subsystem, I/O subsystems, and
    so on, all have varying degrees of performance impact depending upon the use cases.
    In the following sections we look into each of those aspects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种硬件组件可能会以不同的方式影响软件的性能。处理器、缓存、内存子系统、I/O子系统等，它们对性能的影响程度各不相同，这取决于具体的使用场景。在接下来的章节中，我们将探讨这些方面的每一个。
- en: Processors
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理器
- en: 'Since about the late 1980s, microprocessors have been employing pipelining
    and instruction-level parallelism to speed up their performance. Processing an
    instruction at the CPU level consists of typically four cycles: **fetch**, **decode**,
    **execute**, and **writeback**. Modern processors optimize the cycles by running
    them in parallel—while one instruction is executed, the next instruction is being
    decoded, and the one after that is being fetched, and so on. This style is called
    **instruction pipelining**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自1980年代末以来，微处理器一直采用流水线和指令级并行性来提高其性能。在CPU级别处理指令通常包括四个周期：**取指**、**解码**、**执行**和**回写**。现代处理器通过并行运行这些周期来优化它们——当一条指令正在执行时，下一条指令正在解码，再下一条正在取指，依此类推。这种风格被称为**指令流水线**。
- en: In practice, in order to speed up execution even further, the stages are subdivided
    into many shorter stages, thus leading to deeper super-pipeline architecture.
    The length of the longest stage in the pipeline limits the clock speed of the
    CPU. By splitting stages into substages, the processor can be run at a higher
    clock speed, where more cycles are required for each instruction, but the processor
    still completes one instruction per cycle. Since there are more cycles per second
    now, we get better performance in terms of throughput per second even though the
    latency of each instruction is now higher.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，为了进一步加快执行速度，阶段被细分为许多更短的阶段，从而导致了更深的超级流水线架构。流水线中最长阶段的长度限制了CPU的时钟速度。通过将阶段拆分为子阶段，处理器可以以更高的时钟速度运行，每个指令需要更多的周期，但处理器仍然在每个周期内完成一条指令。由于现在每秒有更多的周期，尽管每个指令的延迟现在更高，但我们仍然在每秒吞吐量方面获得了更好的性能。
- en: Branch prediction
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分支预测
- en: The processor must fetch and decode instructions in advance even when it encounters
    instructions of the conditional `if-then` form. Consider an equivalent of the
    (`if (test a) (foo a) (bar a)`) Clojure expression. The processor must choose
    a branch to fetch and decode, the question is should it fetch the `if` branch
    or the `else` branch? Here, the processor makes a guess as to which instruction
    to fetch/decode. If the guess turns out to be correct, it is a performance gain
    as usual; otherwise, the processor has to throw away the result of the fetch/decode
    process and start on the other branch afresh.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 即使处理器遇到条件`if-then`形式的指令，也必须提前取指和解码。考虑一个等价的Clojure表达式(`if (test a) (foo a) (bar
    a))`。处理器必须选择一个分支来取指和解码，问题是它应该取`if`分支还是`else`分支？在这里，处理器对要取指/解码的指令做出猜测。如果猜测是正确的，就像往常一样，这是一个性能提升；否则，处理器必须丢弃取指/解码过程的结果，并从另一个分支重新开始。
- en: Processors deal with branch prediction using an on-chip branch prediction table.
    It contains recent code branches and two bits per branch, indicating whether or
    not the branch was taken, while also accommodating one-off, not-taken occurrences.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器使用片上分支预测表来处理分支预测。它包含最近的代码分支和每个分支两个比特位，指示分支是否被取用，同时也容纳了单次未取用的情况。
- en: Today, branch prediction is extremely important in processors for performance,
    so modern processors dedicate hardware resources and special predication instructions
    to improve the prediction accuracy and lower the cost of a mispredict penalty.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，分支预测在处理器性能方面非常重要，因此现代处理器专门分配硬件资源和特殊的预测指令来提高预测准确性并降低误预测的代价。
- en: Instruction scheduling
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令调度
- en: High-latency instructions and branching usually lead to empty cycles in the
    instruction pipeline known as **stalls** or **bubbles**. These cycles are often
    used to do other work by the means of instruction reordering. Instruction reordering
    is implemented at the hardware level via out of order execution and at the compiler
    level via compile time instruction scheduling (also called **static instruction
    scheduling**).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 高延迟指令和分支通常会导致指令流水线中的空循环，这些循环被称为**停顿**或**气泡**。这些循环通常被用来通过指令重排的方式执行其他工作。指令重排通过硬件层面的乱序执行和编译器层面的编译时指令调度（也称为**静态指令调度**）来实现。
- en: The processor needs to remember the dependencies between instructions when carrying
    out the out-of-order execution. This cost is somewhat mitigated by using renamed
    registers, wherein register values are stored into / loaded from memory locations,
    potentially on different physical registers, so that they can be executed in parallel.
    This necessitates that out-of-order processors always maintain a mapping of instructions
    and corresponding registers they use, which makes their design complex and power
    hungry. With a few exceptions, almost all high-performance CPUs today have out-of-order
    designs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器在执行乱序执行时需要记住指令之间的依赖关系。这种成本可以通过使用重命名寄存器来在一定程度上减轻，其中寄存器值被存储到/从内存位置加载，可能是在不同的物理寄存器上，这样它们就可以并行执行。这要求乱序处理器始终维护指令及其使用的相应寄存器的映射，这使得它们的设计复杂且功耗高。除了一些例外，今天几乎所有高性能CPU都具有乱序设计。
- en: Good compilers are usually extremely aware of processors, and are capable of
    optimizing the code by rearranging processor instructions in a way that there
    are fewer bubbles in the processor instruction pipeline. A few high-performance
    CPUs still rely on only static instruction reordering instead of out-of-order
    instruction reordering and, in turn, save chip area due to simpler design—the
    saved area is used to accommodate extra cache or CPU cores. Low-power processors,
    such as those from the ARM and Atom family, use in-order design. Unlike most CPUs,
    the modern GPUs use in-order design with deep pipelines, which is compensated
    by very fast context switching. This leads to high latency and high throughput
    on GPUs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的编译器通常对处理器有极高的了解，并且能够通过重新排列处理器指令来优化代码，从而减少处理器指令流水线中的气泡。一些高性能CPU仍然只依赖于静态指令重排而不是乱序指令重排，从而简化设计并节省芯片面积——节省的面积被用来容纳额外的缓存或CPU核心。低功耗处理器，如ARM和Atom系列，使用顺序设计。与大多数CPU不同，现代GPU使用顺序设计，具有深管道，这通过非常快的上下文切换得到补偿。这导致GPU具有高延迟和高吞吐量。
- en: Threads and cores
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程和核心
- en: Concurrency and parallelism via context switches, hardware threads, and cores
    are very common today and we have accepted them as a norm to implement in our
    programs. However, we should understand why we needed such a design in the first
    place. Most of the real-world code we write today does not have more than a modest
    scope for instruction-level parallelism. Even with hardware-based, out-of-order
    execution and static instruction reordering, no more than two instructions per
    cycle are truly parallel. Hence, another potential source of instructions that
    can be pipelined and executed in parallel are the programs other than the currently
    running one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上下文切换、硬件线程和核心实现并发和并行性在当今非常普遍，并且我们已经将其视为实现程序的标准。然而，我们应该理解为什么我们最初需要这样的设计。我们今天编写的绝大多数现实世界代码在指令级并行性方面并没有超过适度的范围。即使有基于硬件的乱序执行和静态指令重排，每个周期也真正并行执行的指令不超过两个。因此，除了当前运行的程序之外，另一个潜在的指令来源是可以流水线和并行执行的程序。
- en: The empty cycles in a pipeline can be dedicated to other running programs, which
    assume that there are other currently running programs that need the processor's
    attention. **Simultaneous multithreading** (**SMT**) is a hardware design that
    enables such kinds of parallelism. Intel implements SMT named as **HyperThreading**
    in some of its processors. While SMT presents a single physical processor as two
    or more logical processors, a true multiprocessor system executes one thread per
    processor, thus achieving simultaneous execution. A multicore processor includes
    two or more processors per chip, but has the properties of a multiprocessor system.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 管道中的空闲周期可以专门用于其他正在运行的程序，这些程序假设有其他当前正在运行的程序需要处理器的关注。**同时多线程**（**SMT**）是一种硬件设计，它使这种类型的并行成为可能。英特尔在其某些处理器中实现了名为**HyperThreading**的SMT。虽然SMT将单个物理处理器呈现为两个或更多逻辑处理器，但真正的多处理器系统每个处理器执行一个线程，从而实现同时执行。多核处理器每个芯片包含两个或更多处理器，但具有多处理器系统的特性。
- en: In general, multicore processors significantly outperform SMT processors. Performance
    on SMT processors can vary by the use case. It peaks in those cases where code
    is highly variable or threads do not compete for the same hardware resources,
    and dips when the threads are cache-bound on the same processor. What is also
    important is that some programs are simply not inherently parallel. In such cases
    it may be hard to make them go faster without the explicit use of threads in the
    program.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，多核处理器在性能上显著优于SMT处理器。SMT处理器的性能可能会根据用例而变化。在代码高度可变或线程不竞争相同硬件资源的情况下，性能达到峰值，而当线程在同一个处理器上缓存绑定时，性能会下降。同样重要的是，有些程序本身并不是本质上并行的。在这种情况下，如果没有在程序中显式使用线程，可能很难使它们更快。
- en: Memory systems
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存系统
- en: It is important to understand the memory performance characteristics to know
    the likely impact on the programs we write. Data-intensive programs that are also
    inherently parallel, such as audio/video processing and scientific computation,
    are largely limited by memory bandwidth, not by the processor. Adding processors
    would not make them faster unless the memory bandwidth is also increased. Consider
    another class of programs, such as 3D graphics rendering or database systems that
    are limited mainly by memory latency but not the memory bandwidth. SMT can be
    highly suitable for such programs, where threads do not compete for the same hardware
    resources.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理解内存性能特性对于了解对我们所编写的程序可能产生的影响非常重要。那些既是数据密集型又是本质上并行的程序，例如音频/视频处理和科学计算，在很大程度上受到内存带宽的限制，而不是处理器的限制。除非内存带宽也增加，否则增加处理器并不会使它们更快。考虑另一类程序，例如主要受内存延迟限制但不受内存带宽限制的3D图形渲染或数据库系统。在这种情况下，SMT可能非常适用，因为线程不会竞争相同的硬件资源。
- en: Memory access roughly constitutes a quarter of all instructions executed by
    a processor. A code block typically begins with memory-load instructions and the
    remainder portion depends on the loaded data. This stalls the instructions and
    prevents large-scale, instruction-level parallelism. As if that was not bad enough,
    even superscalar processors (which can issue more than one instruction per clock
    cycle) can issue, at most, two memory instructions per cycle. Building fast memory
    systems is limited by natural factors such as the speed of light. It impacts the
    signal round trip to the RAM. This is a natural hard limit and any optimization
    can only work around it.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 内存访问大致占处理器执行的所有指令的四分之一。代码块通常以内存加载指令开始，其余部分取决于加载的数据。这会导致指令停滞，并防止大规模的指令级并行。更糟糕的是，即使是超标量处理器（每时钟周期可以发出多个指令）每周期最多也只能发出两个内存指令。构建快速内存系统受到自然因素的影响，例如光速。它影响信号往返到RAM。这是一个自然的硬限制，任何优化都只能绕过它。
- en: Data transfer between the processor and motherboard chipset is one of the factors
    that induce memory latency. This is countered using a **faster front-side bus**
    (**FSB**). Nowadays, most modern processors fix this problem by integrating the
    memory controller directly at the chip level. The significant difference between
    the processor versus memory latencies is known as the **memory wall**. This has
    plateaued in recent times due to processor clock speeds hitting power and heat
    limits, but notwithstanding this, memory latency continues to be a significant
    problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器和主板芯片组之间的数据传输是导致内存延迟的因素之一。这可以通过使用**更快的总线前端总线**（**FSB**）来抵消。如今，大多数现代处理器通过在芯片级别直接集成内存控制器来解决这个问题。处理器与内存延迟之间的显著差异被称为**内存墙**。由于处理器时钟速度达到功率和热量限制，近年来这一现象已经趋于平稳，但尽管如此，内存延迟仍然是一个重大问题。
- en: Unlike CPUs, GPUs typically realize a sustained high-memory bandwidth. Due to
    latency hiding, they utilize the bandwidth even during a high number-crunching
    workload.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 不同，GPU 通常实现持续的高内存带宽。由于延迟隐藏，它们在高强度计算工作负载期间也利用带宽。
- en: Cache
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 缓存
- en: 'To overcome the memory latency, modern processors employ a special type of
    very fast memory placed onto the processor chip or close to the chip. The purpose
    of the cache is to store the most recently used data from the memory. Caches are
    of different levels: **L1** cache is located on the processor chip; **L2** cache
    is bigger and located farther away from the processor compared to L1\. There is
    often an **L3** cache, which is even bigger and located farther from the processor
    than L2\. In Intel''s Haswell processor, the L1 cache is generally 64 kilobytes
    (32 KB instruction plus 32 KB data) in size, L2 is 256 KB per core, and L3 is
    8 MB.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服内存延迟，现代处理器在处理器芯片上或芯片附近放置了一种非常快速的内存。缓存的作用是存储最近使用过的内存数据。缓存有不同的级别：**L1** 缓存位于处理器芯片上；**L2**
    缓存比 L1 更大，且距离处理器更远。通常还有一个 **L3** 缓存，它比 L2 更大，且距离处理器更远。在英特尔 Haswell 处理器中，L1 缓存的大小通常是
    64 千字节（32 KB 指令加 32 KB 数据），L2 每核心 256 KB，L3 是 8 MB。
- en: While memory latency is very bad, fortunately caches seem to work very well.
    The L1 cache is much faster than accessing the main memory. The reported cache
    hit rates in real-world programs is 90 percent, which makes a strong case for
    caches. A cache works like a dictionary of memory addresses to a block of data
    values. Since the value is a block of memory, the caching of adjacent memory locations
    has mostly no additional overhead. Note that L2 is slower and bigger than L1,
    and L3 is slower and bigger than L2\. On Intel Sandybridge processors, register
    lookup is instantaneous; L1 cache lookup takes three clock cycles, L2 takes nine,
    L3 takes 21, and main memory access takes 150 to 400 clock cycles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然内存延迟非常糟糕，幸运的是缓存似乎工作得非常好。L1 缓存比访问主内存要快得多。在现实世界的程序中报告的缓存命中率是 90%，这为缓存提供了强有力的论据。缓存就像是一个内存地址到数据值块的字典。由于值是一个内存块，因此相邻内存位置的缓存几乎没有额外的开销。请注意，L2
    比 L1 慢且更大，L3 比 L2 慢且更大。在英特尔 Sandybridge 处理器上，寄存器查找是瞬时的；L1 缓存查找需要三个时钟周期，L2 需要九个，L3
    需要 21 个，而主内存访问需要 150 到 400 个时钟周期。
- en: Interconnect
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互连
- en: 'A processor communicates with the memory and other processors via interconnect
    that are generally of two types of architecture: **Symmetric multiprocessing**
    (**SMP**) and **Non-uniform memory access** (**NUMA**). In SMP, a bus interconnects
    processors and memory with the help of bus controllers. The bus acts as a broadcast
    device for the end points. The bus often becomes a bottleneck with a large number
    of processors and memory banks. SMP systems are cheaper to build and harder to
    scale to a large number of cores compared to NUMA. In a NUMA system, collections
    of processors and memory are connected point to point to other such groups of
    processors and memory. Every such group is called a node. Local memory of a node
    is accessible by other nodes and vice versa. Intel''s **HyperTransport** and **QuickPath**
    interconnect technologies support NUMA.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器通过两种类型的架构的互连与内存和其他处理器通信：**对称多处理**（**SMP**）和**非一致性内存访问**（**NUMA**）。在 SMP 中，总线通过总线控制器将处理器和内存互连。总线充当广播设备。当有大量处理器和内存银行时，总线往往会成为瓶颈。与
    NUMA 相比，SMP 系统的构建成本更低，但扩展到大量核心更困难。在 NUMA 系统中，处理器和内存的集合通过点到点的方式连接到其他这样的处理器和内存组。每个这样的组被称为节点。节点的本地内存可以被其他节点访问，反之亦然。英特尔的
    **HyperTransport** 和 **QuickPath** 互连技术支持 NUMA。
- en: Storage and networking
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储和网络
- en: Storage and networking are the most commonly used hardware components besides
    the processor, cache, and memory. Many of the real-world applications are more
    often I/O bound than execution-bound. Such I/O technologies are continuously advancing
    and there is a wide variety of components available in the market. The consideration
    of such devices should be based on the exact performance and reliability characteristics
    for the use case. Another important criterion is to know how well they are supported
    by the target operating system drivers. Current day storage technologies mostly
    build upon hard disks and solid state drives. The applicability of network devices
    and protocols vary widely as per the business use case. A detailed discussion
    of I/O hardware is beyond the scope of this book.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理器、缓存和内存之外，存储和网络是最常用的硬件组件。许多现实世界中的应用程序往往比执行密集型应用程序更受I/O限制。这类I/O技术不断进步，市场上可供选择的组件种类繁多。考虑这类设备时，应基于具体的使用案例的性能和可靠性特性。另一个重要标准是了解它们在目标操作系统驱动程序中的支持程度。当前存储技术大多基于硬盘和固态硬盘。网络设备和协议的应用范围根据业务用例而大相径庭。I/O硬件的详细讨论超出了本书的范围。
- en: The Java Virtual Machine
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java虚拟机
- en: The Java Virtual Machine is a bytecode-oriented, garbage-collected virtual machine
    that specifies its own instruction set. The instructions have equivalent bytecodes
    that are interpreted and compiled to the underlying OS and hardware by the **Java
    Runtime Environment** (**JRE**). Objects are referred to using symbolic references.
    The data types in the JVM are fully standardized as a single spec across all JVM
    implementations on all platforms and architectures. The JVM also follows the network
    byte order, which means communication between Java programs on different architectures
    can happen using the big-endian byte order. **Jvmtop** ([https://code.google.com/p/jvmtop/](https://code.google.com/p/jvmtop/))
    is a handy JVM monitoring tool similar to the top command in Unix-like systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Java虚拟机是一个以字节码为导向、具有垃圾回收功能的虚拟机，它定义了自己的指令集。这些指令具有等效的字节码，由**Java运行时环境**（**JRE**）进行解释和编译，以适应底层的操作系统和硬件。对象通过符号引用来引用。在JVM中，数据类型在所有平台和架构上的所有JVM实现中都是完全标准化的，作为一个单一的规范。JVM还遵循网络字节序，这意味着在不同架构上的Java程序之间可以使用大端字节序进行通信。**Jvmtop**（[https://code.google.com/p/jvmtop/](https://code.google.com/p/jvmtop/））是一个方便的JVM监控工具，类似于Unix-like系统中的top命令。
- en: The just-in-time compiler
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即时编译器
- en: The **just-in-time** (**JIT**) compiler is part of the JVM. When the JVM starts
    up, the JIT compiler knows hardly anything about the running code so it simply
    interprets the JVM bytecodes. As the program keeps running, the JIT compiler starts
    profiling the code by collecting statistics and analyzing the call and bytecode
    patterns. When a method call count exceeds a certain threshold, the JIT compiler
    applies a number of optimizations to the code. Most common optimizations are inlining
    and native code generating. The final and static methods and classes are great
    candidates for inlining. JIT compilation does not come without a cost; it occupies
    memory to store the profiled code and sometimes it has to revert the wrong speculative
    optimization. However, JIT compilation is almost always amortized over a long
    duration of code execution. In rare cases, turning off JIT compilation may be
    useful if either the code is too large or there are no hotspots in the code due
    to infrequent execution.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**即时编译器**（**JIT**）是JVM的一部分。当JVM启动时，即时编译器对正在运行的代码几乎一无所知，因此它只是简单地解释JVM字节码。随着程序的运行，即时编译器开始通过收集统计数据和分析调用和字节码模式来分析代码。当一个方法调用的次数超过某个阈值时，即时编译器会对代码应用一系列优化。最常见的优化是内联和本地代码生成。最终和静态方法和类是内联的绝佳候选者。即时编译并非没有成本；它占用内存来存储分析过的代码，有时它不得不撤销错误的推测性优化。然而，即时编译几乎总是通过长时间的代码执行来摊销成本。在罕见的情况下，如果代码太大或者由于执行频率低而没有热点，关闭即时编译可能是有用的。'
- en: 'A JRE has typically two kinds of JIT compilers: client and server. Which JIT
    compiler is used by default depends on the type of hardware and platform. The
    client JIT compiler is meant for client programs such as command-line and desktop
    applications. We can start the JRE with the `-server` option to invoke the server
    JIT compiler, which is really meant for long-running programs on a server. The
    threshold for JIT compilation is higher in the server than the client. The difference
    in the two kinds of JIT compilers is that the client targets upfront, visible
    lower latency, and the server is assumed to be running on a high-resource hardware
    and tries to optimize for throughput.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个JRE通常有两种JIT编译器：客户端和服务器。默认使用哪种JIT编译器取决于硬件和平台类型。客户端JIT编译器是为客户端程序，如命令行和桌面应用程序设计的。我们可以通过使用`-server`选项启动JRE来调用服务器JIT编译器，这实际上是为服务器上长时间运行程序设计的。服务器中JIT编译的阈值高于客户端。两种JIT编译器的区别在于，客户端针对的是低延迟，而服务器假定运行在高资源硬件上，并试图优化吞吐量。
- en: The JIT compiler in the Oracle HotSpot JVM observes the code execution to determine
    the most frequently invoked methods, which are hotspots. Such hotspots are usually
    just a fraction of the entire code that can be cheap to focus on and optimize.
    The **HotSpot JIT** compiler is lazy and adaptive. It is lazy because it compiles
    only those methods to native code that have crossed a certain threshold, and not
    all the code that it encounters. Compiling to native code is a time-consuming
    process and compiling all code would be wasteful. It is adaptive to gradually
    increasing the aggressiveness of its compilation on frequently called code, which
    implies that the code is not optimized only once but many times over as the code
    gets executed repeatedly. After a method call crosses the first JIT compiler threshold,
    it is optimized and the counter is reset to zero. At the same time, the optimization
    count for the code is set to one. When the call exceeds the threshold yet again,
    the counter is reset to zero and the optimization count is incremented; and this
    time a more aggressive optimization is applied. This cycle continues until the
    code cannot be optimized anymore.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle HotSpot JVM中的JIT编译器会观察代码执行以确定最频繁调用的方法，这些方法是热点。这些热点通常只是整个代码的一小部分，可以低成本地关注和优化。**HotSpot
    JIT**编译器是懒惰和自适应的。它是懒惰的，因为它只编译那些超过一定阈值的、已经调用的方法，而不是它遇到的全部代码。将代码编译成本地代码是一个耗时的过程，编译所有代码将是浪费。它是自适应的，因为它会逐渐增加对频繁调用代码编译的积极性，这意味着代码不是只优化一次，而是在代码重复执行的过程中多次优化。当一个方法调用超过第一个JIT编译器的阈值后，它将被优化，计数器重置为零。同时，代码的优化计数设置为1。当调用再次超过阈值时，计数器重置为零，优化计数增加；这次应用更积极的优化。这个过程会一直持续到代码不能再优化为止。
- en: 'The HotSpot JIT compiler does a whole bunch of optimizations. Some of the most
    prominent ones are as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HotSpot JIT编译器执行了大量优化。其中一些最显著的优化如下：
- en: '**Inlining**: Inlining of methods—very small methods, the static and final
    methods, methods in final classes, and small methods involving only primitive
    numerics are prime candidates for inlining.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内联（Inlining）**: 方法的内联——非常小的方法、静态和final方法、final类中的方法，以及只涉及原始数值的小方法，是内联的理想候选者。'
- en: '**Lock elimination**: Locking is a performance overhead. Fortunately, if the
    lock object monitor is not reachable from other threads, the lock is eliminated.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁消除（Lock elimination）**: 锁定是一个性能开销。幸运的是，如果锁对象监视器无法从其他线程访问，则可以消除锁。'
- en: '**Virtual call elimination**: Often, there is only one implementation for an
    interface in a program. The JIT compiler eliminates the virtual call and replaces
    that with a direct method call on the class implementation object.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟调用消除（Virtual call elimination）**: 通常，程序中的一个接口只有一个实现。JIT编译器会消除虚拟调用，并用类实现对象上的直接方法调用替换它。'
- en: '**Non-volatile memory write elimination**: The non-volatile data members and
    references in an object are not guaranteed to be visible by the threads other
    than the current thread. This criterion is utilized not to update such references
    in memory and rather use hardware registers or the stack via native code.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非易失性内存写入消除（Non-volatile memory write elimination）**: 对象中的非易失性数据成员和引用不保证对当前线程以外的线程可见。这个标准被用来不更新这样的引用在内存中，而是通过本地代码使用硬件寄存器或栈。'
- en: '**Native code generation**: The JIT compiler generates native code for frequently
    invoked methods together with the arguments. The generated native code is stored
    in the code cache.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地代码生成**：JIT 编译器为频繁调用的方法及其参数生成本地代码。生成的本地代码存储在代码缓存中。'
- en: '**Control flow and local optimizations**: The JIT compiler frequently reorders
    and splits the code for better performance. It also analyzes the branching of
    control and optimizes code based on that.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制流和局部优化**：JIT 编译器经常重新排序和拆分代码以提高性能。它还分析控制流的分支，并根据这些分支优化代码。'
- en: There should rarely be any reason to disable JIT compilation, but it can be
    done by passing the `-Djava.compiler=NONE` parameter when starting the JRE. The
    default compile threshold can be changed by passing `-XX:CompileThreshold=9800`
    to the JRE executable where `9800` is the example threshold. The `XX:+PrintCompilation`
    and `-XX:-CITime` options make the JIT compiler print the JIT statistics and time
    spent on JIT.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有理由禁用 JIT 编译，但可以通过在启动 JRE 时传递 `-Djava.compiler=NONE` 参数来实现。默认的编译阈值可以通过传递 `-XX:CompileThreshold=9800`
    到 JRE 可执行文件来更改，其中 `9800` 是示例阈值。`XX:+PrintCompilation` 和 `-XX:-CITime` 选项使 JIT
    编译器打印 JIT 统计信息和 JIT 所花费的时间。
- en: Memory organization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内存组织
- en: 'The memory used by the JVM is divided into several segments. JVM, being a stack-based
    execution model, one of the memory segments is the stack area. Every thread is
    given a stack where the stack frames are stored in **Last-in-First-out** (**LIFO**)
    order. The stack includes a **program counter** (**PC**) that points to the instruction
    in the JVM memory currently being executed. When a method is called, a new stack
    frame is created containing the local variable array and the operand stack. Contrary
    to conventional stacks, the operand stack holds instructions to load local variable
    / field values and computation results—a mechanism that is also used to prepare
    method parameters before a call and to store the return value. The stack frame
    itself may be allocated on the heap. The easiest way to inspect the order of stack
    frames in the current thread is to execute the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: JVM 使用的内存被分为几个部分。作为基于栈的执行模型，JVM 中的一个内存部分是栈区域。每个线程都有一个栈，栈帧以**后进先出**（**LIFO**）的顺序存储在其中。栈包括一个**程序计数器**（**PC**），它指向
    JVM 内存中当前正在执行的指令。当调用方法时，会创建一个新的栈帧，其中包含局部变量数组和操作数栈。与传统的栈不同，操作数栈包含加载局部变量/字段值和计算结果的指令——这种机制也用于在调用之前准备方法参数，并存储返回值。栈帧本身可能分配在堆上。检查当前线程中栈帧顺序的最简单方法是通过执行以下代码：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When a thread requires more stack space than what the JVM can provide, `StackOverflowError`
    is thrown.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个线程需要的栈空间超过 JVM 可以提供的空间时，会抛出`StackOverflowError`。
- en: The heap is the main memory area where the object and array allocations are
    done. It is shared across all JVM threads. The heap may be of a fixed size or
    expanding, depending on the arguments passed to the JRE on startup. Trying to
    allocate more heap space than what the JVM can make room for results in `OutOfMemoryError`
    to be thrown. The allocations in the heap are subject to garbage collection. When
    an object is no more reachable via any reference, it is garbage collected, with
    the notable exception of weak, soft, and phantom references. Objects pointed to
    by non-strong references take longer to GC.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 堆是对象和数组分配的主要内存区域。它在所有 JVM 线程之间共享。堆的大小可能是固定的或可扩展的，这取决于启动 JRE 时传递的参数。尝试分配比 JVM
    能提供的更多堆空间会导致抛出`OutOfMemoryError`。堆中的分配受垃圾回收的影响。当一个对象不再通过任何引用可访问时，它将被垃圾回收，值得注意的是，弱、软和虚引用除外。由非强引用指向的对象在
    GC 中的回收时间较长。
- en: The method area is logically a part of the heap memory and contains per-class
    structures such as the field and method information, runtime constant pool, code
    for method, and constructor bodies. It is shared across all JVM threads. In the
    Oracle HotSpot JVM (up to Version 7), the method area is found in a memory area
    called the **permanent generation**. In HotSpot Java 8, the permanent generation
    is replaced by a native memory area called **Metaspace**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 方法区域在逻辑上属于堆内存的一部分，包含诸如字段和方法信息、运行时常量池、方法代码和构造函数体等每个类的结构。它在所有 JVM 线程之间共享。在 Oracle
    HotSpot JVM（至版本 7）中，方法区域位于一个称为**永久代**的内存区域中。在 HotSpot Java 8 中，永久代被一个称为**元空间**的本地内存区域所取代。
- en: '![Memory organization](img/3642_04_01.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![内存组织](img/3642_04_01.jpg)'
- en: The JVM contains the native code and the Java bytecode to be provided to the
    Java API implementation and the JVM implementation. The native code call stack
    is maintained separately for each thread stack. The JVM stack contains the Java
    method calls. Please note that the JVM spec for Java SE 7 and 8 does not imply
    a native method stack, but for Java SE 5 and 6, it does.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: JVM包含提供给Java API实现和JVM实现的本地代码和Java字节码。每个线程堆栈维护一个独立的本地代码调用栈。JVM堆栈包含Java方法调用。请注意，Java
    SE 7和8的JVM规范不包含本地方法栈，但Java SE 5和6则包含。
- en: HotSpot heap and garbage collection
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HotSpot堆和垃圾回收
- en: The Oracle HotSpot JVM uses a generational heap. The three main generations
    are **Young**, **Tenured** (old), and **Perm** (permanent) (up to HotSpot JDK
    1.7 only). As objects survive garbage collection, they move from **Eden** to **Survivor**
    and from **Survivor** to **Tenured** spaces. The new instances are allocated in
    the **Eden** segment, which is a very cheap operation (as cheap as a pointer bump,
    and faster than a C `malloc` call), if it already has sufficient free space. When
    the Eden area does not have enough free space, a minor GC is triggered. This copies
    the live objects from **Eden** into the **Survivor** space. In the same operation,
    live objects are checked in **Survivor-1** and copied over to **Survivor-2**,
    thus keeping the live objects only in **Survivor-2**. This scheme keeps **Eden**
    and **Survivor-1** empty and unfragmented to make new allocations, and is known
    as **copy collection**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Oracle HotSpot JVM使用代际堆。主要有三个代：**年轻代**、**持久代**（旧代）和**永久代**（仅限于HotSpot JDK 1.7）。随着对象在垃圾回收中存活，它们从**Eden**移动到**Survivor**空间，再从**Survivor**空间移动到**持久代**空间。新实例在**Eden**段分配，这是一个非常便宜的操作（和指针增加一样便宜，比C的`malloc`调用更快），如果它已经有足够的空闲空间。当Eden区域没有足够的空闲空间时，会触发一次小型的垃圾回收。这次回收会将**Eden**中的活动对象复制到**Survivor**空间。在同样的操作中，活动对象会在**Survivor-1**中进行检查，并复制到**Survivor-2**，从而只保留**Survivor-2**中的活动对象。这种方案保持**Eden**和**Survivor-1**为空且无碎片，以便进行新的分配，这被称为**复制收集**。
- en: '![HotSpot heap and garbage collection](img/3642_04_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![HotSpot堆和垃圾回收](img/3642_04_02.jpg)'
- en: 'After a certain survival threshold in the young generation, the objects are
    moved to the tenured/old generation. If it is not possible to do a minor GC, a
    major GC is attempted. The major GC does not use copying, but rather relies on
    mark-and-sweep algorithms. We can use throughput collectors (**Serial**, **Parallel**,
    and **ParallelOld**) or low-pause collectors (**Concurrent** and **G1**) for the
    old generation. The following table shows a non-exhaustive list of options to
    be used for each collector type:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在年轻代达到一定的存活阈值后，对象会被移动到持久代/旧代。如果无法进行小型的垃圾回收，则会尝试进行一次大型的垃圾回收。大型垃圾回收不使用复制，而是依赖于标记-清除算法。我们可以使用吞吐量收集器（**Serial**、**Parallel**和**ParallelOld**）或低延迟收集器（**Concurrent**和**G1**）来处理旧代。以下表格显示了一些非详尽的选项，用于每个收集器类型：
- en: '| Collector name | JVM flag |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 收集器名称 | JVM标志 |'
- en: '| --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Serial | -XX:+UseSerialGC |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 序列 | -XX:+UseSerialGC |'
- en: '| Parallel | -XX:+UseParallelGC |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 并行 | -XX:+UseParallelGC |'
- en: '| Parallel Compacting | -XX:+UseParallelOldGC |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 并行压缩 | -XX:+UseParallelOldGC |'
- en: '| Concurrent | -XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 并发 | -XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled
    |'
- en: '| G1 | -XX:+UseG1GC |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| G1 | -XX:+UseG1GC |'
- en: 'The previously mentioned flags can be used to start the Java runtime. For example,
    in the following command, we start the server JVM with a 4 GB heap using Parallel
    compacting GC:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的标志可以用来启动Java运行时。例如，在以下命令中，我们使用并行压缩垃圾回收启动了一个4GB堆的服务器JVM：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Sometimes, due to running full GC multiple times, the tenured space may have
    become so fragmented that it may not be feasible to move objects from Survivor
    to Tenured spaces. In those cases, a full GC with compaction is triggered. During
    this period, the application may appear unresponsive due to the full GC in action.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，由于多次运行完全垃圾回收，持久代可能变得非常碎片化，以至于可能无法将对象从Survivor空间移动到持久代空间。在这些情况下，会触发带有压缩的完全垃圾回收。在此期间，由于完全垃圾回收正在进行，应用程序可能看起来没有响应。
- en: Measuring memory (heap/stack) usage
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量内存（堆/栈）使用情况
- en: One of the prime reasons for a performance hit in the JVM is garbage collection.
    It certainly helps to know how heap memory is used by the objects we create and
    how to reduce the impact on GC by means of a lower footprint. Let's inspect how
    the representation of an object may lead to heap space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: JVM性能下降的一个主要原因是垃圾回收。了解我们创建的对象如何使用堆内存以及如何通过降低足迹来减少对GC的影响，这当然是有帮助的。让我们检查对象表示如何导致堆空间。
- en: 'Every (uncompressed) object or array reference on a 64-bit JVM is 16 bytes
    long. On a 32-bit JVM, every reference is 8 bytes long. As the 64-bit architecture
    is becoming more commonplace now, the 64-bit JVM is more likely to be used on
    servers. Fortunately, for a heap size of up to 32 GB, the JVM (Java 7) can use
    compressed pointers (default behavior) that are only 4 bytes in size. Java 8 VMs
    can address up to 64 GB heap size via compressed pointers as seen in the following
    table:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在64位JVM上，每个（未压缩的）对象或数组引用都是16字节长。在32位JVM上，每个引用都是8字节长。由于64位架构现在越来越普遍，64位JVM更有可能在服务器上使用。幸运的是，对于高达32GB的堆大小，JVM（Java
    7）可以使用压缩指针（默认行为），其大小仅为4字节。Java 8虚拟机可以通过压缩指针访问高达64GB的堆大小，如下表所示：
- en: '|   | Uncompressed | Compressed | 32-bit |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|   | 未压缩 | 压缩 | 32位 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Reference (pointer) | 8 | 4 | 4 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 参考指针 | 8 | 4 | 4 |'
- en: '| Object header | 16 | 12 | 8 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 对象头 | 16 | 12 | 8 |'
- en: '| Array header | 24 | 16 | 12 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 数组头 | 24 | 16 | 12 |'
- en: '| Superclass padding | 8 | 4 | 4 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 超类填充 | 8 | 4 | 4 |'
- en: 'This table illustrates pointer sizes in different modes (reproduced with permission
    from Attila Szegedi: [http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20](http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20)).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此表说明了不同模式下的指针大小（经Attila Szegedi授权复制：[http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20](http://www.slideshare.net/aszegedi/everything-i-ever-learned-about-jvm-performance-tuning-twitter/20))。
- en: 'We saw in the previous chapter how many bytes each primitive type takes. Let''s
    see how the memory consumption of the composite types looks with compressed pointers
    (a common case) on a 64-bit JVM with a heap size smaller than 32 GB:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一章中看到了每种原始类型占用多少字节。现在让我们看看在小于32GB的堆大小的64位JVM上，使用压缩指针（常见情况）的复合类型的内存消耗情况：
- en: '| Java Expression | 64-bit memory usage | Description (b = bytes, padding toward
    memory word size in approximate multiples of 8) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Java表达式 | 64位内存使用量 | 描述（b = 字节，填充到内存字大小的近似8的倍数） |'
- en: '| --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `new Object()` | 16 bytes | 12 b header + 4 b padding |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `new Object()` | 16字节 | 12字节头 + 4字节填充 |'
- en: '| `new byte[0]` | 16 bytes | 12 b `obj` header + 4 b `int` length = 16 b array
    header |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `new byte[0]` | 16字节 | 12字节`obj`头 + 4字节`int`长度 = 16字节数组头 |'
- en: '| `new String("foo")` | 40 bytes (interned for literals) | 12 b header + (12
    b array header + 6 b char-array content + 4 b length + 2 b padding = 24 b) + 4
    b hash |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `new String("foo")` | 40字节（字面量内部化） | 12字节头 + (12字节数组头 + 6字节字符数组内容 + 4字节长度
    + 2字节填充 = 24字节) + 4字节哈希 |'
- en: '| `new Integer(3)` | 16 bytes (boxed integer) | 12 b header + 4 b `int` value
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `new Integer(3)` | 16字节（装箱整数） | 12字节头 + 4字节`int`值 |'
- en: '| `new Long(4)` | 24 bytes (boxed long) | 12 b header + 8 b `long` value +
    4 b padding |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `new Long(4)` | 24字节（装箱长整型） | 12字节头 + 8字节`long`值 + 4字节填充 |'
- en: '| `class A { byte x; }``new A();` | 16 bytes | 12 b header + 1 b value + 3
    b padding |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `class A { byte x; }` `new A();` | 16字节 | 12字节头 + 1字节值 + 3字节填充 |'
- en: '| `class B extends A {byte y;}``new B();` | 24 bytes (subclass padding) | 12
    b reference + (1 b value + 7 b padding = 8 b) for A + 1 b for value of `y` + 3
    b padding |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `class B extends A {byte y;}` `new B();` | 24字节（子类填充） | 12字节引用 + (1字节值 +
    7字节填充 = 8字节) 用于A + 1字节用于`y`的值 + 3字节填充 |'
- en: '| `clojure.lang.Symbol.intern("foo")``// clojure ''foo` | 104 bytes (40 bytes
    interned) | 12 b header + 12 b ns reference + (12 b name reference + 40 b interned
    chars) + 4 b `int` hash + 12 b meta reference + (12 b `_str` reference + 40 b
    interned chars) – 40 b interned `str` |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `clojure.lang.Symbol.intern("foo")`// clojure ''foo | 104字节（40字节内部化） | 12字节头
    + 12字节命名空间引用 + (12字节名称引用 + 40字节内部字符) + 4字节`int`哈希 + 12字节元数据引用 + (12字节`_str`引用
    + 40字节内部字符) – 40字节内部`str` |'
- en: '| `clojure.lang.Keyword.intern("foo")``// clojure :foo` | 184 bytes (fully
    interned by factory method) | 12 b reference + (12 b symbol reference + 104 b
    interned value) + 4 b `int` hash + (12 b `_str` reference + 40 b interned `char`)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `clojure.lang.Keyword.intern("foo")`// clojure :foo | 184字节（由工厂方法完全内部化） |
    12字节引用 + (12字节符号引用 + 104字节内部值) + 4字节`int`哈希 + (12字节`_str`引用 + 40字节内部`char`) |'
- en: A comparison of space taken by a symbol and a keyword created from the same
    given string demonstrates that even though a keyword has slight overhead over
    a symbol, the keyword is fully interned and would provide better guard against
    memory consumption and thus GC over time. Moreover, the keyword is interned as
    a weak reference, which ensures that it is garbage collected when no keyword in
    memory is pointing to the interned value anymore.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 比较由相同给定字符串创建的符号和关键字所占用的空间，可以说明尽管关键字相对于符号有轻微的额外开销，但关键字是完全内联的，并且会提供更好的内存消耗和随时间进行的垃圾回收保护。此外，关键字作为弱引用进行内联，这确保了当内存中没有任何关键字指向内联值时，它会被垃圾回收。
- en: Determining program workload type
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确定程序工作负载类型
- en: We often need to determine whether a program is CPU/cache bound, memory bound,
    I/O bound or contention bound. When a program is I/O or contention bound, the
    CPU usage is generally low. You may have to use a profiler (we will see this in
    [Chapter 7](ch07.html "Chapter 7. Performance Optimization"), *Performance Optimization*)
    to find out whether threads are stuck due to resource contention. When a program
    is CPU/cache or memory bound, CPU usage may not be a clear indicator of the source
    of the bottleneck. In such cases, you may want to make an educated guess by inspecting
    cache misses in the program. On Linux systems tools such as **perf** ([https://perf.wiki.kernel.org/](https://perf.wiki.kernel.org/)),
    **cachegrind** ([http://valgrind.org/info/tools.html#cachegrind](http://valgrind.org/info/tools.html#cachegrind))
    and **oprofile** ([http://oprofile.sourceforge.net/](http://oprofile.sourceforge.net/))
    can help determine the volume of cache misses—a higher threshold may imply that
    the program is memory bound. However, using these tools with Java is not straightforward
    because Java's JIT compiler needs a warm-up until meaningful behavior can be observed.
    The project **perf-map-agent** ([https://github.com/jrudolph/perf-map-agent](https://github.com/jrudolph/perf-map-agent))
    can help generate method mappings that you can correlate using the `perf` utility.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要确定一个程序是否受CPU/缓存、内存、I/O或竞争限制。当一个程序受I/O或竞争限制时，CPU使用率通常较低。你可能需要使用分析器（我们将在第7章[性能优化](ch07.html
    "第7章。性能优化")中看到这一点）来找出线程是否因为资源竞争而陷入停滞。当一个程序受CPU/缓存或内存限制时，CPU使用率可能不是瓶颈来源的明确指标。在这种情况下，你可能想要通过检查程序中的缓存未命中来进行有根据的猜测。在Linux系统上，如**perf**
    ([https://perf.wiki.kernel.org/](https://perf.wiki.kernel.org/))、**cachegrind**
    ([http://valgrind.org/info/tools.html#cachegrind](http://valgrind.org/info/tools.html#cachegrind))和**oprofile**
    ([http://oprofile.sourceforge.net/](http://oprofile.sourceforge.net/))等工具可以帮助确定缓存未命中的数量——更高的阈值可能意味着程序受内存限制。然而，由于Java的JIT编译器需要预热才能观察到有意义的行为，因此使用这些工具与Java结合并不简单。项目**perf-map-agent**
    ([https://github.com/jrudolph/perf-map-agent](https://github.com/jrudolph/perf-map-agent))可以帮助生成你可以使用`perf`实用程序关联的方法映射。
- en: Tackling memory inefficiency
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决内存效率低下问题
- en: 'In earlier sections in this chapter we discussed that unchecked memory access
    may become a bottleneck. As of Java 8, due to the way the heap and object references
    work, we cannot fully control the object layout and memory access patterns. However,
    we can take care of the frequently executed blocks of code to consume less memory
    and attempt to make them cache-bound instead of memory-bound at runtime. We can
    consider a few techniques to lower memory consumption and randomness in access:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面的部分，我们讨论了未经检查的内存访问可能成为瓶颈。截至Java 8，由于堆和对象引用的工作方式，我们无法完全控制对象布局和内存访问模式。然而，我们可以关注频繁执行的代码块，以减少内存消耗，并尝试在运行时使它们成为缓存绑定而不是内存绑定。我们可以考虑一些降低内存消耗和访问随机性的技术：
- en: Primitive locals (long, double, boolean, char, etc) in the JVM are created on
    the stack. The rest of the objects are created on the heap and only their references
    are stored in the stack. Primitives have a low overhead and do not require memory
    indirection for access, and are hence recommended.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM中的原始局部变量（long、double、boolean、char等）是在栈上创建的。其余的对象是在堆上创建的，并且只有它们的引用存储在栈上。原始变量具有较低的开销，并且不需要内存间接访问，因此推荐使用。
- en: Data laid out in the main memory in a sequential fashion is faster to access
    than randomly laid out data. When we use a large (say more than eight elements)
    persistent map, the data stored in tries may not be sequentially laid out in memory,
    rather they would be randomly laid out in the heap. Moreover both keys and values
    are stored and accessed. When you use records (`defrecord`) and types (`deftype`),
    not only do they provide array/class semantics for the layout of fields within
    them, they do not store the keys, which is very efficient compared to regular
    maps.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主内存中以顺序方式布局的数据比随机布局的数据访问更快。当我们使用大型（比如说超过八个元素）持久映射时，存储在tries中的数据可能不会在内存中顺序布局，而是在堆中随机布局。此外，键和值都会被存储和访问。当你使用记录（`defrecord`）和类型（`deftype`）时，它们不仅提供了数组/类语义来布局其中的字段，而且它们不存储键，与常规映射相比，这非常高效。
- en: Reading large content from a disk or the network may have an adverse impact
    on performance due to random memory roundtrips. In [Chapter 3](ch03.html "Chapter 3. Leaning
    on Java"), *Leaning on Java*, we briefly discussed memory-mapped byte buffers.
    You can leverage memory-mapped buffers to minimize fragmented object allocation/access
    on the heap. While libraries such as `nio` ([https://github.com/pjstadig/nio/](https://github.com/pjstadig/nio/))
    and `clj-mmap` ([https://github.com/thebusby/clj-mmap](https://github.com/thebusby/clj-mmap))
    help us deal with memory-mapped buffers, `bytebuffer` ([https://github.com/geoffsalmon/bytebuffer](https://github.com/geoffsalmon/bytebuffer)),
    and `gloss` ([https://github.com/ztellman/gloss](https://github.com/ztellman/gloss))
    let us work with byte buffers. There are also alternate abstractions such as iota
    ([https://github.com/thebusby/iota](https://github.com/thebusby/iota)) that help
    us deal with large files as collections.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从磁盘或网络读取大量内容可能会由于随机内存往返而对性能产生不利影响。在[第3章](ch03.html "第3章。依赖Java")《依赖Java》中，我们简要讨论了内存映射字节数据缓冲区。您可以利用内存映射缓冲区来最小化堆上的碎片化对象分配/访问。虽然`nio`([https://github.com/pjstadig/nio/](https://github.com/pjstadig/nio/))和`clj-mmap`([https://github.com/thebusby/clj-mmap](https://github.com/thebusby/clj-mmap))等库帮助我们处理内存映射缓冲区，但`bytebuffer`([https://github.com/geoffsalmon/bytebuffer](https://github.com/geoffsalmon/bytebuffer))和`gloss`([https://github.com/ztellman/gloss](https://github.com/ztellman/gloss))让我们能够处理字节数据缓冲区。还有其他抽象，如iota([https://github.com/thebusby/iota](https://github.com/thebusby/iota))，它帮助我们以集合的形式处理大文件。
- en: Given that memory bottleneck is a potential performance issue in data-intensive
    programs, lowering memory overhead goes a long way in avoiding performance risk.
    Understanding low-level details of the hardware, the JVM and Clojure's implementation
    helps us choose the appropriate techniques to tackle the memory bottleneck issue.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存瓶颈是数据密集型程序中潜在的性能问题，降低内存开销在很大程度上有助于避免性能风险。了解硬件、JVM和Clojure实现的高级细节有助于我们选择适当的技巧来解决内存瓶颈问题。
- en: Measuring latency with Criterium
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Criterium测量延迟
- en: 'Clojure has a neat little macro called `time` that evaluates the body of code
    passed to it, and then prints out the time it took and simply returns the value.
    However, we can note that often the time taken to execute the code varies quite
    a bit across various runs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure有一个叫做`time`的小巧的宏，它评估传递给它的代码的主体，然后打印出所花费的时间，并简单地返回值。然而，我们可以注意到，代码执行所需的时间在不同的运行中变化很大：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are several reasons associated to this variance in behavior. When cold
    started, the JVM has its heap segments empty and is unaware of the code path.
    As the JVM keeps running, the heap fills up and the GC patterns start becoming
    noticeable. The JIT compiler gets a chance to profile the different code paths
    and optimize them. Only after quite some GC and JIT compilation rounds, does the
    JVM performance become less unpredictable.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与这种行为变化相关的有几个原因。当JVM冷启动时，其堆段为空，并且对代码路径一无所知。随着JVM的持续运行，堆开始填满，GC模式开始变得明显。JIT编译器有机会分析不同的代码路径并进行优化。只有在经过相当多的GC和JIT编译轮次之后，JVM的性能才变得不那么不可预测。
- en: 'Criterium ([https://github.com/hugoduncan/criterium](https://github.com/hugoduncan/criterium))
    is a Clojure library to scientifically measure the latency of Clojure expressions
    on a machine. A summary of how it works can be found at the Criterium project
    page. The easiest way to use Criterium is to use it with Leiningen. If you want
    Criterium to be available only in the REPL and not as a project dependency, add
    the following entry to the `~/.lein/profiles.clj` file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Criterium ([https://github.com/hugoduncan/criterium](https://github.com/hugoduncan/criterium))是一个Clojure库，用于在机器上科学地测量Clojure表达式的延迟。关于其工作原理的摘要可以在Criterium项目页面上找到。使用Criterium的最简单方法是将其与Leiningen一起使用。如果你想使Criterium仅在REPL中可用，而不是作为项目依赖项，请将以下条目添加到`~/.lein/profiles.clj`文件中：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Another way is to include `criterium` in your project in the `project.clj`
    file:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是在`project.clj`文件中包含`criterium`：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once done with the editing of the file, launch REPL using `lein repl`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 完成文件编辑后，使用`lein repl`启动REPL：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, we can see that, on average, the expression took 31.6 ms on a certain test
    machine.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到，在某个测试机器上，平均而言，该表达式花费了31.6毫秒。
- en: Criterium and Leiningen
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Criterium和Leiningen
- en: 'By default, Leiningen starts the JVM in a low-tiered compilation mode, which
    causes it to start up faster but impacts the optimizations that the JRE can perform
    at runtime. To get the best effects when running tests with Criterium and Leiningen
    for a server-side use case, be sure to override the defaults in `project.clj`
    as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Leiningen以低级编译模式启动JVM，这使其启动更快，但会影响JRE在运行时可以执行的优化。为了在服务器端用例中使用Criterium和Leiningen运行测试时获得最佳效果，请确保在`project.clj`中覆盖默认设置，如下所示：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `^:replace` hint causes Leiningen to replace its own defaults with what
    is provided under the `:jvm-opts` key. You may like to add more parameters as
    needed, such as a minimum and maximum heap size to run the tests.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`^:replace`提示使Leiningen用`:jvm-opts`键下提供的选项替换其默认设置。你可能需要根据需要添加更多参数，例如运行测试的最小和最大堆大小。'
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The performance of a software system is directly impacted by its hardware components,
    so understanding how the hardware works is crucial. The processor, caches, memory,
    and I/O subsystems have different performance behaviors. Clojure, being a hosted
    language, understanding the performance properties of the host, that is, the JVM,
    is equally important. The Criterium library is useful for measuring the latency
    of the Clojure code—we will discuss Criterium again in [Chapter 6](ch06.html "Chapter 6. Measuring
    Performance"), *Measuring Performance*. In the next chapter we will look at the
    concurrency primitives in Clojure and their performance characteristics.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 软件系统的性能直接受其硬件组件的影响，因此了解硬件的工作原理至关重要。处理器、缓存、内存和I/O子系统具有不同的性能行为。Clojure作为一种托管语言，了解宿主（即JVM）的性能特性同样重要。Criterium库用于测量Clojure代码的延迟——我们将在[第6章](ch06.html
    "第6章。测量性能")“测量性能”中再次讨论Criterium。在下一章中，我们将探讨Clojure的并发原语及其性能特性。
