- en: Centralized Logging with the EFK Stack
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集中日志记录的EFK堆栈
- en: 'In this chapter, we will learn how to collect and store log records from microservice
    instances, as well as how to search and analyze log records. As we mentioned in
    [Chapter 1](282e7b49-42b8-4649-af81-b4b6830d391d.xhtml), *Introduction to Microservices*
    (refer to the *Centralized log analysis* section), it is difficult to get an overview
    of what is going on in a system landscape of microservices when each microservice
    instance writes log records to its local filesystem. We need a component that
    can collect the log records from the microservice''s local filesystem and store
    them in a central database for analysis, search, and visualization. A popular
    open source-based solution for this builds on the following tools:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何从微服务实例收集和存储日志记录，以及如何搜索和分析日志记录。正如我们在[第1章](282e7b49-42b8-4649-af81-b4b6830d391d.xhtml)《微服务简介》（参考*集中日志分析*部分）提到的，当每个微服务实例将日志记录写入其本地文件系统时，很难获得微服务系统景观的概览。我们需要一个组件，它可以从微服务的本地文件系统收集日志记录并将它们存储在中央数据库中进行分析、搜索和可视化。针对这一问题，一个流行的开源解决方案基于以下工具构建：
- en: '**Elasticsearch***,* a distributed database with great capabilities for the
    search and analysis of large datasets'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**，一个用于搜索和分析大数据集的分布式数据库'
- en: '**Fluentd**, a data collector that can be used to collect log records from
    various sources, filter and transform the collected information, and finally send
    it to various consumers, for example, Elasticsearch'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fluentd**，一个数据收集器，可以用来从各种来源收集日志记录，过滤和转换收集的信息，最后发送给各种消费者，例如Elasticsearch'
- en: '**Kibana**, a graphical frontend to Elasticsearch that can be used to visualize
    search results and run analyses of the collected log records'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kibana**，一个Elasticsearch的图形用户界面，可以用来可视化搜索结果和对收集的日志记录进行分析'
- en: Together, these tools are called the **EFK stack**, named after the initials
    of each tool.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具合称为**EFK堆栈**，以每个工具的首字母命名。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Configuring Fluentd
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Fluentd
- en: Deploying the EFK stack on Kubernetes for development and test usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈以供开发和测试使用
- en: 'Trying out the EFK stack by:'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以下方式尝试EFK堆栈：
- en: Analyzing the collected log records
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析收集的日志记录
- en: Discovering log records from the microservices and finding related log records
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从微服务中发现日志记录并查找相关日志记录
- en: Performing root cause analysis
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行根本原因分析
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All of the commands that are described in this book have been run on a MacBook
    Pro using macOS Mojave but should be straightforward to modify so that they can
    be run on another platform, such as Linux or Windows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中描述的所有命令都已经在使用macOS Mojave的MacBook Pro上运行过，但应该很容易修改，以便它们可以在其他平台上运行，例如Linux或Windows。
- en: No new tools need to be installed in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中不需要安装任何新工具。
- en: The source code for this chapter can be found in this book's GitHub repository: [https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19](https://github.com/PacktPublishing/Hands-On-Microservices-with-Spring-Boot-and-Spring-Cloud/tree/master/Chapter19)
- en: 'To be able to run the commands that are described in this book, you need to
    download the source code to a folder and set up an environment variable, `$BOOK_HOME`,
    which points to that folder. Some sample commands are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够运行本书中描述的命令，你需要将源代码下载到一个文件夹中，并设置一个环境变量`$BOOK_HOME`，它指向该文件夹。一些示例命令如下：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: All of the source code examples in this chapter come from the source code in `$BOOK_HOME/Chapter19` and have
    been tested using Kubernetes 1.15.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有源代码示例都来自`$BOOK_HOME/Chapter19`的源代码，并使用Kubernetes 1.15进行了测试。
- en: If you want to take a look at the changes we applied to the source code in this
    chapter, that is, look at the changes we made so that we can use the EFK stack
    for centralized log analyses, you can compare it with the source code for [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*. You can use your favorite `diff` tool and compare
    the two folders, `$BOOK_HOME/Chapter18` and `$BOOK_HOME/Chapter19`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看我们在这个章节中应用到源代码的变化，即查看我们做出的改变以便我们可以使用 EFK 栈进行集中式日志分析，你可以使用你最喜欢的 `diff`
    工具，比较两个文件夹 `$BOOK_HOME/Chapter18` 和 `$BOOK_HOME/Chapter19`。
- en: Configuring Fluentd
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Fluentd
- en: In this section we will learn the basics for how to configure Fluentd. Before
    we do that, let's learn a bit about the background of Fluentd and how it works
    on a high level.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何配置 Fluentd 的基础知识。在那之前，让我们先了解一下 Fluentd 的背景以及它在高层次上是如何工作的。
- en: Introducing Fluentd
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Fluentd
- en: Historically, one of the most popular open source stacks for handling log records
    has been the ELK stack from Elastic ([https://www.elastic.co](https://www.elastic.co)),
    based on Elasticsearch, Logstash (used for log collection and transformation),
    and Kibana. Since Logstash runs on a Java VM, it requires a relatively large amount
    of memory. Over the years, a number of open source alternatives have been developed
    that require significantly less memory than Logstash, one of them being Fluentd ([https://www.fluentd.org](https://www.fluentd.org)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，处理日志记录最受欢迎的开源栈之一就是来自 Elastic 的 ELK 栈（[https://www.elastic.co](https://www.elastic.co)），基于
    Elasticsearch、Logstash（用于日志收集和转换）和 Kibana。由于 Logstash 运行在 Java VM 上，它需要相对较多的内存。多年来，已经开发出许多比
    Logstash 占用更少内存的开源替代品，其中之一就是 Fluentd([https://www.fluentd.org](https://www.fluentd.org))。
- en: Fluentd is managed by the **Cloud Native Computing Foundation** (**CNCF**) ([https://www.cncf.io](https://www.cncf.io)),
    that is, the same organization that manages the Kubernetes project. Therefore,
    Fluentd has become a natural choice as an open source-based log collector that
    runs in Kubernetes. Together with Elastic and Kibana, it forms the EFK stack.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是由 **云原生计算基金会** （**CNCF**）管理的（[https://www.cncf.io](https://www.cncf.io)），也就是管理
    Kubernetes 项目的同一个组织。因此，Fluentd 已经成为在 Kubernetes 中运行的开源日志收集器的自然选择。与 Elastic 和 Kibana
    一起，它构成了 EFK 栈。
- en: Fluentd is written in a mix of C and Ruby, using C for the performance-critical
    parts and Ruby where flexibility is of more importance, for example, allowing
    the simple installation of third-party plugins using Ruby's `gem install` command.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 是用 C 和 Ruby 编写的混合语言，使用 C 处理性能关键部分，而在需要灵活性时使用 Ruby，例如，允许使用 Ruby 的 `gem
    install` 命令简单地安装第三方插件。
- en: 'A log record is processed as an event in Fluentd and consists of the following
    information:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 中的日志记录作为事件处理，并包括以下信息：
- en: A `time` field describing when the log record was created
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `time` 字段，描述了日志记录创建的时间。
- en: A `tag` field that identifies what type of log record it is—the tag is used
    by Fluentd's routing engine to determine how a log record shall be processed
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `tag` 字段，用来标识它是什么类型的日志记录——这个标签由 Fluentd 的路由引擎用来确定日志记录应该如何被处理。
- en: A **record **that contains the actual log information, which is stored as a
    JSON object
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 **记录**，包含实际的日志信息，以 JSON 对象的形式存储。
- en: 'A Fluentd configuration file is used to tell Fluentd how to collect, process,
    and finally send log records to various targets, such as Elasticsearch. A configuration
    file consists of the following types of core elements:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 配置文件用于告诉 Fluentd 如何收集、处理并最终将日志记录发送到各种目标，例如 Elasticsearch。一个配置文件由以下类型的核心元素组成：
- en: '`<source>`: Source elements describe where Fluentd will collect log records.
    For example, tailing log files that have been written to by Docker containers.
    Source elements typically tag the log records, describing the type of log record.
    It could, for example, be used to tag log records to state that they come from
    containers running in Kubernetes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<source>`：源元素描述了 Fluentd 将会从哪里收集日志记录。例如，追踪由 Docker 容器写入的日志文件。源元素通常给日志记录打上标签，描述日志记录的类型。它可以用来自动标记来自运行在
    Kubernetes 中的容器的日志记录。'
- en: '`<filter>`: Filter elements are used to process the log records, for example,
    a filter element can parse log records that come from Spring Boot-based microservices
    and extract interesting parts of the log message into separate fields in the log
    record. Extracting information into separate fields in the log record makes the
    information searchable by Elasticsearch. A filter element selects what log records
    to process based on their tags.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<filter>`：过滤器元素用于处理日志记录，例如，一个过滤器元素可以解析来自基于 Spring Boot 的微服务的日志记录，并将日志消息中的有趣部分提取到日志记录中的单独字段中。将信息提取到日志记录中的单独字段，使得可以通过
    Elasticsearch 搜索这些信息。过滤器元素根据它们的标签选择要处理的日志记录。'
- en: '`<match>`: Output elements are used to perform two main tasks:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`：输出元素用于执行两个主要任务：'
- en: Send processed log records to targets such as Elasticsearch.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将处理后的日志记录发送到目标，如 Elasticsearch。
- en: 'Routing is to decide how to process log records. A routing rule can rewrite
    the tag and reemit the log record into the Fluentd routing engine for further
    processing. A routing rule is expressed as an embedded `<rule>` element inside
    the `<match>` element. Output elements decide what log records to process, in
    the same way as a filter: based on the tag of the log records.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路由是决定如何处理日志记录的方法。路由规则可以重写标签，并将日志记录重新发射到 Fluentd 路由引擎以进行进一步处理。路由规则被表达为 `<match>`
    元素内的嵌入 `<rule>` 元素。输出元素决定根据日志记录的标签处理哪些日志记录，与过滤器的方式相同。
- en: Fluentd comes with a number of built-in and external third-party plugins that
    are used by the source, filter, and output elements. We will see some of them
    in action when we walk through the configuration file in the next section. For
    more information on the available plugins, see Fluentd's documentation, which
    is available at [https://docs.fluentd.org](https://docs.fluentd.org).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 自带了许多内置和外部的第三方插件，这些插件被源、过滤器和输出元素所使用。在我们下一节查看配置文件时，将看到其中的一些插件。关于可用插件的更多信息，请参阅
    Fluentd 的文档，该文档可访问于 [https://docs.fluentd.org](https://docs.fluentd.org)。
- en: With this introduction to Fluentd out of the way, we are ready to see how Fluentd can
    be configured to process the log records from our microservices.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了 Fluentd 之后，我们准备了解 Fluentd 应该如何配置以处理我们微服务中的日志记录。
- en: Configuring Fluentd
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Fluentd
- en: The configuration of Fluentd is based on the configuration files from a Fluentd
    project on GitHub, `fluentd-kubernetes-daemonset`. The project contains Fluentd
    configuration files for how to collect log records from containers that run in
    Kubernetes and how to send them to Elasticsearch once they have been processed.
    We can reuse this configuration without changes and it will simplify our own configuration
    to a great extent. The Fluentd configuration files can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 的配置基于 GitHub 上的 Fluentd 项目的配置文件，`fluentd-kubernetes-daemonset`。该项目包含
    Fluentd 配置文件，说明如何从在 Kubernetes 中运行的容器收集日志记录，以及一旦它们被处理，如何将它们发送到 Elasticsearch。我们可以不进行任何更改地重用此配置，这将极大地简化我们自己的配置。Fluentd
    配置文件可以在 [https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf](https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.4/debian-elasticsearch/conf)
    找到。
- en: 'The configuration files that provide this functionality are `kubernetes.conf`
    and `fluent.conf`. The `kubernetes.conf` configuration file contains the following
    information:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提供这种功能的配置文件有 `kubernetes.conf` 和 `fluent.conf`。`kubernetes.conf` 配置文件包含以下信息：
- en: Source elements that tail container log files and log files from processes that
    run outside of Kubernetes, for example, the `kubelet` and the Docker daemon. The
    source elements also tag the log records from Kubernetes with the full name of
    the log file with `/` replaced by `.` and prefixed with `kubernetes`. Since the
    tag is based on the full filename, the name contains the name of the namespace,
    pod, and container, among other things. So, the tag is very useful for finding
    log records of interest by matching the tag. For example, the tag from the `product-composite` microservice
    could be something like `kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`,
    while the tag for the corresponding `istio-proxy` in the same pod could be something
    like `kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源元素用于监控容器日志文件以及运行在Kubernetes之外的过程的日志文件，例如`kubelet`和Docker守护进程。源元素还将为Kubernetes中的日志记录打上完整的日志文件名，并用`/`替换`.'，并加上前缀`kubernetes`。由于标签基于完整的文件名，其中包含命名空间、Pod和容器的名称等信息，因此标签对于通过匹配标签找到感兴趣的日志记录非常有用。例如，`product-composite`微服务的标签可能类似于`kubernetes.var.log.containers.product-composite-7...s_hands-on_comp-e...b.log`，而同一Pod中相应的`istio-proxy`的标签可能类似于`kubernetes.var.log.containers.product-composite-7...s_hands-on_istio-proxy-1...3.log`。
- en: A filter element that enriches the log records that come from containers running
    inside Kubernetes, along with Kubernetes-specific fields that contain information
    such as the names of the containers and the namespace they run in.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过滤器元素，用于丰富来自Kubernetes内部运行的容器以及包含容器名称和它们运行的命名空间等信息的Kubernetes特定字段的日志记录。
- en: 'The main configuration file, `fluent.conf`, contains the following information:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 主配置文件`fluent.conf`包含以下信息：
- en: '`@include` statements for other configuration files, for example, the `kubernetes.conf` file
    we described previously. It also includes custom configuration files that are
    placed in a specific folder, making it very easy for us to reuse these configuration
    files without any changes and provide our own configuration file that only handles
    processing related to our own log records. We simply need to place our own configuration
    file in the folder specified by the `fluent.conf` file.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@include`语句用于其他配置文件，例如我们之前描述的`kubernetes.conf`文件。它还包括放置在特定文件夹中的自定义配置文件，使我们能够不进行任何更改就重用这些配置文件，并只处理与自身日志记录相关的处理。我们只需将自定义配置文件放置在`fluent.conf`文件指定的文件夹中。'
- en: An output element that sends log records to Elasticsearch.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出元素，用于将日志记录发送到Elasticsearch。
- en: As we described in the *Deploying Fluentd* section, these two configuration
    files will be packaged into the Docker image we will build for Fluentd.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*部署Fluentd*部分所描述的，这两个配置文件将被打包到我们将为Fluentd构建的Docker镜像中。
- en: 'What''s left to cover in our own configuration file is the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自己的配置文件中需要覆盖的是以下内容：
- en: Detect and parse Spring Boot formatted log records from our microservices.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测并解析我们微服务中的Spring Boot格式的日志记录。
- en: Handling multiline stack traces. Stack traces, for example, are written to log
    files using multiple lines. This makes it hard for Fluentd to handle a stack trace
    as a single log record.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理多行堆栈跟踪。例如，堆栈跟踪是使用多行写入日志文件的。这使得Fluentd难以将堆栈跟踪作为单个日志记录处理。
- en: Separating log records from the `istio-proxy` sidecars from the log records
    that were created by the microservices running in the same pod. The log records
    that are created by `istio-proxy` don't follow the same pattern as the log patterns
    that are created by our Spring Boot-based microservices. Therefore, they must
    be handled separately so that Fluentd doesn't try to parse them as Spring Boot
    formatted log records.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`istio-proxy`侧边的日志记录与在同一Pod中运行的微服务生成的日志记录分开。由`istio-proxy`生成的日志记录不遵循我们基于Spring
    Boot的微服务生成的日志模式。因此，它们必须分开处理，以便Fluentd不要尝试将它们解析为Spring Boot格式的日志记录。
- en: To achieve this, the configuration is, to a large extent, based on using the
    `rewrite_tag_flter` plugin. This plugin can be used for routing log records based
    on the concept of changing the name of a tag and then reemitting the log record
    to the Fluentd routing engine.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，配置很大程度上是基于使用`rewrite_tag_filter`插件。这个插件可以用于根据改变标签名称的概念来路由日志记录，然后将日志记录重新发射到Fluentd路由引擎。
- en: 'This processing is summarized by the following UML activity diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此处理总结如下UML活动图：
- en: '![](img/4aa9c779-4a07-422d-af6c-00539062d30a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4aa9c779-4a07-422d-af6c-00539062d30a.png)'
- en: 'At a high level, the design of the configuration file looks as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，配置文件的设计如下所示：
- en: The tags of all of the log records from Istio, including `istio-proxy`, are
    prefixed with `istio` so that they can be separated from the Spring Boot-based
    log records.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Istio的所有日志记录的标签，包括`istio-proxy`，都加上`istio`前缀，以便将它们与基于Spring Boot的日志记录区分开来。
- en: The tags of all of the log records from the `hands-on` namespace (except for
    the log records from `istio-proxy`) are prefixed with `spring-boot`.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有来自`hands-on`命名空间的日志记录（除了来自`istio-proxy`的日志记录）的标签都加上`spring-boot`前缀。
- en: The log records from Spring Boot are checked for the presence of multiline stack
    traces. If the log record is part of a multiline stack trace, it is processed
    by the third-party `detect-exceptions` plugin to recreate the stack trace. Otherwise,
    it is parsed using a regular expression to extract information of interest. See
    the *Deploying Fluentd* section for details on this third-party plugin.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自Spring Boot的日志记录检查是否有多行堆栈跟踪。如果日志记录是多行堆栈跟踪的一部分，它将使用第三方`detect-exceptions`插件来重新创建堆栈跟踪。否则，它将使用正则表达式提取感兴趣的信息。关于这个第三方插件的详细信息，请参见*部署Fluentd*部分。
- en: 'The `fluentd-hands-on.conf` configuration file follows this activity diagram
    closely. The configuration file is placed inside a Kubernetes config map (see
    `kubernetes/efk/fluentd-hands-on-configmap.yml`). Let''s go through this step
    by step, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`fluentd-hands-on.conf`配置文件紧密遵循这个活动图。该配置文件放在一个Kubernetes配置映射中（参见`kubernetes/efk/fluentd-hands-on-configmap.yml`）。让我们一步一步地讲解，如下所述：'
- en: 'First comes the definition of the config map and the filename of the configuration
    file, `fluentd-hands-on.conf`. It looks as follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先是配置映射和配置文件filename的定义，`fluentd-hands-on.conf`。它看起来像这样：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From the preceding source code, we understand that the `data` element will contain
    the configuration of Fluentd. It starts with the filename and uses a vertical
    bar `|` to mark the beginning of the embedded configuration file for Fluentd.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的源代码中，我们了解到`data`元素将包含Fluentd的配置。它从文件名开始，并使用竖线`|`标记Fluentd嵌入式配置文件的开始。
- en: 'The first `<match>` element matches the log records from Istio, that is, tags
    that are prefixed with `kubernetes` and contain either `istio` as part of its
    namespace or part of its container name. It looks like this:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个`<match>`元素匹配来自Istio的日志记录，即以`kubernetes`为前缀并包含作为其命名空间或容器名称一部分的`istio`的标签。它看起来像这样：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s explain the preceding source code in more detail:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地解释前面的源代码：
- en: The `<match>` element matches any tags that follow the `kubernetes.**istio**` pattern,
    that is, it starts with `kubernetes` and then contains the word `istio` somewhere
    in the tag name. `istio` can either come from the name of the namespace or the
    container; both are part of the tag.
  id: totrans-66
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素匹配任何符合`kubernetes.**istio**`模式的标签，即，它以`kubernetes`开始，然后在标签名中包含单词`istio`。`istio`可以来自命名空间名称或容器名称；两者都是标签的一部分。'
- en: The `<match>` element contains only one `<rule>` element, which prefixes the
    tag with `istio`. The `${tag}` variable holds the current value of the tag.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素只包含一个`<rule>`元素，它为标签加上`istio`前缀。`${tag}`变量持有当前标签的值。'
- en: 'Since this is the only `<rule>` element in the `<match>` element, it is configured
    to match all of the log records like so:'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这是`<match>`元素中唯一的`<rule>`元素，它被配置为如此匹配所有日志记录：
- en: Since all of the log records that come from Kubernetes have a `log` field, the
    `key` field is set to `log`, that is, the rule looks for a `log` field in the
    log records.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于所有来自Kubernetes的日志记录都有一个`log`字段，所以`key`字段被设置为`log`，即规则在日志记录中查找一个`log`字段。
- en: To match any string in the `log` field, the `pattern` field is set to the `^(.*)$` regular
    expression. `^` marks the beginning of a string, while `$` marks the end of a
    string. `(.*)` matches any number of characters, except for line breaks.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了匹配`log`字段中的任何字符串，`pattern`字段被设置为`^(.*)$`正则表达式。`^`标志着一个字符串的开始，而`$`标志着一个字符串的结束。`(.*)`匹配任何数量的字符，除了换行符。
- en: The log records are reemitted to the Fluentd routing engine. Since no other
    elements in the configuration file match tags starting with `istio`, they will
    be sent directly to the output element for Elasticsearch, which is defined in
    the `fluent.conf` file we described previously.
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录被重新发送到Fluentd路由引擎。由于配置文件中没有其他元素与以`istio`开头的标签匹配，它们将被直接发送到我们之前描述的`fluent.conf`文件中定义的Elasticsearch输出元素。
- en: 'The second `<match>` element matches all of the log records from the `hands-on`
    namespace, that is, the log records that are emitted by our microservices. It
    looks like this:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个`<match>`元素匹配来自`hands-on`命名空间的全部日志记录，也就是说，是我们微服务发出的日志记录。它看起来像这样：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the preceding source code we can see that:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从前的源代码我们可以看出：
- en: The log records emitted by our microservices use formatting rules for the log
    message defined by Spring Boot so their tags are prefixed with `spring-boot`.
    Then, they are re-emitted for further processing.
  id: totrans-75
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们微服务发出的日志记录使用Spring Boot定义的日志消息格式化规则，因此它们的标签前缀为`spring-boot`。然后，它们被重新发出以进行进一步处理。
- en: The `<match>` element is configured in the same way as the `<match kubernetes.**istio**>`
    element we looked at previously.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<match>`元素的配置方式与我们在前面查看的`<match kubernetes.**istio**>`元素相同。'
- en: 'The third `<match>` element matches `spring-boot` log records and determines
    whether they are ordinary Spring Boot log records or are part of a multiline stack
    trace. It looks like this:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个`<match>`元素匹配`spring-boot`日志记录，并确定它们是普通的Spring Boot日志记录还是多行堆栈跟踪的一部分。它看起来像这样：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As seen in the preceding source code, this is determined by using two `<rule>` elements:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所见的源代码，这是通过使用两个`<rule>`元素来确定的：
- en: The first uses a regular expression to check whether the `log` field in the
    log element starts with a timestamp or not.
  id: totrans-80
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个使用正则表达式检查日志元素中的`log`字段是否以时间戳开头。
- en: If the `log` field starts with a timestamp, the log record is treated as an
    ordinary Spring Boot log record and its tag is prefixed with `parse`.
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`log`字段以时间戳开头，则将日志记录视为普通的Spring Boot日志记录，并在其标签前加上`parse`前缀。
- en: Otherwise, the second `<rule>` element will match and the log record is handled
    as a multiline log record. Its tag is prefixed with `check.exception`.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，第二个`<rule>`元素将匹配，并将日志记录作为多行日志记录处理。其标签前缀为`check.exception`。
- en: The log record is re-emitted in either case and its tag will either start with
    `check.exception.spring-boot.kubernetes` or `parse.spring-boot.kubernetes` after
    this process.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论如何，在这种情况下，日志记录都会被重新发出，其标签将在此过程后以`check.exception.spring-boot.kubernetes`或`parse.spring-boot.kubernetes`开头。
- en: 'In the fourth `<match>` element, the selected log records have a tag that starts
    with `check.exception.spring-boot`, that is, log records that are part of a multiline
    stack trace. It looks like this:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第四个`<match>`元素中，所选的日志记录具有以`check.exception.spring-boot`开头的标签，即，是多行堆栈跟踪的一部分的日志记录。它看起来像这样：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The source code for the `detect_exceptions` plugin works like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect_exceptions`插件的源代码像这样工作：'
- en: The `detect_exceptions` plugin is used to combine multiple one-line log records
    into a single log record that contains a complete stack trace.
  id: totrans-87
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`detect_exceptions`插件用于将多个单行日志记录组合成一个包含完整堆栈跟踪的日志记录。'
- en: Before a multiline log record is reemitted into the routing engine, the `check` prefix
    is removed from the tag to prevent a never-ending processing loop of the log record.
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将多行日志记录重新输入路由引擎之前，将标签中的`check`前缀移除，以防止日志记录的处理循环无限进行。
- en: 'Finally, the configuration file consists of a filter element that parses Spring
    Boot log messages using a regular expression, extracting information of interest.
    It looks like this:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，配置文件由一个过滤器元素组成，该元素使用正则表达式解析Spring Boot日志消息，提取感兴趣的信息。它看起来像这样：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s explain the preceding source code in more detail:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地解释前面的源代码：
- en: Note that filter elements don't re-emit log records; instead, they just pass
    them on to the next element in the configuration file that matches the log record's
    tag.
  id: totrans-92
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，过滤器元素不会重新发出日志记录；相反，它们只是将它们传递给配置文件中与日志记录的标签匹配的下一个元素。
- en: 'The following fields are extracted from the Spring Boot log message that''s
    stored in the `log` field in the log record:'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从存储在日志记录中的`log`字段的Spring Boot日志消息中提取以下字段：
- en: '`<time>`: The timestamp for when the log record was created'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<time>`：创建日志记录的时间戳'
- en: '`<spring.level>`: The log level of the log record, for example, `FATAL`, `ERROR`,
    `WARN`, `INFO`, `DEBUG`, or `TRACE`'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.level>`：日志记录的日志级别，例如，`FATAL`、`ERROR`、`WARN`、`INFO`、`DEBUG`或`TRACE`'
- en: '`<spring.service>`: The name of the microservice'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.service>`：微服务名称'
- en: '`<spring.trace>`: The trace ID used to perform distributed tracing'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.trace>`：用于执行分布式跟踪的跟踪ID'
- en: '`<spring.span>`: The span ID, the ID of the part of the distributed processing
    that this microservice executed'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.span>`：跨度ID，即这个微服务执行的分布式处理的部分的ID'
- en: '`<spring.pid>`: The process ID'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.pid>`：进程ID'
- en: '`<spring.thread>`: The thread ID'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.thread>`：线程ID'
- en: '`<spring.class>`: The name of the Java class'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<spring.class>`：Java类的名称'
- en: '`<log>`: The actual log message'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<log>`：实际的日志消息'
- en: The names of Spring Boot-based microservices are specified using the `spring.application.name` property.
    This property has been added to each microservice-specific property file in the
    config repository, in the `config-repo` folder.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`spring.application.name`属性指定基于Spring Boot的微服务名称。此属性已添加到配置存储库中的每个微服务特定属性文件中，在`config-repo`文件夹中。
- en: Getting regular expressions right can be challenging, to say the least. Thankfully,
    there are several websites that can help. When it comes to using regular expressions
    together with Fluentd, I recommend using the following site: [https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 准确地编写正则表达式可以说是一项挑战。幸运的是，有多个网站可以提供帮助。当涉及到与Fluentd一起使用正则表达式时，我推荐使用以下网站：[https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/)。
- en: Now that you've been introduced to how Fluentd works and how the configuration
    file is constructed, we are ready to deploy the EKF stack.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了Fluentd如何工作以及配置文件是如何构建的，我们就可以部署EKF堆栈了。
- en: Deploying the EFK stack on Kubernetes
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈
- en: 'Deploying the EFK stack on Kubernetes will be done in the same way as we have
    deployed our own microservices: using Kubernetes definition files for objects
    such as deployments, services, and configuration maps.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes上部署EFK堆栈的方式将与我们部署自己的微服务的方式相同：使用Kubernetes定义文件为部署、服务和配置映射等对象。
- en: 'The deployment of the EFK stack is divided into two parts:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: EFK堆栈的部署分为两部分：
- en: One part where we deploy Elasticsearch and Kibana
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们部署Elasticsearch和Kibana的一部分
- en: One part where we deploy Fluentd
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们部署Fluentd的一部分
- en: But first, we need to build and deploy our own microservices.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，我们需要构建和部署我们自己的微服务。
- en: Building and deploying our microservices
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和部署我们的微服务
- en: 'Building, deploying, and verifying the deployment using the `test-em-all.bash` test
    script is done in the same way as it was done in [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, in the *Running commands
    to create the service mesh* section. Run the following commands to get started:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`test-em-all.bash`测试脚本构建、部署并验证部署的方式与第18章中的*使用服务网格提高可观测性和管理*，*运行创建服务网格的命令*部分相同。开始运行以下命令：
- en: 'First, build the Docker images from the source with the following commands:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令从源代码构建Docker镜像：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Recreate the namespace, `hands-on`, and set it as the default namespace:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新创建命名空间`hands-on`，并将其设置为默认命名空间：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Execute the deployment by running the `deploy-dev-env.bash` script with the
    following command:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令执行部署：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Start the Minikube tunnel, if it''s not already running (see [Chapter 18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using
    a Service Mesh to Improve Observability and Management*, the *Setting up access
    to Istio services* section for a recap, if required):'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果Minikube隧道尚未运行，请启动（如果需要，请参阅第18章*使用服务网格提高可观测性和管理*，*设置对Istio服务的访问*部分）：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remember that this command requires that your user has `sudo` privileges and
    that you enter your password during startup and shutdown. It takes a couple of
    seconds before the command asks for the password, so it is easy to miss!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，此命令要求您的用户具有`sudo`权限，并且在启动和关闭时输入您的密码。在命令要求输入密码之前需要几秒钟，所以很容易错过！
- en: 'Run the normal tests to verify the deployment with the following command:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行正常测试以验证部署：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Expect the output to be similar to what we have seen from the previous chapters:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 期望输出与前几章看到的内容类似：
- en: '![](img/042de643-01de-4776-8b29-ab05be9fe519.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/042de643-01de-4776-8b29-ab05be9fe519.png)'
- en: 'You can also try out the APIs manually by running the following commands:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以通过运行以下命令手动测试API：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Expect the requested product ID, `2`, in the response.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 期望在响应中收到请求的产品ID，`2`。
- en: With the microservices deployed, we can move on and deploy Elasticsearch and
    Kibana!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务部署完成后，我们可以继续部署Elasticsearch和Kibana！
- en: Deploying Elasticsearch and Kibana
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署Elasticsearch和Kibana
- en: 'We will deploy Elasticsearch and Kibana to its own namespace, `logging`. Both
    Elasticsearch and Kibana will be deployed for development and test usage using
    a Kubernetes deployment object. This will be done with a single pod and a Kubernetes
    node port service. The services will expose the standard ports for Elasticsearch
    and Kibana internally in the Kubernetes cluster, that is, port `9200` for Elasticserach
    and port `5601` for Kibana. Thanks to the `minikube tunnel` command, we will be
    able to access these services locally using the following URLs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Elasticsearch 和 Kibana 部署到其自己的命名空间 `logging`。Elasticsearch 和 Kibana 将使用
    Kubernetes 部署对象部署用于开发和测试。这将通过单个 pod 和 Kubernetes 节点端口服务完成。服务将内部在 Kubernetes 集群中暴露
    Elasticsearch 和 Kibana 的标准端口，即 Elasticsearch 的端口 `9200` 和 Kibana 的端口 `5601`。多亏了
    `minikube tunnel` 命令，我们将能够使用以下 URL 本地访问这些服务：
- en: '`elasticsearch.logging.svc.cluster.local:9200` for Elasticserch'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`elasticsearch.logging.svc.cluster.local:9200` 对于 Elasticsearch。'
- en: '`kibana.logging.svc.cluster.local:5601` for Kibana'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kibana.logging.svc.cluster.local:5601` 对于 Kibana。'
- en: For the recommended deployment in a production environment on Kubernetes, see [https://www.elastic.co/elasticsearch-kubernetes](https://www.elastic.co/elasticsearch-kubernetes).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看在 Kubernetes 生产环境中推荐部署，请参阅 [https://www.elastic.co/elasticsearch-kubernetes](https://www.elastic.co/elasticsearch-kubernetes)。
- en: 'We will use the versions that were available when this chapter was written:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在本章写作时可用的版本：
- en: Elasticsearch version 7.3.0
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch 版本 7.3.0
- en: Kibana version 7.3.0
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kibana 版本 7.3.0
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行部署之前，让我们看看定义文件中最有趣的部分。
- en: A walkthrough of the definition files
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义文件的逐步讲解
- en: 'The definition file for Elasticsearch, `kubernetes/efk/elasticsearch.yml`,
    contains a standard Kubernetes deployment and service object that we have seen
    multiples times before, for example, in [Chapter 15](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml),
    *Introduction to Kubernetes*, in the *Trying out a sample deployment* section.
    The most interesting part, as we explained previously, of the definition file
    is the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch** 的定义文件 `kubernetes/efk/elasticsearch.yml` 包含了一个标准的 Kubernetes
    部署和服务对象，我们在之前见过多次，例如在 [第 15 章](87949e5b-2761-4dc1-a70c-d9d21f03d530.xhtml) *Kubernetes
    简介* 的 *尝试样本部署* 部分。正如我们之前解释的，定义文件最有趣的部分如下：'
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: We use an official Docker image from Elastic that's available at `docker.elastic.co`
    with a package that only contains open source components. This is ensured by using
    the `-oss` suffix on the name of the Docker image, `elasticsearch-oss`. The version
    is set to `7.3.0`.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用来自 Elastic 的官方 Docker 镜像，可在 `docker.elastic.co` 获得，以及只包含开源组件的包。这通过在 Docker
    镜像名称中使用 `-oss` 后缀来保证，即 `elasticsearch-oss`。版本设置为 `7.3.0`。
- en: The Elasticsearch container is allowed to allocate a relatively large amount
    of memory—2 GB—to be able to perform queries with good performance. The more memory,
    the better the performance.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch 容器被允许分配相对较多的内存 - 2 GB，以能够良好地执行查询。内存越多，性能越好。
- en: 'The definition file for Kibana, `kubernetes/efk/kibana.yml`, also contains
    a standard Kubernetes deployment and service object. The most interesting parts
    in the definition file are as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kibana** 的定义文件 `kubernetes/efk/kibana.yml` 也包含了一个标准的 Kubernetes 部署和服务对象。定义文件中最有趣的部分如下：'
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: For Kibana, we also use an official Docker image from Elastic that's available
    at `docker.elastic.co`, along with a package that only contains open source components, `kibana-oss`. The
    version is set to `7.3.0`.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Kibana，我们还使用来自 Elastic 的官方 Docker 镜像，可在 `docker.elastic.co` 获得，以及只包含开源组件的包
    `kibana-oss`。版本设置为 `7.3.0`。
- en: To connect Kibana with the Elasticsearch pod, an environment variable, `ELASTICSEARCH_URL`,
    is defined to specify the address to the Elasticsearch service, `http://elasticsearch:9200`.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将 Kibana 与 Elasticsearch pod 连接，定义了一个环境变量 `ELASTICSEARCH_URL`，以指定 Elasticsearch
    服务的地址，`http://elasticsearch:9200`。
- en: With these insights, we are ready to perform the deployment of Elasticsearch
    and Kibana.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些洞察，我们准备执行 Elasticsearch 和 Kibana 的部署。
- en: Running the deploy commands
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行部署命令
- en: 'Deploy Elasticsearch and Kibana by performing the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下步骤部署 Elasticsearch 和 Kibana：
- en: 'Create a namespace for Elasticsearch and Kibana with the following command:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令为 Elasticsearch 和 Kibana 创建一个命名空间：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To make the deploy steps run faster, prefetch the Docker images for Elasticsearch
    and Kibana with the following commands:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使部署步骤运行得更快，使用以下命令预取 Elasticsearch 和 Kibana 的 Docker 镜像：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Deploy Elasticsearch and wait for its pod to be ready with the following commands:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署 Elasticsearch 并等待其 pod 准备好：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify that Elasticsearch is up and running with the following command:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证 Elasticsearch 是否正在运行：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Expect `You Know, for Search` as a response.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 期待响应为“You Know, for Search”。
- en: Depending on your hardware, you might need to wait for a minute or two before
    Elasticsearch responds with this message.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的硬件，您可能需要等待一两分钟，直到 Elasticsearch 回应此消息。
- en: 'Deploy Kibana and wait for its pod to be ready with the following commands:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令部署 Kibana 并等待其 pod 准备好：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Verify that Kibana is up and running with the following command:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证 Kibana 是否正在运行：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Expect `200` as the response.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 期待响应为 `200`。
- en: With Elasticsearch and Kibana deployed, we can start to deploy Fluentd.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署 Elasticsearch 和 Kibana 之后，我们可以开始部署 Fluentd。
- en: Deploying Fluentd
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Fluentd
- en: Deploying Fluentd is a bit more complex compared to deploying Elasticsearch
    and Kibana. To deploy Fluentd, we will use a Docker image that's been published
    by the Fluentd project on Docker Hub, `fluent/fluentd-kubernetes-daemonset`, and
    sample the Kubernetes definition files from a Fluentd project on GitHub, `fluentd-kubernetes-daemonset`.
    It is located at [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
    As it's implied by the name of the project, Fluentd will be deployed as a daemon
    set, running one pod per node in the Kubernetes cluster. Each Fluentd pod is responsible
    for collecting log output from processes and containers that run on the same node
    as the pod. Since we are using Minikube, that is, a single node cluster, we will
    only have one Fluentd pod.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与部署 Elasticsearch 和 Kibana 相比，部署 Fluentd 稍微复杂一些。为了部署 Fluentd，我们将使用 Fluentd 项目在
    Docker Hub 上发布的 Docker 镜像，`fluent/fluentd-kubernetes-daemonset`，并从 GitHub 上的 Fluentd
    项目中采样 Kubernetes 定义文件，`fluentd-kubernetes-daemonset`。它位于 [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset)。正如项目名称所暗示的，Fluentd
    将作为 daemon set 部署，在 Kubernetes 集群中的每个节点运行一个 pod。每个 Fluentd pod 负责收集运行在同一节点上的进程和容器的日志输出。由于我们使用的是
    Minikube，即单节点集群，所以我们只有一个 Fluentd pod。
- en: To handle multiline log records that contain stack traces from exceptions, we
    will use a third-party Fluentd plugin provided by Google, `fluent-plugin-detect-exceptions`,
    which is available at [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions). To
    be able to use this plugin, we will build our own Docker image where the `fluent-plugin-detect-exceptions` plugin
    will be installed. Fluentd's Docker image, `fluentd-kubernetes-daemonset`, will
    be used as the base image.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理包含异常堆栈跟踪的多行日志记录，我们将使用 Google 提供的一个第三方 Fluentd 插件，`fluent-plugin-detect-exceptions`，该插件可在
    [https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions](https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions)
    获得。为了能够使用这个插件，我们将构建我们自己的 Docker 镜像，其中将安装 `fluent-plugin-detect-exceptions` 插件。将使用
    Fluentd 的 Docker 镜像 `fluentd-kubernetes-daemonset` 作为基础镜像。
- en: 'We will use the versions that were available when this chapter was written:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在本章编写时可用的版本：
- en: Fluentd version 1.4.2
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 版本 1.4.2
- en: fluent-plugin-detect-exceptions version 0.0.12
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fluent-plugin-detect-exceptions 版本 0.0.12
- en: Before we perform the deployments, let's look at the most interesting parts
    of the definition files.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行部署之前，让我们看看定义文件的最有趣的部分。
- en: Discovering the log records from microservices
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从微服务中发现日志记录
- en: In this section, we will learn how to utilize one of the main features of centralized
    logging, that is, finding log records from our microservices. We will also learn
    how to use the trace ID in the log records to find log records from other microservices
    that belong to one and the same process, for example, a request to the API.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何利用集中日志的一个主要功能，那就是从我们的微服务中找到日志记录。我们还将学习如何在日志记录中使用 trace ID 来查找属于同一进程的其他微服务的日志记录，例如
    API 的请求。
- en: Let's start by creating some log records that we can look up with the help of
    Kibana. We will use the API to create a product with a unique product ID and then
    retrieve information about the product. After that, we can try to find the log
    records that were created when retrieving the product information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过API创建一些我们可以用Kibana查找的日志记录。我们将使用API创建一个具有唯一产品ID的产品，然后检索有关产品的信息。之后，我们可以尝试找到在检索产品信息时创建的日志记录。
- en: 'The creation of log records in the microservices has updated a bit from the
    previous chapter so that the product composite and the three core microservices,
    `product`, `recommendation`, and `review`, all write a log record with the log
    level set to `INFO` when they begin processing a get request. Let''s go over the
    source code that''s been added to each microservice:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一章相比，微服务中的日志记录创建稍有更新，以便产品组合和三个核心微服务（`product`、`recommendation`和`review`）在处理get请求时都设置日志级别为`INFO`。让我们来看看为每个微服务添加的源代码：
- en: 'Product composite microservice log creation:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品组合微服务日志创建：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Product microservice log creation:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品微服务日志创建：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Recommendation microservice log creation:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐微服务日志创建：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Review microservice log creation:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论微服务日志创建：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For more details, see the source code in the `microservices` folder.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅`microservices`文件夹中的源代码。
- en: 'Perform the following steps to use the API to create log records and then use
    Kibana to look up the log records:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤使用API创建日志记录，然后使用Kibana查找日志记录：
- en: 'Get an access token with the following command:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取访问令牌：
- en: '[PRE25]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As mentioned in the introduction to this section we will start by creating
    a product with a unique product ID. Create a minimalistic product (without recommendations
    and reviews) for `"productId" :1234` by executing the following command:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本节介绍中所述，我们将首先通过API创建具有唯一产品ID的产品。为`"productId" :1234`创建一个简约的产品（不包含推荐和评论）通过执行以下命令：
- en: '[PRE26]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Read the product with the following command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令读取产品：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Expect a response similar to the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 期待如下响应：
- en: '![](img/258fd470-f7ae-432b-933e-0098656e7199.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/258fd470-f7ae-432b-933e-0098656e7199.png)'
- en: Hopefully, we got some log records created by these API calls. Let's jump over
    to Kibana and find out!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些API调用创建了一些日志记录。让我们跳到Kibana去看看！
- en: 'On the Kibana web page, click on the `Discover` menu on the left. You will
    see something like the following:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kibana网页上，点击左侧的`Discover`菜单。你会看到类似以下内容：
- en: '![](img/75f6dd32-2743-454f-a301-007444bb2cfd.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75f6dd32-2743-454f-a301-007444bb2cfd.png)'
- en: On the left-top corner, we can see that Kibaba has found 326,642 log records.
    The time picker shows that they are from the last 7 days. In the histogram, we
    can see how the log records are spread out over time. Following that is a table showing
    the most recent log events that were found by the query.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在左上角，我们可以看到Kibaba找到了326,642条日志记录。时间选择器显示它们来自过去7天。在直方图中，我们可以看到日志记录随时间如何分布。之后是一个表，显示查询找到的最新日志事件。
- en: If you want to change the time interval, you can use the time picker. Click
    on its calendar icon to adjust the time interval.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想更改时间间隔，可以使用时间选择器。点击其日历图标调整时间间隔。
- en: 'To get a better view of the content in the log records, add some fields from
    the log records to the table under the histogram. Select the fields from the list
    of available fields to the left. Scroll down until the field is found. Hold the
    cursor over the field and an add button will appear; click on it to add the field
    as a column in the table. Select the following fields, in order:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了更好地查看日志记录中的内容，请将日志记录中的某些字段添加到直方图下的表中。从左侧的可用字段列表中选择字段。滚动到底部直到找到字段。将光标悬停在字段上，会出现一个添加按钮；点击它将字段作为列添加到表中。选择以下字段，按顺序：
- en: spring.level, the log level
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: spring.level，日志级别
- en: kubernetes.container_name, the name of the container
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: kubernetes.container_name，容器名称
- en: spring.trace, the trace ID used for distributed tracing
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: spring.trace，用于分布式跟踪的跟踪ID
- en: 'log, the actual log message. The web page should look something similar to
    the following:'
  id: totrans-208
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: log，实际日志消息。网页应看起来与以下类似：
- en: '![](img/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e42ce2ba-2d4c-4436-95d7-cde3621c572f.png)'
- en: The table now contains information that is of interest regarding the log records!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表格现在包含了有关日志记录的信息！
- en: 'To find log records from the call to the `GET` API, we can ask Kibana to find
    log records where the log field contains the text product.id=1234\. This matches
    the log output from the product composite microservice that was shown previously.This
    can be done by entering `log:"product.id=1234"` in the Search field and clicking
    on the Update button (this button can also be labeled Refresh). Expect one log
    record to be found:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找来自`GET` API调用的日志记录，我们可以请Kibana查找日志字段包含product.id=1234的日志记录。这匹配了前面显示的产品组合微服务的日志输出。这可以通过在搜索字段中输入`log:"product.id=1234"`并点击更新按钮（这个按钮也可以标记为刷新）来完成。预期会找到一条日志记录：
- en: '![](img/c5370730-05a8-4dc0-a414-3020057d3bb6.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5370730-05a8-4dc0-a414-3020057d3bb6.png)'
- en: Verify that the timestamp is from when you called the `GET` API and verify that
    the name of the container that created the log record is comp, that is, verify
    that the log record was sent by the product composite microservice.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证时间戳是否来自您调用`GET` API的时间，并验证创建日志记录的容器名称是`comp`，即验证日志记录是由产品组合微服务发送的。
- en: Now, we want to see the related log records from the other microservices that
    participated in the process of returning information about the product with productId
    1234, that is, finding log records with the same trace ID as that of the log record
    we found. To do that, place the cursor over the `spring.trace` field for the log
    record. Two small magnifying glasses will be shown to the right of the field,
    one with a `+` sign and one with a `-` sign. Click on the magnifying glass with
    the `+` sign to filter on the trace ID.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们想要查看其他参与返回产品ID 1234信息过程的微服务的相关日志记录，即查找具有与找到的日志记录相同的跟踪ID的日志记录。为此，将光标悬停在日志记录的`spring.trace`字段上。字段右侧将显示两个小放大镜图标，一个带加号，一个带减号。点击带有加号的放大镜图标以根据跟踪ID进行过滤。
- en: 'Clean the Search field so that the only search criteria is the filter of the
    trace field. Then, click on the Update button to see the result. Expect a response
    similar to the following:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清空搜索字段，以便唯一的搜索条件是跟踪字段的过滤。然后，点击更新按钮以查看结果。预期会有类似于下面的响应：
- en: '![](img/909125b4-7868-4ed9-b33c-3a5197408304.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/909125b4-7868-4ed9-b33c-3a5197408304.png)'
- en: We can see a lot of detailed debug and trace messages that clutter the view;
    let's get rid of them!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到很多详细的debug和trace消息，让视图变得杂乱，让我们去掉它们！
- en: Place the cursor over a TRACE value and click on the magnifying glass with the
    - sign to filter out log records with the log level set to TRACE.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将光标悬停在TRACE值上，然后点击带有减号的放大镜图标以过滤出日志级别设置为TRACE的日志记录。
- en: Repeat the preceding step for the DEBUG log record.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复上述步骤以处理DEBUG日志记录。
- en: 'We should now be able to see the four expected log records, one for each microservice
    involved in the lookup of product information for the product with product ID
    1234:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们应该能够看到四个预期的日志记录，每个记录都涉及查找产品信息的产品ID 1234的每个微服务：
- en: '![](img/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/437104c3-c1c1-463a-a9d9-9fe2e57f6461.png)'
- en: Also, note the filters that were applied included the trace ID but excluded
    log records with the log level set to DEBUG or TRACE.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意应用的过滤器包括跟踪ID，但不包括日志级别设置为DEBUG或TRACE的日志记录。
- en: Now that we know how to find the expected log records, we are ready to take
    the next step. This will be to learn how to find unexpected log records, that
    is, error messages, and how to perform root cause analysis, that is, find the
    reason for these error messages.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道如何查找预期的日志记录，我们就可以进行下一步了。这将是要学习如何查找意外的日志记录，即错误消息，以及如何进行根本原因分析，即找到这些错误消息的原因。
- en: A walkthrough of the definition files
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义文件的概览
- en: 'The Dockerfile that''s used to build the Docker image, `kubernetes/efk/Dockerfile`,
    looks as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建Docker镜像的Dockerfile，`kubernetes/efk/Dockerfile`，如下所示：
- en: '[PRE28]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释前面的源代码：
- en: The base image is Fluentd's Docker image, `fluentd-kubernetes-daemonset`. The `v1.4.2-debian-elasticsearch-1.1` tag
    specifies that version v1.4.2 shall be used with a package that contains built-in
    support for sending log records to Elasticsearch. The base Docker image contains
    the Fluentd configuration files that were mentioned in the *Configuring Fluentd* section.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础镜像使用的是Fluentd的Docker镜像，`fluentd-kubernetes-daemonset`。标签`v1.4.2-debian-elasticsearch-1.1`指定了应使用版本v1.4.2，并且包含发送日志记录到Elasticsearch的内置支持的包。基础Docker镜像包含了在*配置Fluentd*部分提到的Fluentd配置文件。
- en: The Google plugin, `fluent-plugin-detect-exceptions`, is installed using Ruby's
    package manager, `gem`.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ruby的包管理器`gem`安装Google插件`fluent-plugin-detect-exceptions`。
- en: 'The definition file of the daemon set, `kubernetes/efk/fluentd-ds.yml`, is
    based on a sample definition file in the `fluentd-kubernetes-daemonset` project,
    which can be found at [https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml).
    This file is a bit complex, so let''s go through the most interesting parts separately:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 守护进程集的定义文件`kubernetes/efk/fluentd-ds.yml`基于`fluentd-kubernetes-daemonset`项目中的一个示例定义文件，该文件可在此处找到[https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml](https://github.com/fluent/fluentd-kubernetes-daemonset/blob/master/fluentd-daemonset-elasticsearch.yaml)。这个文件有点复杂，所以让我们分别查看最有趣的部分：
- en: 'First, here''s the declaration of the daemon set:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，以下是守护进程集的声明：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释一下前面的源代码：
- en: The `kind` key specifies that this is a daemon set.
  id: totrans-234
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kind`键指定了这是一个守护进程集。'
- en: The `namespace` key specifies that the daemon set shall be created in the `kube-system`
    namespace and not in the `logging` namespace where Elasticseach and Kibana are
    deployed.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`namespace`键指定守护进程集应创建于`kube-system`命名空间中，而不是部署Elasticsearch和Kibana的`logging`命名空间。'
- en: 'The next part specifies the template for the pods that are created by the daemon
    set. The most interesting parts are as follows:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一部分指定了由守护进程集创建的Pod模板。最有趣的部分如下：
- en: '[PRE30]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释一下前面的源代码：
- en: The Docker image that's used for the pods is `hands-on/fluentd:v1`. We will
    build this Docker image after walking through the definition files using the Dockerfile
    we described previously.
  id: totrans-239
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于Pod的Docker镜像为`hands-on/fluentd:v1`。我们在之前描述的Dockerfile中走过定义文件后，将构建这个Docker镜像。
- en: 'A number of environment variables are supported by the Docker image and are
    used to customize it. The two most important ones are as follows:'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker镜像支持许多环境变量，用于自定义它。其中最重要的两个如下：
- en: '`FLUENT_ELASTICSEARCH_HOST`, which specifies the hostname of the Elasticsearch
    service, that is, `elasticsearch.logging`'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FLUENT_ELASTICSEARCH_HOST`，指定Elasticsearch服务的主机名，即`elasticsearch.logging`'
- en: '`FLUENT_ELASTICSEARCH_PORT`, which specifies the port that''s used to communicate
    with Elasticsearch, that is, `9200`'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FLUENT_ELASTICSEARCH_PORT`，指定与Elasticsearch通信的端口，即`9200`'
- en: Since the Fluentd pod runs in another namespace than Elasticsearch, the hostname
    cannot be specified using its short name, that is, `elasticsearch`. Instead, the
    namespace part of the DNS name must also be specified, that is, `elasticsearch.logging`. As
    an alternative, the **fully qualified domain name** (**FQDN**), `elasticsearch.logging.svc.cluster.local`,
    can also be used. But since the last part of the DNS name, `svc.cluster.local`,
    is shared by all DNS names inside a Kubernetes cluster, it does not need to be
    specified.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Fluentd Pod在Elasticsearch之外的命名空间中运行，不能使用其短名称（即`elasticsearch`）来指定主机名。相反，DNS名称的命名空间部分也必须指定，即`elasticsearch.logging`。作为替代方案，可以使用**完全合格域名**（**FQDN**），`elasticsearch.logging.svc.cluster.local`。但由于DNS名称的最后一部分，`svc.cluster.local`，在Kubernetes集群内的所有DNS名称中是共享的，因此不需要指定。
- en: 'Finally, a number of volumes, that is, filesystems, are mapped into the pod,
    as follows:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，有一系列卷，即文件系统，被映射到Pod中，如下所示：
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s explain the preceding source code in detail:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释一下前面的源代码：
- en: Three folders on the host (that is, the node) are mapped into the Fluentd pod.
    These folders contain the log files that Fluentd will tail and collect log records
    from. The folders are: `/var/log`, `/var/lib/docker/containers` and `/run/log/journal`.
  id: totrans-247
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宿主机（即节点）上的三个文件夹被映射到Fluentd Pod中。这些文件夹包含Fluentd将跟踪和收集日志记录的日志文件。这些文件夹是：`/var/log`、`/var/lib/docker/containers`和`/run/log/journal`。
- en: Our own configuration file that specifies how Fluentd shall process log records
    from our microservices is mapped using a config map called `fluentd-hands-on-config`
    to the `/fluentd/etc/conf.d` folder. The base Docker image that's used for preceding
    Fluentd, `fluentd-kubernetes-daemonset`, configures Fluentd to include any configuration
    file that's found in the `/fluentd/etc/conf.d` folder. See the *Configuring Fluentd* section
    for details.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们自己的配置文件，指定Fluentd如何处理来自我们微服务的日志记录，通过一个名为`fluentd-hands-on-config`的config map映射到`/fluentd/etc/conf.d`文件夹。之前使用的Fluentd的Docker镜像`fluentd-kubernetes-daemonset`，配置Fluentd包括在`/fluentd/etc/conf.d`文件夹中找到的任何配置文件。有关详细信息，请参见*配置Fluentd*部分。
- en: For the full source code of the definition file for the daemon set, see the
    `kubernetes/efk/fluentd-ds.yml` file.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 关于daemon set定义文件的完整源代码，请参阅`kubernetes/efk/fluentd-ds.yml`文件。
- en: Now that we've walked through everything, we are ready to perform the deployment
    of Fluentd.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了所有内容，我们就可以准备部署Fluentd了。
- en: Running the deploy commands
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行部署命令
- en: 'To deploy Fluentd, we have to build the Docker image, create the config map,
    and finally deploy the daemon set. Run the following commands to perform these
    steps:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署Fluentd，我们必须构建Docker镜像，创建config map，最后部署daemon set。运行以下命令以执行这些步骤：
- en: 'Build the Docker image and tag it with `hands-on/fluentd:v1` using the following
    command:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令构建Docker镜像并标记为`hands-on/fluentd:v1`：
- en: '[PRE32]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create the config map, deploy Fluentd''s daemon set, and wait for the pod to
    be ready with the following commands:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令创建config map，部署Fluentd的daemon set，并等待pod就绪：
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Verify that the Fluentd pod is healthy with the following command:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证Fluentd pod是否正常：
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Expect a response of `2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is
    now running worker=0`.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '期望的响应是`2019-08-16 15:11:33 +0000 [info]: #0 fluentd worker is now running worker=0`。'
- en: 'Fluentd will start to collect a considerable amount of log records from the
    various processes and containers in the Minkube instance. After a minute or so,
    you can ask Elasticsearch how many log records have been collected with the following
    command:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Fluentd将开始从Minkube实例中的各种进程和容器收集大量的日志记录。大约一分钟后，您可以使用以下命令询问Elasticsearch已经收集了多少日志记录：
- en: '[PRE35]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The command can be a bit slow the first time it is executed, but should return
    a response similar to the following:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令首次执行时可能会有点慢，但应该返回类似于以下的响应：
- en: '![](img/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14a27b89-c4e5-4d29-86c2-4652b7fbe575.png)'
- en: In this example, Elasticsearch contains `144750` log records.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，Elasticsearch包含`144750`条日志记录。
- en: This completes the deployment of the EFK stack. Now, it's time to try it out
    and find out what all of the collected log records are about!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了EFK堆栈的部署。现在，是时候尝试它并找出收集的所有日志记录都关于什么了！
- en: Trying out the EFK stack
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试EFK堆栈
- en: 'The first thing we need to do before we can try out the EFK stack is initialize
    Kibana so it knows what search indices to use in Elasticsearch. Once that is done,
    we will try out the following, in my experience, common tasks:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以尝试EFK堆栈之前，我们需要做的第一件事是初始化Kibana，这样它就知道在Elasticsearch中使用哪些搜索索引。一旦完成，我们将尝试以下，根据我的经验，常见的任务：
- en: We will start by analyzing of what types of log records Fluentd has collected
    and stored in Elasticsearch. Kibana has a very useful visualization capability
    that can be used for this.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先分析Fluentd已经收集并在Elasticsearch中存储了哪些类型的日志记录。Kibana具有非常实用的可视化功能，可用于此目的。
- en: Next, we will learn how to discover log records from different microservices
    that belong to one and the same processing of an external request to the API. We
    will use the **trace ID** in the log records as a correlation ID to find related
    log records.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何发现属于同一外部API请求处理的不同微服务的日志记录。我们将使用日志记录中的**追踪ID**作为相关日志记录的关联ID来找到相关的日志记录。
- en: Thirdly, we will learn how to use Kibana to perform **root cause analysis**, that
    is, find the actual reason for an error.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，我们将学习如何使用Kibana进行**根本原因分析**，也就是说，找到错误的实际原因。
- en: Initializing Kibana
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化Kibana
- en: Before we start to use Kibana, we must specify what search indices to use in
    Elasticsearch and what field in the indices holds the timestamps for the log records.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用Kibana之前，我们必须指定在Elasticsearch中使用哪些搜索索引以及索引中哪个字段持有日志记录的时间戳。
- en: 'Perform the following steps to initialize Kibana:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤初始化Kibana：
- en: Open Kibana's web UI using the `http://kibana.logging.svc.cluster.local:5601` URL
    in a web browser.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在web浏览器中使用`http://kibana.logging.svc.cluster.local:5601` URL打开Kibana的web UI。
- en: On the welcome page, Welcome to Kibana, click on the Explore on my own button.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在欢迎页面上，点击“自行探索”按钮。
- en: Click on the Expand button in the lower-left corner to view the names of the
    menu choices. These will be shown on the left-hand side.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左下角的展开按钮以查看菜单选择的名称。这些将在左侧显示。
- en: Click on Discover in the menu to the left. You will be asked to define a pattern
    that's used by Kibana to identify what Elasticsearch indices it shall retrieve
    log records from.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左侧菜单中的发现。你将被要求定义一个模式，Kibana用它来确定应从哪个Elasticsearch索引中检索日志记录。
- en: Enter the `logstash-*` index pattern and click on Next Step.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入`logstash-*`索引模式，然后点击下一步。
- en: On the next page, you will be asked to specify the name of the field that contains
    the timestamp for the log records. Click on the drop-down list for the Time Filter
    field name and select the only available field, @timestamp.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页上，你将被要求指定包含日志记录时间戳的字段名称。点击时间过滤字段名称的下拉列表，选择唯一可用的字段，即`@timestamp`。
- en: Click on the Create index pattern button.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击创建索引模式按钮。
- en: Kibana will show a page that summarizes the fields that are available in the
    selected indices.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kibana将显示一个页面，总结在选定的索引中可用的字段。
- en: Indices are, by default, named `logstash` for historical reasons, even though
    it is Flutentd that is used for log collection.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，索引以`logstash`命名，这是出于历史原因，尽管用于日志收集的是Flutentd。
- en: With Kibana initialized, we are ready to examine the log records we have collected.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化Kibana后，我们准备检查我们收集的日志记录。
- en: Analyzing the log records
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析日志记录
- en: From the deployment of Fluentd, we know that it immediately started to collect
    a significant number of log records. So, the first thing we need to do is get
    an understanding of what types of log records Fluentd has collected and stored
    in Elasticsearch.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从Fluentd的部署来看，我们知道它立即开始收集大量的日志记录。因此，我们首先需要了解Fluentd已经收集并存储在Elasticsearch中的日志记录的类型。
- en: 'We will use Kibana''s visualization feature to divide the log records per Kubernetes
    namespace and then ask Kibana to show us how the log records are divided per type
    of container within each namespace. A pie chart is a suitable chart type for this
    type of analysis. Perform the following steps to create a pie chart:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Kibana的可视化功能，按Kubernetes命名空间分割日志记录，然后要求Kibana显示每个命名空间内按容器类型分割的日志记录。饼图是这种分析的合适图表类型。按照以下步骤创建饼图：
- en: In Kibana's web UI, click on Visualize in the menu to the left.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Kibana的Web UI中，点击左侧菜单中的可视化。
- en: Click on the Create new visualization button.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击创建新可视化按钮。
- en: Select Pie as the visualization type.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择饼图作为可视化类型。
- en: Select logstash-* as the source.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`logstash-*`作为源。
- en: In the time picker (a date interval selector) above the pie chart, set a date
    interval of your choice (set to the last 7 days in the following screenshot).
    Click on its calendar icon to adjust the time interval.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在饼图上方的日期选择器（日期间隔选择器）中，设置一个你喜欢的日期间隔（以下屏幕截图设置为最后7天）。点击其日历图标调整时间间隔。
- en: 'Click on Add to create the first bucket, as follows:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击添加以创建第一个桶，如下所示：
- en: Select the bucket type, that is, Split slices.
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即分割切片。
- en: For the aggregation type, select Terms from the drop-down list.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于聚合类型，从下拉列表中选择项。
- en: As the field, select kubernetes.namespace_name.keyword.
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择`kubernetes.namespace_name.keyword`。
- en: For the size, select 10.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小，选择10。
- en: Enable Group other values in separate bucket.
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用将其他值分组到单独的桶中。
- en: Enable Show missing values.
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用显示缺失值。
- en: 'Press the Apply changes button (the blue play icon above the Bucket definition).
    Expect a pie chart that looks similar to the following:'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下应用更改按钮（Bucket定义上方蓝色播放图标）。期待一个类似于以下的饼图：
- en: '![](img/1d79e158-3af1-495b-8415-e2b769b59454.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d79e158-3af1-495b-8415-e2b769b59454.png)'
- en: We can see that the log records are divided over the namespaces we have been
    working with in the previous chapters: `kube-system`, `istio-system`, `logging`, `cert-manager`,
    and our own `hands-on` namespace. To see what containers have created the log
    records divided per namespace, we need to create a second bucket.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，日志记录已经分布在我们在前几章中工作的命名空间上：`kube-system`、`istio-system`、`logging`、`cert-manager`以及我们自己的`hands-on`命名空间。为了查看按命名空间分割的日志记录是由哪些容器创建的，我们需要创建第二个桶。
- en: 'Click on Add again to create a second bucket:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击再次添加以创建第二个桶：
- en: Select the bucket type, that is, Split slices.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即分割切片。
- en: As the sub-aggregation type, select Terms from the drop-down list.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于子聚合类型，从下拉列表中选择项。
- en: As the field, select kubernetes.container_name.keyword.
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择`kubernetes.container_name.keyword`。
- en: For the size, select 10.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于大小，选择“10”。
- en: Enable Group other values in separate bucket.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用“将其他值分组到单独的桶中”。
- en: Enable Show missing values.
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用“显示缺失值”。
- en: 'Press the Apply changes button again. Expect a pie chart that looks similar
    to the following:'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次点击“应用更改”按钮。预期会出现类似下面的饼图：
- en: '![](img/660ceb82-205e-401d-8595-5a23cc328e26.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/660ceb82-205e-401d-8595-5a23cc328e26.png)'
- en: Here, we can find the log records from our microservices. Most of the log records
    come from the `product-composite` microservice.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以找到来自我们微服务的日志记录。大多数日志记录来自`product-composite`微服务。
- en: At the top of the pie chart, we have a group of log records labeled `missing`,
    that is, they neither have a Kubernetes namespace nor a container name specified.
    What's behind these missing log records? These log records come from processes
    running outside of the Kubernetes cluster in the Minikube instance and they are
    stored using Syslog. They can be analyzed using Syslog-specific fields, specifically
    the *identifier field.* Let's create a third bucket that divides log records based
    on their Syslog identifier field, if any.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在饼图的顶部，有一组被标记为`missing`的日志记录，即它们既没有Kubernetes命名空间也没有容器名称指定。这些缺失的日志记录背后有什么？这些日志记录来自在Minikube实例外的Kubernetes集群中运行的进程，并且它们是使用Syslog存储的。它们可以使用Syslog特定的字段进行分析，特别是*标识符字段*。让我们创建一个第三个桶，根据它们的Syslog标识符字段（如果有）来划分日志记录。
- en: 'Click on `Add` again to create a third bucket:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“添加”再次创建一个第三个桶：
- en: Select the bucket type, that is, Split slices.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择桶类型，即“分割切片”。
- en: As the sub-aggregation type, select Terms from the drop-down list.
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为子聚合类型，从下拉列表中选择“Terms”。
- en: As the field, select SYSLOG_IDENTIFIER.keyword.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为字段，选择`SYSLOG_IDENTIFIER.keyword`。
- en: Enable Group other values in separate bucket.
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用“将其他值分组到单独的桶中”。
- en: Enable Show missing values.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用“显示缺失值”。
- en: 'Press the Apply changes button and expect a pie chart that looks similar to
    the following:'
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“应用更改”按钮，预期会出现类似下面的饼图：
- en: '![](img/56a1d5be-61bf-49e6-87c2-a18290f65e38.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56a1d5be-61bf-49e6-87c2-a18290f65e38.png)'
- en: The `missing` log records turn out to come from the `kubelet` process, which
    manages the node from a Kubernetes perspective, and `dockerd`, the Docker daemon
    that manages all of the containers.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`missing`日志记录最终来自`kubelet`进程，该进程从Kubernetes的角度管理节点，以及`dockerd`，管理所有容器的Docker守护进程。'
- en: Now that we have found out where the log records come from, we can start to
    locate the actual log records from our microservices.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经找到了日志记录的来源，我们就可以开始从我们的微服务中定位实际的日志记录了。
- en: Performing root cause analyses
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行根本原因分析
- en: One of the most important features of centralized logging is that it makes it
    possible to analyze errors using log records from many sources and, based on that,
    perform root cause analysis, that is, find the actual reason for the error message.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 集中日志的最重要特性之一是，它使得使用来自许多源的日志记录来分析错误成为可能，并且基于此执行根本原因分析，即找到错误消息的实际原因。
- en: 'In this section, we will simulate an error and see how we can find information
    about it, all of the way down to the line of source code that caused the error
    in one of the microservices in the system landscape. To simulate an error, we
    will reuse the fault parameter we introduced in [Chapter 13](23795d34-4068-4961-842d-989cde26b642.xhtml),
    *Improving Resilience Using Resilience4j*, in the *Adding programmable delays
    and random errors* section. We can use this to force the product microservice
    to throw an exception. Perform the following steps:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟一个错误，并看看我们如何能够找到有关它的信息，一直找到系统中微服务中的某一个微服务中的源代码行引起了错误。为了模拟错误，我们将重新使用我们在[第13章](23795d34-4068-4961-842d-989cde26b642.xhtml)中引入的故障参数*使用Resilience4j提高弹性*节的*添加可编程延迟和随机错误*。我们可以使用这个来强制产品微服务抛出异常。按照以下步骤进行操作：
- en: 'Run the following command to generate a fault in the product microservice while
    searching for product information on the product with product ID `666`:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令，在搜索具有产品ID `666`的产品信息时，在产品微服务中生成故障：
- en: '[PRE36]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Expect the following error in response:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 预期响应中会出现以下错误：
- en: '![](img/8d0a558f-7fc3-4f83-bb92-320d419859ab.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d0a558f-7fc3-4f83-bb92-320d419859ab.png)'
- en: Now, we have to pretend that we have no clue about the reason for this error!
    Otherwise, the root cause analysis wouldn't be very exciting, right? Let's assume
    that we work in a support organization and have been asked to investigate some
    problems that just occurred while an end user tried to look up information regarding
    a product with product ID `666`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须假装我们对这个错误的原因一无所知！否则，根本原因分析将不会非常令人兴奋，对吧？假设我们工作在支持组织中，并被要求调查一些刚刚在最终用户尝试查找有关产品ID
    `666`的信息时发生的问题。
- en: 'Before we start to analyze the problem, let''s delete the previous search filters
    in the Kibana web UI so that we can start from scratch. For each filter we defined
    in the previous section, click on their close icon (an x) to remove them. After
    all of the filters have been removed, the web page should look similar to the
    following:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始分析问题之前，让我们在Kibana Web UI中删除之前的搜索过滤器，以便我们可以从头开始。对于我们在上一节中定义的每个过滤器，点击它们的关闭图标（一个X）以删除它们。删除所有过滤器后，网页应看起来与以下类似：
- en: '![](img/82e6c766-b5d4-4bf8-900d-081dfff40319.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82e6c766-b5d4-4bf8-900d-081dfff40319.png)'
- en: Start by selecting a time interval that includes the point in time when the
    problem occurred using the time picker. For example, search the last seven days
    if you know that the problem occurred within the last seven days.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用时间选择器选择一个包括问题发生时间的时间间隔。例如，如果您知道问题发生在过去七天之内，就搜索最后七天。
- en: 'Next, search for log records with the log level set to ERROR within this timeframe.
    This can be done by clicking on the spring.level field in the list of selected
    fields. When you click on this field, its most commonly used values will be displayed
    under it. Filter on the ERROR value by clicking on its magnifier, shown with the
    + sign. Kibana will now show log records within the selected time frame with its
    log level set to ERROR, like so:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在此时间范围内搜索日志记录，日志级别设置为ERROR。这可以通过点击列表中的`spring.level`字段来实现。当你点击这个字段时，它最常用的值将显示在其下方。通过点击带有加号标志的放大镜过滤`ERROR`值。现在Kibana将显示在此选定时间范围内，其日志级别设置为ERROR的日志记录，如下所示：
- en: '![](img/70b40e75-4168-4207-90fb-e528fad127f7.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70b40e75-4168-4207-90fb-e528fad127f7.png)'
- en: We can see a number of error messages related to product ID `666`. The top four
    have the same trace ID, so this seems like a trace ID of interest to use for further
    investigation.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到许多与产品ID `666`相关的错误信息。其中前四个具有相同的跟踪ID，因此这似乎是一个值得进一步调查的跟踪ID。
- en: We can also see more error messages below the top four that seem to be related
    to the same error but with different trace IDs. Those are caused by the retry
    mechanism in the product composite microservice, that is, it retries the request
    a couple of times before giving up and returning an error message to the caller.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以看到在顶部四个下面还有更多与同一错误相关的错误信息，但具有不同的跟踪ID。这些是由产品组合微服务中的重试机制引起的，即在放弃并向调用者返回错误消息之前，它将请求重试几次。
- en: Filter on the trace ID of the first log record in the same way we did in the
    previous section.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以与上一节相同的方式过滤第一个日志记录的跟踪ID。
- en: 'Remove the filter of the ERROR log level to be able to see all of the records
    belonging to this trace ID. Expect Kibana to respond with a lot of log records.
    Look to the oldest log record, that is, the one that occurred first, that looks
    suspicious. For example, it may have a WARN or ERROR log level or a strange log
    message. The default sort order is showing the latest log record at the top, so
    scroll down to the end and search backward (you can also change the sort order
    to show the oldest log record first by clicking on the small up/down arrow next
    to the `Time` column header). The WARN log message that says `Bad luck, and error
    occurred` looks like it could be the root cause of the problem. Let''s investigate
    it further:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除`ERROR`日志级别的过滤器，以便能够查看属于此跟踪ID的所有记录。预计Kibana将响应大量的日志记录。查看最古老的日志记录，即最先发生的记录，该记录看起来可疑。例如，它可能具有`WARN`或`ERROR`日志级别或奇怪的日志消息。默认的排序顺序是显示最新的日志记录在顶部，因此向下滚动到底部并向后搜索（您还可以通过点击`Time`列标题旁边的向上/向下箭头小图标来更改排序顺序，以首先显示最古老的日志记录）。显示`Bad
    luck, and error occurred`的`WARN`日志消息看起来可能是问题的根本原因。让我们进一步调查：
- en: '![](img/47ac8d1a-707a-410f-869f-d0923b4392b2.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47ac8d1a-707a-410f-869f-d0923b4392b2.png)'
- en: Once a log record has been found that might be the root cause of the problem,
    it is of great interest to be able to find the nearby stack trace describing where
    exceptions were thrown in the source code**. **Unfortunately, the Fluentd plugin
    we use for collecting multiline exceptions, `fluent-plugin-detect-exceptions`,
    is unable to relate stack traces to the trace ID that was used. Therefore, stack
    traces will not show up in Kibana when we filter on a trace ID. Instead, we can
    use a feature in Kibana for finding surrounding log records that show log records
    that have occurred in near time to a specific log record.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦找到了可能成为问题根源的日志记录，能够找到描述在源代码中异常抛出的附近堆栈跟踪就显得非常重要了**.**不幸的是，我们用于收集多行异常的Fluentd插件`fluent-plugin-detect-exceptions`，无法将堆栈跟踪与使用的跟踪ID关联起来。因此，当我们在跟踪ID上过滤时，堆栈跟踪不会在Kibana中显示。相反，我们可以使用Kibana中的一个功能来查找显示在特定日志记录附近发生的日志记录的周围日志记录。
- en: 'Expand the log record that says bad luck using the arrow to the left of the
    log record. Detailed information about this specific log record will be revealed.
    There is also a link named View surrounding documents; click on it to see nearby
    log records. Expect a web page similar to the following:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用日志记录左侧的箭头展开显示“坏运气”的日志记录。详细信息关于这个具体的日志记录将显示出来。还有一个名为“查看周围文档”的链接；点击它，就能看到附近的日志记录。期待一个类似于以下的网页：
- en: '![](img/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d533fe87-485e-4fe5-95c6-8de6c2ae04c6.png)'
- en: 'The log record above the bad luck log record with the stack trace for the error
    message Something went wrong... looks interesting and was logged by the product
    microservice just two milliseconds after it logged the *bad luck* log record.
    They seem to be related! The stack trace in that log record points to line 96
    in `ProductServiceImpl.java`. Looking in the source code (see `microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`),
    line 96 looks as follows:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示“坏运气”日志记录上面的带有错误消息“Something went wrong...”的堆栈跟踪的日志记录看起来很有趣，并且是由产品微服务在它记录了*坏运气*日志记录的两毫秒后记录的。它们似乎有关联！那个日志记录中的堆栈跟踪指向了`ProductServiceImpl.java`的第96行。查看源代码（见`microservices/product-service/src/main/java/se/magnus/microservices/core/product/services/ProductServiceImpl.java`），第96行看起来如下：
- en: '[PRE37]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is the root cause of the error. We did know this in advance, but now we
    have seen how we can navigate to it as well.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 这是错误的根本原因。我们之前知道这一点，但现在我们也看到了如何导航到它。
- en: 'In this case, the problem is quite simple to resolve: simply omit the `faultPercent`
    parameter in the request to the API. In other cases, the resolution of the root
    cause can be much harder to figure out!'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，问题的解决相当简单：只需在API请求中省略`faultPercent`参数即可。在其他情况下，找出根本原因的解决可能要困难得多！
- en: This concludes this chapter on using the EFK stack for centralized logging.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 本章关于使用EFK堆栈进行集中日志记录的内容就此结束。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the importance of collecting log records from
    microservices in a system landscape into a common centralized database where analysis
    and searches among the stored log records can be performed. We used the EFK stack,
    that is, Elasticsearch, Fluentd, and Kibana, to collect, process, store, analyze,
    and search for log records.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了收集系统景观中微服务的日志记录到一个共同的集中数据库的重要性，在该数据库中可以对存储的日志记录进行分析和搜索。我们使用了EFK堆栈，即Elasticsearch、Fluentd和Kibana，来收集、处理、存储、分析和搜索日志记录。
- en: Fluentd was used to collect log records not only from our microservices but
    also from the various supporting containers and processes in the Kubernetes cluster.
    Elasticsearch was used as a text search engine. Together with Kibana, we saw how
    easy it is to get an understanding of what types of log records we have collected.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅用Fluentd收集微服务的日志记录，还收集了Kubernetes集群中各种支持容器和进程的日志记录。Elasticsearch被用作文本搜索引擎。与Kibana一起，我们了解了识别我们收集了哪些类型的日志记录是多么容易。
- en: We also learned how to use Kibana to perform important tasks such as finding
    related log records from cooperating microservices and how to perform root cause
    analysis, that is, finding the real problem for an error message. Finally, we
    learned how to update the configuration of Fluentd and how to get the change reflected
    by the executing Fluentd pod.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学会了如何使用Kibana执行重要任务，例如查找来自合作微服务的相关日志记录以及如何进行根本原因分析，即找到错误消息的真实问题。最后，我们学会了如何更新Fluentd的配置以及如何让执行的Fluentd
    pod反映出这个更改。
- en: Being able to collect and analyze log records in this way is an important capability
    in a production environment, but these types of activities are always done afterward,
    once the log record has been collected. Another important capability is to be
    able to monitor the current health of the microservices, that is, collect and
    visualize runtime metrics in terms of the use of hardware resources, response
    times, and so on. We touched on this subject in the previous chapter, [Chapter
    18](422649a4-94bc-48ae-b92b-e3894c014962.xhtml), *Using a Service Mesh to Improve
    Observability and Management*, and in the next chapter, [Chapter 20](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml),
    *Monitoring Microservices*, we will learn more about monitoring microservices.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 能够以这种方式收集和分析日志记录是在生产环境中的一项重要能力，但是这类活动总是要在日志记录被收集之后进行。另一项重要能力是能够监控微服务的当前健康状况，也就是说，收集并可视化关于硬件资源使用、响应时间等方面的运行时指标。我们在上一章，[第18章](422649a4-94bc-48ae-b92b-e3894c014962.xhtml)，*使用服务网格提高可观测性和管理*，和下一章，[第20章](5e6cce2d-d426-4f55-95c9-52b596769a57.xhtml)，*监控微服务*，将会了解更多关于监控微服务的内容。
- en: Questions
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: A user searched for ERROR log messages in the `hands-on` namespace for the last
    30 days using the search criteria shown in the following screenshot, but none
    were found. Why?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用户在过去30天里在`hands-on`命名空间中搜索了ERROR日志信息，使用的是下面截图中的搜索条件，但是没有找到任何结果。为什么？
- en: '![](img/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d818708-6286-4a4c-b9f5-1ad9d67581cd.png)'
- en: A user has found a log record of interest. How can the user find related log
    records from this and other microservices, for example, that come from processing
    an external API request?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用户找到了一个感兴趣的日志记录。用户如何从这个以及其他微服务中找到相关日志记录，例如，那些来自处理外部API请求的记录？
- en: '![](img/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/292c8785-1180-40d6-8ad2-2b7e564cc8ef.png)'
- en: A user has found a log record that seems to indicate the root cause of a problem
    that was reported by an end user. How can the user find the stack trace that shows
    wherein the source code the error occurred?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用户找到了一个似乎指示了一个由终端用户报告的问题的根源的日志记录。用户如何找到显示错误发生在源代码中的堆栈跟踪？
- en: '![](img/6e935782-d0e3-46aa-8d97-141af6289a9c.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e935782-d0e3-46aa-8d97-141af6289a9c.png)'
- en: Why doesn't the following Fluentd configuration element work?
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的Fluentd配置元素为什么不起作用？
- en: '[PRE38]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How can you determine whether Elasticsearch is up and running?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你怎么确定Elasticsearch是否正在运行？
- en: Suddenly, you lose connection to Kibana from your web browser. What caused this
    problem?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 突然之间，你从网页浏览器上失去了对Kibana的连接。这个问题是由什么引起的？
