- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Core Architectural Design Patterns
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心架构设计模式
- en: In the previous chapters, we learned how to architect data engineering solutions
    for both batch-based and real-time processing using specific use cases. However,
    we haven’t discussed the various options available concerning architectural design
    patterns for batch and real-time stream processing engines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用特定用例来构建批处理和实时处理的数据工程解决方案。然而，我们还没有讨论有关批处理和实时流处理引擎的架构设计模式的多种选择。
- en: In this chapter, we will learn about a few commonly used architectural patterns
    for data engineering problems. We will start by learning about a few common patterns
    in batch-based data processing and common scenarios where they are used. Then,
    we will learn about various streaming-based processing patterns in modern data
    architectures and how they can help solve business problems. We will also discuss
    the two famous hybrid data architectural patterns. Finally, we will learn about
    various serverless data ingestion patterns commonly used in the cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一些常用的数据工程架构模式，用于解决数据工程问题。我们将从学习基于批处理的常见模式和它们常用的场景开始。然后，我们将学习现代数据架构中基于流的各种处理模式以及它们如何帮助解决业务问题。我们还将讨论两个著名的混合数据架构模式。最后，我们将学习在云中常用的一些无服务器数据摄取模式。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Core batch processing patterns
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心批处理模式
- en: Core stream processing patterns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心流处理模式
- en: Hybrid data processing patterns
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合数据处理模式
- en: Serverless patterns for data ingestion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取的无服务器模式
- en: Core batch processing patterns
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心批处理模式
- en: In this section, we will look at a few commonly used data engineering patterns
    to solve batch processing problems. Although there can be many variations of the
    implementation, these patterns are generic, irrespective of the technologies used
    to implement the patterns. In the following sections, we’ll discuss the commonly
    used batch processing patterns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些常用的数据工程模式，用于解决批处理问题。尽管实现方式可能有多种变化，但这些模式是通用的，与实现模式所使用的具体技术无关。在接下来的章节中，我们将讨论常用的批处理模式。
- en: The staged Collect-Process-Store pattern
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段性收集-处理-存储模式
- en: 'The **staged Collect-Process-Store pattern** is the most common batch processing
    pattern. It is also commonly known as the **Extract-Transform-Load** (**ETL**)
    pattern in data engineering. This architectural pattern is used to ingest data
    and store it as information. The following diagram depicts this architectural
    pattern:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段性收集-处理-存储模式**是批处理中最常见的模式。它也常被称为数据工程中的**提取-转换-加载**（**ETL**）模式。这种架构模式用于摄取数据并将其存储为信息。以下图展示了这种架构模式：'
- en: '![Figure 7.1 – The staged Collect-Process-Store pattern ](img/B17084_07_001.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 阶段性收集-处理-存储模式](img/B17084_07_001.jpg)'
- en: Figure 7.1 – The staged Collect-Process-Store pattern
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 阶段性收集-处理-存储模式
- en: 'We can break this pattern into a series of stages, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此模式分解为一系列阶段，如下所示：
- en: In this architectural pattern, one or more data sources are extracted and kept
    in a form of data storage called a raw zone or landing zone. The landing zone
    data is often raw data, which consists of noise such as extra spaces, junk characters,
    important fields missing, and so on. The extraction or collection job has the
    responsibility to extract and store the data in the raw zone. The data storage
    that’s used for the landing zone can vary from a filesystem, the **Hadoop Distributed
    File System** (**HDFS**), an S3 bucket, or some relational database based on the
    use case and the platform chosen to solve the problem.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种架构模式中，一个或多个数据源被提取并保存在称为原始区域或着陆区的数据存储形式中。着陆区数据通常是原始数据，它包含诸如额外空格、垃圾字符、重要字段缺失等噪声。提取或收集作业负责从原始区域提取和存储数据。用于着陆区的数据存储可以是从文件系统、**Hadoop分布式文件系统**（**HDFS**）、S3存储桶或基于用例和选择的平台来解决问题的某些关系型数据库。
- en: The processing jobs read the data from the raw zone and perform a series of
    transformations on the data, such as data cleansing, data standardization, and
    data validation. The job stores its output in the intermediate processing zone(s).
    There can be one or more transformation jobs, as well as multiple intermediate
    processing zones, based on the project and technology. Sometimes, processing jobs
    fetch related information from the intermediate data zone to enrich the processed
    data. In such a scenario, it reads the data from the intermediate processing zone
    or any external reference database. The final intermediate processing zone contains
    the data, which is cleansed, transformed, validated, and well organized.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理作业从原始区域读取数据，并对数据进行一系列转换，例如数据清洗、数据标准化和数据验证。作业将输出存储在中间处理区域（s）。根据项目和技术的不同，可能有一个或多个转换作业，以及多个中间处理区域。有时，处理作业会从中间数据区域获取相关信息以丰富处理后的数据。在这种情况下，它从中间处理区域或任何外部参考数据库读取数据。最终的中间处理区域包含经过清洗、转换、验证和良好组织的数据。
- en: The fetch and load process picks up the transformed data and loads it into the
    sorted dataset layer. The sorted dataset layer contains clean and usable data
    in a specific format that can easily be consumed by downstream applications for
    data analytics, reference, and so on. The sorted dataset layer is also popularly
    known as the **Organized Data Layer** (**ODL**). There is no hard and fast rule
    regarding the type of database or data store used for the sorted dataset layer.
    However, based on whether the sorted data will be used for **Online Transaction
    Processing** (**OLTP**) or **Online Analytical Processing** (**OLAP**), a database
    is chosen. Generally, this pattern is used to ingest and store data for OLAP purposes.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索和加载过程检索转换后的数据并将其加载到排序数据集层。排序数据集层包含以特定格式存储的干净和可用的数据，这种格式可以轻松被下游应用程序用于数据分析、参考等。排序数据集层也通常被称为**组织数据层**（**ODL**）。关于用于排序数据层的数据库或数据存储的类型没有硬性规定。然而，根据排序数据是否将用于**在线事务处理**（**OLTP**）或**在线分析处理**（**OLAP**），选择数据库。通常，这种模式用于摄入和存储用于OLAP目的的数据。
- en: The jobs for this architectural pattern typically run periodically based on
    a predetermined schedule, such as once daily or once weekly, or every Friday and
    Wednesday at 8 P.M. One of the advantages of this pattern is that it ingests the
    data and processes in a series of stages. The output of each stage is stored in
    an intermediate processing zone, and the next stage fetches data from the output
    of the previous stage. This staged architecture makes the design loosely coupled.
    Often, in production, the data processing job fails. In such a situation, we don’t
    need to rerun the full ingestion pipeline; instead, we can restart the pipeline
    from the job that failed.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构模式的作业通常根据预定的计划定期运行，例如每天一次或每周一次，或者每周五和周三晚上8点。这种模式的一个优点是它以一系列阶段摄入和处理数据。每个阶段的输出存储在中间处理区域，下一个阶段从上一个阶段的输出中检索数据。这种分阶段架构使得设计松散耦合。通常，在生产中，数据处理作业会失败。在这种情况下，我们不需要重新运行整个摄入管道；相反，我们可以从失败的作业重新启动管道。
- en: Now, let’s look at a real-world use case where this pattern will be a good fit.
    A health insurance firm receives tons of insurance claims every day. To process
    these claims and determine the cost that will be paid by the insurance firm, the
    data needs to be cleansed, enriched, organized, and stored. In such a use case,
    this architectural pattern can be used to ingest different kinds of claims, such
    as medical, dental, and vision, from various sources; then, they can be extracted,
    transformed, and loaded into ODL. Another example implementation of this pattern
    was discussed in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load
    – A Batch-Based Solution to Ingesting Data in a Data Warehouse*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个现实世界的用例，这个模式将非常适合。一家健康保险公司每天都会收到大量的保险索赔。为了处理这些索赔并确定保险公司将支付的成本，需要对数据进行清洗、丰富、组织和存储。在这种情况下，可以使用这种架构模式来摄入来自不同来源的各种索赔，例如医疗、牙科和视力索赔；然后，它们可以被提取、转换并加载到ODL。这种模式的另一个示例实现已在[*第4章*](B17084_04.xhtml#_idTextAnchor062)中讨论，*ETL数据加载
    – 数据仓库中摄入数据的基于批处理解决方案*。
- en: Common file format processing pattern
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见文件格式处理模式
- en: 'Suppose there is a scenario where there are multiple files (say, for example,
    25 source files) for the data sources and the structure of these sources are quite
    different from each other. Now, the question is, *Can the staged collect-process-store
    pattern handle such a use case?* Yes, it can. But is it optimized to do so? No,
    it’s not. The problem is that for all 25 different kinds of source files, we need
    to have a separate set of transformation logic written to process and store them
    into a sorted dataset. We may require 25 separate data pipelines to ingest the
    data. This not only increases development effort but also increases analysis and
    testing effort. Also, we may need to fine-tune all the jobs in all 25 data pipelines.
    The **common file format processing pattern** is well suited to overcome such
    problems. The following diagram depicts how the common file format processing
    pattern works:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设存在一个场景，其中存在多个文件（例如，25个源文件）用于数据源，并且这些源的结构彼此之间相当不同。现在的问题是，*分阶段收集-处理-存储模式能否处理此类用例？*
    是的，它可以。但是它是否优化了这样做？不是的。问题是，对于所有25种不同的源文件，我们需要编写一组单独的转换逻辑来处理并将它们存储到排序数据集中。我们可能需要25个单独的数据管道来摄取数据。这不仅增加了开发工作量，也增加了分析和测试工作量。此外，我们可能需要调整所有25个数据管道中的所有作业。**公共文件格式处理模式**非常适合克服此类问题。以下图表描述了公共文件格式处理模式的工作原理：
- en: '![Figure 7.2 – The common file format processing pattern ](img/B17084_07_002.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 公共文件格式处理模式](img/B17084_07_002.jpg)'
- en: Figure 7.2 – The common file format processing pattern
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 公共文件格式处理模式
- en: 'This pattern is divided into the following stages:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此模式分为以下阶段：
- en: In this architectural pattern, multiple source files with distinctively different
    source file structures are stored or sent to the landing zone from the sources.
    The landing zone can be a filesystem, a NAS mount, an SFTP location, or an HDFS
    location.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种架构模式中，具有明显不同源文件结构的多个源文件从源存储或发送到目标区域。目标区域可以是文件系统、NAS挂载、SFTP位置或HDFS位置。
- en: A common file format conversion process runs, which takes the different incoming
    source files and converts them into a uniform structure called a **common file
    format**. The job or pipeline that runs to do this conversion is lightweight.
    It should not do cleansing or business transformation in this layer. The common
    file format conversion process writes its output in the common file format zone.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个公共文件格式转换过程，它将不同的传入源文件转换为称为**公共文件格式**的统一结构。执行此转换的作业或管道应该是轻量级的。它不应该在这一层进行清理或业务转换。公共文件格式转换过程将其输出写入公共文件格式区域。
- en: Now that all the files are in the same format or structure, a single set of
    process and load jobs can run on top of those files present in the common file
    format zone. The process and load process can be a single job or a series of jobs
    that writes the final organized and sorted data into ODL or the sorted dataset
    layer. The process and load job may write its intermediate result to temporary
    storage zones if required.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在所有文件都处于相同的格式或结构中，一组处理和加载作业可以在位于公共文件格式区域的文件上运行。处理和加载过程可以是一个作业或一系列作业，它们将最终组织好的和排序好的数据写入ODL或排序数据集层。如果需要，处理和加载作业可能会将其中间结果写入临时存储区域。
- en: Now, let’s look at a real-world scenario. A credit card company wants to generate
    and provide offers for its customers based on their buying and spending patterns,
    as well as a set of complex rules. However, transaction data can be received from
    various kinds of sources, which includes web-based payment gateways, physical
    transaction gateways, payment apps such as PayPal and Cash App, foreign payment
    gateways, and various other similar apps. However, the transaction files that
    are received from all these sources are in different formats. One option is to
    create a separate set of transformation mappings for each source and apply the
    rules differently. However, that will cause a lot of development time and costs,
    as well as a maintenance challenge. In such a scenario, the common file format
    processing pattern can be used to convert all transaction files coming from different
    source systems into a common file format. Then, a single set of rule engine jobs
    can process transactions received from different sources.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个现实世界的场景。一家信用卡公司希望根据客户的购买和消费模式，以及一系列复杂的规则，为其客户生成和提供优惠。然而，交易数据可以从各种来源接收，包括基于网络的支付网关、实体交易网关、如
    PayPal 和 Cash App 这样的支付应用、外国支付网关以及各种类似的 app。然而，从所有这些来源接收的交易文件格式各不相同。一个选择是为每个来源创建一组单独的转换映射，并分别应用规则。然而，这将导致大量的开发时间和成本，以及维护挑战。在这种情况下，可以使用常见的文件格式处理模式将来自不同源系统的所有交易文件转换为通用的文件格式。然后，一套规则引擎作业可以处理来自不同来源的交易。
- en: The Extract-Load-Transform pattern
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取-加载-转换模式
- en: 'Previously in this book, we learned about the classical ETL-based pattern,
    where we extract the data first, transform and process the data, and finally store
    it in the final data store. However, with modern processing capabilities and the
    scalability that the cloud offers, we have seen many **Massive Parallel Processing**
    (**MPP**) databases such as Snowflake, Redshift, and Google’s Big Query becoming
    popular. These MPP databases have enabled a new pattern of data ingestion where
    we extract and load the data into these MPP databases first and then process the
    data. This pattern is commonly known as the **Extract-Load-Transform** (**ELT**)
    pattern or the Collect-Store-Process pattern. This pattern is useful for building
    high-performing data warehouses that contain a huge amount of data. The following
    diagram provides an overview of the ELT pattern:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书之前的部分，我们学习了基于经典 ETL 的模式，其中我们首先提取数据，然后转换和处理数据，最后将其存储在最终数据存储中。然而，随着现代处理能力和云提供的可扩展性，我们已经看到许多
    **大规模并行处理（MPP**）数据库，如 Snowflake、Redshift 和 Google 的 Big Query 变得越来越流行。这些 MPP 数据库使数据摄入的新模式成为可能，即我们首先提取和加载数据到这些
    MPP 数据库中，然后处理数据。这种模式通常被称为 **提取-加载-转换（ELT**）模式或收集-存储-处理模式。这种模式对于构建包含大量数据的高性能数据仓库非常有用。以下图提供了
    ELT 模式的概述：
- en: '![Figure 7.3 – The Extract-Load-Transform (ELT) pattern ](img/B17084_07_003.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 提取-加载-转换（ELT）模式](img/B17084_07_003.jpg)'
- en: Figure 7.3 – The Extract-Load-Transform (ELT) pattern
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 提取-加载-转换（ELT）模式
- en: 'The preceding diagram depicts the typical flow of an ELT pattern. This can
    be described as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前图描述了 ELT 模式的典型流程。这可以描述如下：
- en: As shown in the preceding diagram, raw data is extracted and loaded into the
    MPP database. This data is stored in the staging zone of the MPP database.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前图所示，原始数据被提取并加载到 MPP 数据库中。这些数据存储在 MPP 数据库的暂存区。
- en: Then, using MPP queries and a transformation pipeline, the data is transformed
    into the final set of tables. These final tables are exposed as a data warehouse.
    For security purposes, sometimes, views are created and exposed as a data warehouse
    on top of the tables.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 MPP 查询和转换管道，数据被转换成最终的表格集。这些最终表格作为数据仓库公开。出于安全考虑，有时会在表格之上创建视图并作为数据仓库公开。
- en: 'Again, let’s look at an example of how this pattern is used in the industry.
    As the customer experience continues to rise, businesses face a gap between the
    data needed to meet customer expectations and the ability to deliver using the
    current data management practice. Customer 360 involves building a complete and
    accurate repository of all the structured and unstructured data across the organization
    related to the customer. It is an aggregation of all customer data into a single
    unified location so that it can be queried and used for analytics to improve customer
    experience. To build Customer 360 solutions, we can leverage the power of MPP
    databases to create a single unified Customer 360 data warehouse. An example of
    a Customer 360 design using Snowflake on AWS is shown here:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们看看这个模式在行业中的应用示例。随着客户体验的持续提升，企业在满足客户期望所需的数据和利用当前数据管理实践提供的能力之间面临差距。客户360涉及构建一个包含组织内与客户相关的所有结构化和非结构化数据的完整且准确的数据库。它是将所有客户数据聚合到单个统一位置，以便可以查询并用于分析以改善客户体验。为了构建客户360解决方案，我们可以利用MPP数据库的力量来创建一个单一的统一客户360数据仓库。以下是在AWS上使用Snowflake的客户360设计示例：
- en: '![Figure 7.4 – Example of an ELT pattern on AWS ](img/B17084_07_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – AWS上的ELT模式示例](img/B17084_07_004.jpg)'
- en: Figure 7.4 – Example of an ELT pattern on AWS
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – AWS上的ELT模式示例
- en: Here, data from cloud storage, event streams, and third-party sources all land
    in the staging area of Snowflake (an MPP database). Then, using Snowflake pipelines,
    data is cleansed, transformed, and enriched and is stored in the final tables
    to be consumed by the organization as the centralized enterprise data warehouse.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，来自云存储、事件流和第三方数据源的所有数据都落在Snowflake（一个MPP数据库）的临时区域。然后，使用Snowflake管道，数据被清洗、转换和丰富，并存储在最终表中，供组织作为集中式企业数据仓库使用。
- en: The compaction pattern
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 压缩模式
- en: Data warehouses are not only built on MPP databases. For big data needs, a lot
    of the time, they are built on top of HDFS using Hive as the querying engine.
    However, in modern pipelines, a lot of data is dumped in the landing zone by real-time
    processing engines such as Kafka or Pulsar. Although the use case needs our processing
    jobs to run a few times a day or once daily, the files are landed when any records
    come in. This creates a different kind of problem. Due to the scenario described
    earlier, too many small files containing few records are created. HDFS is not
    designed to work with small files, especially if it is significantly smaller than
    the HDFS block size; for example, 128 MB. HDFS works better if a smaller number
    of huge files are stored instead of a huge number of small files.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库不仅建立在MPP数据库之上。对于大数据需求，很多时候它们是建立在HDFS之上，使用Hive作为查询引擎。然而，在现代管道中，大量数据由Kafka或Pulsar等实时处理引擎直接倒入着陆区。尽管使用案例需要我们的处理作业每天运行几次或每天运行一次，但文件是在任何记录到达时落地的。这创造了一个不同的问题。由于前面描述的场景，创建了包含少量记录的过多小文件。HDFS不是为处理小文件而设计的，尤其是如果它比HDFS块大小显著小；例如，128
    MB。如果存储的是少量大文件而不是大量小文件，HDFS的工作效果会更好。
- en: 'Eventually, as the small files grow, the query performance reduces, and eventually,
    Hive is unable to query those records. To overcome this problem, a pattern is
    commonly used. This is called the compaction pattern. The following diagram provides
    an overview of the compaction pattern:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，随着小文件的增多，查询性能降低，最终Hive无法查询这些记录。为了克服这个问题，通常使用一种模式。这被称为压缩模式。以下图表提供了压缩模式的概述：
- en: '![Figure 7.5 – The compaction pattern ](img/B17084_07_005.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 压缩模式](img/B17084_07_005.jpg)'
- en: Figure 7.5 – The compaction pattern
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 压缩模式
- en: In this architectural pattern, the small files are stored in the landing zone.
    A batch-based periodical job runs and compacts those small files to create a single
    large file. In between, it uses the status and state storage to store job audit
    information. It is also used to store state information that may be used by subsequent
    compaction jobs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构模式中，小文件存储在着陆区。一个基于批次的定期作业运行并压缩这些小文件以创建一个大型文件。在此期间，它使用状态和状态存储来存储作业审计信息。它还用于存储可能由后续压缩作业使用的状态信息。
- en: The staged report generation pattern
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段性报告生成模式
- en: 'We have discussed multiple patterns to show how data is ingested and stored
    as a sorted dataset or in a data warehouse. This pattern, on the other hand, focuses
    on running data analytics jobs and generating report(s) from the **ODL** or data
    warehouse. The following diagram shows the generic architecture of this pattern:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了多个模式，以展示数据是如何作为排序数据集或数据仓库中的数据被摄取和存储的。另一方面，本模式侧重于运行数据分析作业并从**ODL**或数据仓库生成报告。以下图表显示了该模式的通用架构：
- en: '![Figure 7.6 – The staged report generation pattern ](img/B17084_07_006.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 阶段性报告生成模式](img/B17084_07_006.jpg)'
- en: Figure 7.6 – The staged report generation pattern
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 阶段性报告生成模式
- en: 'The staged report generation pattern consists of primarily two stages and an
    auxiliary step, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段性报告生成模式主要由两个阶段和一个辅助步骤组成，具体如下：
- en: '**Report generation stage**: Various analytics jobs run on top of the sorted
    data or organized data layer. Such analytics jobs may even run on the data stored
    in the data warehouse. These jobs then save the report of the analysis in a reporting
    database. A reporting database can be a relational database, a NoSQL database,
    or a search engine such as Elasticsearch.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**报告生成阶段**：各种分析作业在排序数据或组织数据层上运行。这些作业甚至可以在数据仓库中存储的数据上运行。然后，这些作业将分析报告保存到报告数据库中。报告数据库可以是关系数据库、NoSQL数据库或如Elasticsearch这样的搜索引擎。'
- en: '**Summary generation stage**: The summary reporting jobs fetch data from the
    reporting database and report the summary data in the summary database. Summary
    databases are usually relational databases, data warehouses, or search engines.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摘要生成阶段**：摘要报告作业从报告数据库中获取数据，并在汇总数据库中报告摘要数据。汇总数据库通常是关系数据库、数据仓库或搜索引擎。'
- en: Using exporters and connectors, the data present in either the reporting database
    or the summary database can be visualized using BI tools or used for data science
    and analytics purposes, or simply used to extract flat files containing reports.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导出器和连接器，可以可视化报告数据库或汇总数据库中存在的数据，或用于数据科学和分析目的，或者简单地用于提取包含报告的平面文件。
- en: Now, let’s look at a real-world scenario where this pattern is suitable. Let’s
    say that a company has an on-premises data center. Every day, monitoring and resolution
    logs are generated for all the servers and storage, backup storage, and networking
    devices present in the data center. This data is ingested and stored in a data
    warehouse that contains daily, weekly, and monthly outage and resolution details.
    Using this data warehouse, the organization wants to generate various reports
    for the average SLA for various kinds of incidents, the performance or KPI ratio
    before and after the resolutions, and the team-wise velocity of closing incidents.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个适合此模式的实际场景。假设一家公司有一个本地数据中心。每天，数据中心中所有服务器和存储、备份存储和网络设备都会生成监控和解决日志。这些数据被摄取并存储在一个包含每日、每周和每月故障和解决详情的数据仓库中。利用这个数据仓库，组织希望生成各种报告，包括各种类型事件的平均SLA、解决前后的性能或KPI比率以及按团队解决事件的速度。
- en: Finally, the company wants to generate a summary of all incidents on a weekly,
    monthly, and quarterly basis. This use case is well-suited for using this pattern.
    In this use case, we can generate all the reports and store them in a reporting
    database and generate the summary reports to the summary database. Both general
    reports and summary reports can be visualized using BI tools such as Tableau by
    pulling the data from the reporting databases using proper connectors.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，公司希望按周、月和季度生成所有事件的摘要。这个用例非常适合使用此模式。在这个用例中，我们可以生成所有报告并将它们存储在报告数据库中，同时生成摘要报告到汇总数据库。一般报告和摘要报告都可以使用BI工具（如Tableau）通过使用适当的连接器从报告数据库中提取数据来可视化。
- en: In this section, we learned about a few popular batch processing architectural
    patterns and a few real-world scenarios that can be applied. In the next section,
    we will cover a few common patterns used for real-time stream processing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了一些流行的批量处理架构模式和几个可以应用的实际场景。在下一节中，我们将介绍一些用于实时流处理的常见模式。
- en: Core stream processing patterns
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心流处理模式
- en: In the previous section, we learned about a few commonly used batch processing
    patterns. In this section, we will discuss various stream processing patterns.
    Let’s get started.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了一些常用的批量处理模式。在本节中，我们将讨论各种流处理模式。让我们开始吧。
- en: The outbox pattern
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 出盒模式
- en: 'With modern data engineering, monolithic applications have been replaced by
    a series of microservices application working in tandem. Also, it is worth noting
    that microservices usually don’t share their databases with other microservices.
    The database session commits and interservice communications should be atomic
    and in real time to avoid inconsistencies and bugs. Here, the outbox pattern comes
    in handy. The following diagram shows the generic architecture of the outbox pattern:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代数据工程的进步，单体应用已被一系列协同工作的微服务应用所取代。值得注意的是，微服务通常不会与其他微服务共享数据库。数据库会话提交和跨服务通信应该是原子性和实时的，以避免不一致性和错误。在这里，出盒模式非常有用。以下图表显示了出盒模式的通用架构：
- en: '![Figure 7.7 – The outbox pattern ](img/B17084_07_007.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 出盒模式](img/B17084_07_007.jpg)'
- en: Figure 7.7 – The outbox pattern
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 出盒模式
- en: As we can see, a microservice (here, **Service 1**) writes a transaction to
    not only the required table where online reads and writes happen (denoted in the
    diagram as **Online Table**) but also to an **Outbox Table**, whose structure
    is where messages to the message broker should be published. Just like the physical
    trays on office desks that once held outgoing letters and documents, the outbox
    pattern uses an **Outbox Table** to send messages to the message broker. A **Change
    Data Capture** (**CDC**) publisher picks the CDC events from the **Outbox Table**
    area and publishes them to our **Message Broker**. Downstream services that need
    data from Service 1 consume the data from the topic.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，一个微服务（在此处为**服务1**）不仅将事务写入在线读取和写入所需的表（在图中表示为**在线表**），还将写入一个**出盒表**，其结构是消息应该发布到消息代理的地方。就像办公桌上曾经存放过发出的信件和文件的物理托盘一样，出盒模式使用**出盒表**将消息发送到消息代理。一个**变更数据捕获**（**CDC**）发布者从**出盒表**区域选择CDC事件并将它们发布到我们的**消息代理**。需要从服务1获取数据的下游服务消费这些数据。
- en: The saga pattern
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 悲剧模式
- en: The saga pattern is a design pattern that is used to manage and handle distributed
    transactions across multiple applications or services successfully. In a real-world
    scenario, a single business transaction can never be done with one application
    or backend service. Usually, multiple applications work in tandem to complete
    a successful business transaction. However, we need to have an asynchronous, reliable,
    and scalable way to communicate between these systems. Each business transaction
    that spans multiple services is called a saga. The pattern to implement such a
    transaction is called the saga pattern.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 悲剧模式是一种设计模式，用于成功管理和处理跨多个应用程序或服务的分布式事务。在现实世界的场景中，单个业务交易不可能仅通过一个应用程序或后端服务完成。通常，多个应用程序协同工作以完成一个成功的业务交易。然而，我们需要有一种异步、可靠和可扩展的方式来在这些系统之间进行通信。跨越多个服务的每个业务交易都称为悲剧。实现此类交易的模式称为悲剧模式。
- en: 'To understand the saga pattern, let’s take a look at an e-commerce application.
    The following diagram shows the workflow of a simplified ordering system in an
    e-commerce application:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解悲剧模式，让我们看看一个电子商务应用程序。以下图表显示了电子商务应用程序中简化订单系统的流程：
- en: '![Figure 7.8 – A simplified e-commerce ordering system ](img/B17084_07_008.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 简化的电子商务订单系统](img/B17084_07_008.jpg)'
- en: Figure 7.8 – A simplified e-commerce ordering system
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 简化的电子商务订单系统
- en: 'As we can see, an ordering system consists of multiple services, each of which
    has its own set of functions to perform. Essentially, there are three services:
    the ordering service, the credit management service, and the payment service.
    For a successful ordering transaction, the ordering service receives the order.
    If the order is received successfully, it goes to the credit management service,
    which checks the credit card’s balance and validates the card. If the credit check
    is successful, the system uses the payment service to request payment. If the
    payment goes through successfully, the order is marked as accepted. If it fails
    at any stage, the transaction is aborted, and the order gets rejected.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，一个订单系统由多个服务组成，每个服务都有其执行的一套功能。本质上，有三个服务：订单服务、信用管理服务和支付服务。为了成功完成订单交易，订单服务接收订单。如果订单成功接收，它将转到信用管理服务，该服务检查信用卡余额并验证卡片。如果信用检查成功，系统将使用支付服务请求支付。如果支付成功，订单将被标记为已接受。如果在任何阶段失败，交易将被终止，订单将被拒绝。
- en: 'Now, let’s see how the saga pattern is implemented in this situation. Here
    inter-service communication is decoupled and made asynchronous by introducing
    a message broker platform to exchange messages between them. The following diagram
    shows how the saga pattern is used to implement the ordering system for an e-commerce
    application:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在这个情况下如何实现叙事模式。这里通过引入一个消息代理平台来交换它们之间的消息，实现了服务间通信的解耦和异步化。以下图表显示了如何使用叙事模式来实现电子商务应用的订单系统：
- en: '![Figure 7.9 – The saga pattern applied to implement an ordering system ](img/B17084_07_009.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 应用到实现订单系统的叙事模式](img/B17084_07_009.jpg)'
- en: Figure 7.9 – The saga pattern applied to implement an ordering system
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 应用到实现订单系统的叙事模式
- en: Here, the saga pattern is applied to the saga transaction of placing an order.
    Here, the ordering service stores the orders in a local database. A CDC publisher
    is used to publish the messages containing the order to **Topic 2** in the streaming
    platform. Data sent to Topic 2 is consumed by the credit management service to
    do the credit check functionality (marked as flow *1* in the preceding diagram).
    The output of the credit check functionality is sent to a local database. The
    message containing the credit check result is sent from the local database to
    **Topic 1**. The ordering service consumes and stores the output for further processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，叙事模式应用于放置订单的叙事交易。在这里，订单服务将订单存储在本地数据库中。使用CDC发布者将包含订单的消息发布到流平台中的**主题2**。发送到主题2的数据被信用管理服务消费以执行信用检查功能（在前面的图表中标记为流程*1）。信用检查功能的输出被发送到本地数据库。包含信用检查结果的消息从本地数据库发送到**主题1**。订单服务消费并存储输出以供进一步处理。
- en: If the credit check report is positive, a payment request event is published
    in **Topic 3** using the CDC processor (depicted as flow *3* in the preceding
    diagram). The event that’s published in flow *3* is picked up by the payment service
    and requests payment. The result of the payment request is saved in the local
    payment database. The CDC publisher from the payment database produces the payment
    output to **Topic 4**, which is denoted as flow *4* in the preceding diagram.
    Using the information that’s been shared over **Topic 4**, the ordering service
    determines whether the order was placed or whether it was rejected. One of the
    interesting things that you can see is that each step of the saga pattern follows
    the outbox pattern, as described earlier. We can say a series of outbox patterns
    are knitted in a certain manner to create the saga pattern.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果信用检查报告是积极的，则使用CDC处理器（在前面的图表中描述为流程*3）在**主题3**发布一个支付请求事件。在流程*3*中发布的事件被支付服务捕获并请求支付。支付请求的结果被保存在本地支付数据库中。支付数据库的CDC发布者将支付输出生产到**主题4**，在前面图表中标记为流程*4**。使用在**主题4**上共享的信息，订单服务确定订单是已放置还是被拒绝。你可以看到的一个有趣的事情是，叙事模式的每个步骤都遵循前面描述的出箱模式。我们可以说一系列出箱模式以某种方式编织在一起，以创建叙事模式。
- en: The choreography pattern
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编舞模式
- en: 'This pattern is used specifically where each component independently takes
    part in the decision-making process to complete a business transaction. All these
    independent components talk to a centralized orchestrator application or system.
    Just like how in choreography, the choreography pattern enables all independent
    dancers to perform separately and create a wonderfully synchronized show, so an
    orchestrator orchestrates decentralized decision-making components to complete
    a business transaction. This is the reason that this pattern is called the choreography
    pattern. The following diagram provides an overview of the choreography pattern:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式特别适用于每个组件独立参与决策过程以完成一项业务交易的情况。所有这些独立组件都与一个集中的编排器应用程序或系统进行通信。就像在编舞中，编舞模式使得所有独立的舞者可以分别表演并创造出一个精彩同步的表演一样，编排器协调去中心化的决策组件以完成一项业务交易。这就是为什么这种模式被称为编舞模式。以下图表提供了编舞模式的概述：
- en: '![Figure 7.10 – The choreography pattern ](img/B17084_07_010.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 编舞模式](img/B17084_07_010.jpg)'
- en: Figure 7.10 – The choreography pattern
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 编舞模式
- en: As we can see, events from the client are streamed to the topic. Each event
    contains a specific message header or a message key. Based on the type of message
    header value or key value, each consuming application can filter and process the
    messages required by that application. Once it processes the event, it generates
    a result event to the same topic but with a different key or header value. The
    client consumes all the resulting events to create the final output or decision.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，来自客户端的事件被流式传输到主题。每个事件都包含一个特定的消息头或消息键。根据消息头值或键值的类型，每个消费应用程序都可以过滤和处理该应用程序所需的消息。一旦处理了事件，它就会生成一个结果事件发送到同一个主题，但具有不同的键或头部值。客户端消费所有结果事件以创建最终输出或决策。
- en: This pattern is useful when you have scenarios where applications may be frequently
    added, removed, or updated or there is a bottleneck in the centralized orchestration
    layer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有频繁添加、删除或更新应用程序的场景，或者集中编排层存在瓶颈时，这种模式非常有用。
- en: Let’s take a look at a real-world use case where this pattern may come in handy.
    A service provider receives different events whenever a client does a recharge.
    In a single recharge, the client can buy different bundles, such as a top-up bundle,
    data bundle, and so on. Each bundle adds a message header to the event. Different
    client applications provide customized offers for each kind of bundle. This use
    case is suitable for the choreography pattern. Suppose an event comes with both
    the top-up and data bundles; this will add two pieces of header information, so
    there will be two consuming applications based on the type of bundle that will
    be consumed; its own set of offers will be generated and sent back to the client
    using the topic. It makes sense to use the choreography pattern here as the type
    of bundles are dynamic, which can vary year-to-year and season-to-season. So,
    the consuming applications may be frequently added or removed from the ecosystem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个现实世界的用例，在这个用例中，这种模式可能会派上用场。当客户进行充值时，服务提供商会接收到不同的事件。在单次充值中，客户可以购买不同的套餐，例如充值套餐、数据套餐等。每个套餐都会向事件添加一个消息头。不同的客户端应用程序为每种套餐提供定制化的优惠。这个用例适合编排模式。假设一个事件同时包含充值套餐和数据套餐；这将添加两块头部信息，因此将有两个基于套餐类型消费的应用程序；它们将根据主题生成并发送回客户端的自己的优惠套餐。在这里使用编排模式是有意义的，因为套餐类型是动态的，可以逐年和季节变化。因此，消费应用程序可能会频繁地添加或从生态系统中移除。
- en: The Command Query Responsibility Segregation (CQRS) pattern
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命令查询责任分离（CQRS）模式
- en: 'This is a very famous pattern, where the read responsibility and write responsibility
    is segregated out. This means the data is written to a different data store and
    read from another data store. While the write data store is optimized for fast
    writes, the read data store is optimized for fast data reads. The following diagram
    shows how the CQRS pattern works:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常著名的模式，其中读取责任和写入责任被分离出来。这意味着数据被写入不同的数据存储，并从另一个数据存储中读取。虽然写入数据存储针对快速写入进行了优化，但读取数据存储针对快速数据读取进行了优化。以下图表显示了CQRS模式的工作原理：
- en: '![Figure 7.11 – The CQRS pattern ](img/B17084_07_011.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – CQRS模式](img/B17084_07_011.jpg)'
- en: Figure 7.11 – The CQRS pattern
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – CQRS模式
- en: 'The preceding diagram depicts how the CQRS pattern works. The flow of this
    pattern is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表描述了CQRS模式的工作原理。该模式的流程如下：
- en: First, the producer or publisher writes the event to a topic.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，生产者或发布者将事件写入主题。
- en: Using a streaming application, this record is streamed into the read database.
    While the topic is optimized for fast data writes, the read database is optimized
    for high-performance data reads. This kind of pattern is very useful for scenarios
    where we need to have a high write as well as high read speed.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用流应用程序，此记录被流式传输到读取数据库。虽然主题针对快速数据写入进行了优化，但读取数据库针对高性能数据读取进行了优化。这种模式在需要高写入速度和高读取速度的场景中非常有用。
- en: For example, for a big e-commerce website such as Amazon, the traffic increases
    heavily on Amazon sale days. In this scenario, there is the possibility of a high
    number of writes as well as a high number of searches. In this case, various sources
    such as mobile apps, web portals, and so on will accept orders and update the
    inventory. Also, offers and discounts are changed hourly using the Amazon Big
    Day sale event management portal by sellers and Amazon representatives. Although
    there will be a high number of reads and writes, customers expect subsecond response
    times regarding search results. This can be achieved by maintaining separate write
    and search databases.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于像亚马逊这样的大型电子商务网站，在亚马逊促销日，流量会大幅增加。在这种情况下，可能会有大量的写入和搜索。在这种情况下，各种来源，如移动应用、网络门户等，将接受订单并更新库存。此外，卖家和亚马逊代表使用亚马逊大日促销活动管理门户每小时更改优惠和折扣。尽管会有大量的读取和写入，但客户对搜索结果期望有亚秒级的响应时间。这可以通过维护单独的写入和搜索数据库来实现。
- en: Hence, this use case is ideal for the CQRS pattern. Here, when a customer searches,
    data is fetched from the search database, and when the customer orders or adds
    something to the cart, it is written to the write database. The information available
    in the write database will be streamed in real time to a search database such
    as Elasticsearch or AWS OpenSearch. So, users who are searching for products and
    discounts should get the search results in a fraction of a second.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个用例非常适合CQRS模式。在这里，当客户搜索时，数据从搜索数据库中检索，而当客户下单或添加到购物车时，它被写入写入数据库。写入数据库中的信息将实时流式传输到搜索数据库，如Elasticsearch或AWS
    OpenSearch。因此，搜索产品和折扣的用户应该能在几秒钟内得到搜索结果。
- en: The strangler fig pattern
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奇异榕树模式
- en: The strangler fig pattern derives its name from a species of tropical fig plants
    that grow around their host trees, slowly strangling the host tree so that it
    dies. This pattern was first proposed by Martin Fowler. Although the basic pattern
    may be implemented in different ways, the streaming pipeline gives us an indigenous
    way to use this pattern. To understand this pattern, let’s look at an example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异榕树模式的名字来源于一种热带榕树，这种榕树在其宿主树周围生长，逐渐勒死宿主树，导致其死亡。这种模式最初由马丁·福勒提出。尽管基本模式可以以不同的方式实现，但流式管道为我们提供了一种本地化的方式来使用这种模式。为了理解这种模式，让我们来看一个例子。
- en: 'Suppose there is a monolithic application that consists of three modules –
    A, B, and C. A, B, and C read and write data to the database. Initially, the architecture
    looked as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个由三个模块组成（A、B和C）的单体应用。A、B和C读取和写入数据库。最初，架构如下所示：
- en: '![Figure 7.12 – Initial state of the monolithic application ](img/B17084_07_012.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 – 单体应用的初始状态](img/B17084_07_012.jpg)'
- en: Figure 7.12 – Initial state of the monolithic application
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 单体应用的初始状态
- en: 'As we can see, all the modules have double-ended arrows, denoting both reads
    and writes are happening. Now, using the strangler fig pattern, we can convert
    this monolithic legacy application into a microservices-based application by slowly
    migrating the individual modules as separate microservices – one at a time. The
    following diagram shows that **Module A** is being moved to the microservices
    pattern from the monolithic app:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有模块都有双向箭头，表示读取和写入都在发生。现在，使用奇异榕树模式，我们可以通过逐步迁移各个模块作为独立的微服务——一次一个——将这个单体遗留应用转换为基于微服务的应用。以下图表显示了**模块A**正在从单体应用迁移到微服务模式：
- en: '![Figure 7.13 – Module A replaced with Microservice A ](img/B17084_07_013.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – 模块A被微服务A取代](img/B17084_07_013.jpg)'
- en: Figure 7.13 – Module A replaced with Microservice A
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 模块A被微服务A取代
- en: 'As we can see, **Microservice A** (which has successfully replaced **Module
    A**) reads and writes data to an event stream. This event stream is, in turn,
    connected to the database using an event source or sink connector. Slowly, the
    monolithic application will be strangled, and the final transformed architecture
    will look as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，**微服务A**（它已成功取代了**模块A**）读取和写入数据到事件流。这个事件流反过来又通过事件源或接收器连接器连接到数据库。慢慢地，单体应用将被勒死，最终的转换架构将如下所示：
- en: '![Figure 7.14 – All modules migrated using the strangler fig pattern ](img/B17084_07_014.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 使用奇异榕树模式迁移所有模块](img/B17084_07_014.jpg)'
- en: Figure 7.14 – All modules migrated using the strangler fig pattern
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 使用奇异榕树模式迁移所有模块
- en: As we can see, all the modules have been migrated from the monolithic application
    to the federated microservice pattern, allowing the monolithic application to
    retire.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有模块都已从单体应用迁移到联邦微服务模式，从而使单体应用得以退役。
- en: The log stream analytics pattern
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志流分析模式
- en: 'In this pattern, we will learn how logs collected across various apps, web
    portals, backend services, and IoT devices are used for analytics and monitoring.
    The following diagram shows a typical log streaming pattern used to facilitate
    log analytics and monitoring:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模式中，我们将学习如何使用来自各种应用、Web门户、后端服务和物联网设备的日志进行分析和监控。以下图展示了用于促进日志分析和监控的典型日志流模式：
- en: '![Figure 7.15 – The log stream analytics pattern ](img/B17084_07_015.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 日志流分析模式](img/B17084_07_015.jpg)'
- en: Figure 7.15 – The log stream analytics pattern
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 日志流分析模式
- en: 'Let’s learn how this pattern works:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习这个模式是如何工作的：
- en: As evident from the preceding diagram, all log events from various IoT devices,
    apps, web portals, and services are streamed into an event stream.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，来自各种物联网设备、应用、Web门户和服务的所有日志事件都流式传输到一个事件流中。
- en: Then, using the event sink connector, the events are sent to both a search database
    and a querying database.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用事件接收器连接器，事件被发送到搜索数据库和查询数据库。
- en: A search database can be a search engine such as Elasticsearch, AWS OpenSearch,
    Splunk, or Apache Solar. This database facilitates quick searches with complex
    query patterns. It also enables visualization and analytics using the capabilities
    of the search engine. The query database is either an MPP database such as Redshift
    or Snowflake or a query engine such as Athena. A query engine allows users to
    run SQL queries on top of ObjectStores such as S3 objects.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索数据库可以是搜索引擎，如Elasticsearch、AWS OpenSearch、Splunk或Apache Solr。这个数据库可以支持使用复杂查询模式的快速搜索。它还利用搜索引擎的功能进行可视化和分析。查询数据库可以是MPP数据库，如Redshift或Snowflake，或者查询引擎，如Athena。查询引擎允许用户在ObjectStores（如S3对象）上运行SQL查询。
- en: 'The following diagram shows a sample implementation of this kind of pattern
    in AWS:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了在AWS中此类模式的一个示例实现：
- en: '![Figure 7.16 – Example of a log analytics pattern in AWS ](img/B17084_07_016.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图7.16 – AWS中的日志分析模式示例](img/B17084_07_016.jpg)'
- en: Figure 7.16 – Example of a log analytics pattern in AWS
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – AWS中的日志分析模式示例
- en: Here, log events from various AWS services such as EC2, ECR, EKS, and others
    are streamed to a Kinesis topic using Kinesis Firehose. Kinesis Analytics transformation
    is done and, using Kinesis Firehose, streamed to AWS OpenSearch for search and
    analytics purposes. On the other hand, data gets streamed into S3 from the first
    Kinesis Firehose. Athena tables are created on top of the S3 objects. Athena then
    provides an easy-to-use query interface for batch-based analytic queries to be
    performed on the log data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，来自各种AWS服务（如EC2、ECR、EKS等）的日志事件通过Kinesis Firehose流式传输到一个Kinesis主题。使用Kinesis
    Analytics进行转换，并通过Kinesis Firehose流式传输到AWS OpenSearch进行搜索和分析。另一方面，数据从第一个Kinesis
    Firehose流式传输到S3。在S3对象上创建了Athena表。然后Athena提供了一个易于使用的查询界面，以便对日志数据进行基于批次的查询分析。
- en: In this section, we learned about various streaming patterns that are popular
    and can be used to solve common data engineering problems. We also looked at a
    few examples and learned when these patterns should be used. Next, we will investigate
    a few popular patterns that are a mix of both batch and stream processing. These
    are known as hybrid data processing patterns.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了各种流行的流处理模式，这些模式可以用来解决常见的数据工程问题。我们还查看了一些示例，并了解了何时应该使用这些模式。接下来，我们将研究一些流行的混合模式，这些模式结合了批处理和流处理。这些被称为混合数据处理模式。
- en: Hybrid data processing patterns
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合数据处理模式
- en: In this section, we will discuss two very famous patterns that support both
    batch and real-time processing. Since these patterns support both batch processing
    and stream processing, they are categorized as hybrid patterns. Let’s take a look
    at the most popular hybrid architectural patterns.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种非常著名的模式，它们支持批处理和实时处理。由于这些模式同时支持批处理和流处理，它们被归类为混合模式。让我们看看最流行的混合架构模式。
- en: The Lambda architecture
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lambda架构
- en: 'First, let’s understand the need for Lambda architecture. In distributed computing,
    the CAP theorem states that any distributed data can guarantee only two out of
    the three features of the data – that is, consistency, availability, and partition
    tolerance. However, Nathan Marz proposed a new pattern in 2011 that made it possible
    to have all three characteristics present in a distributed data store. This pattern
    is called the Lambda pattern. The Lambda architecture consists of three layers,
    as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解Lambda架构的需求。在分布式计算中，CAP定理表明任何分布式数据只能保证数据的三种特性中的两种——即一致性、可用性和分区容错性。然而，Nathan
    Marz在2011年提出了一种新的模式，使得分布式数据存储中可以同时具备这三种特性。这种模式被称为Lambda模式。Lambda架构由三层组成，如下所示：
- en: '**Batch layer**: This layer is responsible for batch processing'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量层**：这一层负责批量处理'
- en: '**Speed layer**: This layer is responsible for real-time processing'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度层**：这一层负责实时处理'
- en: '**Serving layer**: This layer serves as the unified serving layer where querying
    can be done by downstream applications'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务层**：这一层作为统一的查询服务层，下游应用程序可以在此进行查询'
- en: 'The following diagram shows an overview of the Lambda architecture:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图展示了Lambda架构的概述：
- en: '![Figure 7.17 – The Lambda architecture ](img/B17084_07_017.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图7.17 – Lambda架构](img/B17084_07_017.jpg)'
- en: Figure 7.17 – The Lambda architecture
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – Lambda架构
- en: In the Lambda architecture, the input data or the source data is written to
    the master data store present in the batch layer, as well as the event streams
    present in the speed layer. The master data store may be a relational or NoSQL
    database or a filesystem such as HDFS. Batch processing jobs run on top of this
    data store to do any data processing, as well as to load the data into batch views
    present in the serving layer. Events written in the event stream are picked up,
    processed, and loaded into the real-time view by stream processing jobs (as shown
    in *Figure 7.17*). Queries can be made separately to query batch views and real-time
    views, or they can be queried simultaneously on both views to view the results.
    Batch views are mainly for historical data, while real-time views are for Delta
    data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lambda架构中，输入数据或源数据被写入批量层中的主数据存储，以及速度层中的事件流。主数据存储可能是一个关系型数据库或NoSQL数据库，或者是一个如HDFS的文件系统。在数据存储之上运行批量处理作业，以进行数据处理，并将数据加载到服务层中存在的批量视图中。事件流中写入的事件被流处理作业拾取、处理，并加载到实时视图中（如图7.17所示）。可以分别查询批量视图和实时视图，或者同时在这两个视图中查询以查看结果。批量视图主要用于历史数据，而实时视图用于Delta数据。
- en: Although it solves the problem of eventual consistency as queries can combine
    data from both real-time views and batch-based views, it comes with a few shortcomings.
    One of the major shortcomings is that we must maintain two different workflows
    – one for the batch layer and another for the speed layer. Since, in a lot of
    scenarios, the technology to implement a streaming application is quite different
    from a batch-based application, we must maintain two different source codes. Also,
    debugging and monitoring for both batch and stream processing systems becomes
    an overhead. We will discuss how to overcome these challenges in the next pattern.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它解决了最终一致性查询的问题，因为查询可以结合实时视图和基于批量的视图中的数据，但它也有一些缺点。其中一个主要的缺点是我们必须维护两个不同的工作流程——一个用于批量层，另一个用于速度层。由于在许多场景中，实现流式应用程序的技术与基于批量的应用程序的技术有很大不同，我们必须维护两个不同的源代码。此外，对批量处理和流处理系统进行调试和监控也成为一个额外的负担。我们将在下一个模式中讨论如何克服这些挑战。
- en: The Kappa architecture
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kappa架构
- en: 'One of the reasons the Lambda architecture is widely accepted is because it
    can overcome the limitation of the CAP theorem and enable more use of stream processing
    across the industry. Before the Lambda architecture, businesses were skeptical
    to use stream processing as they feared losing messages during real-time processing.
    However, this assumption is not true with modern distributed streaming platforms
    such as Kafka and Pulsar. Let’s take a look at how the Kappa architecture provides
    a simpler alternative to the Lambda architecture. The following diagram depicts
    the Kappa architecture:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构被广泛接受的一个原因是它能够克服CAP定理的限制，并使整个行业更多地使用流处理。在Lambda架构之前，企业对使用流处理持怀疑态度，因为他们担心在实时处理中丢失消息。然而，这种假设在现代分布式流平台（如Kafka和Pulsar）中并不成立。让我们看看Kappa架构如何为Lambda架构提供一个更简单的替代方案。以下图表描述了Kappa架构：
- en: '![Figure 7.18 – The Kappa architecture ](img/B17084_07_018.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图7.18 – Kappa架构](img/B17084_07_018.jpg)'
- en: Figure 7.18 – The Kappa architecture
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – Kappa架构
- en: In the Kappa architecture, the idea is not to use two different flows – one
    for batch and one for streaming. Instead, it proposes all processing should be
    done using a stream processing engine. This means that both batch-based workloads
    and stream-based workloads can be handled by a single pipeline. Here, input data
    or source data is written in a special event stream. This event stream is an immutable
    append-only transaction log. Since it is an append-only log, it has fast writing
    capabilities. To read data, we can read from the offset where the data read stopped
    earlier. In addition to this last read offset, it should support replayability,
    which means we can read from the first message as well.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kappa架构中，理念不是使用两种不同的流程——一个用于批处理，一个用于流处理。相反，它提出所有处理都应使用流处理引擎来完成。这意味着基于批处理的工作负载和基于流的工作负载都可以由单个管道处理。在这里，输入数据或源数据被写入一个特殊的事件流。这个事件流是一个不可变的追加只事务日志。由于它是一个追加日志，因此具有快速的写入能力。要读取数据，我们可以从之前读取数据停止的位置读取。除了这个最后的读取偏移量之外，它还应支持可重放性，这意味着我们可以从第一条消息开始读取。
- en: Since we are talking about distributed computing, this transaction log will
    be partitioned, which will improve the read and write performance as well. Stream
    processing jobs read the events, process them, and write the output to unified
    views (containing both batch and real-time data). One question that comes to mind
    is, *How does this kind of flow support high-volume batch loads?* Huge volumes
    of data are also sent to the transaction log, which is then picked up by stream
    processing jobs. The output is stored in the view present in the serving layer.
    To process such a high-volume event stream, we need to do more parallelism by
    increasing the number of partitions in the event stream.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在讨论分布式计算，这个事务日志将被分区，这将提高读写性能。流处理作业读取事件，处理它们，并将输出写入统一视图（包含批处理和实时数据）。一个可能的问题就是，*这种类型的流程如何支持高容量的批处理负载？*
    巨大的数据量也被发送到事务日志，然后由流处理作业拾取。输出存储在服务层中现有的视图中。为了处理这种高容量的事件流，我们需要通过增加事件流中的分区数量来增加更多的并行性。
- en: This event stream log should have retention capabilities. Also, consumers should
    be able to replay the stream using either the event time or event offset. Each
    event has an offset and an event timestamp. The replayability feature allows consumers
    to re-read already fetched data by setting the event offset or event timestamp
    to an older value.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此事件流日志应该具有保留功能。此外，消费者应能够使用事件时间或事件偏移量重新播放流。每个事件都有一个偏移量和事件时间戳。可重放功能允许消费者通过将事件偏移量或事件时间戳设置为较旧值来重新读取已检索的数据。
- en: So far, we have discussed commonly used batch-based, real-time, and hybrid architectural
    patterns. In the penultimate section, we will quickly look at a few common serverless
    patterns.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了常用的基于批处理、实时和混合架构模式。在最后一节，我们将快速浏览一些常见的无服务器模式。
- en: Serverless patterns for data ingestion
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据摄取的无服务器模式
- en: 'We will start by answering the question, *What is serverless computing?* Serverless
    computing is a cloud execution model in which a cloud provider takes care of allocating
    resources such as storage and compute based on demand while taking care of the
    servers on behalf of customers. Serverless computing removes the burden of maintaining
    and managing servers and resources associated with it. Here, the customers of
    serverless computing don’t care how and where the jobs or applications are running.
    They just focus on the business logic and let the cloud provider take care of
    managing the resources for running and executing that code. A few examples of
    serverless computing are as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回答问题，*什么是无服务器计算？* 无服务器计算是一种云执行模型，其中云服务提供商根据需求分配资源，如存储和计算，同时代表客户管理服务器。无服务器计算消除了维护和管理服务器及其相关资源的负担。在这里，无服务器计算的客户不关心作业或应用程序如何以及在哪里运行。他们只关注业务逻辑，并让云服务提供商负责管理运行和执行该代码的资源。以下是一些无服务器计算的示例：
- en: '**AWS Lambda Function or Azure Function**: This is used to run any application
    or service'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda 函数或 Azure 函数**：这用于运行任何应用程序或服务'
- en: '**AWS Glue**: This is used to run big data-based ETL jobs'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue**：用于运行基于大数据的 ETL 作业'
- en: '**AWS Kinesis**: This is a serverless event streaming and analytics platform'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Kinesis**：这是一个无服务器事件流和数据分析平台'
- en: 'Although there are many useful serverless patterns, in this section, we will
    discuss the two most relevant patterns that can help us architect data engineering
    solutions. The following are the serverless patterns that we will be discussing:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多有用的无服务器模式，但在此部分中，我们将讨论两个最相关的模式，这些模式可以帮助我们构建数据工程解决方案。以下是我们将讨论的无服务器模式：
- en: '**The event-driven trigger pattern**: This is a very common pattern that’s
    used in cloud architectures. In this pattern, upon creating or updating any file
    in object storage such as an S3 bucket, a serverless function gets triggered.
    The following diagram provides an overview of this pattern:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件驱动触发模式**：这是一个在云架构中非常常见的模式。在此模式中，在对象存储（如 S3 存储桶）中创建或更新任何文件时，将触发无服务器函数。以下图表提供了此模式的概述：'
- en: '![Figure 7.19 – The event-driven trigger pattern ](img/B17084_07_019.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 事件驱动触发模式](img/B17084_07_019.jpg)'
- en: Figure 7.19 – The event-driven trigger pattern
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 事件驱动触发模式
- en: In this pattern, any change to an object, such as it being created or deleted,
    in object storage can trigger a serverless function. This serverless function
    may either directly process the data or be used to trigger a big data job. Serverless
    functions such as AWS Lambda and Azure Function can set triggers that can trigger
    them. For example, a Lambda function can be configured to have an S3 trigger from
    Bucket1 for any new object being created or updated in Bucket1\. The triggered
    Lambda function can, in turn, trigger an EMR job or a serverless Glue job, which
    transforms and processes the necessary data and writes the final output to a data
    store. Alternatively, the Lambda function can do some data processing and store
    the output result in the final data store. The final data store can be a SQL database,
    NoSQL database, MPP database, or object storage such as AWS S3.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模式中，对象存储中任何对象的变化，例如创建或删除，都可以触发无服务器函数。此无服务器函数可以直接处理数据或用于触发大数据作业。AWS Lambda
    和 Azure Function 等无服务器函数可以设置触发器来触发它们。例如，可以配置 Lambda 函数以从 Bucket1 的任何新创建或更新的对象中触发
    S3 触发器。触发的 Lambda 函数可以反过来触发 EMR 作业或无服务器 Glue 作业，这些作业转换并处理必要的数据，并将最终输出写入数据存储。或者，Lambda
    函数可以进行一些数据处理并将输出结果存储在最终数据存储中。最终数据存储可以是 SQL 数据库、NoSQL 数据库、MPP 数据库或对象存储，如 AWS S3。
- en: A real-world scenario for using this pattern and its solution was explained
    in detail in [*Chapter 5*](B17084_05.xhtml#_idTextAnchor074), *Architecting a
    Batch Processing Pipeline*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 5 章*](B17084_05.xhtml#_idTextAnchor074)“构建批处理管道”中详细解释了使用此模式及其解决方案的实际场景。
- en: '**The serverless real-time pattern**: This is an oversimplistic serverless
    pattern that is quite popular for data ingestion in the cloud. An overview of
    this pattern can be seen in the following diagram:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无服务器实时模式**：这是一个非常流行的无服务器模式，用于云中的数据摄取。以下图表中可以看到此模式的概述：'
- en: '![Figure 7.20 – The serverless real-time pattern ](img/B17084_07_020.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.20 – 无服务器实时模式](img/B17084_07_020.jpg)'
- en: Figure 7.20 – The serverless real-time pattern
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 无服务器实时模式
- en: In the serverless real-time pattern, event or data streaming, as well as data
    processing, happens using serverless services in the cloud. Events, logs, and
    messages from different source systems publish the events to a serverless data
    streaming platform such as AWS Kinesis. The data stream triggers one or a series
    of serverless functions chained one after another to do the data processing on
    the fly. Once the data has been processed, it is written back to a final data
    store. The final data store can be SQL, NoSQL, an MPP database, object storage,
    or a search engine.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器实时模式中，事件或数据流以及数据处理都是通过云中的无服务器服务来完成的。来自不同源系统的事件、日志和消息将事件发布到无服务器数据流平台，如AWS
    Kinesis。数据流触发一个或一系列无服务器函数，这些函数一个接一个地执行数据处理。一旦数据处理完成，它就被写回到最终数据存储中。最终数据存储可以是SQL、NoSQL、MPP数据库、对象存储或搜索引擎。
- en: 'A real-world example where this pattern may be used is in a real-time fraud
    detection system for credit card usage. The following diagram depicts a sample
    solution of fraud detection in AWS using this pattern:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能使用此模式的实际示例是在信用卡使用的实时欺诈检测系统中。以下图展示了使用此模式在AWS中进行欺诈检测的示例解决方案：
- en: '![Figure 7.21 – A sample implementation of the serverless real-time pattern
    in AWS ](img/B17084_07_021.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21 – AWS中无服务器实时模式的示例实现](img/B17084_07_021.jpg)'
- en: Figure 7.21 – A sample implementation of the serverless real-time pattern in
    AWS
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – AWS中无服务器实时模式的示例实现
- en: Here, API Gateway streams real-time credit card transactions directly into Kinesis
    Data Streams (a serverless data streaming platform). The transaction events written
    in Kinesis Data Streams trigger the Lambda function to perform fraud and anomaly
    detection on the event. The Lambda function makes use of AWS SageMaker, which,
    in turn, uses the already stored data science models stored in S3 to determine
    fraud and anomalies in the transaction. The output is then passed to Kinesis Data
    Firehose, which captures the result from the Lambda function and stores it in
    an S3 bucket. This S3 bucket contains the results in real time. We can use a service
    such as Amazon QuickSight to visualize the results and take any action if required.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，API网关将实时信用卡交易直接流式传输到Kinesis数据流（一个无服务器数据流平台）。在Kinesis数据流中写入的交易事件触发Lambda函数对事件进行欺诈和异常检测。Lambda函数利用AWS
    SageMaker，它反过来使用存储在S3中的已存储数据科学模型来确定交易中的欺诈和异常。然后，输出传递给Kinesis数据火 hose，它捕获Lambda函数的结果并将其存储在一个S3桶中。这个S3桶包含实时结果。我们可以使用像Amazon
    QuickSight这样的服务来可视化结果，并在需要时采取任何行动。
- en: With that, we have discussed what serverless computing is and discussed two
    highly used patterns for serverless computing for data ingestion. Now, let’s summarize
    what we learned in this chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经讨论了什么是无服务器计算，并讨论了两种高度使用的无服务器计算数据摄取模式。现在，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started by discussing various popular batch processing patterns.
    We covered five commonly used patterns to solve batch processing problems. We
    also looked at examples of those patterns and real-world scenarios where such
    patterns are used. Then, we looked at five popular patterns available to architect
    stream processing pipelines and how they are used to solve real-world problems
    in data engineering. Next, we learned about the Lambda and Kappa architectures
    and how they are useful for both batch and stream processing. Finally, we learned
    what serverless architecture is and looked at two popular serverless architectures
    that are used to solve many data engineering problems in the cloud.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了各种流行的批处理模式。我们涵盖了五种常用的模式来解决批处理问题。我们还研究了这些模式的示例以及在实际场景中使用这些模式的情况。然后，我们探讨了可用于构建流处理管道的五种流行模式以及它们如何用于解决数据工程中的实际问题。接下来，我们学习了Lambda和Kappa架构以及它们在批处理和流处理中的有用性。最后，我们学习了无服务器架构是什么，并探讨了两种流行的无服务器架构，这些架构用于解决云中许多数据工程问题。
- en: At this point, we know how to implement batch and streaming solutions, as well
    as have a fair idea of different data engineering patterns that are commonly used
    across the industry. Now, it is time to put some amount of security and data governance
    into our solutions. In the next chapter, we will discuss various data governance
    techniques and tools. We will also cover how and why data security needs to be
    applied to data engineering solutions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经知道如何实现批处理和流式处理解决方案，并对行业内常用的不同数据工程模式有一个相当的了解。现在，是时候在我们的解决方案中加入一些安全性和数据治理措施了。在下一章中，我们将讨论各种数据治理技术和工具。我们还将涵盖为什么以及如何将数据安全应用于数据工程解决方案。
