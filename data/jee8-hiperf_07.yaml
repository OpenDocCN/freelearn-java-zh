- en: Be Fault-Tolerant
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有容错性
- en: For years, Java EE has been about putting the maximum number of applications
    inside a single application server, but it has been changing for a few years now.
    It has become more common to deploy a single application in a container instance
    and to reduce the application size to handle a single responsibility. The direct
    implication of such a paradigm change is that a system, as a whole, is now composed
    of far more applications than before, and we rely more and more on remote communications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Java EE一直致力于将尽可能多的应用程序放入单个应用服务器中，但现在这种做法已经改变。现在，在容器实例中部署单个应用程序并减小应用程序大小以处理单一责任变得更加常见。这种范式转变的直接影响是，整个系统现在由比以前多得多的应用程序组成，我们越来越依赖于远程通信。
- en: 'In such a context, the performance of one application directly depends on another
    application, and it is important to be able to limit the side effects between
    applications. To ensure that your applications identified the impact of its environment and
    can work with such constraints, we will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个应用程序的性能直接取决于另一个应用程序，因此限制应用程序之间的副作用变得非常重要。为了确保您的应用程序能够识别其环境的影响并能够在这种约束下工作，我们将在本章中介绍以下主题：
- en: Load balancing on clients and servers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端和服务器上的负载均衡
- en: Fail-overs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败转移
- en: Circuit breaker
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电路断路器
- en: Bulkhead usage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隔离舱
- en: Timeout handling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超时处理
- en: It will fail, no doubt!
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它肯定会失败，毫无疑问！
- en: When developing an application, we often spend most of the time on the *passing* code
    path, as the code path gives the application its actual feature. However, it is
    important to not forget all the unexpected cases. It can sound weird to try to
    solve something we don't control but, here, the idea is to follow the Murphy's
    law which is often summarized as follows: *anything that can go wrong, will go
    wrong*. It doesn't mean that the system will never work, but it means that if
    there is a potential issue, it will become your reality one day or another.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发应用程序时，我们通常花费大部分时间在*通过*代码路径上，因为代码路径赋予了应用程序实际的功能。然而，不要忘记所有意外情况也很重要。试图解决我们无法控制的事情可能听起来很奇怪，但在这里，我们的想法是遵循墨菲定律，它通常被总结如下：*任何可能出错的事情，最终都会出错*。这并不意味着系统永远不会工作，但它的意思是，如果存在潜在的问题，它最终会成为你的现实。
- en: In terms of a modern system and, more particularly, Java EE deployment, the
    typical consequence is that you can lose the connectivity to a related resource
    or application. Another common failure case you can desire to address is about
    the JVM failing (no more memory, OS issue, and so on), but this is linked to the
    infrastructure (potentially Kubernetes), and it is beyond the scope of this book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代系统和Java EE部署方面，典型的后果是你可能会失去与相关资源或应用程序的连接。你还可以希望解决另一个常见的故障案例，即JVM失败（没有更多内存、操作系统问题等），但这与基础设施（可能是Kubernetes）有关，并且超出了本书的范围。
- en: 'We will illustrate this with a very simple system where three Java EE applications
    are chained:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个非常简单的系统来阐述这一点，其中三个Java EE应用程序是串联的：
- en: '![](img/a2530673-657f-4fdb-94fd-111c3f6500e5.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2530673-657f-4fdb-94fd-111c3f6500e5.jpg)'
- en: With such an architecture, we can assume that a front layer exposes some customer
    features or API. Then, the front application delegates the actual logic to an
    internal system owned by another team. Finally, the internal system relies on
    a data system which is again owned and developed by another team.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的架构中，我们可以假设一个前端层暴露了一些客户功能或API。然后，前端应用程序将实际逻辑委托给另一个团队拥有的内部系统。最后，内部系统依赖于一个数据系统，该系统同样由另一个团队拥有和开发。
- en: It is not rare to use the same sort of architecture, but with external systems.
    In such a case, you often have a support phone number, but it is rarely as efficient
    as calling a colleague, which makes you even more dependent on this system/company
    in the case of a failure.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种类似的架构并不罕见，但与外部系统一起使用。在这种情况下，你通常有一个支持电话号码，但它很少像打电话给同事那样高效，这使得你在系统失败的情况下更加依赖这个系统/公司。
- en: What is important with such systems is that if the data fails (because the data
    engineers did an upgrade that didn't work as expected), then all the internal
    systems will start failing because the data doesn't answer anymore. Transitively,
    the front system will fail because the internal system fails.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的系统来说，重要的是如果数据失败（因为数据工程师进行了一个未按预期工作的升级），那么所有内部系统都将开始失败，因为数据不再响应。递归地，前端系统将因为内部系统失败而失败。
- en: You may think that this will just make the system inefficient and that there
    is no link with the performance. This is not really the case. In the previous
    schema, the data system looks quite central. If the company adds a second internal
    system (let's call it *internal2*), then we can assume that the load on the data
    store will be multiplied by two. Nonetheless, if the data is not sized for the
    load increase, it will be slower to answer and will potentially return more errors.
    Here again, all consumer services, including transitive services, will start being
    slower, as they depend on the data system.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这只会使系统效率低下，并且与性能无关。实际上并非如此。在先前的架构中，数据系统看起来相当集中。如果公司增加第二个内部系统（我们可以称之为*internal2*），那么我们可以假设数据存储的负载将翻倍。然而，如果数据没有为负载增加而调整大小，它将回答得更慢，并且可能会返回更多的错误。在这里，所有消费者服务，包括传递性服务，都将开始变慢，因为它们依赖于数据系统。
- en: This is not something you can actually avoid. You can limit the effect of an
    unexpected failure, but it is almost impossible to guarantee that it will not
    happen. If you are in a big company with an operation team in charge of all the
    applications, this kind of issue will likely be sorted by priority and performance
    degradation will be less important than a failing system. When a distributed system
    like this starts to fail, each brick often fails slowly, just because of the relationships.
    Thus, all the applications will be seen in *red* by the monitoring team, which
    doesn't help them to solve the issue, when only one part of the whole system is
    failing (our data system in this example). This is why ensuring that your system
    is ready to fail will make sure that your system is fixed faster if there is an
    issue, and that the performance impact on the other parts of the system will be
    reduced if some related application go uncontrolled.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是你可以真正避免的事情。你可以限制意外故障的影响，但几乎不可能保证它不会发生。如果你是一家大公司，有一个负责所有应用程序的操作团队，这类问题可能会根据优先级得到解决，性能下降将不如系统故障重要。当一个分布式系统像这样开始失败时，每个组件通常会因为关系而缓慢失败，这就是为什么所有应用程序都会被监控团队视为*红色*，这并不帮助他们解决问题，因为只有系统的一部分（在这个例子中是我们的数据系统）正在失败。这就是为什么确保你的系统准备好失败，可以确保在出现问题时系统修复得更快，如果某些相关应用程序失控，对系统其他部分的影响也会降低。
- en: Load balancing – pick the best one
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡 – 选择最佳方案
- en: Load balancing is about defining how to select the backend node that will process
    the request. It can be done on the server or client side, but strategies are globally
    the same. The fact that it is **Client** or **Server** is mainly a deployment
    concern because when the **Load Balancer** is an instance (software), then you
    actually add a **Client** in the chain between the final clients and your middlewares.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是关于定义如何选择处理请求的后端节点。它可以在服务器或客户端上进行，但策略总体上是相同的。它主要是**客户端**或**服务器**的部署问题，因为当**负载均衡器**是一个实例（软件）时，你实际上在最终客户端和你的中间件之间添加了一个**客户端**。
- en: 'At very high level, a **Load Balancer** can be schematized as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常高的层面上，**负载均衡器**可以简化如下：
- en: '![](img/55bfe10b-d8e7-4760-ba44-e58315df4fb4.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/55bfe10b-d8e7-4760-ba44-e58315df4fb4.jpg)'
- en: The global idea is to add a layer in between the **Client** and the **Server**,
    which will orchestrate the way the requests are distributed to the servers depending
    on different strategies. This picture represents four clients calling the same
    application through a **Load Balancer**, which will delegate the request processing
    to three servers (one server will process two of the four client requests).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 全球的想法是在**客户端**和**服务器**之间添加一层，这将根据不同的策略来协调请求如何分布到服务器。这张图展示了四个客户端通过**负载均衡器**调用相同的应用程序，负载均衡器将请求处理委托给三个服务器（一个服务器将处理四个客户端请求中的两个）。
- en: This is a common representation of server-side load balancing, but it can also
    be applied on the client side. The only difference is that the **Load Balancer**
    will be *deployed* inside the JVM of the client and, instead of taking the incoming
    requests through a network protocol (websocket, HTTP, and so on), it will take
    it from the inside of the JVM (normal method invocation).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种常见的服务器端负载均衡表示，但它也可以应用于客户端。唯一的区别是**负载均衡器**将在客户端的JVM（Java虚拟机）内部**部署**，而不是通过网络协议（如websocket、HTTP等）接收传入的请求，而是从JVM内部（正常方法调用）接收。
- en: Transparent load balancing – client versus server
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 透明负载均衡 – 客户端与服务器
- en: The main advantage of using a server load balancer is that the clients don't
    really care about a load balancer. Concretely, all the clients will use the same
    endpoint (let's say, *quote-manager.demo.packt.com*) and the load balancer will
    distribute the requests without requiring any knowledge of the clients. This is
    very important in terms of the infrastructure, since you can update your infrastructure
    without notifying or updating the clients (which can be impossible if not owned
    by your own system).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务器负载均衡器的主要优势是客户端实际上并不关心负载均衡器。具体来说，所有客户端都将使用相同的端点（比如说，*quote-manager.demo.packt.com*），而负载均衡器将分配请求而不需要了解任何客户端信息。这在基础设施方面非常重要，因为你可以更新你的基础设施而无需通知或更新客户端（如果客户端不属于你自己的系统，这可能是不可能的）。
- en: 'For instance, if you start with two machines and decide to add a third one
    a month later because you get more load or to support the *black friday* additional
    load, then you will just register this third machine against the load balancer
    and it will distribute the work load against the three machines instead of only
    two. It is also true for the opposite way: if you need to do some maintenance
    on a machine, you can remove it from the cluster behind the load balancer, assuming
    that removing one machine still supports the load, but it should be taken into
    account during the sizing phase of the infrastructure. Then, do your maintenance
    offline, and once done, just add back the machine into the cluster.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你一开始使用两台机器，然后在一个月后决定添加第三台机器，因为负载增加或为了支持*黑色星期五*的额外负载，那么你只需将这台第三台机器注册到负载均衡器上，它将把工作负载分配到三台机器上，而不是只有两台。反之亦然：如果你需要对一台机器进行维护，你可以将其从负载均衡器后面的集群中移除，假设移除一台机器仍然可以支持负载，但在基础设施的规模阶段应该考虑到这一点。然后，离线进行维护，完成后，只需将机器重新添加到集群中即可。
- en: So, this analysis makes it sound like server load balancing is the best solution
    and the one to choose. However, modern systems have efficient client-side load
    balancers if you own the clients (which is often the case for microservice-oriented
    systems). What makes server load balancing strategy better than client load balancing?—The
    fact that the server can be updated without notifying the clients. This means
    that if the clients are autoupdated from the server/backend changes, then we achieve
    the same on the server side. In practice, this is done using a service registry
    that can enumerate the list of URLs that you can use to contact a service. In
    practice, the client load balancer will contact this registry service to get the
    list of endpoints it can use for a particular service and update this list from
    time to time with a configuration policy close to the pool ones that we saw in
    the previous chapter. It sill means that this *registry* service must be reliable
    and should likely use a server load balancer solution, but then, all other services
    can use point-to-point connection (without an intermediate load balancer instance).
    In terms of application impact, it means that adding (or removing) a server must
    imply (de)registration against the registry instead of the load balancer, but
    it is the same sort of work in both the cases.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种分析听起来像是服务器负载均衡是最好的解决方案，也是应该选择的一个。然而，如果你拥有客户端，现代系统有高效的客户端负载均衡器（这在面向微服务的系统中通常是情况）。是什么让服务器负载均衡策略比客户端负载均衡更好？——事实是服务器可以在不通知客户端的情况下进行更新。这意味着如果客户端从服务器/后端更改中自动更新，那么我们在服务器端也实现了同样的效果。在实践中，这是通过一个服务注册表来完成的，它可以列出你可以用来联系服务的URL列表。在实践中，客户端负载均衡器将联系这个注册表服务以获取特定服务的端点列表，并定期使用与上一章中看到的池配置策略相似的配置策略更新这个列表。这意味着这个*注册表*服务必须是可靠的，并且可能需要使用服务器负载均衡器解决方案，但然后，所有其他服务都可以使用点对点连接（无需中间负载均衡器实例）。从应用程序的影响来看，这意味着添加（或删除）服务器必须意味着（去）注册到注册表，而不是负载均衡器，但在两种情况下都是同样的工作。
- en: 'At this point, we see that both the client and server load balancing can achieve
    the same sort of features, so what can be differentiating? There are two main
    criteria you can use to choose between both:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到客户端和服务器负载均衡都可以实现类似的功能，那么有什么可以区分它们呢？你可以使用两个主要标准来在两者之间进行选择：
- en: Who is responsible for the infrastructure and load balancing? If it is the dev(ops)
    team, both the solutions will work well. However, if you are working in a company
    that splits development and operations into teams, you will likely prefer to delegate
    the part to the operations team and, thus, use a server load balancer, which they
    will fully control without impacting the application development.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁负责基础设施和负载均衡？如果是开发（运维）团队，这两种解决方案都能很好地工作。然而，如果你在一个将开发和运维分开成不同团队的公司工作，你可能会更倾向于将这部分工作委托给运维团队，因此使用服务器负载均衡器，他们将完全控制，而不会影响应用程序开发。
- en: What kind of logic do you want to put in place inside the load balancer? Server-side
    load balancers have the most common strategies already implemented and, often,
    a small scripting language that you can use to customize. However, if you have
    a very custom strategy (potentially depending on your application state), then
    you will want to code the load balancing strategy inside the client.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想在负载均衡器内部实现什么样的逻辑？服务器端负载均衡器已经实现了最常见的策略，并且通常还提供了一种小型的脚本语言，你可以用它来自定义。然而，如果你有一个非常定制的策略（可能依赖于你的应用程序状态），那么你将需要在客户端编写负载均衡策略。
- en: To summarize, client-side load balancing is more impacting in terms of development
    because you need to handle it on the client side, which means in all the clients
    instead of a single instance for the server side, but it gives you really more
    power for very advanced needs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，客户端负载均衡在开发方面影响更大，因为你需要在客户端处理它，这意味着在所有客户端而不是服务器端的一个实例上，但它为你提供了针对非常高级需求的真实更多权力。
- en: Common strategies
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见策略
- en: How to distribute the request is the central piece of a load balancer. In this
    section, we will go through the most common solutions that you will encounter
    while configuring a load balancer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如何分配请求是负载均衡器的核心部分。在本节中，我们将介绍你在配置负载均衡器时可能会遇到的最常见的解决方案。
- en: The round-robin algorithm
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮询算法
- en: The round-robin algorithm is certainly the most known of all the strategies.
    It considers the list of available members of the cluster (the *servers*) as a
    ring and continuously iterates over this ring each time a request comes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询算法无疑是所有策略中最为人所知的。它将集群（*服务器*）中可用的成员列表视为一个环，并在每次请求到来时连续遍历这个环。
- en: 'For instance, if you have three servers (`server1.company.com`, `server2.company.com`, `server3.company.com`),
    here is how the first requests will be served:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你有三个服务器（`server1.company.com`，`server2.company.com`，`server3.company.com`），以下是第一个请求的处理方式：
- en: '| **Request number** | **Selected server** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **请求编号** | **选择的服务器** |'
- en: '| 1 | `server1.company.com` |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `server1.company.com` |'
- en: '| 2 | `server2.company.com` |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `server2.company.com` |'
- en: '| 3 | `server3.company.com` |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `server3.company.com` |'
- en: '| 4 | `server1.company.com` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 4 | `server1.company.com` |'
- en: '| 5 | `server2.company.com` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 5 | `server2.company.com` |'
- en: '| 6 | `server3.company.com` |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 6 | `server3.company.com` |'
- en: '| 7 | `server1.company.com` |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 7 | `server1.company.com` |'
- en: '| ... | ... |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... |'
- en: You will note that to have a *fair* distribution, the load-balancer strategy
    must lock or synchronize the list every time it selects a server. There are other
    flavors of this algorithm where the implementation is lock-free but the fairness
    of the distribution is not fully guaranteed. However, keep in mind that it is
    rarely something you'd really care about, as you want to have something that looks
    like being fair.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，为了实现*公平*的分配，负载均衡器策略必须在每次选择服务器时锁定或同步列表。还有其他这种算法的变体，其实现是无锁的，但分布的公平性不能完全保证。然而，请记住，这通常不是你真正关心的事情，因为你希望有一个看起来像是公平的解决方案。
- en: Random load balancing
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机负载均衡
- en: Random load balancing also takes a list of servers to target but every time
    a request comes, it picks one randomly. If random implementation is equally distributed,
    it leads to a distribution close to the round-robin solution. However, it can
    potentially scale better, since it doesn't need to synchronize the list to pick
    the *current* server to use.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机负载均衡也针对服务器列表进行操作，但每次请求到来时，它会随机选择一个。如果随机实现是均匀分布的，它会导致接近轮询解决方案的分布。然而，它可能具有更好的可扩展性，因为它不需要同步列表来选择要使用的*当前*服务器。
- en: Link to failover
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接到故障转移
- en: 'We will talk more about failover in the next section, but it is important here
    to mention that load balancing can be used to implement a failover strategy. Here,
    the goal will be to try the request against another machine if the *current* one
    fails. This can be sort of seen as round-robin, but instead of using each request
    as a trigger for the iteration over the hosts (to change targeted instance), a
    failure would be the trigger. Here is an example sequence using the failover strategy,
    considering that we have the same three hosts as in the round-robin part:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中更多地讨论故障转移，但在此处提到负载均衡可以用来实现故障转移策略是很重要的。在这里，目标将是如果当前服务器失败，尝试将请求发送到另一台机器。这可以看作是轮询，但与使用每个请求作为触发器来迭代主机列表（以更改目标实例）不同，失败将是触发器。以下是一个使用故障转移策略的示例序列，考虑到我们与轮询部分相同的三个主机：
- en: '| **Request number** | **Selected server** | **Status** |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| **请求编号** | **选择的服务器** | **状态** |'
- en: '| 1 | `server1.company.com` | OK |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 1 | `server1.company.com` | 正常 |'
- en: '| 2 | `server1.company.com` | OK |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 2 | `server1.company.com` | 正常 |'
- en: '| 3 | `server1.company.com` | OK |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 3 | `server1.company.com` | 正常 |'
- en: '| 4 | `server1.company.com` | OK |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 4 | `server1.company.com` | 正常 |'
- en: '| *5* | `server1.company.com` | *OK* |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| *5* | `server1.company.com` | *正常* |'
- en: '| 6 | `server2.company.com` | OK |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 6 | `server2.company.com` | 正常 |'
- en: '| 7 | `server2.company.com` | OK |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 7 | `server2.company.com` | 正常 |'
- en: '| ... | ... |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... |  |'
- en: 'As you can see in the preceding table, each request is using the same host
    (`server1.company.com`) until a request fails (request #5), in which case, the
    algorithm iterates over the host list and starts using `server2.company.com`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，每个请求都使用相同的宿主（`server1.company.com`），直到请求失败（请求编号5），在这种情况下，算法将迭代主机列表并开始使用
    `server2.company.com`。
- en: Indeed, there are some variants to this algorithm. For instance, the failed
    request can be retried (or not) with the *next* host in the list, or you can configure
    a number of failures to wait before switching the host or even configure what
    failure means (the default is generally a 5xx HTTP status, but you can also configure
    it to be any HTTP status > 399, or base this choice on a header or any other part
    of the response).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，这个算法有一些变体。例如，失败的请求可以与列表中的下一个主机重试（或不重试），或者你可以配置在切换主机之前等待的失败次数，甚至可以配置失败的含义（默认情况下通常是5xx
    HTTP状态，但你也可以将其配置为任何HTTP状态> 399，或者基于响应的标题或其他部分来做出这个选择）。
- en: Sticky session
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 粘性会话
- en: 'Sticky session routing is generally used because of a business use case. The
    idea is to always route a client to the same backend server if a session is started.
    Java EE defines three session modes through `SessionTrackingMode`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 粘性会话路由通常用于业务用例。其想法是在会话启动时始终将客户端路由到同一后端服务器。Java EE 通过 `SessionTrackingMode` 定义了三种会话模式：
- en: '**COOKIE**: The session is tracked through its identifier (`JSESSIONID`) inside
    cookies, so it hits the browser (client) and is sent with each request in the
    cookies.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**COOKIE**：会话通过其标识符（`JSESSIONID`）在cookie中跟踪，因此它击中浏览器（客户端）并随每个请求在cookie中发送。'
- en: '**URL**: The `JSESSIONID` is sent to the client through the URL. Example:  `http://sample.packt.com/quote-manager/index.html;jessionid=1234`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**URL**：`JSESSIONID` 通过URL发送到客户端。例如：`http://sample.packt.com/quote-manager/index.html;jessionid=1234`'
- en: '**SSL**: This uses the HTTPS native mechanism to identify sessions.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SSL**：这使用HTTPS原生机制来识别会话。'
- en: 'Each time, the tracking works by passing a shared *identifier* between the
    client and the server. If you add a load balancer in between, the communication
    can be broken if you don''t target the same host. Here is a diagram representing
    this statement:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每次跟踪都是通过在客户端和服务器之间传递一个共享的 *标识符* 来实现的。如果在两者之间添加一个负载均衡器，如果不针对同一主机，通信可能会中断。以下是表示这个陈述的图表：
- en: '![](img/4b60f497-449b-4f73-a2e9-b2294897c114.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b60f497-449b-4f73-a2e9-b2294897c114.jpg)'
- en: This diagram represents a **Client** serving two requests. The first request
    (**1**) will hit the **Load Balancer**, which will redirect the request to **Server
    1** (**1'**) and the request processing will create a session on **Server 1**.
    It implies that the response of this first request will create a `JSESSIONID` (or
    its SSL replacement). Now, the client issues a second request (**2**) and, here,
    the **Load Balancer** redirects the request, respecting the stickiness of the
    strategy, to the second server (**2'**). During the processing on **Server 2**,
    the application tries to access back the session information created during the
    first request (the identified user for instance), but since we switched the node,
    the session is not here. So, the request processing fails (red cross).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表示一个**客户端**服务两个请求。第一个请求（**1**）将击中**负载均衡器**，它将请求重定向到**服务器1**（**1'**），请求处理将在**服务器1**上创建一个会话。这意味着第一个请求的响应将创建一个`JSESSIONID`（或其SSL替代品）。现在，客户端发出第二个请求（**2**），在这里，**负载均衡器**根据策略的粘性将请求重定向到第二个服务器（**2'**）。在**服务器2**上的处理过程中，应用程序试图访问在第一个请求期间创建的会话信息（例如，已识别的用户），但由于我们切换了节点，会话不在这里。因此，请求处理失败（红色交叉）。
- en: 'To ensure that this workflow works, there are two main solutions:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保此工作流程正常工作，有两种主要解决方案：
- en: Ensure that the session state is distributed and shared across all the nodes.
    This solution sets up a kind of distributed storage space between the servers.
    It generally either implies a lot of latency (if synchronously done), or some
    potential miss (failure) if asynchronously done, which can lead to the same issue
    as the previous schema. It also implies to configure a solution other than the
    server's default session handling, which is local only out of the box.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保会话状态在所有节点之间分布和共享。此解决方案在服务器之间设置了一种类型的分布式存储空间。这通常意味着大量的延迟（如果同步完成），或者在异步完成时，可能会出现一些潜在的遗漏（故障），这可能导致与先前方案相同的问题。它还意味着要配置除服务器默认会话处理之外的解决方案，该处理默认情况下仅限于本地。
- en: Ensure that the load balancer always hits the same backend node once a session
    is created. This is what we call the *sticky session* mode. The load balancer
    will check if a `JSESSIONID` (or an SSL connection) exists and, if so, will store
    which node created it. If it sees again this identifier in a request, it will
    redirect the request to the same node ignoring any distribution strategy.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保负载均衡器在创建会话后始终击中相同的后端节点。这就是我们所说的**粘性会话**模式。负载均衡器将检查是否存在`JSESSIONID`（或SSL连接），如果存在，则存储创建它的节点。如果它再次在请求中看到此标识符，它将重定向请求到同一节点，忽略任何分布策略。
- en: This means that the sticky session mode is often coupled with another strategy,
    which will define the distribution, since the sticky session only applies once
    a request has already been served for a client.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着粘性会话模式通常与另一种策略相结合，该策略将定义分布，因为粘性会话仅在已经为客户端服务了一个请求之后才适用。
- en: The scheduling algorithm
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度算法
- en: 'The scheduling algorithm is a wide category of strategies based on some statistical
    criteria. The idea here is to be more accurate about the way load distribution
    is done regarding the available resources in the backend servers. The most common
    criteria are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 调度算法是基于某些统计标准的一类广泛策略。这里的想法是更准确地关于负载分布的方式，考虑到后端服务器上的可用资源。最常用的标准如下：
- en: '**By request**: The distribution is based on the number of requests served
    by the node. Note that each server node can have a weight associated with this
    distribution to bias the distribution if a machine is less powerful than the other.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按请求**：分布基于节点服务的请求数量。请注意，每个服务器节点可以与这种分布相关联一个权重，以偏置分布，如果一台机器不如其他机器强大。'
- en: '**By traffic**: This is the same sort of distribution as the previous one,
    but instead of counting the requests, it uses the transmitted bytes.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按流量**：这与先前的分布类似，但不是计数请求，而是使用传输的字节数。'
- en: '**By busyness**: This is also based on the number of requests, but only the
    live number. The least *busy* node is selected.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按繁忙程度**：这也是基于请求数量，但仅限于活动数量。选择最不繁忙的节点。'
- en: '**Heartbeat**: This is not a distribution solution per se but is more of an
    alternate evaluation solution. It uses a heartbeat or *agent* to evaluate the
    load a node has and, based on this information, it distributes to the node that
    can handle the most load. It is generally a time statistics, which is, therefore,
    dynamic and autoadaptive.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心跳**：这本身不是一个分发解决方案，而更像是一种替代评估解决方案。它使用心跳或*代理*来评估节点所承受的负载，并根据这些信息将负载分配给可以处理最多负载的节点。这通常是一种时间统计，因此它是动态和自适应的。'
- en: Load balancing or proxy – additional features
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡或代理 – 额外功能
- en: 'Even if you set up a load balancing to distribute the load with one of the
    previous strategies, a load balancer solution is often a full proxy and, therefore,
    also provides additional features. It generally concerns the server middlewares
    more than the client ones, but it remains very interesting. Part of the features
    that you can get from the backend (servers) point of view are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即使您设置了负载均衡以使用之前的一种策略来分配负载，负载均衡器解决方案通常是一个完整的代理，因此也提供了额外的功能。这通常更多地涉及服务器中间件而不是客户端，但它仍然非常有趣。从后端（服务器）的角度来看，您可以获得的部分功能如下：
- en: '**Compression:** Your backend server can serve plain text and the load balancer/proxy
    layer will automatically add GZIP compression on text resources (HTML, JavaScript,
    CSS, and so on). Since the client (browser) to load balancer communication is
    generally slower than the load balancer to server/backend communication, it will
    allow you to save precious time for these requests.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**压缩**：您的后端服务器可以提供纯文本，而负载均衡器/代理层将自动在文本资源（HTML、JavaScript、CSS等）上添加GZIP压缩。由于客户端（浏览器）到负载均衡器的通信通常比负载均衡器到服务器/后端通信慢，这将允许您为这些请求节省宝贵的时间。'
- en: '**TCP buffering:** Here, the idea is to buffer the response sent by the backend
    server in the load balancer layer to free the backend from this load and let it
    serve other requests. This is useful for slow clients that would hold a connection
    on the backend but not imply any processing/computing work.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TCP缓冲**：在这里，想法是在负载均衡器层缓冲后端服务器发送的响应，以减轻后端的负担，并让它处理其他请求。这对于缓慢的客户端很有用，它们会在后端保持连接，但不会引起任何处理/计算工作。'
- en: '**HTTP caching:** We saw, in the previous section, that HTTP defines some caching.
    The proxy layer can handle it for you for free without having to request the backend
    server. This generally concerns only static resources that are moved to the proxy
    layer in this condition.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP缓存**：在上一节中，我们看到了HTTP定义了一些缓存。代理层可以免费为您处理，而无需请求后端服务器。这通常仅涉及静态资源，在这种情况下被移动到代理层。'
- en: '**Encryption:** The proxy layer can encrypt a part of the request to prevent
    the end users from knowing enough about the backend to understand how it works
    or even access some sensitive data.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加密**：代理层可以加密请求的一部分，以防止最终用户了解足够关于后端的信息，从而理解其工作方式或甚至访问一些敏感数据。'
- en: 'When the load balancer layer adds features more business-oriented than communication/network-oriented,
    we often call it a *gateway.* However, technically, it is pretty much the same
    sort of middleware. Here are the features you can find in gateways:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当负载均衡器层添加比通信/网络导向更偏向于商业化的功能时，我们通常称之为*网关*。然而，从技术上来说，它基本上是一种类似的中间件。以下是您可以在网关中找到的功能：
- en: '**Security handling**: The load balancer layer can authenticate and validate
    the permissions from the request (generally from a header).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全处理**：负载均衡器层可以验证请求（通常来自标题）中的权限。'
- en: '**Version handling:** Depending on the incoming request, the route (requested
    endpoint of the backend) can change, allowing us to automatically handle the versioning
    of the backend points.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本处理**：根据传入的请求，路由（后端请求的端点）可以改变，使我们能够自动处理后端点的版本。'
- en: '**Rate limiting:** This limits the access to the backend with a particular
    rate, either by application or per user, if the authentication is associated with
    the rate limiting. It is generally expressed as the number of allowed requests
    per unit time, for example, 1,000 requests per minute.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速率限制**：这限制了后端访问的速率，无论是通过应用程序还是按用户，如果认证与速率限制相关联。这通常以每单位时间允许的请求数量来表示，例如，每分钟1,000个请求。'
- en: '**Concurrent limiting:** This controls the number of requests that can be sent
    in parallel/concurrently. As for rate limiting, it can be done for the entire
    application or per user (or other units if relevant). For example, 512 requests.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发限制**：这控制了可以并行/同时发送的请求数量。至于速率限制，它可以针对整个应用程序或每个用户（或相关其他单位）进行。例如，512个请求。'
- en: As you can see there are several features and all are not related to the performance.
    However, most of them will have performance impacts depending on your final environment.
    HTTP caching, for instance, will allow your backend server to handle more actual
    load and, therefore, will scale more easily. The rate/concurrent limiting features
    can enable you to control the performances and ensure that they are not degraded
    under unexpected load circumstances, but other features such as the security ones
    can have a very strong impact on your performance if the gateway layer can use
    a hardware encryption solution instead of a software encryption, like you would
    generally do in a Java EE application.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，有几个特性，而且它们都不与性能相关。然而，根据你的最终环境，大多数特性都会对性能产生影响。例如，HTTP缓存将允许你的后端服务器处理更多的实际负载，因此更容易进行扩展。速率/并发限制特性可以使你控制性能，并确保在意外负载情况下不会降低性能，但其他特性，如安全性特性，如果网关层可以使用硬件加密解决方案而不是通常在Java
    EE应用程序中使用的软件加密，可能会对你的性能产生非常强烈的影响。
- en: What is important to keep in mind here is to think of the application as a system
    solution and not to try to put everything inside the application just because
    it is easier or more portable. Relying on well-optimized hardware solutions will
    yield good performance compared with optimizing a software solution, which can
    lead you to use native integration, particularly, when it comes to security and
    cryptography,  and will affect the portability of your application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要记住的重要一点是将应用程序视为一个系统解决方案，而不是试图将所有内容都放在应用程序中，仅仅因为这样做更容易或更便携。依赖于经过良好优化的硬件解决方案，与优化软件解决方案相比，将产生良好的性能，尤其是在安全性和加密方面，这会影响你应用程序的可移植性。
- en: Failover
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 备用
- en: In a distributed system, it is very important to ensure that you know how to
    handle failures. Java EE applications being more and more connected to other systems,
    they face this challenge more and more, so it is important to know how you will
    deal with failover when it happens.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式系统中，确保你知道如何处理故障非常重要。Java EE应用程序越来越多地连接到其他系统，它们面临越来越多的挑战，因此了解在发生故障转移时如何处理非常重要。
- en: 'The first meaning of failover is, indeed, to fail over. It can be rephrased
    as *the capability to switch to a backup system when the primary system fails*. In
    Java EE applications, there are lots of places where this can be set up, but they
    are all related to external systems:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 备用的第一个含义确实是“故障转移”。它可以重新表述为*在主系统失败时切换到备用系统的能力*。在Java EE应用程序中，有很多地方可以设置这个，但它们都与外部系统相关：
- en: '**Databases**: If a database connection fails, how to still handle the requests?'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库**：如果数据库连接失败，如何仍然处理请求？'
- en: '**JMS**: If a broker fails, what to do?'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JMS**：如果代理失败，怎么办？'
- en: '**Other network API (such as SOAP or REST API)**: If the remote server is down,
    what to do?'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他网络API（如SOAP或REST API）**：如果远程服务器宕机，怎么办？'
- en: '**WebSocket**: If the target server closes the connection or fails, what to
    do?'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WebSocket**：如果目标服务器关闭连接或失败，怎么办？'
- en: In general, each time your application relies on something it doesn't control
    (a.k.a. external systems), it can need a plan B to still be functional if the
    primary solution is no more responding or working.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，每次你的应用程序依赖于它无法控制的东西（即外部系统），它可能需要一个备用计划，以便在主要解决方案不再响应或工作时仍然能够正常工作。
- en: There are several ways to handle the failovers, which either rely on selecting
    another system or are based on some default/caching implementation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 处理故障转移有几种方法，它们要么依赖于选择另一个系统，要么基于某些默认/缓存实现。
- en: Switching to another system
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切换到另一个系统
- en: The easiest implementation of a failover is switching to another system when
    an error occurs. This is what we saw in the previous section with load balancing.
    The only condition for us to be able to implement a failover is to be able to
    identify the error encountered by the system.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 备用的最简单实现是在发生错误时切换到另一个系统。这就是我们在上一节中看到的负载均衡。我们能够实施备用的唯一条件是能够识别系统遇到的错误。
- en: 'Here is an example using the JAX-RS client API of Java EE to illustrate this
    logic:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用Java EE的JAX-RS客户端API的示例来说明这个逻辑：
- en: '[PRE0]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This snippet replaces the direct call to the remote API with `Stream`. The
    usage of `Stream` is fancy here compared with the usage of `Collection`, since
    the leaf of the stream will trigger the flow to be executed (but by element) and
    will enable us to stop the *iteration* if we encounter a final condition early.
    Concretely, it prevents us from iterating over all the elements if irrelevant,
    which is exactly what we want for failover. In terms of implementation, here is
    the flow:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段用`Stream`替换了对远程API的直接调用。在这里使用`Stream`比使用`Collection`更复杂，因为流的叶子将触发（按元素）执行流程，并允许我们在遇到最终条件时提前停止*迭代*。具体来说，它防止我们在不相关的情况下迭代所有元素，这正是我们想要的故障转移。在实现方面，这里是流程：
- en: From a server, we invoke the endpoint we want.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从服务器端，我们调用我们想要的端点。
- en: We process the response of the server. If it requires a failover, we return
    `null`; otherwise, we keep the default behavior of the invocation.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们处理服务器的响应。如果它需要故障转移，我们返回`null`；否则，我们保持调用的默认行为。
- en: We remove the `null` response from the flow, since the previous step defines
    `null` as a failover condition.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从流程中移除了`null`响应，因为前一步定义`null`为故障转移条件。
- en: We use the first available response as being valid, which will avoid doing the
    invocation for all servers.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用第一个可用的响应作为有效响应，这将避免对所有服务器进行调用。
- en: If all the servers fail, then we throw `IllegalStateException`.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果所有服务器都失败，那么我们抛出`IllegalStateException`。
- en: 'What is missing in the previous snippet is the way to evaluate that we want
    a failover. In the previous code, we are basing this decision on `WebApplicationExceptinon`,
    so the client can throw in the case of an error. A default implementation of `supportsFailover()`
    will just be to return `true`, but we can be more fancy:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 之前片段中缺少的是评估我们想要故障转移的方式。在之前的代码中，我们是基于`WebApplicationExceptinon`来做出这个决定的，所以客户端可以在出错时抛出异常。`supportsFailover()`的默认实现只是返回`true`，但我们可以做得更复杂一些：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is still a simple implementation but we use the HTTP status code to retry
    only if its value is more than 412, which means we will not retry if we get HTTP
    404 (not found) or HTTP 412 (precondition failed), since both the cases will lead
    to sending the same request to another server and getting the same response.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个简单的实现，但我们使用HTTP状态码来重试，只有当其值大于412时，这意味着如果得到HTTP 404（未找到）或HTTP 412（预处理失败），我们不会重试，因为这两种情况都会导致向另一个服务器发送相同的请求并得到相同的响应。
- en: Of course, you can customize a lot of this logic (and this can even be service-dependent),
    but luckily, Java EE provides you with all you need to do.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以自定义很多这种逻辑（这甚至可能取决于服务），但幸运的是，Java EE为你提供了所有你需要的东西。
- en: Local fallback
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地回退
- en: The previous failover implementation was considering that there is an alternative
    system to contact if the primary one fails. This is not always the case and you
    may desire to replace a remote invocation by a local default in the case of an
    error. This sort of solution is available in the hystrix framework, which can
    be integrated with Java EE using the concurrency utilities that we saw earlier.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的故障转移实现是考虑到如果主系统失败，有一个替代系统可以联系。这并不总是情况，你可能在出错的情况下希望用本地默认值替换远程调用。这种解决方案在hystrix框架中可用，可以使用我们之前看到的并发工具与Java
    EE集成。
- en: 'The high-level logic of defaulting is as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 默认逻辑的高级逻辑如下：
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is actually a generalization of the previous implementation. Instead of
    seeing a list of hosts to contact, you need to consider the remote invocations
    that you can do as a list of tasks. Concretely, we can rewrite this as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是对之前实现的泛化。你不需要看到要联系的主机列表，而是需要将远程调用视为一系列任务。具体来说，我们可以这样重写：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we just moved the client invocation to a method and we replaced our stream
    of hosts by a stream of invocations. The stream logic is exactly the same—that
    is, it will take the first working result of the list.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是将客户端调用移动到方法中，并用调用流替换了我们的主机流。流逻辑完全相同——也就是说，它将取列表中的第一个有效结果。
- en: 'The direct gain of such a solution is that, since you are passing tasks to
    the failover logic and not hosts, you can implement every task as you wish. For
    instance, if you want to default to a hardcoded value, you can do this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案的直接收益是，由于你传递的是任务给故障转移逻辑，而不是主机，你可以按自己的意愿实现每个任务。例如，如果你想默认使用硬编码的值，你可以这样做：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With this definition, we will first try to contact `http://server1.company.com`,
    and if it fails, we will just create a `Data` instance locally.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此定义，我们首先尝试联系`http://server1.company.com`，如果失败，我们将在本地创建一个`Data`实例。
- en: Before seeing what type of strategy can be used for these fallbacks, let's just
    take a moment to see what it can mean in terms of code organization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到可以使用哪些策略进行这些回退之前，让我们先花一点时间看看它在代码组织方面可以意味着什么。
- en: Fallback code structure
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回退代码结构
- en: 'When you were just handling a failover across servers, it was not very complicated,
    as you would probably have the list of hosts in the `Client` class, and iterating
    over them was almost natural. Now that we iterate over different implementations,
    it is less natural and we need an *orchestrator* bean. Concretely, for the previous
    example, which first calls a remote service and then falls back on a local hardcoded
    instantiation, we would need the following:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当您只是处理服务器之间的故障转移时，它并不复杂，因为您可能已经在`Client`类中有主机列表，并且遍历它们几乎是自然的。现在，当我们遍历不同的实现时，这就不那么自然了，我们需要一个*编排器*豆。具体来说，对于之前的例子，它首先调用远程服务，然后回退到本地硬编码的实例化，我们需要以下内容：
- en: A remote client
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 远程客户端
- en: A local *mock* implementation
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地*模拟*实现
- en: A facade (orchestration) service, which is used everywhere, so we can leverage
    this failover logic
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个门面（编排）服务，它被用于各个地方，因此我们可以利用这个故障转移逻辑
- en: When you start integrating lots of services, it is not very convenient.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始集成大量服务时，这并不方便。
- en: Microprofile to the rescue
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Microprofile来拯救
- en: 'Microprofile includes in its scope a specification helping you to handle your
    fallback logic in a "standard" way. The specification allows to define a fallback
    method reference or handler on a method in case this last one fails. Here what
    it can look like:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Microprofile在其范围内包含一个规范，帮助您以“标准”方式处理您的回退逻辑。该规范允许在方法失败的情况下定义回退方法引用或处理程序。以下是一个例子：
- en: '[PRE5]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, you will call `getData` from all the consumers of the enclosing service,
    and if the method fails, the microprofile fallback handling will automatically
    call `getDataFallback`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您将从封装服务的所有消费者中调用`getData`，如果方法失败，微Profile回退处理将自动调用`getDataFallback`。
- en: This implementation also supports `@Retry`, which allows you to define how many
    times you execute the primary method (`getData` in the previous example) before
    falling back on the fallback handling/method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现还支持`@Retry`，它允许您定义在回退到回退处理/方法之前，您将执行主方法（在之前的例子中为`getData`）多少次。
- en: This API is nice and straightforward but couples the different implementations
    together, since the primary and secondary methods are linked with the `@Fallback`
    API.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个API很简洁，但将不同的实现耦合在一起，因为主方法和次要方法通过`@Fallback` API链接。
- en: Failover extension
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障转移扩展
- en: 'With CDI, you can define a small extension, which will automatically handle
    the failover, exactly as we did with streams previously, without much effort.
    The extension will be composed of two main parts:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CDI，您可以定义一个小扩展，该扩展将自动处理故障转移，就像我们之前处理流一样，无需太多努力。该扩展将由两个主要部分组成：
- en: It will identify all the implementations of a particular logic
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将识别特定逻辑的所有实现
- en: It will register a bean for chaining the implementations in the right order
    with the failover logic
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将注册一个豆，以正确顺序使用故障转移逻辑链接实现
- en: 'To do so, we need a few API elements:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要一些API元素：
- en: To find a service implementation, we will mark an interface method with `@Failoverable`
    to identify that we need to create a failover implementation for this interface;
    we will also use this annotation to mark the implementations.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了找到一个服务实现，我们将使用`@Failoverable`标记接口方法，以标识我们需要为该接口创建一个故障转移实现；我们还将使用此注解来标记实现。
- en: To sort the services, we will use `@Priority`. We will just use the priority
    value as a sorting order for the sake of simplicity.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了对服务进行排序，我们将使用`@Priority`。为了简单起见，我们将只使用优先级值作为排序顺序。
- en: 'In terms of what it will look like from a user''s point of view, here is the
    previous example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从用户的角度来看，这里有一个之前的例子：
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE7]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The interface defines the methods that we want to support failover. Then, we
    have two different implementations with their priority. Such an organization will
    allow us to add another strategy and insert it into the chain, easily and automatically,
    without having to modify all other implementations and respecting CDI loose coupling.
    Now, any user can just inject the `GetData` bean into any service and call `fetch()`
    with automatic failover.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接口定义了我们想要支持故障转移的方法。然后，我们有两种不同的实现及其优先级。这种组织方式将允许我们添加另一种策略并将其轻松自动地插入链中，而无需修改所有其他实现，并尊重CDI的松耦合。现在，任何用户都可以将`GetData`bean注入任何服务并调用`fetch()`以实现自动故障转移。
- en: This example doesn't define any parameter to the method, but this is not a limitation
    and you can use this strategy with any method, even very complex ones.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子没有为方法定义任何参数，但这并不是限制，你可以用这种策略使用任何方法，即使是复杂的。
- en: 'In terms of caller code, it really looks like any CDI bean invocation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用者代码方面，它看起来就像任何CDI bean调用：
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: And that's it! No need to implement the `GetData` failover for the end user;
    it is done by the extension.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！不需要为最终用户实现`GetData`故障转移；这是由扩展完成的。
- en: Now that we saw how the API looks, let's see how the CDI allows us to do it
    easily.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了API的样子，让我们看看CDI如何使我们轻松地做到这一点。
- en: 'The first step is to define our API; the only API that is not in Java EE is `@Failoverable`.
    It is a plain annotation without anything special, except that it must be usable
    on an interface:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义我们的API；唯一不在Java EE中的API是`@Failoverable`。它是一个普通的注解，没有特别之处，除了它必须能够应用于接口：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we just need an extension identifying the implementations of the interfaces
    decorated with this annotation, sorting them, and defining a bean for each interface:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需要一个扩展来识别带有此注解的接口的实现，对它们进行排序，并为每个接口定义一个bean：
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This extension has four main entry points:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此扩展有四个主要入口点：
- en: '`captureFailoverable`: This will ensure that any `@Failoverable` interface
    is registered and will automatically get a default implementation even if there
    is no service implementing it. It avoids having a `bean not found` error at the
    time of deployment and will instead throw our failover implementation exception
    to ensure a consistent exception handling for all our beans. Note that it only
    works if the scanning mode of the module containing the interface includes interfaces
    (that is, not `annotated` ). If not, we may get `UnsatisfiedResolutionException`
    or equivalent during deployment.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`captureFailoverable`：这将确保任何`@Failoverable`接口都被注册，并且即使没有服务实现它，也会自动获得默认实现。它避免了在部署时出现“bean未找到”错误，而是会抛出我们的故障转移实现异常，以确保对所有bean的一致异常处理。请注意，它仅在包含接口的模块的扫描模式包括接口（即，不是`annotated`）时才有效。如果不是，我们可能在部署期间遇到`UnsatisfiedResolutionException`或等效异常。'
- en: '`findService`: This captures all the implementations of a `@Failoverable` interface.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`findService`：这会捕获所有`@Failoverable`接口的实现。'
- en: '`addFailoverableImplementations`: This adds a bean with the `@Default` qualifier
    implementing the `@Failoverable` interface.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`addFailoverableImplementations`：这会添加一个带有`@Default`限定符的bean，实现`@Failoverable`接口。'
- en: '`addQualifier`: This just adds our `@Failoverable` API as a qualifier to avoid
    ambiguous resolutions, since all the services (implementations) will implement
    the same API and we want to use the `@Default` qualifier (implicit qualifier)
    for our facade. Note that we could have added `@Qualifier` on the annotation as
    well.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`addQualifier`：这只是在我们的`@Failoverable`API上添加了一个限定符，以避免模糊解析，因为所有服务（实现）都将实现相同的API，我们希望使用`@Default`限定符（隐式限定符）来使用我们的门面。请注意，我们也可以在注解上添加`@Qualifier`。'
- en: Also, to register this extension, don't forget to create a `META-INF/services/javax.enterprise.inject.spi.Extension`
    file containing the fully qualified name of the class.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了注册此扩展，别忘了创建一个包含该类完全限定名称的`META-INF/services/javax.enterprise.inject.spi.Extension`文件。
- en: 'The implementation of the facade bean is done with a proxy. All the failover
    logic will be passed to the handler, which takes, as input, a list of delegates
    that are actually the implementations we identified in `findService`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 门面bean的实现是通过代理完成的。所有故障转移逻辑都将传递给处理程序，处理程序作为输入接收一个代表我们在`findService`中确定的实现者的代理列表：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This implementation is probably the most straightforward:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现可能是最直接的：
- en: The list of delegates is already sorted (see the previous extension).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理列表已经排序（参见上一个扩展）。
- en: It iterates over the delegate and tries to do the invocation for each of them;
    the first one to succeed provides the returned value.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它遍历代理并尝试对每个代理进行调用；第一个成功的是返回值。
- en: If no invocation succeeds, then `FailoverException` is thrown, which includes
    the case where no implementation was provided (that is, the `delegates` list is
    empty).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果没有调用成功，则抛出`FailoverException`，这包括没有提供实现的情况（即`delegates`列表为空）。
- en: If an exception is thrown, it is tested to see if the failover should occur
    and the next delegate should be used. In this implementation, it is done by ensuring
    that the exception has `@Failoverable` on it, but it could also test some well-known
    exceptions such as `WebApplicationException` or `IllegalStateException`, for instance.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果抛出异常，会测试是否应该发生故障转移，并使用下一个代理。在这个实现中，这是通过确保异常上有`@Failoverable`注解来完成的，但也可以测试一些众所周知的异常，例如`WebApplicationException`或`IllegalStateException`等。
- en: Fallback handling – caching, an alternative solution
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回退处理 - 缓存，一种替代解决方案
- en: In the previous subsection, we saw how to handle a fallback using another strategy,
    which can be a hardcoded default value, or can be an alternative way to compute
    the service, including how to contact another service from another provider potentially.
    This is the straightforward implementation of a failover.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们看到了如何使用另一种策略来处理回退，这可以是一个硬编码的默认值，或者可以是一种计算服务的方法，包括如何从另一个提供商联系另一个服务。这是故障转移的直接实现。
- en: However, if you step back and think about why you set up some failover mechanisms,
    you'll realize that it was to ensure your service can run even if an external
    system is down. Therefore, there is another solution, which is not a failover,
    strictly speaking, but fulfills the same goal, that is, caching. We saw in a previous
    section how JCache can help your application go faster, enabling you to bypass
    computation. Caching data from external systems also allows you to be more resilient
    and can potentially prevent you from implementing a failover mechanism for them.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你退一步思考你为什么要设置一些故障转移机制，你会意识到这是为了确保即使外部系统出现故障，你的服务也能运行。因此，还有一个解决方案，严格来说它不是一个故障转移，但实现了相同的目标，那就是缓存。在前一节中，我们看到了JCache如何帮助你的应用更快地运行，使你能够绕过计算。从外部系统缓存数据也使你更加健壮，并且可能防止你为它们实现故障转移机制。
- en: Let's take a very simple case to illustrate this. In our quote application ([Chapter
    1](f8931396-0636-41a9-8bf7-2b67bb424b76.xhtml), *Money – The Quote Manager Application*),
    we grab the list of symbols to use from CBOE and query Yahoo!Finance to get the
    price of each quote. If one of the two services is down, then we don't get any
    price update. However, if we've already executed this logic once, then the price
    and the list of symbols will be in our database, which is a sort of *persistent
    caching*. This means that our application, which serves the price of quotes through
    our JAX-RS endpoint, will still work for clients even if the background update
    process fails. If we want to go one step further, we can update this logic to
    fall back on selecting all the symbols of the database if the CBOE service is
    no more available, which will allow the application to at least get price updates
    and still be more accurate than if we fail the whole update process because CBOE
    is down but not Yahoo!Finance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个非常简单的例子来说明这一点。在我们的报价应用（[第1章](f8931396-0636-41a9-8bf7-2b67bb424b76.xhtml)，*货币
    - 报价管理应用*）中，我们从CBOE获取要使用的符号列表，并查询Yahoo!Finance以获取每个报价的价格。如果这两个服务中的任何一个出现故障，那么我们就不会收到任何价格更新。然而，如果我们已经执行过这个逻辑一次，那么价格和符号列表将存储在我们的数据库中，这可以看作是一种*持久缓存*。这意味着即使后台更新过程失败，我们的应用仍然可以通过我们的JAX-RS端点为客户端提供服务。如果我们想更进一步，可以将这个逻辑更新为在CBOE服务不再可用时回退到选择数据库中的所有符号，这将允许应用至少获取价格更新，并且比如果整个更新过程因为CBOE故障而失败要更准确。
- en: More generally, if a remote system is not fully reliable and data can be cached,
    which implies that the data is regularly (re)used and can be a bit outdated for
    your business, then caching is a very good alternative to failover.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地说，如果一个远程系统并不完全可靠，并且数据可以被缓存，这意味着数据会被定期（重新）使用，并且对于你的业务来说可能有点过时，那么缓存是故障转移的一个非常好的替代方案。
- en: 'In terms of implementation, you have two main options:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，你有两个主要选项：
- en: Manually handle the cache and the fallback (not recommended)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动处理缓存和回退（不推荐）
- en: Use the cache as a data source and fall back on the external system if the data
    is outdated or missing
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用缓存作为数据源，如果数据过时或缺失则回退到外部系统
- en: The first option is plain failover handling but the fallback implementation
    is based on cache access. This solution, indeed, considers that you'll fill the
    cache when the primary source works; otherwise, the fallback will just return
    `null`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项是直接的故障转移处理，但回退实现基于缓存访问。实际上，这种解决方案认为您将在主源工作时会填充缓存；否则，回退将只返回`null`。
- en: The second option can be implemented through the solutions we saw in the previous
    part, either using JCache CDI integration or the `Cache` API as the primary source
    manually in your application. You will note that this reverses the failover paradigm,
    as the primary source (remote system) becomes secondary because the cache is checked
    first. But that's how caching works and if the remote system supports caching,
    you will, most of the time, get more benefit from it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项可以通过我们在上一部分看到的解决方案实现，无论是使用JCache CDI集成还是将`Cache` API作为应用程序中的主源手动使用。您会注意到，这反转了故障转移范式，因为主源（远程系统）成为次要的，因为首先检查缓存。但这就是缓存的工作方式，如果远程系统支持缓存，您将大多数时候从中获得更多的好处。
- en: 'To provision the cache, you can use the `@CacheResult` API but don''t forget
    to add `skipGet=true` to just provision the cache and not bypass the logic. For
    instance, here is what it can look like:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要提供缓存，您可以使用`@CacheResult` API，但不要忘记添加`skipGet=true`以仅提供缓存而不绕过逻辑。例如，它可以看起来像这样：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Enforcing JCache to skip the get phase of the interceptor associated with `@CacheResult`
    enables you to put the result in the cache when the method succeeds, but to not
    use the cached data if it is already in the cache. Therefore, if this service
    is chained with a fallback service reading the data from the cache, it will correctly
    implement the failover based on the cached data.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 强制JCache跳过与`@CacheResult`关联的拦截器的获取阶段，使您在方法成功时可以将结果放入缓存，但如果结果已经在缓存中，则不使用缓存数据。因此，如果此服务与一个后备服务链式连接，该后备服务从缓存中读取数据，它将正确实现基于缓存数据的故障转移。
- en: 'However, note that there is a trick here—you need to use the right cache name
    and key. To do so, don''t hesitate to just use another method relying on JCache
    as well:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意这里有一个技巧——您需要使用正确的缓存名称和键。为此，不要犹豫，也可以使用另一个依赖于JCache的方法：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The implementation is quite straightforward; it returns `null` to represent
    it doesn't have any data if it is not already in the cache. An alternative implementation
    could be to throw an exception, depending on the caller behavior you want to provide.
    Then, to ensure that we use the same cache as the previous primary service, we
    name the cache with the previous method's name. Here, we used the default name
    for the primary service and set this name to the secondary service but you can
    also use a more business-oriented cache name through the `cacheName` configuration
    that we saw in the [Chapter 6](8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml), *Be
    Lazy; Cache Your Data*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 实现相当简单；如果它不在缓存中，则返回`null`来表示没有数据。另一种实现可能是抛出异常，具体取决于您想提供的调用者行为。然后，为了确保我们使用与之前主服务相同的缓存，我们使用之前方法的名字命名缓存。在这里，我们使用了主服务的默认名称，并将其设置为次要服务，但您也可以使用更面向业务的缓存名称，通过我们在[第6章](8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml)中看到的`cacheName`配置来实现。*懒一点；缓存你的数据*。
- en: 'Now, if we go back to the caching-first solution, reversing the primary and
    secondary sources, we can implement it a bit differently. If the cache is the
    source, we can still use the CDI integration, but the provisioning of the cache
    (which is the secondary source now) can be done through a native JCache mechanism.
    Concretely, our service can look as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们回到以缓存为第一的解决方案，反转主次源，我们可以稍作不同的实现。如果缓存是源，我们仍然可以使用CDI集成，但缓存的提供（现在是次要源）可以通过本地的JCache机制完成。具体来说，我们的服务可以看起来如下：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the standard way of using it, but there is an alternative way to do
    it that would also work better with manual cache handling—that is, without CDI
    integration. Instead of using the method as a fallback and cache its result, we
    programmatically set the way the cache is lazily provisioned when configuring
    the cache. In this case, our service can become the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用它的标准方式，但还有一种替代方式可以更好地与手动缓存处理配合使用——即不使用CDI集成。我们不是将方法作为后备并缓存其结果，而是在配置缓存时程序化地设置缓存懒加载的方式。在这种情况下，我们的服务可以变成以下这样：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Yes, you saw it correctly: we don''t even implement the loading logic into
    the service! So where does it go? This service will trigger `cache.get(...)`,
    so we need to inject our data when `get()` is called if the data is not already
    available. To do so, we can use the `CacheLoader` API, which is initialized on
    the cache itself.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，你正确地看到了：我们甚至没有在服务中实现加载逻辑！那么它去哪里了？这个服务将触发 `cache.get(...)`，所以当数据不可用时，我们需要在调用
    `get()` 时注入我们的数据。为此，我们可以使用 `CacheLoader` API，该 API 在缓存本身上初始化。
- en: 'To configure the cache, you can use a custom `CacheResolver` (see the previous
    chapter for more details), which will set `CacheLoader` into the cache configuration:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置缓存，你可以使用自定义的 `CacheResolver`（有关更多详细信息，请参阅上一章），它将 `CacheLoader` 设置到缓存配置中：
- en: '[PRE17]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The loader implementation can now be the following:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 加载器的实现现在可以是以下内容：
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `loadAll` method just delegates to the `load` method, so it is not very
    interesting, but in some cases you can bulk-load multiple values at once and it
    makes sense to have a different implementation. The `load` method delegates the
    loading to a CDI bean. We can consider that we call the remote service here without
    any failover.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`loadAll` 方法只是委托给 `load` 方法，所以它并不很有趣，但在某些情况下，你可以一次性批量加载多个值，并且有不同实现是有意义的。`load`
    方法将加载委托给 CDI 容器。我们可以认为在这里我们调用远程服务时没有进行故障转移。'
- en: The important point for this solution is to have a custom `GeneratedKey` key
    to be able to unwrap it and extract the business key (`extractSymbol()` in the
    previous example) to be able to execute the actual business. As a quick reminder
    of the previous chapter, `GeneratedKey` is the key deduced from the method signature
    in JCache CDI integration, so you need to ensure you can work with such a key using `@CacheResult`.
    As we saw in [Chapter 6](8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml), *Be Lazy;
    Cache Your Data*, using a custom `CacheKeyGenerator` allows you to fulfill this
    requirement for this solution.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的重要点是有一个自定义的 `GeneratedKey` 键，以便能够解包它并提取业务键（在先前的示例中是 `extractSymbol()`），以便能够执行实际业务。作为对上一章的快速回顾，`GeneratedKey`
    是从 JCache CDI 集成中的方法签名中推导出的键，因此你需要确保你可以使用 `@CacheResult` 与此类键一起工作。正如我们在[第6章](8db12a5f-dba9-449b-af38-4963ac0adec0.xhtml)中看到的，*要懒散；缓存你的数据*，使用自定义的
    `CacheKeyGenerator` 允许你满足这个解决方案的要求。
- en: 'In terms of usage, when should you use `CacheLoader` instead of a method implementation
    that behaves as an implicit cache loader after all? The cache loader makes more
    sense when you don''t use the CDI integration because, in such a case, you manipulate
    a more natural key (such as a string for symbols) and get the same behavior:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用方面，你何时应该使用 `CacheLoader` 而不是表现得像隐式缓存加载器的方法实现？当你不使用 CDI 集成时，缓存加载器更有意义，因为在这种情况下，你操作一个更自然的键（例如，对于符号的字符串）并得到相同的行为：
- en: '[PRE19]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If the cache is set up to load the data from a remote service if not already
    present in the cache, then the second line of this snippet will call the remote
    service and transparently initialize the cache with the data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缓存被设置为在缓存中不存在数据时从远程服务加载数据，那么这个片段的第二行将调用远程服务并透明地使用数据初始化缓存。
- en: This kind of caching usage also works in the case of a remote service, which
    is rate-limited. It will allow you to rely on its data more than you would be
    allowed to without a cache. For instance, if the service accepts only 1,000 requests
    per minute with your credentials, you can, with the cache, call it 10,000 times
    per minute.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种缓存使用方式也适用于远程服务的情况，该服务受到速率限制。这将允许你比没有缓存时更多地依赖其数据。例如，如果服务每分钟只接受1000次带有你的凭证的请求，那么使用缓存，你每分钟可以调用它10000次。
- en: Circuit breaker
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 断路器
- en: The circuit breaker involves allowing the application to disable a code path
    if it is known or estimated as failing.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器涉及允许应用程序在已知或估计为失败时禁用代码路径。
- en: 'For instance, if you call a remote service and this service has failed 10 times
    already, then you can say: *don''t call this service anymore for 5 minutes*. The
    main idea is to bypass errors when possible.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你调用一个远程服务并且这个服务已经失败了10次，那么你可以这样说：*不要再次调用这个服务5分钟*。主要思想是在可能的情况下绕过错误。
- en: 'A circuit breaker generally has three states:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器通常有三个状态：
- en: '**CLOSED**: The system is considered to be working, so use it (default case).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关闭（CLOSED）**：系统被认为是正常工作的，所以使用它（默认情况）。'
- en: '**OPEN**: The system is considered not working, so bypass it.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**打开（OPEN）**：系统被认为是无法工作的，所以绕过它。'
- en: '**HALF-OPEN**: The system must be reevaluated. Try an invocation: if it fails,
    go back to the OPEN state; otherwise, go to the CLOSED state.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半开式（HALF-OPEN）**：系统必须重新评估。尝试一个调用：如果失败，则回到**开放（OPEN）**状态；否则，进入**关闭（CLOSED）**状态。'
- en: Then, all the conditions to go from a state to the other are configurable. For
    instance, what triggers a CLOSED state depends on the way you configure it (it
    can be an exception, an HTTP status, a timeout, and so on). The same applies for
    when the system enters into the HALF-OPEN state—it can be a timeout, a number
    of requests, and so on.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从一个状态到另一个状态的所有条件都是可配置的。例如，触发**关闭（CLOSED）**状态的是什么取决于您如何配置它（可以是异常、HTTP状态、超时等）。当系统进入**半开（HALF-OPEN）**状态时，情况也适用——可以是超时、请求数量等。
- en: 'There are multiple available implementations of circuit breaks but the most
    known ones for Java EE are in these projects:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种断路器实现方式可用，但Java EE中最知名的是这些项目：
- en: Hystrix
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hystrix
- en: Failsafe
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Failsafe
- en: Microprofile fault-tolerant specification
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Microprofile fault-tolerant specification`'
- en: commons-lang3 project.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commons-lang3`项目。'
- en: 'Using a circuit breaker is important for the system health to ensure that your
    system is always healthy even if one functionality is not. However, it can also
    be used for performance because it will keep them under control if the system
    starts failing and avoid a domino effect where each connection between a failing
    system and another system implies the other system to fail too. To ensure that
    the impact of the circuit breaker is as expected, you need to associate two solutions
    with it:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用断路器对于确保系统健康非常重要，以确保即使某个功能不正常，系统也能始终保持健康。然而，它也可以用于性能，因为它会在系统开始失败时保持它们受控，避免连锁反应，即一个失败系统与另一个系统之间的每个连接都意味着另一个系统也会失败。为了确保断路器的影响符合预期，您需要将其与两个解决方案相关联：
- en: A failover solution to ensure that your system behaves correctly (as much as
    possible, since it is not always feasible)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个故障转移解决方案，以确保您的系统（尽可能）正确地运行。
- en: A monitoring solution to ensure that you properly report that you are no more
    fully functional to let the support/operation team efficiently work and enable
    your circuit breaker to automatically recover once the failing system is fixed
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个监控解决方案，以确保您正确报告您不再完全功能正常，以便支持/运维团队能够高效工作，并使您的断路器在失败系统修复后自动恢复。
- en: Bulk head
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bulk head
- en: Bulk head is a pattern designed to ensure that no part of the system deeply
    impacts other system areas. The name comes from a common solution used in ships
    to ensure that if there is a hole in the hull, it doesn't lead to the boat sinking.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Bulk head是一种设计模式，旨在确保系统的任何部分都不会对其他系统区域产生深远影响。这个名字来源于在船只上常用的一个解决方案，以确保如果船体有洞，它不会导致船沉没。
- en: '![](img/c266fe64-4177-4da8-961b-1552fcdb04ce.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c266fe64-4177-4da8-961b-1552fcdb04ce.jpg)'
- en: Here, for instance, the second area has a hole and the water is coming into
    the section but the boat will not sink, as the other sections are isolated.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里第二个区域有一个洞，水进入了该部分，但船不会沉没，因为其他部分是隔离的。
- en: The Titanic used this technique but the isolation was not fully done from down
    to top for passengers and crew members comfort. And we all know the outcome of
    that choice. That is why, if you go with isolation, it is important to make sure
    that it is complete; otherwise better not do anything.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号使用了这种技术，但隔离并没有从底部到顶部完全完成，以保障乘客和船员舒适。我们都知道那个选择的结果。这就是为什么，如果您选择隔离，确保它是完整的很重要；否则最好不要做任何事情。
- en: What does it mean for an application? It means that each service that can sink
    your application should be isolated from the other ones. This isolation is mainly
    about the execution and, therefore, the execution environment of the service invocation.
    Concretely, it is generally about which pool (thread, connection, and so on) and
    which context to load. To be able to isolate services, you need to be able to
    identify which service you are calling. If we take our quote manager application,
    we can identify the *quote finder* service, which can be isolated from the *quote
    update* service, for instance. This is a business criteria for the isolation,
    but in practice, you will often want to go further to isolate the execution of
    your services, including the use of the tenants. It is not rare to want to use
    a different pool for different tenants. This is actually often even related to
    different contracts clauses.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应用程序意味着什么？这意味着每个可能导致你的应用程序崩溃的服务都应该与其他服务隔离开。这种隔离主要关于执行和因此服务调用的执行环境。具体来说，通常关于哪个池（线程、连接等）和哪个上下文要加载。为了能够隔离服务，你需要能够识别你正在调用的服务。如果我们以我们的报价管理应用程序为例，我们可以识别出*quote
    finder*服务，它可以与*quote update*服务隔离开，例如。这是隔离的业务标准，但在实践中，你通常会想更进一步，隔离服务的执行，包括使用租户。使用不同池为不同租户是常见的情况。这实际上通常与不同的合同条款有关。
- en: 'To illustrate this concept in a Java EE application, we will update our `QuoteResource#findId`
    method to apply this pattern. The first step will be to isolate the invocation
    from the servlet container context/threads. To do so, we will make the method
    asynchronous, using the JAX-RS `@Suspended` API and a Java EE concurrency utilities
    thread pool:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Java EE应用程序中说明这个概念，我们将更新我们的`QuoteResource#findId`方法以应用此模式。第一步将是将调用与servlet容器上下文/线程隔离开。为此，我们将使方法异步，使用JAX-RS的`@Suspended`
    API和Java EE并发实用工具线程池：
- en: '[PRE20]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we created and configured, into the container, a dedicated pool for a
    part of the application called `threads/quote-manager/quote/findById`. The `findById`
    method executes its original logic in a task submitted to this dedicated pool.
    Using the JAX-RS asynchronous API, we manually `resume` the request response,
    since the container doesn't handle the execution of the logic anymore, but we
    do it ourselves.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建并配置了一个专用的池，用于应用的一部分，称为`threads/quote-manager/quote/findById`。`findById`方法在这个专用池中执行其原始逻辑。使用JAX-RS异步API，我们手动`resume`请求响应，因为容器不再处理逻辑的执行，而是我们自己执行。
- en: This implementation works only if the thread pool has a maximum size to ensure
    that the execution is controlled. If you use an unbounded thread pool, this will
    not help control your application at all.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现仅在线程池有一个最大大小时才有效，以确保执行受控。如果你使用无界线程池，这根本不能帮助你控制应用程序。
- en: There are other ways to implement a bulkhead not relying on different threads,
    such as using `Semaphore` as we saw in the threading chapter, but they don't allow
    you to isolate the application logic from the container threads. Thus, it can
    have side-effects on the overall application (or even cross-applications if you
    use the same HTTP container thread pool). The main advantage of using a no thread-related
    implementation is that it is generally faster even if it doesn't isolate the executions
    as well as the thread-based implementation. Here again, make sure to benchmark
    your application to know which implementation best fits your case.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他方法实现bulkhead，不依赖于不同的线程，例如使用我们在线程章节中看到的`Semaphore`，但它们不允许你将应用程序逻辑与容器线程隔离开。因此，它可能会对整体应用程序（或者如果你使用相同的HTTP容器线程池，甚至可能是跨应用程序）产生副作用。使用无线程相关实现的主要优势是它通常更快，即使它没有像基于线程的实现那样隔离执行。在这里，再次确保对应用程序进行基准测试，以了解哪种实现最适合你的情况。
- en: Timeouts
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超时
- en: The last and very important criteria to ensure control over the performance
    and to ensure that the performance is bounded (your application doesn't start
    being very slow) is related to timeouts.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 确保对性能的控制并确保性能有界（你的应用程序不会变得非常慢）的最后一个非常重要的标准与超时有关。
- en: 'An application has timeouts everywhere even if you don''t always see them:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你并不总是看到它们，应用程序到处都有超时：
- en: The HTTP connector, or any network connector in general, has timeouts to force
    the release of clients connected for too long.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP连接器，或者更一般地说，任何网络连接器都有超时设置，以强制释放长时间连接的客户端。
- en: Databases generally have timeouts as well. It can be a client-side (network)
    timeout or a server-side setting. For instance, MySQL will cut any connection
    that lasts for more than 8 hours by default.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库通常也有超时。这可能是一个客户端（网络）超时或服务器端设置。例如，MySQL默认情况下会切断任何持续超过8小时的联系。
- en: Thread pools can handle timeouts if an execution is too long.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程池可以处理执行时间过长的超时。
- en: The JAX-RS client supports vendor-specific timeout configuration to avoid blocking
    the network later.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX-RS客户端支持供应商特定的超时配置，以避免稍后阻塞网络。
- en: Configuring timeouts enables you to ensure that if something starts being wrong
    in your system, including a remote system being slow or unresponsive, you will
    be able to respond in a correct (or, at least, bounded) duration. Of course, you
    will cumulate all the timeouts in the worst case, considering you use them synchronously
    and not concurrently, but at least, you will know the maximum duration a request
    can take in your system.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 配置超时可以确保如果系统中的某些事情开始出错，包括远程系统运行缓慢或无响应，你将能够以正确（或者至少是有界的）持续时间做出响应。当然，在考虑你使用它们是同步而不是并发的情况下，你将累积所有超时，但至少，你将知道请求在你的系统中可以持续的最大持续时间。
- en: Timeouts for code without timeouts
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无超时代码的超时
- en: 'A trick for adding timeouts to methods that weren''t designed to handle timeouts
    is to use a thread pool. A thread pool allows you to execute tasks and wait for
    them for a certain duration. Of course, it means that you will block the calling
    thread, but you will block it for a certain amount of time:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为那些未设计为处理超时的方法添加超时的一个技巧是使用线程池。线程池允许你执行任务并在一定时间内等待它们。当然，这意味着你将阻塞调用线程，但你将阻塞它一段时间：
- en: '[PRE21]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This code will compute some value or throw `TimeoutException` if it lasts for
    more than 10 seconds. Wrapping any code inside such a block will allow you to
    handle a timeout for any sort of code. However, this doesn''t mean that the wrapped
    code ends after 10 seconds; it just means that the caller doesn''t wait for it
    anymore. The task, however, is still submitted and can last forever taking a thread.
    To stop you from needing to keep a reference on `Future`, the `submit` method
    will return and cancel the task, allowing you to interrupt the execution thread:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将计算某个值或如果持续超过10秒则抛出`TimeoutException`。将任何代码包裹在这样的代码块中将允许你为任何类型的代码处理超时。然而，这并不意味着包裹的代码在10秒后结束；它只是意味着调用者不再等待它。然而，任务仍然被提交，并且可以无限期地占用一个线程。为了防止你需要保持对`Future`的引用，`submit`方法将返回并取消任务，允许你中断执行线程：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, if we get a timeout, we can cancel the running task, so as to not leak
    tasks, and potentially fill the thread pool even if we have timeouts.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们遇到超时，我们可以取消正在运行的任务，以避免任务泄露，并且即使我们有超时，也可能填满线程池。
- en: If you want to handle some failover on a timeout, you can add it in a block
    that catches `TimeoutException`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在超时时处理一些故障转移，你可以在捕获`TimeoutException`的代码块中添加它。
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw that properly handling faults in an application is the
    key to ensuring that you can still control the response time of your system and
    keep it under control in any circumstance. This is true for any system but with
    the spreading of microservices, it is even more true for systems using this architecture.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了正确处理应用程序中的故障是确保你可以继续控制系统的响应时间并在任何情况下保持其控制的关键。这对于任何系统都是正确的，但随着微服务的普及，对于使用这种架构的系统来说更是如此。
- en: Well, defining how your Java EE application is deployed and what is needed to
    ensure you control all the potential failures of your system is a full time job;
    we often forget to work on the *main* codepath of our applications, but doing
    so ensures that the production deployment and behavior are sane.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，定义如何部署你的Java EE应用程序以及确保你控制你系统的所有潜在故障是一项全职工作；我们常常忘记工作在应用程序的*主要*代码路径上，但这样做可以确保生产部署和行为是合理的。
- en: This chapter showed you some common patterns used to handle fault tolerance
    more or less transparently and reliably.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了处理容错的一些常见模式，这些模式在某种程度上是透明和可靠的。
- en: 'Logging is another important factor when issues start popping up in a system.
    It will allow you to investigate what happened and identify the issue to fix it.
    However, logging too much or without reflection can be very costly. This is what
    our next chapter will be about: ensure that you are correctly leveraging your
    logging regarding the performance factor.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 记录日志是系统出现问题时另一个重要的因素。它将使你能够调查发生了什么，并确定需要修复的问题。然而，过度记录日志或缺乏反思可能会非常昂贵。这正是我们下一章将要讨论的内容：确保你正确地利用日志来考虑性能因素。
