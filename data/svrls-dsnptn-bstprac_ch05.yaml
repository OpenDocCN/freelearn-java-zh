- en: Scaling Out with the Fan-Out Pattern
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用扇出模式进行横向扩展
- en: The next turn in our serverless journey takes us away from web-centric patterns
    and towards those suitable for a variety of problems, web and otherwise. In this
    chapter, we'll discuss the fan-out pattern, which may be used in many different
    contexts, either by itself as a standalone system or within a larger project as
    a sub-unit. Conceptually, the fan-out pattern is precisely what it sounds like—one
    serverless entry point results in multiple invocations of downstream systems.
    Big data platforms and computer science algorithms have been using this trick
    for a very long time; by taking a sizable computational problem and breaking it
    into smaller pieces, a system can get to the result faster by working on those
    smaller pieces concurrently. Conceptually, this is precisely how MapReduce works
    in the mapping step.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的无服务器之旅中，下一步将离开以网络为中心的模式，转向适合各种问题的模式，无论是网络还是其他。在本章中，我们将讨论扇出模式，该模式可用于许多不同的上下文，无论是作为一个独立系统还是作为更大项目的一个子单元。从概念上讲，扇出模式正是其名称所暗示的——一个无服务器入口点导致下游系统的多个调用。大数据平台和计算机科学算法已经使用了很长时间的这个技巧；通过将一个大的计算问题分解成更小的部分，系统可以通过同时处理这些较小的部分来更快地得到结果。从概念上讲，这正是MapReduce在映射步骤中工作的方式。
- en: In this chapter, we will discuss how to split a single unit of work into multiple
    smaller groups of work using the fan-out pattern. We will go through use cases
    for this pattern and the various problems for which it's well suited.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何使用扇出模式将单个工作单元拆分为多个较小的作业组。我们将探讨此模式的使用案例以及它非常适合的各种问题。
- en: 'By the end of this chapter, you can expect to know the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你可以期待了解以下内容：
- en: How to set up a fan-out architecture for resizing images in parallel
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何设置扇出架构以并行调整图像大小
- en: How to use the fan-out pattern to split a single large input file into smaller
    files and process those pieces in parallel
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用扇出模式将单个大型输入文件拆分为较小的文件，并并行处理这些片段
- en: What types of problem is the fan-out pattern is suitable for?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扇出模式适用于哪些类型的问题？
- en: System architecture
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统架构
- en: In many ways, this is the most straightforward pattern covered in this book.
    A single entry point, whether it be an HTTP request, some event notification,
    or anything else supported on your cloud provider of choice, triggers multiple invocations of
    some other serverless function in parallel. What one gains in this architecture
    is parallelism and hence speed. Our first example is one which is easy to understand
    and which you can view as the `Hello World` of serverless architectures.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，这是本书中涵盖的最直接的模式。一个入口点，无论是HTTP请求、某些事件通知还是你选择的云提供商支持的任何其他内容，都会触发对其他服务器端函数的多个并行调用。在这种架构中，你获得的是并行性和因此速度的提升。我们的第一个例子是容易理解的一个，你可以将其视为无服务器架构的“Hello
    World”。
- en: 'Imagine a system which takes an image and creates multiple versions of the
    original image with different sizes smaller than the original. How can this be
    solved at its simplest? Once a user uploads an image, our system notices the new
    image upload and, using a `for` loop, iterates and creates the various thumbnails.
    Some fictitious code to do this may look like the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个系统，它接收一个图像并创建多个不同尺寸的原始图像版本，这些尺寸小于原始图像。如何以最简单的方式解决这个问题？一旦用户上传了图像，我们的系统就会注意到新的图像上传，并使用`for`循环迭代并创建各种缩略图。执行此操作的虚构代码可能如下所示：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This can work just fine but runs the risk of slowing down drastically as a
    single process is in charge of the entire pipeline for one image. Logically speaking,
    each resize event is completely independent, only depending on the original image
    to perform its task. As such, this is a perfect task to run in parallel. The following
    diagram shows the general design for the fan-out pattern, where a single entry
    point triggers multiple downstream processes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以很好地工作，但存在风险，即单个进程负责整个图像的整个管道，可能会大幅降低速度。从逻辑上讲，每个调整大小的事件都是完全独立的，仅依赖于原始图像来完成其任务。因此，这是一个并行运行的完美任务。以下图表显示了扇出模式的通用设计，其中单个入口点触发多个下游进程：
- en: '![](img/94ee5054-cece-42f2-b8f4-547b50747179.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![系统架构](img/94ee5054-cece-42f2-b8f4-547b50747179.jpg)'
- en: Some event or trigger will result in a call to an entry point function. In the
    image resize example, this event occurs when an image is uploaded to an AWS S3
    bucket. Admittedly, setting this up is very simple, and AWS makes the invocation
    of Lambda functions quite easy due to all their cross-service integrations. However,
    one may apply this pattern to any cloud provider, and the trigger could very well
    be the uploading of an image over an `HTTP POST` rather than to an S3 bucket.
    Once the entry function is invoked, it will be responsible for triggering multiple
    `worker` functions, passing them the needed data to do their jobs. The key to
    this entire architecture is that the triggering of the worker processes occurs
    in such a way that they all run in parallel, or as close to parallel as possible.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 某些事件或触发器将导致调用入口函数。在图像缩放示例中，这个事件发生在图像上传到AWS S3存储桶时。诚然，设置这个非常简单，AWS通过所有跨服务集成使得Lambda函数的调用变得非常容易。然而，可以将这种模式应用于任何云提供商，触发器可能是通过`HTTP
    POST`上传图像，而不是上传到S3存储桶。一旦入口函数被调用，它将负责触发多个`worker`函数，并将所需数据传递给它们以完成工作。这个架构的关键在于触发工作进程的方式，使得它们都能并行运行，或者尽可能接近并行。
- en: Synchronous versus asynchronous invocation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步调用与异步调用
- en: The sole job of the entry point function is to initiate the fan-out and distribute
    work to multiple sub-functions. With this design, it's important to remember our
    goal, to parallelize the workload such that all of the worker functions are running
    in parallel. Just as our naive implementation with a `for` loop works synchronously,
    it's entirely possible to attempt to build an architecture as pictured above,
    but wind up with one which is synchronous. How can this happen?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 入口函数的唯一任务是启动扇出（fan-out）并将工作分配给多个子函数。在这种设计中，记住我们的目标很重要，即并行化工作负载，使得所有工作函数都能并行运行。正如我们使用`for`循环的原始实现是同步的，完全有可能尝试构建如上图所示的架构，但最终得到的是一个同步的架构。这是如何发生的呢？
- en: 'Eventually, the entry point must make multiple calls to kick off the sub-tasks.
    For that, we''ll use some form of looping. Using AWS as an example, the entry
    point can use the AWS APIs to invoke Lambda functions within that loop. The following
    code block demonstrates invoking a `lambda` function via the JavaScript AWS SDK:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，入口函数必须进行多次调用以启动子任务。为此，我们将使用某种形式的循环。以AWS为例，入口函数可以使用AWS API在该循环中调用Lambda函数。以下代码块演示了通过JavaScript
    AWS SDK调用`lambda`函数：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It''s safe to ignore much of the detail in the preceding code block. At a high
    level, our Node.js code iterates around an array of three items and invokes a
    named Lambda function each time. You may think that this code implements the architecture
    shown in the diagram above. However, running this, you''d quickly learn that this
    code operates entirely synchronously. That is, each iteration of the loop waits
    for a response to be returned from the `lambda.invoke` call. The reason for this
    is that, by default, Lambda invocation APIs assume a request type of request/response.
    More plainly, the default mode of operation is synchronous, where the client is
    expecting a return value from the invoked function. The good news is that this
    is trivial to fix by calling the `invoke` function with the correct parameter,
    which instructs it that we don''t care about a return value. Merely add `InvocationType:
    "Event"` to the `params` and you''re all done.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '可以安全地忽略前面代码块中的许多细节。从高层次来看，我们的Node.js代码遍历一个包含三个项目的数组，并在每次迭代中调用一个命名的Lambda函数。你可能认为这段代码实现了上图所示的架构。然而，运行这段代码，你会很快发现它完全是以同步方式运行的。也就是说，循环的每次迭代都等待从`lambda.invoke`调用返回响应。之所以会这样，是因为Lambda调用API默认假设请求类型为请求/响应。更简单地说，默认的操作模式是同步的，客户端期望从被调用的函数中获取返回值。好消息是，通过调用`invoke`函数并使用正确的参数来修复这个问题，这个参数指示我们不需要返回值。只需将`InvocationType:
    "Event"`添加到`params`中，就完成了。'
- en: Resizing images in parallel
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行缩放图像
- en: This example will be implemented in Node.js for no other reason than to change
    things from the Python code in previous chapters. There is a single dependency
    in this example, which we use for the image resizing, called `jimp`. I'll touch
    on some of the steps to get going with a new Node project using the Serverless
    Framework.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将使用Node.js实现，没有其他原因，只是为了将之前章节中的Python代码进行更改。在这个例子中，有一个依赖项，我们使用它来进行图像缩放，称为`jimp`。我将简要介绍使用Serverless
    Framework开始新Node项目的一些步骤。
- en: Setting up the project
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置项目
- en: 'Setting up a new Node.js project isn''t any different from doing so with any
    other supported language. We''ll tell serverless to use the `aws-nodejs` template
    and name our project `fanout`. The `-p` argument simply tells Serverless to place
    all of the generated code in the `serverless` directory, which is relative to
    the location where we execute this command. Consider the following code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个新的Node.js项目与使用任何其他支持的语言设置没有区别。我们将告诉Serverless使用`aws-nodejs`模板，并将我们的项目命名为`fanout`。`-p`参数只是告诉Serverless将所有生成的代码放置在`serverless`目录中，该目录相对于我们执行此命令的位置。考虑以下代码：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we''ll add our single dependency for `jimp`. Here, I''m using `yarn`,
    but `npm` works fine as well:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加我们的单个依赖项`jimp`。在这里，我使用的是`yarn`，但`npm`同样可以正常工作：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once all that is done, our code layout looks like the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成所有这些，我们的代码布局看起来如下所示：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Setting up trigger and worker functions
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置触发器和工作函数
- en: 'Next, let''s wire up the `trigger` function and define the `worker` function.
    As noted earlier, this entire process will begin upon uploading an image to S3\.
    The Serverless Framework makes this type of wiring straightforward with the `events`
    section in `serverless.yml`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们连接`trigger`函数并定义`worker`函数。如前所述，整个过程将在上传图像到S3时开始。Serverless Framework通过`serverless.yml`中的`events`部分使这种类型的连接变得简单：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What this says is that the `uploadImage` function will be called whenever an
    object is created in the named S3 bucket. That's all there is to it. Again, this
    event could have been anything else supported in AWS, provided the trigger gives
    access to some image which needs resizing. If you're using a cloud provider other
    than AWS, you'll need to figure out which trigger makes sense for your platform.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每当在命名的S3桶中创建对象时，都会调用`uploadImage`函数。就是这样。再次强调，这个事件可以是AWS支持的其他任何事件，只要触发器提供了需要调整大小的图像的访问权限。如果你使用的是除AWS之外的云服务提供商，你需要找出对你平台有意义的触发器。
- en: You'll also notice the definition of the `ResizeImage` function. What's curious
    is that there are no `events` listed. That is because, in our case, the `uploadImage`
    function will act as the trigger, calling this Lambda function manually using `aws-sdk`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到`ResizeImage`函数的定义。奇怪的是，没有列出`events`。这是因为，在我们的案例中，`uploadImage`函数将作为触发器，通过`aws-sdk`手动调用这个Lambda函数。
- en: Setting up permissions
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置权限
- en: As with all things AWS, we'll need to ensure IAM permissions are set up correctly.
    The `UploadImage` function will interact with a single AWS resource other than
    itself, and that is the `ResizeImage` function. For `UploadImage` to invoke `ResizeImage`,
    we need to grant it explicit permission.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有AWS相关的事物一样，我们需要确保IAM权限设置正确。`UploadImage`函数将与其自身之外的单一AWS资源交互，那就是`ResizeImage`函数。为了使`UploadImage`能够调用`ResizeImage`，我们需要授予它明确的权限。
- en: Additionally, `ResizeImage` needs access to write data to the final resting
    place of the resized photos. We'll place these images in a different S3 bucket
    and again grant access via the `iamRoleStatements` section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`ResizeImage`需要访问写入调整大小后照片的最终存储位置。我们将将这些图像放置在不同的S3桶中，并通过`iamRoleStatements`部分再次授予访问权限。
- en: 'You can see both of these statements in the following code, along with other
    configurations in the full `serverless.yml` file:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的代码中看到这两个语句，以及`serverless.yml`文件中的其他配置：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From a security perspective, there is a slight imperfection in our IAM roles
    above since both functions are granted the same permissions. That is, `ResizeImage`
    is allowed to call itself, and `uploadImage` is allowed access to the results
    S3 bucket. Ideally, only the functions which need the permissions would be granted
    those permissions. It is possible to set up per-function IAM access using the
    Serverless Framework but it's a bit verbose and outside the scope of this book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全角度来看，我们上面的IAM角色中存在一个小缺陷，因为这两个函数被授予了相同的权限。也就是说，`ResizeImage`允许调用自己，而`uploadImage`允许访问结果S3桶。理想情况下，只有需要权限的函数才会被授予这些权限。使用Serverless
    Framework可以设置每个函数的IAM访问权限，但这有点冗长，并且超出了本书的范围。
- en: Implementing the application code
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现应用程序代码
- en: With the setup done, we can now focus on the application code. Let's take a
    look at the `uploadImage` function, as that is the gateway to the entire process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，我们现在可以专注于应用程序代码。让我们看看`uploadImage`函数，因为那是整个过程的入口。
- en: 'We first need to initialize our two dependencies at the top of this `handler.js`
    file, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要在`handler.js`文件顶部初始化我们的两个依赖项，如下所示：
- en: The `aws-sdk`, which is automatically available in the Lambda runtime
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aws-sdk`，它在Lambda运行时自动可用'
- en: The `jimp` library for doing the image manipulation
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于图像操作的`jimp`库
- en: Going from the top down, our `uploadImage` function defines a few things. First,
    our `params` object contains the base of the Lamda invocation parameters. Note
    here that we're using an `InvocationType` of `"Event"`, which is extremely important
    in order to get the asynchronous fan-out described earlier. Next, we'll hardcode
    a few image widths to which we'll resize the original image. `jimp` is capable
    of taking a single dimension (height or width) and automatically calculating the
    other dimension to retain the original aspect ratio.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从上到下，我们的`uploadImage`函数定义了一些东西。首先，我们的`params`对象包含Lambda调用参数的基础。注意这里我们使用的是`InvocationType`为`"Event"`，这对于获得前面描述的异步扇出至关重要。接下来，我们将硬编码一些图像宽度，我们将将这些宽度用于调整原始图像。`jimp`能够接受一个单维（高度或宽度），并自动计算另一个维度以保留原始的宽高比。
- en: The `uploadImage` function, when invoked, receives quite a bit of metadata about
    the invocation in the `event` parameter. In our case, the information about the
    uploaded images will be contained in this `event` object. All of that data ends
    up in an array of `Records`. In reality, there should only be a single record
    to deal with. Just to be safe, we'll continue working as if there are a variable
    number of items in here and grab them all.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当`uploadImage`函数被调用时，它在`event`参数中接收有关调用的相当多的元数据。在我们的情况下，上传的图像信息将包含在这个`event`对象中。所有这些数据最终都结束在一个`Records`数组中。实际上，应该只有一个记录需要处理。为了安全起见，我们将继续假设这里有一个可变数量的项目，并获取它们所有。
- en: 'Finally, this function will iterate around the array of different sizes and
    invoke the appropriate callback as many times, with a slightly different payload.
    The list of `S3Objects` is the same for each iteration, but the size field for
    each `resizeImage` invocation will be different. The following code block shows
    the full implementation of the `uploadImage` function, which invokes the `ResizeImage`
    Lambda function asynchronously:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，此函数将遍历不同大小的数组，多次调用适当的回调，每次调用都带有略微不同的负载。每次迭代的`S3Objects`列表都是相同的，但每次`resizeImage`调用的尺寸字段将不同。以下代码块显示了`uploadImage`函数的完整实现，该函数异步调用`ResizeImage`
    Lambda函数：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With that, we can turn our attention to the work on actually resizing the images.
    Just as in `uploadImage`, `resizeImage` receives a payload of data in the `event`
    parameter, which is an object type. Remember that the `S3Objects` attribute passed
    over to this function is an array of S3 images. It's safe to say that this will
    be an array of length one for this example.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以将注意力转向实际调整图像大小的工作。就像在`uploadImage`中一样，`resizeImage`接收一个包含在`event`参数中的数据负载，该参数是一个对象类型。记住，传递给此函数的`S3Objects`属性是一个S3图像数组。可以肯定的是，在这个例子中这将是一个长度为1的数组。
- en: 'As this function iterates around the list of `S3Objects`, it will extract the
    pertinent data needed for it to perform the following tasks:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当此函数遍历`S3Objects`列表时，它将提取执行以下任务所需的相关数据：
- en: Get the original image from S3
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3获取原始图像
- en: Resize the in-memory image contents
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整内存中的图像内容
- en: Write the resized image to a local buffer
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将调整大小的图像写入本地缓冲区
- en: Upload the resized image to the destination bucket with the updated name
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将调整大小的图像上传到带有更新名称的目标存储桶
- en: 'The following code block shows the full implementation of the `resizeImage`
    function, which is responsible for downsizing an image:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块显示了`resizeImage`函数的完整实现，该函数负责缩小图像：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Testing our code
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试我们的代码
- en: 'To test this, all that''s needed is to upload an image to our target directory.
    I''ll use one of my photos from the High Sierra in California, along with the
    AWS command-line interface. The original photo is 2,816 × 2,112 pixels:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这一点，只需将一张图片上传到我们的目标目录即可。我将使用我在加利福尼亚州希拉里拉山的一张照片，以及AWS命令行界面。原始照片的分辨率为2,816
    × 2,112像素：
- en: '![](img/eeba7e7c-feff-48f6-830b-1488988bb923.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eeba7e7c-feff-48f6-830b-1488988bb923.jpg)'
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s inspect the logs from the `ResizeImage` function. What we expect to
    see is three invocations right around the same time. Bear in mind that these may
    finish at entirely different times since the workload between them may vary; however,
    the starting times should be very close together. Looking at the CloudWatch log
    results, we can see what we''re hoping for:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查 `ResizeImage` 函数的日志。我们期望看到的是大约在同一时间的三次调用。请注意，由于它们之间的工作负载可能不同，这些调用可能完全在不同的时间完成；然而，它们的开始时间应该非常接近。查看
    CloudWatch 日志结果，我们可以看到我们希望看到的内容：
- en: '![](img/d0d66f3c-56cd-4a4d-84fc-1dc1ca12ca32.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0d66f3c-56cd-4a4d-84fc-1dc1ca12ca32.png)'
- en: Success! Each one of these log streams corresponds to a unique invocation of
    `ResizeImage`. Additionally, the Last Event Time is precisely the same across
    all three invocations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！这些日志流中的每一个都对应于 `ResizeImage` 的一个唯一调用。此外，最后一次事件时间在所有三个调用中都是精确相同的。
- en: In this case, each Log Streams corresponds to a single invocation, but that
    isn't always necessarily true. As more and more requests come in, CloudWatch will
    group Log statements into existing streams. Here, I started with no Log Streams
    at all for clarity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个日志流对应于一个单独的调用，但这并不总是必然的。随着越来越多的请求到来，CloudWatch 将将日志语句组合到现有的流中。在这里，为了清晰起见，我一开始没有任何日志流。
- en: 'It''s possible to view the logs in the AWS console or use the `sls logs` command
    to see them all together. Unfortunately, the start times are not automatically
    added to the `CloudWatch`, `Log` statement when using the AWS API (which is what
    `sls` commands ultimately use). However, we can see the results from any of our
    `console.log` statements along with the ending times:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 AWS 控制台中查看日志，或使用 `sls logs` 命令将它们全部一起查看。不幸的是，当使用 AWS API（`sls` 命令最终使用的是这个）时，开始时间不会自动添加到
    `CloudWatch`、`Log` 语句中。然而，我们可以看到任何 `console.log` 语句的结果，以及结束时间：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These results also make sense. The smallest resizing job uses the least amount
    of memory and completes first. The largest resize job uses the most memory and
    finishes last. We need to acknowledge that the 128 px job starts early and gets
    a tiny head start. However, looking at the duration, it's also clear that the
    total execution time is higher when the resized file is bigger. I suspect this
    is due to the uploading to S3 and not the resizing process itself. Regardless,
    for this example, it's unimportant which takes longer and why. What is important
    is that we now have a system which receives a single input and invokes multiple
    worker processes in parallel. Had this work been done synchronously, the execution
    would have taken approximately 20 seconds, which is the sum of all three resize
    durations. Using the fan-out pattern, this is cut down to 7 seconds, which is
    the time it takes for the longest running task to complete.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果也是有意义的。最小的缩放作业使用的内存最少，并且首先完成。最大的缩放作业使用的内存最多，最后完成。我们需要承认，128 像素的作业开始得早，并且有一个小小的领先优势。然而，从持续时间来看，当缩放后的文件更大时，总执行时间也更高。我怀疑这是由于上传到
    S3 而不是缩放过程本身。无论如何，对于这个例子来说，哪个耗时更长以及为什么并不重要。重要的是，我们现在有一个系统，它接收单个输入并并行调用多个工作进程。如果这项工作是以同步方式完成的，执行时间将大约是
    20 秒，这是三个缩放持续时间的总和。使用扇出模式，这缩短到 7 秒，这是最长运行任务完成所需的时间。
- en: 'Looking into the S3 bucket of the results, we can view the three new images
    with the new widths embedded in the name. Additionally, you can see the image
    sizes vary, where the smallest image has the smallest file size and the largest
    image the largest file size:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果 S3 存储桶，我们可以看到三个新图像，其名称中嵌入的新宽度。此外，您还可以看到图像大小不同，其中最小的图像文件大小最小，最大的图像文件大小最大：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Alternate Implementations
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交替实现
- en: The preceding example is merely one, albeit common, example of how you can implement
    a fan-out pattern. There are many ways to turn one event or trigger into multiple
    parallel processes. Some options for this are specific to the cloud service you're
    using. Since the vast majority of my cloud experience is with AWS, I'll cover
    some alternative architectures. These may be portable to other cloud providers
    with corollary service offerings under different names.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例仅是一个，尽管是常见的，但展示了您如何实现扇出模式。有许多方法可以将一个事件或触发器转换为多个并行过程。其中一些选项特定于您使用的云服务。由于我的大部分云经验都是与
    AWS 相关，我将介绍一些替代架构。这些可能可以移植到其他云提供商，在具有不同名称的相关服务下。
- en: Using notifications with subscriptions
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用带有订阅的通知
- en: My preceding example was controlled by a master receiver function, which was
    invoked on some trigger and then performed the work of calling the worker processes
    manually. One alternative is to replace the entry point function with a **Simple
    Notification Service** (**SNS**) topic. If you're unfamiliar with SNS, it's just
    what it sounds like—a system which, when triggered, notifies subscribers that
    something has happened.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前的示例是由一个主接收器函数控制的，该函数在某个触发器上被调用，然后手动执行调用工作进程的工作。一个替代方案是将入口点函数替换为**简单通知服务**（**SNS**）主题。如果您不熟悉SNS，它就像它的名字一样——一个系统，当被触发时，会通知订阅者发生了某些事情。
- en: 'Because our example was focused on transforming an image in multiple ways,
    it makes sense to set up a trigger when a new file is added. However, what happens
    when we want to start doing some processing when another type of event occurs?
    For example, a new user signs up on our website via their Facebook account, and
    we want to do the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的示例专注于以多种方式转换图像，所以在新文件添加时设置一个触发器是有意义的。然而，当我们想要在另一个类型的事件发生时开始进行一些处理时会发生什么？例如，一位新用户通过他们的Facebook账户在我们的网站上注册，我们想要执行以下操作：
- en: Send them a welcome email
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给他们发送欢迎邮件
- en: Set up their account
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置他们的账户
- en: Pull their Facebook social graph
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉取他们的Facebook社交图
- en: This workflow is all made up, but the main idea is the same—a single event results
    in multiple jobs, which may operate in parallel.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流程完全是虚构的，但主要思想是相同的——一个事件导致多个作业，这些作业可能并行运行。
- en: In cases like this, an event of interest would trigger an SNS notification on
    a particular topic. SNS payloads can contain whatever your application code decides
    to send. Downstream, zero or more subscribers may be listening to that topic and
    choose to do some work when a new notification arrives. On AWS, Lambda functions
    may be triggered by SNS notifications.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一个感兴趣的事件会触发特定主题上的SNS通知。SNS有效载荷可以包含您的应用程序代码决定发送的任何内容。在下游，零个或多个订阅者可能正在监听该主题，并在收到新的通知时选择执行一些工作。在AWS上，Lambda函数可以通过SNS通知触发。
- en: 'Our fan-out architecture looks slightly different if using SNS to trigger one
    or more Lambda workers:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用SNS触发一个或多个Lambda工作进程，我们的扇出架构看起来略有不同：
- en: '![](img/4354b836-08d6-420d-8ffb-33d1ee9d847c.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4354b836-08d6-420d-8ffb-33d1ee9d847c.jpg)'
- en: Now we're freed from the burden of keeping track of what worker processes need
    to be called when an interesting event occurs. With the SNS design, individual
    workers subscribe to the SNS topic and trigger upon receiving an SNS notification.
    It's important to note that each one of these workers is a unique Lambda function
    with separate application code. In case, where the parallel jobs are all performing
    disparate tasks, there is no problem, since these would always need to be separated
    out and managed as unique functions. This design fits very nicely when the work
    to be parallelized can occur based on a singular event, but where each job is
    unique and can stand alone. Crawling a user's Facebook social graph and inserting
    some records in your database are entirely separable. For that reason, this architecture
    is an excellent choice. When a new job needs to run based on the same event, the
    work involves implementing the application code and subscribing to the existing
    SNS topic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经摆脱了在有趣事件发生时跟踪需要调用哪些工作进程的负担。使用SNS设计，单个工作进程订阅SNS主题，并在接收到SNS通知时触发。重要的是要注意，这些工作进程中的每一个都是一个独特的Lambda函数，具有独立的应用程序代码。在所有并行作业都执行不同的任务的情况下，没有问题，因为这些始终需要分离出来并作为独立函数进行管理。当要并行化的工作可以根据单一事件发生时，这种设计非常适合，但每个作业都是独特的，可以独立存在。爬取用户的Facebook社交图和在您的数据库中插入一些记录是完全可分离的。因此，这种架构是一个非常好的选择。当基于同一事件需要运行新的作业时，涉及的工作包括实现应用程序代码和订阅现有的SNS主题。
- en: 'This model doesn''t work very well if all of the workers are performing the
    same task. The reason for this is that an SNS trigger occurs on a single topic
    and delivers the same payload to all subscribers. In the image resize example,
    the `UploadImage` function invoked `ResizeImage` three times with three different
    payloads. If we had built the image resize example with the SNS design, each resizing
    worker would need to be its own independently managed Lambda function with the
    knowledge of what size to use when resizing images. To be more clear, there would
    be three different Lambda functions which corresponded to the three different
    image sizes we wanted to resize:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有的工作者都在执行相同的任务，这种模型可能工作得不是很好。原因在于SNS触发是在单个主题上发生的，并将相同的负载发送给所有订阅者。在图像缩放示例中，`UploadImage`函数调用了三次`ResizeImage`，分别使用了三个不同的负载。如果我们用SNS设计构建图像缩放示例，每个缩放工作者都需要是一个独立管理的Lambda函数，并且知道在缩放图像时使用什么大小。更明确地说，会有三个不同的Lambda函数，对应于我们想要缩放的三个不同图像大小：
- en: '`ResizeImage256`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResizeImage256`'
- en: '`ResizeImage512`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResizeImage512`'
- en: '`ResizeImage1024`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ResizeImage1024`'
- en: If we wanted to add a new image size, it would mean implementing a new function.
    That rapidly becomes unwieldy, and problems such as this aren't a good fit for
    this design.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要添加一个新的图像大小，这意味着需要实现一个新的函数。这很快就会变得难以控制，而且像这样的问题不适合这种设计。
- en: Using notifications with queues
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用队列进行通知
- en: 'Another take on this is to use an SNS notification to deliver to multiple queues.
    When an SNS event is triggered, that notification takes place fairly quickly (a
    few seconds, at most). If there are 100 subscribers attached to that topic, all
    100 subscribers will wake up and start working in parallel. That behavior may
    be exactly what you need in certain scenarios. However, in those cases where you
    may not want your system to operate at full capacity, it''s possible to deliver
    SNS data to one or more **Simple Queuing Service** (**SQS**) queues:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用SNS通知将消息发送到多个队列。当SNS事件被触发时，该通知会相当快地发生（最多几秒钟）。如果有100个订阅者附加到该主题，所有100个订阅者都会唤醒并开始并行工作。这种行为可能正是你在某些场景下所需要的。然而，在那些你可能不希望系统全速运行的情况下，可以将SNS数据发送到一个或多个**简单队列服务**（**SQS**）队列：
- en: '![](img/cf58db97-ceba-4de6-9811-4836491136a7.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf58db97-ceba-4de6-9811-4836491136a7.jpg)'
- en: Here, the subscribers to an SNS topic are SQS queues rather than Lamba functions.
    This sort of design may work well when you don't necessarily want, or need to
    keep up with, a high volume of events. By throwing data on queues, it's easier
    to control the consumption rate of the data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，SNS主题的订阅者是SQS队列而不是Lamba函数。这种设计在你不一定要，或者不需要跟上大量事件时可能工作得很好。通过将数据投放到队列中，更容易控制数据的消费速率。
- en: SQS behaves as one would expect a queue to act; that is, data placed into the
    queue remains in the queue until some process comes along and consumes it, finally
    marking it as consumed. This pattern would be a great design to protect some backing
    service such as a relational database. Take the case where a high number of transactions
    arrives all at once and they ultimately need to be written to a database. Using
    the previous example, this could result in an equally high number of database
    writes since there is nothing to slow down the workers being invoked once the
    SNS event is triggered. To buffer that work, the SNS notifications trigger writes
    to the SQS queues, which result in all of the data queuing up for future processing.
    Workers process then poll the queues at some acceptable and known rate so as not
    to overwhelm the database either saturating the number of open connections or
    putting too much load on it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SQS的行为就像人们预期队列应该表现的那样；也就是说，放入队列中的数据会留在队列中，直到某个进程过来消费它，最终将其标记为已消费。这种模式会是一个很好的设计，用于保护某些后端服务，例如关系型数据库。考虑这种情况，大量事务同时到达，最终需要写入数据库。使用之前的示例，这可能导致同样数量的数据库写入，因为一旦SNS事件被触发，就没有什么可以减缓被调用的工作者。为了缓冲这项工作，SNS通知触发写入到SQS队列，导致所有数据排队等待未来的处理。工作者以某种可接受和已知的速率轮询队列，这样就不会压倒数据库，既不会饱和打开连接的数量，也不会给它带来过多的负载。
- en: Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced the fan-out pattern and discussed its overall merits
    and basic architecture when using a serverless platform. We discussed, in detail,
    the implementation of an example serverless application, which created multiple
    resized images in parallel using this pattern. In this example, we also learned
    the basics of deploying a Node.js application using the Serverless Framework on
    top of AWS. We also discussed different implementations of the fan-out pattern
    using different AWS services and when those alternative designs may be suitable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了扇出模式，并讨论了在使用无服务器平台时其整体优点和基本架构。我们详细讨论了一个示例无服务器应用程序的实现，该应用程序使用此模式并行创建多个调整大小的图像。在这个例子中，我们还学习了如何使用AWS上的Serverless
    Framework部署Node.js应用程序的基础知识。我们还讨论了使用不同AWS服务实现扇出模式的不同方法，以及何时这些替代设计可能适用。
- en: Readers should understand the fan-out pattern well and be ready to use this
    pattern in future chapters in this book as part of more complex patterns.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该很好地理解扇出模式，并准备好在本书的后续章节中将此模式作为更复杂模式的一部分使用。
- en: Next, we'll work on processing data using queues and the messaging pattern.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用队列和消息模式来处理数据。
