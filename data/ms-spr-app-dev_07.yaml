- en: Chapter 7. Spring with Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。与Hadoop一起使用Spring
- en: Processing large chunks of data has been a major challenge in architecting modern
    day web applications. Hadoop is an open source framework from Apache that provides
    libraries to process and store large chunks of data. It offers a scalable, cost-effective,
    and fault-tolerant solution to store and process large chunks of data. In this
    chapter, let us demonstrate how the Spring Framework supports Hadoop. Map and
    Reduce, Hive, and HDFS are some of the Hadoop key terminology used with cloud-based
    technologies. Google has also come with its own Map and Reduce and distributed
    file system framework, apart from Apache Hadoop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建现代Web应用程序的架构中，处理大量数据一直是一个主要挑战。 Hadoop是Apache的开源框架，提供了处理和存储大量数据的库。它提供了一种可扩展、成本效益和容错的解决方案，用于存储和处理大量数据。在本章中，让我们演示Spring框架如何支持Hadoop。
    Map和Reduce、Hive和HDFS是与基于云的技术一起使用的一些Hadoop关键术语。除了Apache Hadoop之外，Google还推出了自己的Map和Reduce以及分布式文件系统框架。
- en: Apache Hadoop modules
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Hadoop模块
- en: 'Apache Hadoop consists of the following modules:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop由以下模块组成：
- en: '**Hadoop Common**: This is a common module used by other modules of Hadoop.
    It is like a utility package.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Common**：这是Hadoop的其他模块使用的通用模块。它类似于一个实用程序包。'
- en: '**Hadoop Distributed File System**: Hadoop Distributed File System can be considered
    when we have to store large amounts of data across various machines or machine
    clusters.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**：当我们需要在各种机器或机器集群上存储大量数据时，可以考虑使用Hadoop分布式文件系统。'
- en: '**Hadoop Yarn**: Think of a scenario where we have many servers on the cloud
    that need to be scheduled to restart or reboot at a particular time by sending
    an e-mail intimation to the tenants. Hadoop Yarn can be used for scheduling resources
    across computers or clusters.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Yarn**：想象一种情景，我们在云上有许多需要在特定时间通过发送电子邮件通知租户重新启动或重启的服务器。 Hadoop Yarn可用于在计算机或集群之间调度资源。'
- en: '**Hadoop Map and Reduce**: If we have to process a large set of data, we can
    break it into small clusters and process them as units and merge them back later.
    This can be done with the libraries provided in Apache map and reduce.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop Map和Reduce**：如果我们需要处理大量数据集，可以将其分解为小集群并将它们作为单元进行处理，然后稍后合并它们。这可以通过Apache
    map和reduce提供的库来实现。'
- en: Spring namespace for Hadoop
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop的Spring命名空间
- en: Following is the namespace that needs to be used to integrate the Hadoop framework
    with Spring. [http://www.springframework.org/schema/hadoop/spring-hadoop.xsd](http://www.springframework.org/schema/hadoop/spring-hadoop.xsd)
    defines the XSD for Spring-Hadoop, which is normally used in the `application-context.xml`
    file. The XSD details how to configure Hadoop as jobs with Spring Framework.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要用来将Hadoop框架与Spring集成的命名空间。[http://www.springframework.org/schema/hadoop/spring-hadoop.xsd](http://www.springframework.org/schema/hadoop/spring-hadoop.xsd)定义了Spring-Hadoop的XSD，通常在`application-context.xml`文件中使用。
    XSD详细说明了如何使用Spring框架配置Hadoop作业。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Hadoop Distributed Files System
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Hadoop分布式文件系统
- en: The **Hadoop Distributed File System** (**HDFS**) is used for storing large
    amounts of data on a distributed file system. HDFS stores metadata and application
    data separately on different servers. The servers used to store metadata are called
    `NameNode` servers. The servers used to store application data are called `DataNode`
    servers. The `NameNode` and `DataNodes` behave in a master-slave architecture.
    Usually, one `NameNode` will have many `DataNodes`. `NameNodes` stores the file's
    namespace, and the file will spilt it into many small chunks across `DataNodes`.
    `DataNodes` usually function as per the instruction from `NameNode` and per functions
    such as block creation, replication, and deletion. So, the major tasks with Hadoop
    will involve interacting with the file system. This may include creating files,
    parsing file for processes, or deleting files.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统**（**HDFS**）用于在分布式文件系统上存储大量数据。 HDFS将元数据和应用程序数据分别存储在不同的服务器上。用于存储元数据的服务器称为`NameNode`服务器。用于存储应用程序数据的服务器称为`DataNode`服务器。`NameNode`和`DataNodes`以主从架构运行。通常，一个`NameNode`会有许多`DataNodes`。`NameNodes`存储文件的命名空间，并将文件分割成许多小块存储在`DataNodes`上。`DataNodes`通常根据`NameNode`的指令执行功能，如块创建、复制和删除。因此，与Hadoop的主要任务将涉及与文件系统的交互。这可能包括创建文件、解析文件进行处理或删除文件。'
- en: 'The Hadoop file system can be accessed in a number of ways. We have listed
    a few here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多种方式访问Hadoop文件系统。我们在这里列出了一些：
- en: '`hdfs`: It uses RPC for communication, and the protocol used is `hdfs://`.
    It requires the client, server, and clusters to have the same versions, else a
    serialization error will occur.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfs`：它使用RPC进行通信，使用的协议是`hdfs://`。客户端、服务器和集群需要具有相同的版本，否则将发生序列化错误。'
- en: '`hftp` and `hsftp`: These are HTTP-based, version-independent protocols with
    the prefix `hftp://`.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hftp`和`hsftp`：这些是基于HTTP的、与版本无关的协议，前缀为`hftp://`。'
- en: '`webhdfs`: This is based on HTTP with REST API and is also version independent.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`webhdfs`：这是基于REST API的HTTP，并且也是版本无关的。'
- en: The abstract class `org.apache.hadoop.fs.FileSystem` behaves like an entry point
    into the Hadoop File System implementation. This class has been extended by Spring
    Framework with the subclass `SimplerFileSystem`. This subclass contains all the
    methods that serve file operations, such as copying from one location to another.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象类`org.apache.hadoop.fs.FileSystem`的行为类似于Hadoop文件系统实现的入口点。Spring框架通过子类`SimplerFileSystem`扩展了这个类。这个子类包含了所有的文件操作方法，比如从一个位置复制到另一个位置。
- en: Spring Framework comes with a package in Hadoop to handle Hadoop Distributed
    File Systems. The package `org.springframework.data.hadoop.fs` has classes to
    handle the file resources.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spring框架提供了一个处理Hadoop分布式文件系统的包。包`org.springframework.data.hadoop.fs`中有处理文件资源的类。
- en: '`HdfsResourceLoader` is a class found in Sping''s Hadoop File System package
    and is used to load the resources in Hadoop File System. It has constructors that
    take the configuration object as the input. The `HdfsResourceLoader` constructors
    are shown in the following code snippet. It also has methods to get the resource
    from a path specified and to close the file stream after use.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`HdfsResourceLoader`是Spring的Hadoop文件系统包中的一个类，用于加载Hadoop文件系统中的资源。它有以配置对象作为输入的构造函数。`HdfsResourceLoader`的构造函数如下所示。它还有从指定路径获取资源和在使用后关闭文件流的方法。'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Use the following command to configure Spring to use `webhdfs`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令配置Spring使用`webhdfs`：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To manually configure the URI and File System ID, the following configuration
    can be given:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要手动配置URI和文件系统ID，可以给出以下配置：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Languages such as **Rhino** and **Groovy** have provided Java scripting or use
    Python to do the HDFS configuration. A sample one is shown in the following code.
    The scripts can be configured to run on start up or conditional start up. Two
    script variables that can be used for this configuration are `run-at-start-up`
    and `evaluate`. Scripts can also be configured to get started as tasklets (which
    means as a batch job).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如**Rhino**和**Groovy**之类的语言提供了Java脚本或使用Python来进行HDFS配置。以下是一个示例。可以配置脚本在启动时或有条件的启动时运行。可以用于此配置的两个脚本变量是`run-at-start-up`和`evaluate`。脚本也可以配置为作为任务启动（这意味着作为批处理作业启动）。
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Some of the implicit variables and classes associated with the implicit variables
    are shown here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了一些与隐式变量和与隐式变量相关的类：
- en: '`hdfsRL-org.springframework.data.hadoop.io.HdfsResourceLoader`: a HDFS resource
    loader (relies on `hadoop-resource-loader` or singleton type match, falls back
    to creating one automatically based on ''`cfg`'').'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hdfsRL-org.springframework.data.hadoop.io.HdfsResourceLoader`：一个HDFS资源加载器（依赖于`hadoop-resource-loader`或单例类型匹配，根据''`cfg`''自动创建）。'
- en: '`distcp-org.springframework.data.hadoop.fs.DistributedCopyUtil`: Programmatic
    access to `DistCp`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`distcp-org.springframework.data.hadoop.fs.DistributedCopyUtil`：对`DistCp`进行编程访问。'
- en: '`fs-org.apache.hadoop.fs.FileSystem`: a Hadoop File System (relies on ''`hadoop-fs`''
    bean or singleton type match, falls back to creating one based on ''cfg'').'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs-org.apache.hadoop.fs.FileSystem`：一个Hadoop文件系统（依赖于''`hadoop-fs`'' bean或单例类型匹配，根据''cfg''创建）。'
- en: '`fsh-org.springframework.data.hadoop.fs.FsShell`: a File System shell, exposing
    hadoop `fs` commands as an API.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fsh-org.springframework.data.hadoop.fs.FsShell`：一个文件系统shell，将hadoop `fs`命令作为API暴露出来。'
- en: HBase
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HBase
- en: Apache HBase is mainly the key value storage for Hadoop. It's actually a database
    that is easily scalable and can accommodate millions of rows and columns. It can
    be scaled across hardware and is similar to a NoSQL database. It integrates with
    Map and Reduce and works best with the RESTFUL API. HBase was derived from Google's
    bigdata. It has been used by Netflix, Yahoo, and Facebook. It is also memory intensive,
    since it's meant to handle large amounts of data and has to scale against hardware.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Apache HBase主要是Hadoop的键值存储。它实际上是一个易于扩展的数据库，可以容纳数百万行和列。它可以跨硬件进行扩展，类似于NoSQL数据库。它与Map和Reduce集成，并且最适合使用RESTFUL
    API。HBase源自Google的bigdata。它已经被Netflix、Yahoo和Facebook使用。它也是内存密集型的，因为它旨在处理大量数据并且必须针对硬件进行扩展。
- en: 'Let''s create a simple employee table using Eclipse and Hadoop HBase. In Eclipse,
    just add the following JAR files, or if you are using Maven, ensure that the following
    JAR files are updated in Maven''s `pom.xml` file:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Eclipse和Hadoop HBase创建一个简单的员工表。在Eclipse中，只需添加以下JAR文件，或者如果您使用Maven，请确保在Maven的`pom.xml`文件中更新以下JAR文件：
- en: '`hbase-0.94.8.jar`'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hbase-0.94.8.jar`'
- en: '`commons-logging-1.1.1.jar`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commons-logging-1.1.1.jar`'
- en: '`log4j-1.2.16.jar`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log4j-1.2.16.jar`'
- en: '`zookeeper-3.4.5.jar`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zookeeper-3.4.5.jar`'
- en: '`hadoop-core-1.1.2.jar`'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hadoop-core-1.1.2.jar`'
- en: '`commons-configuration-1.6.jar`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`commons-configuration-1.6.jar`'
- en: '`common-lang-2.5.jar`'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`common-lang-2.5.jar`'
- en: '`protobuf-java-2.4.0a.jar`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`protobuf-java-2.4.0a.jar`'
- en: '`slf4j-api-1.4.3.jar`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slf4j-api-1.4.3.jar`'
- en: '`slf4j-log4j12-1.4.3.jar`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slf4j-log4j12-1.4.3.jar`'
- en: Create a `Main` class to with the following code. This class will create an
    employee table with ID and Name as its two columns using the `HbaseAdmin` class.
    This class has methods for creating, modifying, and deleting tables in Hadoop.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`Main`类，并使用以下代码。这个类将使用`HbaseAdmin`类创建一个包含ID和Name两列的员工表。这个类有用于在Hadoop中创建、修改和删除表的方法。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: HBase is supported by the Spring Framework, and `factoryBean` is also created
    in a Spring Hadoop package to support it. The `HbaseConfigurationFactoryBean`
    bean is available in the package `org.springframework.data.hadoop.hbase`. The
    `HBaseAccessor` class is an abstract class and has been extended by two subclasses,
    `HbaseTemplate` and `HbaseInterceptors`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: HBase得到了Spring Framework的支持，并且Spring Hadoop包中还创建了一个`factoryBean`来支持它。`HbaseConfigurationFactoryBean`
    bean位于`org.springframework.data.hadoop.hbase`包中。`HBaseAccessor`类是一个抽象类，并且已经被两个子类`HbaseTemplate`和`HbaseInterceptors`扩展。
- en: '![HBase](img/B02116_07_01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![HBase](img/B02116_07_01.jpg)'
- en: Spring offers a core class called `HBaseTemplate`. This class is the first point
    of contact for the application when HBase is implemented. This class has all the
    methods to access tables, such as `execute`, `find`, `find all`, and so on.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Spring提供了一个名为`HBaseTemplate`的核心类。当HBase被实现时，这个类是应用程序的第一个接触点。这个类有访问表的所有方法，比如`execute`、`find`、`find
    all`等等。
- en: 'This class has the following constructor:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类有以下构造函数：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This is the HBase template configuration that can be used in the application''s
    `context.xml` or `Hbasecontext.xml` files:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可以在应用程序的`context.xml`或`Hbasecontext.xml`文件中使用的HBase模板配置：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s also look at how `HBaseTemplate` is used for retrieving table information
    with a sample code snippet:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看如何使用`HBaseTemplate`来检索表信息，以下是一个示例代码片段：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Spring also supports AOP integration with Hadoop HBase and has a package to
    handle all the AOP events using `HBaseInterceptors`. This class implements the
    following interfaces:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spring还支持AOP与Hadoop HBase的集成，并有一个包来处理所有AOP事件，使用`HBaseInterceptors`。这个类实现了以下接口：
- en: '`org.aopalliance.aop.Advice`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.aopalliance.aop.Advice`'
- en: '`org.aopalliance.intercept.Interceptor`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.aopalliance.intercept.Interceptor`'
- en: '`org.aopalliance.intercept.MethodInterceptor`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`org.aopalliance.intercept.MethodInterceptor`'
- en: '`InitializingBean`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InitializingBean`'
- en: '`HBaseInterceptors` with `HBaseSynchronizationManager` can be used to bind
    an HBase table to a thread before a method call, or detach from it after a method
    call.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`HBaseInterceptors`与`HBaseSynchronizationManager`可用于在方法调用之前将HBase表绑定到线程，或在方法调用之后将其分离。'
- en: 'This is Spring''s Hadoop HBase configuration for creating an HBase configuration
    object to manage HBase configuration connections:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是Spring的Hadoop HBase配置，用于创建一个HBase配置对象来管理HBase配置连接：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is Spring''s Hadoop HBase configuration to manage proxies and connections
    when the application context is null or not available for some reason:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是Spring的Hadoop HBase配置，用于在应用程序上下文为空或由于某种原因不可用时管理代理和连接：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is the configuration for a high performance coordination server called
    `ZooKeeper` which is used in Hadoop distributed systems:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个名为`ZooKeeper`的高性能协调服务器的配置，它用于Hadoop分布式系统：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can also load the properties from the file as shown here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从文件中加载属性，如下所示：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Map and Reduce
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Map和Reduce
- en: '**Map and Reduce** is a programming approach that allows a lot of scalability.
    The term "Map and Reduce" implies that we will be using maps to process the data.
    We can see two steps here. The first one is map creation (a map is created with
    a key-value pair), and the second one is reduction, which reads the map created
    in the first step and breaks it into many smaller maps.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**Map和Reduce**是一种允许大规模扩展的编程方法。术语“Map和Reduce”意味着我们将使用映射来处理数据。我们可以看到这里有两个步骤。第一个是创建映射（创建具有键值对的映射），第二个是减少，它读取第一步创建的映射，并将其分解成许多较小的映射。'
- en: 'Let''s think of a scenario that can be related to Map and Reduce—let''s say
    that we need to get the population of tigers in India and do some work to enhance
    their living conditions so that they don''t go extinct. We may have an average
    figure of the population of tigers. Say that we dispatch people to different states
    and they collect information like this: Karnataka (100), TamilNadu (150), and
    so on. We would then combine these figures into a single figure to get the total
    population of tigers. The mapping of population can be seen as a parallel process
    (mapping job), and combining the result can be seen as a reducing job.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个与Map和Reduce相关的场景——假设我们需要获取印度老虎的数量，并做一些工作来改善它们的生存条件，以免它们灭绝。我们可能有老虎数量的平均数字。假设我们派遣人员到不同的邦，他们收集到的信息如下：卡纳塔克邦（100），泰米尔纳德邦（150），等等。然后我们将这些数字合并成一个数字，以得到老虎的总数量。人口的映射可以被看作是一个并行过程（映射作业），而合并结果可以被看作是一个减少作业。
- en: Creating a configuration object in Spring for Map and Reduce
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为Map和Reduce在Spring中创建一个配置对象
- en: The configuration object holds information about the Map and Reduce job. The
    configuration object itself is a bean definition mapped to the class `ConfigurationFactoryBean`,
    with the default name `hadoopConfiguration`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象保存有关Map和Reduce作业的信息。配置对象本身是一个映射到类`ConfigurationFactoryBean`的bean定义，具有默认名称`hadoopConfiguration`。
- en: 'The configuration object can be simply configured as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 配置对象可以简单地配置如下：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is another variation of configuring the configuration object:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是配置对象的另一种变化：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Another variation is to configure Hadoop resources using `java.properties`
    directly in the `configuration` tag, like so:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种变化是直接在`configuration`标记中使用`java.properties`直接配置Hadoop资源，如下所示：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can also use Spring''s property placeholder to externalize the properties,
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用Spring的属性占位符来外部化属性，如下所示：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Map and Reduce jobs with Spring
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spring创建Map和Reduce作业
- en: Map and Reduce can be scheduled as a job using Spring Framework. Spring Framework
    comes with `spring-data-hadoop` package which supports Map and Reduce. With this,
    we need to ensure that we have the Apache Hadoop core package.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Spring Framework将Map和Reduce安排为作业。Spring Framework带有`spring-data-hadoop`包，支持Map和Reduce。为此，我们需要确保我们有Apache
    Hadoop核心包。
- en: Let us implement a simple scenario of counting the occurrence of each word in
    the input file. Create a simple Maven Java project with the following mentioned
    dependencies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的场景，统计输入文件中每个单词的出现次数。创建一个简单的Maven Java项目，具有以下所述的依赖关系。
- en: Dependencies for Maven project
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Maven项目的依赖关系
- en: 'We need to add these dependencies in the `pom.xml` file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在`pom.xml`文件中添加这些依赖项：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Apache Hadoop Map and Reduce comes with a mapper class that can be used to create
    maps to solve the problem of reading the contents and storing the occurrence of
    the word with key-value pairs. Each line in the file will be broken into words
    to be stored in maps.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop Map和Reduce带有一个映射器类，可用于创建映射，以解决读取内容并存储单词出现次数的问题，使用键值对。文件中的每一行将被分解为要存储在映射中的单词。
- en: 'We can create a custom mapper by extending the `ApacheMapper` class and overriding
    the map method, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展`ApacheMapper`类并覆盖map方法来创建自定义映射器，如下所示：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `CustomWordMapper` class does the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`CustomWordMapper`类执行以下操作：'
- en: Creates an `myword` instance of the `Text()` class.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`Text()`类的`myword`实例。
- en: 'Overrides the `map` method of the super class `Mapper` and implements these
    steps:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 覆盖超类`Mapper`的`map`方法，并实现以下步骤：
- en: Text object is converted to string and assigned to string `line`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本对象转换为字符串，并赋值给字符串`line`。
- en: Line is a string object that is passed to string tokenizer.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Line是一个传递给字符串标记器的字符串对象。
- en: String tokenizer is looped using `while` and calls the `removeNonLettersNonNumbers`
    method. The returning string is assigned to a `myword` text instance.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`while`循环遍历字符串标记器，并调用`removeNonLettersNonNumbers`方法。返回的字符串赋值给`myword`文本实例。
- en: The `context.write(myword,newIntwritable(1))` method is called.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`context.write(myword,newIntwritable(1))`方法。
- en: Has a method to remove non-letters and non-number that uses the `string.replaceAll()`
    method. It finally returns a string object which has only number and letters.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个方法可以删除非字母和非数字，使用`string.replaceAll()`方法。最后返回一个只包含数字和字母的字符串对象。
- en: 'We shall next create a reducer component. The reducer component will do the
    following tasks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将创建一个reducer组件。reducer组件将执行以下任务：
- en: Extend the `reducer` class.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展`reducer`类。
- en: Create a string attribute for the reducer class which accepts the string that
    needs to be searched and whose occurrence needs to be found.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为reducer类创建一个字符串属性，该属性接受需要搜索的字符串及其需要找到的出现次数。
- en: Override the `reduce` method.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 覆盖`reduce`方法。
- en: Remove unwanted key-value pairs.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除不需要的键值对。
- en: Keep key-value pairs that are required.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留所需的键值对。
- en: Check whether the input key is already present. If it is present, it will get
    the occurrence and the latest value will be stored.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查输入键是否已经存在。如果存在，它将获取出现次数，并将最新值存储。
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Configure the `application.properties` file with HDFS ports and input and output
    file paths.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用HDFS端口和输入输出文件路径配置`application.properties`文件。
- en: 'Here is the sample `application.properties` file:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是示例`application.properties`文件：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once the properties are configured, it should be available in the Spring context.
    So, configure the properties file in Spring's `application-context.xml` file using
    `property-placeholder`. This is the configuration snippet that needs to be added
    in the `application-conext.xml` file.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦属性被配置，它应该在Spring上下文中可用。因此，在Spring的`application-context.xml`文件中使用`property-placeholder`配置属性文件。这是需要在`application-conext.xml`文件中添加的配置片段。
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can directly configure Apache Hadoop in the `application-context.xml` file
    or use the properties file and read the key-value pair from the properties file.
    Since we have used the properties file, we shall read the values from the properties
    file. The following code snippet shows that `${mapred.job.tracker}` is a key in
    the properties file. You can see that the default name is also configured from
    the properties file using the key `${fs.default.name}`. Configure Apache Hadoop
    in the `application-context.xml` file as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接在`application-context.xml`文件中配置Apache Hadoop，也可以使用属性文件并从属性文件中读取键值对。由于我们使用了属性文件，我们将从属性文件中读取值。以下代码片段显示`${mapred.job.tracker}`是属性文件中的一个键。您可以看到默认名称也是使用键`${fs.default.name}`从属性文件中配置的。在`application-context.xml`文件中配置Apache
    Hadoop如下：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to configure Hadoop as a job in Spring:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要在Spring中配置Hadoop作业：
- en: Provide a job ID.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供作业ID。
- en: Specify the input path; it will be read from the properties file.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定输入路径；它将从属性文件中读取。
- en: Specify the output path; it will be read from the properties file.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定输出路径；它将从属性文件中读取。
- en: Jar-by class.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按类进行Jar。
- en: Mapper class-reference to the custom mapper class.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mapper类引用自定义mapper类。
- en: Reducer class-reference to the custom reducer class.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reducer类引用自定义reducer类。
- en: 'This is the configuration snippet that needs to be available in the `application-xccontext.xml`
    file. Configure the Hadoop job in the `application-context.xml` file, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是需要在`application-xccontext.xml`文件中可用的配置片段。在`application-context.xml`文件中配置Hadoop作业如下：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Lastly, we need to configure the job runner in the `application-context.xml`
    file. The job runner configuration tells Spring Framework when to start the job.
    Here we have configured the job runner to start `wordcountjob` on start up.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要在`application-context.xml`文件中配置作业运行器。作业运行器配置告诉Spring框架何时启动作业。在这里，我们已经配置了作业运行器在启动时启动`wordcountjob`。
- en: Here is the configuration snippet for job runner. Configure the `application-context.xml`
    file to run the Hadoop job.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是作业运行器的配置片段。配置`application-context.xml`文件以运行Hadoop作业。
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Since this is a standalone Spring application, we do not have a web module that
    will invoke the application context. The context needs to be loaded in a class
    file. So, let's create a `Main` class with a `static` method to load the `application-context.xml`
    file.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个独立的Spring应用程序，我们没有一个将调用应用程序上下文的web模块。上下文需要在一个类文件中加载。因此，让我们创建一个带有`static`方法的`Main`类来加载`application-context.xml`文件。
- en: 'We can create a class that loads the `application-context.xml` file on start
    up, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个在启动时加载`application-context.xml`文件的类，如下所示：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s create a file named `myinput.txt` with content, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为`myinput.txt`的文件，内容如下：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we need to give an input file to HDFS by executing this command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过执行此命令向HDFS提供输入文件：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Run the `Main` class to see the output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`Main`类以查看输出。
- en: Map and Reduce jobs using Hadoop streaming and Spring DataApache Hadoop
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hadoop流和Spring DataApache Hadoop进行Map和Reduce作业
- en: In this section, we shall demonstrate Map and Reduce data streaming with Unix
    shell commands. Since this is related to Hadoop streaming, we shall set up a Hadoop
    instance on the Unix system. A Hadoop instance is always is run on a Unix machine
    for production mode, while for development, a Windows Hadoop instance is used.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示使用Unix shell命令进行Map和Reduce数据流。由于这与Hadoop流相关，我们将在Unix系统上设置一个Hadoop实例。Hadoop实例始终在Unix机器上以生产模式运行，而在开发中，将使用Windows
    Hadoop实例。
- en: 'These are the requirements to set up the requirement:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些是设置要求的要求：
- en: JAVA 1.7.x
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAVA 1.7.x
- en: SSH must be installed
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须安装SSH
- en: Download the latest Apache Hadoop Distribution Binary Package.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载最新的Apache Hadoop分发二进制包。
- en: Unzip and extract the package into a folder.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压并将包提取到一个文件夹中。
- en: 'Set up the following environment variables:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置以下环境变量：
- en: '`JAVA_HOME`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JAVA_HOME`'
- en: '`HADOOP_HOME`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HADOOP_HOME`'
- en: '`HADOOP_LOG_DIR`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HADOOP_LOG_DIR`'
- en: '`PATH`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PATH`'
- en: 'We also need to configure the files that are present in the `conf` folder of
    the Hadoop installation directory:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要配置Hadoop安装目录的`conf`文件夹中存在的文件：
- en: '`Core-site.xml`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Core-site.xml`'
- en: '`Hdfs-site.xml`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Hdfs-site.xml`'
- en: '`Mapred-site.xml`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mapred-site.xml`'
- en: We need to set a default Hadoop file system.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置一个默认的Hadoop文件系统。
- en: To configure a default Hadoop file system, provide setting information in the
    `core-site.xml` file.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置默认的Hadoop文件系统，请在`core-site.xml`文件中提供设置信息。
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Also configure the replication factor. Replication factor configuration ensures
    that a copy of the file gets stored in the Hadoop file system. A property `dfs.replication`
    and its value is set in the `hdfs-site.xml` file.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还要配置复制因子。复制因子配置确保文件的副本存储在Hadoop文件系统中。在`hdfs-site.xml`文件中设置属性`dfs.replication`及其值。
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Lastly, configure the job tracker; this configuration is done in the `mapred-site.xml`
    file.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，配置作业跟踪器；此配置在`mapred-site.xml`文件中完成。
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To run Hadoop in pseudo distributed mode, we just need the format; in the `bin`
    folder, there are `start` and `stop` Hadoop instance commands.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在伪分布式模式下运行Hadoop，我们只需要格式；在`bin`文件夹中，有`start`和`stop` Hadoop实例命令。
- en: Next, we shall demonstrate how to integrate Python with Apache Hadoop data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将演示如何将Python与Apache Hadoop数据集成。
- en: 'We shall create a simple project using Maven. These are the dependencies:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Maven创建一个简单的项目。这些是依赖关系：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We need a mapper and reducer Python script. A mapper script in Python should
    be implemented to do the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个mapper和reducer Python脚本。Python中的mapper脚本应该实现以下功能：
- en: The script should read from a standard input stream, read the input one line
    at time, and convert it into UTF-8
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脚本应该从标准输入流中读取，一次读取一行输入，并将其转换为UTF-8
- en: The words in the line have to be split into words
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行中的单词必须分割成单词
- en: The special characters from the line need to be replaced with blank characters,
    and then get a key value pair as a tab; they are delimited to standard output
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行中的特殊字符需要替换为空字符，然后得到一个键值对作为制表符；它们被限定到标准输出
- en: 'Here is the mapper script in Python:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Python中的mapper脚本：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The Reducer script in Python should be implemented to do the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的Reducer脚本应该实现以下功能：
- en: The script should read the key-value pair output generated from the `mapper`
    class. Then, count the occurrence of keywords.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本应该读取从`mapper`类生成的键值对输出。然后，计算关键字的出现次数。
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once the Python script is ready, we need to provide the mapper and reducer
    class names and configurations in the properties files. This is the `.properties`
    file:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦Python脚本准备就绪，我们需要在属性文件中提供mapper和reducer类名和配置。这是`.properties`文件：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We also need to configure `property-placeholder` and Apache Hadoop in the `context.xml`
    file. Here is the configuration:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要在`context.xml`文件中配置`property-placeholder`和Apache Hadoop。这是配置：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Lastly, we need to configure the Hadoop job and assign the job to job runner,
    which will initialize the job.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要配置Hadoop作业并将作业分配给作业运行器，该运行器将初始化作业。
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, we need to invoke the configuration using the application context, so that
    the application context is loaded with all the configurations in the Spring Framework.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要使用应用程序上下文来调用配置，以便应用程序上下文加载Spring框架中的所有配置。
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Run the following command in Command Prompt to provide an input file. Let the
    file be placed in a folder named `input`:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符中运行以下命令以提供输入文件。让文件放在名为`input`的文件夹中：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is made available in the output directory, which can be read using
    the following commands:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出可以在输出目录中使用以下命令读取。
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: You should see an output that shows the occurrence of the word "Amily" in the
    provided text.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个输出，显示提供的文本中单词“Amily”的出现次数。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: So far, we have seen how Spring integrates with Apache Hadoop and provides a
    Map and Reduce process to search and count data. We have also discussed the integration
    of Python with Apache Hadoop. We have demonstrated how we can configure Hadoop
    jobs in Spring Framework, and have also seen HDFS configurations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了Spring如何与Apache Hadoop集成，并提供了搜索和计数数据的Map和Reduce过程。我们还讨论了Python与Apache
    Hadoop的集成。我们已经演示了如何在Spring框架中配置Hadoop作业，并且还看到了HDFS配置。
- en: Hadoop is vast concept. For further information, refer to [http://docs.spring.io/spring-hadoop/docs/current/reference/html/](http://docs.spring.io/spring-hadoop/docs/current/reference/html/)
    and [https://github.com/spring-projects/spring-hadoop-samples](https://github.com/spring-projects/spring-hadoop-samples).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop是一个庞大的概念。有关更多信息，请参阅[http://docs.spring.io/spring-hadoop/docs/current/reference/html/](http://docs.spring.io/spring-hadoop/docs/current/reference/html/)和[https://github.com/spring-projects/spring-hadoop-samples](https://github.com/spring-projects/spring-hadoop-samples)。
- en: We have demonstrated how we can install a Hadoop instance on Unix machines.
    In the next chapter, we shall see how to use Spring Dynamic Modules with OSGI.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经演示了如何在Unix机器上安装Hadoop实例。在下一章中，我们将看到如何在OSGI中使用Spring动态模块。
