- en: Distributed Logging and Tracing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式日志记录和追踪
- en: When breaking down a monolith into microservices, we usually spend a lot of
    time thinking about business boundaries or the partitioning of our application
    logic, but we forget about the logs. From my own experience as a developer and
    software architect, I can say that developers do not usually pay much attention
    to logging. On the other hand, operation teams, which are responsible for application
    maintenance, are mainly dependent on logs. Regardless of one's area of expertise,
    it is indisputable that logging is something that all applications have to do,
    whether they have monolithic or microservices architecture. However, microservices
    force adding a whole new dimension to design and arrangement of application logs. There
    are many small, independent, horizontally scaled, intercommunicating services
    that are running on multiple machines. Requests are often processed by multiple
    services. We have to correlate these requests and store all the logs in a single,
    central place in order to make it easier to view them. Spring Cloud introduces
    a dedicated library that implements a distributed tracing solution, Spring Cloud
    Sleuth.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当将单体应用拆分为微服务时，我们通常会花很多时间思考业务边界或应用逻辑的划分，但我们忘记了日志。根据我自己作为开发者和软件架构师的经验，我可以说明开发者通常不会支付太多注意力到日志上。另一方面，负责应用程序维护的操作团队主要依赖日志。无论你的专业领域是什么，毫无疑问，日志是所有应用程序都必须做的工作，无论它们是有单体架构还是微服务架构。然而，微服务在设计和安排应用程序日志方面增加了一个全新的维度。有许多小型的、独立的、水平扩展的、相互通信的服务在多台机器上运行。请求通常由多个服务处理。我们必须关联这些请求，并将所有日志存储在单一的、中心位置，以便更容易查看它们。Spring
    Cloud引入了一个专门的库，实现了分布式追踪解决方案，即Spring Cloud Sleuth。
- en: There is also one thing that should be discussed here. Logging is not the same
    as tracing! It is worth pointing out the differences between them. Tracing is
    following your program's data flow. It is typically used by technical support
    teams to diagnose where a problem occurs. You have to trace your system flow to
    discover performance bottlenecks or times when the error occurs. Logging is used
    for error reporting and detecting. It should always be enabled, in contrast to
    tracing. When you design a large system and you would like to have good and flexible
    error reporting across machines, you should definitely think about collecting
    log data in a centralized way. The recommended and most popular solution for this
    is the **ELK** stack (**Elasticsearch** + **Logstash** + **Kibana**). There is
    no dedicated library for this stack in Spring Cloud, but the integration may be
    realized with Java logging frameworks, such as Logback or Log4j. There is another
    tool that will be discussed in this chapter, Zipkin. It is a typical tracing tool
    that helps gather timing data that can be used to troubleshoot latency problems
    in microservice architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里还应该讨论一件事情。日志记录不同于追踪！指出它们之间的区别是值得的。追踪是跟随你的程序的数据流。它通常被技术支持团队用来诊断问题出现的位置。你要追踪你的系统流程以发现性能瓶颈或错误发生的时间。日志记录用于错误报告和检测。与追踪相比，它应该始终是启用的。当你设计一个大型系统，并且你希望跨机器有良好的、灵活的错误报告时，你肯定应该考虑以集中式方式收集日志数据。实现这一目标的推荐和最受欢迎的解决方案是**ELK**栈（**Elasticsearch**
    + **Logstash** + **Kibana**）。Spring Cloud中没有为这个栈提供专门的库，但是可以通过Java日志框架（如Logback或Log4j）来实现集成。在本章中还将讨论另一个工具，Zipkin。它是一个典型的追踪工具，帮助收集可以用来解决微服务架构中延迟问题的计时数据。
- en: 'The topics we will cover in this chapter include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将要覆盖的主题包括以下内容：
- en: The best practices for logging in microservices-based systems
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微服务基础系统日志的最佳实践
- en: Using Spring Cloud Sleuth to append tracing information to messages and correlating
    events
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spring Cloud Sleuth向消息添加追踪信息并关联事件
- en: Integrating the Spring Boot application with Logstash
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Spring Boot应用程序与Logstash集成
- en: Displaying and filtering log entries using Kibana
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kibana显示和筛选日志条目
- en: Using Zipkin as a distributed tracing tool and integrating it with the application
    through Spring Cloud Sleuth
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zipkin作为分布式追踪工具，并通过Spring Cloud Sleuth与应用程序集成
- en: Best logging practices for microservices
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务最佳的日志实践
- en: 'One of the most important best practices for dealing with logging is to trace
    all the incoming requests and outgoing responses. Maybe it seems obvious to you,
    but I have seen a couple of applications that did not comply with that requirement.
    If you meet this demand, there is one consequence that occurs with microservices-based
    architecture. The overall number of logs in your system increases compared to
    monolithic applications, where there is no messaging. This, in turn, requires
    us to pay even more attention to logging than before. We should do our best to
    generate as little information as possible, even though this information can tell
    us much about the situation. How do we achieve this? First of all, it is good
    to have the same log message format across all the microservices. For example,
    let''s consider how to print variables in the application logs. I suggest you
    use the JSON notation in view of the fact that, usually, messages exchanged between
    microservices are formatted with JSON. This format has a very straightforward
    standard, which makes your logs easily readable and parseable, as shown in the
    following code fragment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 处理日志最重要的最佳实践之一是跟踪所有传入请求和传出响应。这可能对你来说很显然，但我见过几个不符合这一要求的应用程序。如果你满足这个需求，微服务架构有一个后果。与单片应用程序相比，系统的日志总数会增加，其中没有消息传递。这反过来又要求我们比以前更加关注日志记录。我们应该尽最大努力生成尽可能少的信息，尽管这些信息可以告诉我们很多情况。我们如何实现这一点？首先，拥有所有微服务相同的日志消息格式是很好的。例如，考虑如何在应用程序日志中打印变量。我建议你使用JSON表示法，因为通常，微服务之间交换的消息格式是JSON。这种格式有一个非常直接的标准化，使得你的日志容易阅读和解析，如下面的代码片段所示：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding format is much easier to analyze than something like the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的格式比以下内容更容易分析：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But generally, the most important thing here is standardization. No matter
    which format you choose, it is crucial to use it everywhere. You should also be
    careful to ensure that your logs are meaningful. Try to avoid sentences that do
    not contain any information. For example, from the following format, it is not
    clear which order is being processed:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常，这里最重要的是标准化。无论你选择哪种格式，关键是在到处使用它。你还应该小心确保你的日志是有意义的。尽量避免不包含任何信息的句子。例如，从以下格式来看，不清楚哪个顺序正在处理：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, if you really want this kind of log entry format, try to assign it
    to different log levels. It is really a bad practice to log everything with the
    same level of `INFO`. Some kinds of information are more important than others,
    so the one difficulty here is to decide what level the log entry should be logged
    at. Here are some suggestions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你真的想要这种日志条目格式，尽量把它分配给不同的日志级别。将所有内容都以`INFO`相同的级别记录，真的是一种糟糕的做法。有些信息比其他信息更重要，所以这里的困难在于决定日志条目应该记录在哪个级别。以下是一些建议：
- en: '`TRACE`: This is for very detailed information, intended only for development.
    You might keep it for a short period of time, just after deployment to a production
    environment, but treat it as a temporary file.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TRACE`：这是非常详细的信息，仅用于开发。你可能会在部署到生产环境后短时间内保留它，但将其视为临时文件。'
- en: '`DEBUG`: At this level, log anything that happens in the program. This is mostly
    used for debugging or troubleshooting by developers. The distinction between `DEBUG`
    and `TRACE` is probably the most difficult.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEBUG`：在这个级别，记录程序中发生的任何事件。这主要用于开发人员的调试或故障排除。`DEBUG`和`TRACE`之间的区别可能是最难的。'
- en: '`INFO`: At this level, you should log the most important information during
    the operation. These messages have to be easily understandable, not just for developers,
    but also for administrators or advanced users, to let them quickly find out what
    the application is doing.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INFO`：在这个级别，你应该记录操作期间最重要的信息。这些信息必须易于理解，不仅对开发者，对管理员或高级用户也是如此，让他们能够快速找出应用程序正在做什么。'
- en: '`WARN`: At this level, log all events that could potentially become errors.
    Such a process may be continued, but you should take extra caution with it.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WARN`：在这个级别，记录所有可能变成错误的潜在事件。这样的过程可以继续进行，但你应该对此特别小心。'
- en: '`ERROR`: Usually, you print exceptions at this level. The important thing here
    is not to throw exceptions everywhere if, for example, only one business logic
    execution has not succeeded.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ERROR`：通常，你会在这个级别打印异常。这里的关键不是到处都抛出异常，例如，如果只有一个业务逻辑执行没有成功的话。'
- en: '`FATAL`: This Java logging level designates very severe error events that will
    probably cause the application to stop.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FATAL`：这个Java日志级别表示非常严重的错误事件，可能会导致应用程序停止运行。'
- en: There are other good logging practices, but I have mentioned the most important
    ones for use in microservices-based systems. It is also worth mentioning one more
    aspect of logging, normalization. If you would like to easily understand and interpret
    your logs, you should definitely know how and when they were collected, what they
    contain, and why they were emitted. There are some especially important characteristics
    that should be normalized across all microservices, such as `Time` (when), `Hostname`
    (where), and `AppName` (who). As you will see in the next part of this chapter,
    this kind of normalization is very useful when a centralized method of collecting
    logs is implemented in your system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些好的日志实践，但我已经提到了在基于微服务的系统中使用的一些最重要的实践。还值得提到日志的一个方面，即规范化。如果您想轻松理解和解释您的日志，您肯定要知道它们是在何时何地收集的，它们包含什么，以及为什么要发出它们。在所有微服务中特别重要的特性应该进行规范化，例如`Time`（何时）、`Hostname`（何地）和`AppName`（何人）。正如您将在本章的下一部分看到的，这种规范化在系统中实现集中日志收集方法时非常有用。
- en: Logging with Spring Boot
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spring Boot进行日志记录
- en: Spring Boot uses Apache Commons Logging for internal logging, but if you are
    including dependencies with starters, Logback will be used by default in your
    application. It doesn't inhibit the possibility of using other logging frameworks
    in any way. The default configurations are also provided for Java Util Logging,
    Log4J2, and SLF4J. Logging settings may be configured in the `application.yml`
    file with `logging.*` properties. The default log output contains the date and
    time in milliseconds, log level, process ID, thread name, the full name of the
    class that has emitted the entry, and the message. It may be overridden by using
    the `logging.pattern.console` and `logging.pattern.file` properties respectively
    for the console and file appenders.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Boot内部日志使用Apache Commons Logging，但如果您包含启动器中的依赖项，默认情况下您的应用程序将使用Logback。它以任何方式都不妨碍使用其他日志框架的可能性。还提供了Java
    Util Logging、Log4J2和SLF4J的默认配置。日志设置可以在`application.yml`文件中使用`logging.*`属性进行配置。默认日志输出包含日期和时间（毫秒）、日志级别、进程ID、线程名称、发出条目的类的全名和消息。可以通过分别使用`logging.pattern.console`和`logging.pattern.file`属性为控制台和文件附加器来覆盖它。
- en: By default, Spring Boot only logs on to a console. In order to allow the writing
    of log files in addition to a console output, you should set a `logging.file`
    or `logging.path` property. If you specify the `logging.file` property, the logs
    would be written to the file at an exact location or a location relative to the
    current directory. If you set `logging.path`, it creates a `spring.log` file in
    the specified directory. Log files will be rotated after reaching 10 MB.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spring Boot只向控制台记录日志。为了允许除了控制台输出之外还写入日志文件，您应该设置`logging.file`或`logging.path`属性。如果您指定`logging.file`属性，日志将被写入确切位置或当前位置的文件。如果您设置`logging.path`，它将在指定目录中创建一个`spring.log`文件。日志文件在达到10
    MB后会被轮换。
- en: 'The last thing that can be customized in the `application.yml` settings file
    is the log levels. By default, Spring Boot writes messages with `ERROR`, `WARN`,
    and `INFO` levels. We may override this setting for every single package or class
    with `logging.level.*` properties. The root logger can also be configured using
    `logging.level.root`. Here''s an example configuration in the `application.yml`
    file, which changes the default pattern format, as well as a few log levels, and
    sets the location of the logging file:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在`application.yml`设置文件中可以自定义的最后一件事情是日志级别。默认情况下，Spring Boot记录`ERROR`、`WARN`和`INFO`级别的消息。我们可以使用`logging.level.*`属性为每个单独的包或类覆盖此设置。还可以使用`logging.level.root`配置根日志记录器。以下是在`application.yml`文件中的一个示例配置，它更改了默认模式格式，以及一些日志级别，并设置了日志文件的存储位置：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see in the preceding example, such a configuration is pretty simple,
    but, in some cases, it is not enough. If you would like to define additional appenders
    or filters, you should definitely include the configuration for one of the available
    logging systems—Logback (`logback-spring.xml`), Log4j2 (`log4j2-spring.xml`),
    or Java Util Logging (`logging.properties`). As I have mentioned earlier, by default, Spring
    Boot uses Logback for the application logs. If you provide the `logback-spring.xml`
    file in the root of the classpath, it will override all settings defined in `application.yml`.
    For example, you may create file appenders that rotate logs daily and retain a
    maximum history of 10 days. This feature is very commonly used in applications.
    In the next section of this chapter, you will also learn that a custom appender
    is required to integrate your microservice with Logstash. Here''s an example Logback
    configuration file''s fragment that sets a daily rolling policy for the `logs/order.log`
    file:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在之前的示例中所看到的，这样的配置相当简单，但在某些情况下，这并不足够。如果您想要定义额外的appender或过滤器，您肯定应该包括其中一个可用的日志系统的配置——Logback(`logback-spring.xml`)，Log4j2(`log4j2-spring.xml`)，或Java
    Util Logging(`logging.properties`)。正如我之前提到的，Spring Boot默认使用Logback来记录应用程序日志。如果您在类路径的根目录提供`logback-spring.xml`文件，它将覆盖`application.yml`中定义的所有设置。例如，您可以创建每日轮转日志的文件appender，并保留最多10天的历史记录。这个功能在应用程序中非常常用。在本章的下一节中，您还将了解到，要集成您的微服务与Logstash，需要一个自定义的appender。以下是一个设置`logs/order.log`文件每日轮转策略的Logback配置文件片段的例子：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It is also worth mentioning that Spring recommends using `logback-spring.xml`
    for Logback instead of the default `logback.xml`. Spring Boot includes a couple
    of extensions to Logback that may be helpful for an advanced configuration. They
    cannot be used in the standard `logback.xml`, but only with `logback-spring.xml`.
    We have listed some of these extensions that will allow you to define profile-specific
    configurations or surface properties from the Spring Environment:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，Spring建议使用`logback-spring.xml`而不是默认的`logback.xml`对Logback进行配置。Spring
    Boot包含对Logback的一些扩展，这些扩展对于高级配置可能很有帮助。它们不能用在标准的`logback.xml`中，只能与`logback-spring.xml`一起使用。我们已经列出了其中一些扩展，这些扩展将允许您定义特定于配置文件或从Spring
    Environment公开属性的配置：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Centralizing logs with ELK Stack
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ELK栈集中日志
- en: ELK is the acronym for three open source tools—Elasticsearch, Logstash, and
    Kibana. It is also called **Elastic Stack**. The heart of this system is **Elasticsearch**, a
    search engine based on another open source project written in Java, Apache Lucene. This
    library is especially suitable for applications that require full-text searches
    in cross-platform environments. The main reason for the popularity of Elasticsearch
    is its performance. Of course, it has some other advantages, such as scalability,
    flexibility, and easy integration by providing a RESTful, JSON-based API for searching
    stored data. It has a large community and many use cases, but the most interesting
    one for us is its ability to store and search logs generated by applications.
    Logging is the main reason for including Logstash in ELK Stack. This open source
    data-processing pipeline allows us to collect, process, and input data into Elasticsearch.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ELK是三个开源工具的缩写——Elasticsearch、Logstash和Kibana。它也被称为**Elastic Stack**。这个系统的核心是**Elasticsearch**，一个基于另一个开源Java项目Apache
    Lucene的搜索引擎。这个库特别适合于需要在跨平台环境中进行全文搜索的应用程序。Elasticsearch流行的主要原因是它的性能。当然，它还有一些其他优势，如可扩展性、灵活性和通过提供基于RESTful、JSON格式的API来搜索存储的数据，易于集成。它有一个庞大的社区和许多用例，但对我们来说最有趣的是它存储和搜索应用程序生成的日志的能力。日志是包含Logstash在ELK栈中的主要原因。这个开源数据处理管道允许我们收集、处理并将数据输入到Elasticsearch中。
- en: '**Logstash** supports many inputs that pull events from external sources. What
    is interesting is that it has many outputs, and Elasticsearch is only one of them.
    For example, it can write events to Apache Kafka, RabbitMQ, or MongoDB, and it
    can write metrics to InfluxDB or Graphite. It not only receives and forwards data
    to their destinations, but can also parse and transform it on the fly.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**Logstash**支持许多输入，这些输入可以从外部来源提取事件。有趣的是，它有许多输出，而Elasticsearch只是其中之一。例如，它可以将事件写入Apache
    Kafka、RabbitMQ或MongoDB，并且可以将指标写入InfluxDB或Graphite。它不仅接收并将数据转发到它们的目的地，还可以实时解析和转换它们。'
- en: '**Kibana** is the last element of ELK Stack. It is an open source, data-visualization
    plugin for Elasticsearch. It allows you to visualize, explore, and discover data
    from Elasticsearch. We may easily display and filter all the logs collected from
    our application by creating search queries. On this basis, we can export data
    to PDF or CSV formats to provide reports.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kibana** 是ELK堆栈的最后一个元素。它是一个开源的数据可视化插件，用于Elasticsearch。它允许您可视化、探索和发现来自Elasticsearch的数据。我们可以通过创建搜索查询轻松地显示和筛选我们应用程序收集的所有日志。在此基础上，我们可以将数据导出为PDF或CSV格式以提供报告。'
- en: Setting up ELK Stack on the machine
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器上设置ELK堆栈
- en: Before we try to send any logs from our application to Logstash, we have to
    configure ELK Stack on the local machine. The most suitable way to run it is through
    Docker containers. All the products in the stack are available as Docker images.
    There is a dedicated Docker registry hosted by Elastic Stack's vendor. A full
    list of published images and tags can be found at [www.docker.elastic.co](http://www.docker.elastic.co).
    All of them use `centos:7` as the base image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将应用程序的任何日志发送到Logstash之前，我们必须在本地机器上配置ELK堆栈。最合适的方法是使用Docker容器运行它。堆栈中的所有产品都可以作为Docker镜像使用。ELastic
    Stack的供应商提供了一个专用的Docker注册表。可以在[www.docker.elastic.co](http://www.docker.elastic.co)找到所有发布镜像和标签的完整列表。它们都使用`centos:7`作为基础镜像。
- en: 'We will begin from the Elasticsearch instance. Its development can be started
    with the following command:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Elasticsearch实例开始。其开发可以通过以下命令启动：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running Elasticsearch in development mode is the most convenient way of running
    it because we don''t have to provide any additional configuration. If you would
    like to launch it in production mode, the `vm.max_map_count` Linux kernel setting
    needs to be set to at least `262144`. The procedure for modifying it is different
    depending on the OS platform. For Windows with Docker Toolbox, it must be set
    via `docker-machine`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发模式下运行Elasticsearch是最方便的，因为我们不需要提供任何其他配置。如果您想要在生产模式下启动它，`vm.max_map_count`
    Linux内核设置至少需要设置为`262144`。根据不同的操作系统平台，修改它的过程是不同的。对于带有Docker Toolbox的Windows，必须通过`docker-machine`来设置：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The next step is to run a container with Logstash. In addition to launching
    a container with Logstash, we should also define an input and output. The output
    is obvious—Elasticsearch, which is now available under the default Docker machine
    address, `192.168.99.100`. As an input, we define the simple TCP plugin `logstash-input-tcp`,
    which is compatible with `LogstashTcpSocketAppender` used as a logging appender
    in our sample application. All the logs from our microservices will be sent in
    JSON format. For now, it is important to set the `json` codec for that plugin. Each
    microservice will be indexed in Elasticsearch with its name and `micro` prefix.
    Here''s the Logstash configuration file, `logstash.conf`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是运行带有Logstash的容器。除了启动带有Logstash的容器外，我们还应该定义一个输入和一个输出。输出是显而易见的——Elasticsearch，现在在默认的Docker机器地址`192.168.99.100`下可用。作为输入，我们定义了与我们的示例应用程序中用作日志附加器的`LogstashTcpSocketAppender`兼容的简单TCP插件`logstash-input-tcp`。我们所有的微服务日志都将以JSON格式发送。现在，重要的是为该插件设置`json`编码器。每个微服务都将以其名称和`micro`前缀在Elasticsearch中索引。以下是Logstash配置文件`logstash.conf`：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here''s the command that runs Logstash and exposes it on port `5000`. It also
    copies the file with  the preceding settings to the container and overrides the
    default location of the Logstash configuration file:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个运行Logstash并将其暴露在端口`5000`上的命令。它还将带有前述设置的文件复制到容器中，并覆盖Logstash配置文件的默认位置：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we can run the last element of the stack, Kibana. By default, this
    is exposed on port `5601` and connects to the Elasticsearch API available on port `9200` in
    order to be able to load data from there:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行堆栈的最后一个元素，Kibana。默认情况下，它暴露在端口`5601`上，并连接到端口`9200`上的Elasticsearch API，以便能够从那里加载数据：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you would like to run all Elastic Stack products on your Docker machine on
    Windows, you would probably have to increase the default RAM memory for your Linux
    virtual image to a minimum of 2 GB. After launching all containers, you may finally
    access the Kibana dashboard available under `http://192.168.99.100:5601` and then
    proceed to integrate your application with Logstash.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在带有Docker的Windows机器上运行Elastic Stack的所有产品，您可能需要将Linux虚拟机图像的默认RAM内存增加到至少2
    GB。在启动所有容器后，您最终可以通过`http://192.168.99.100:5601`访问可用的Kibana仪表板，然后继续将您的应用程序与Logstash集成。
- en: Integrating an application with ELK Stack
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将应用程序与ELK堆栈集成
- en: There are many ways of integrating Java applications with ELK Stack via Logstash.
    One of the methods involves using Filebeat, which is a log data shipper for local
    files. This approach requires a beats (`logstash-input-beats`) input configured
    for the instance of Logstash, which is, in fact, the default option. You should
    also install and launch a Filebeat daemon on the server machine. It is responsible
    for the delivery of the logs to Logstash.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以通过Logstash将Java应用程序与ELK堆栈集成。其中一种方法涉及到使用Filebeat，它是一个用于本地文件的日志数据传输器。这种方法需要为Logstash实例配置一个beats（`logstash-input-beats`）输入，实际上这就是默认选项。你还需要在服务器机器上安装并启动一个Filebeat守护进程。它负责将日志传递给Logstash。
- en: Personally, I prefer a configuration based on Logback and dedicated appenders.
    It seems to be simpler than using a Filebeat agent. Besides having to deploy an
    additional service, Filebeat requires us to play with a parsing expression, such
    as the Grok filter. When using a Logback appender, you don't require any log shippers.
    This appender is available within the project Logstash JSON encoder. You may enable
    it for your application by declaring the `net.logstash.logback.appender.LogstashSocketAppender`
    appender inside the `logback-spring.xml` file.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 个人而言，我更喜欢基于Logback和专用追加器的配置。这似乎比使用Filebeat代理简单。除了需要部署一个附加服务外，Filebeat还要求我们使用诸如Grok过滤器的解析表达式。使用Logback追加器时，你不需要任何日志传输器。这个追加器可在项目中的Logstash
    JSON编码器内使用。你可以通过在`logback-spring.xml`文件内声明`net.logstash.logback.appender.LogstashSocketAppender`追加器来为你的应用程序启用它。
- en: We will also discuss an alternative approach for sending data to Logstash, using
    a message broker. In the example that we will shortly examine, I'm going to show
    you how to use Spring `AMQPAppender` to publish logging events to a RabbitMQ exchange.
    In this case, Logstash subscribes to the exchange and consumes published messages.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将讨论一种将数据发送到Logstash的替代方法，使用消息代理。在我们即将研究的示例中，我将向你展示如何使用Spring `AMQPAppender`将日志事件发布到RabbitMQ交换。在这种情况下，Logstash订阅该交换并消费发布的消息。
- en: Using LogstashTCPAppender
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LogstashTCPAppender
- en: 'The library `logstash-logback-encoder` provides three types of appenders—UDP,
    TCP, and async. The TCP appender is most commonly used. What is worth mentioning
    is that TCP appenders are asynchronous, and all the encoding and communication
    is delegated to a single thread. In addition to appenders, the library also provides
    some encoders and layouts to enable you to log in the JSON format. Because Spring
    Boot includes a Logback library by default, as well as `spring-boot-starter-web`,
    we only have to add one dependency to Maven `pom.xml`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 库`logstash-logback-encoder`提供了三种类型的追加器——UDP、TCP和异步。TCP追加器最常用。值得一提的是，TCP追加器是异步的，所有的编码和通信都委托给一个线程。除了追加器，该库还提供了一些编码器和布局，以使你能够以JSON格式记录日志。因为Spring
    Boot默认包含一个Logback库，以及`spring-boot-starter-web`，我们只需在Maven `pom.xml`中添加一个依赖项：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The next step is to define the appender with the `LogstashTCPAppender` class in
    the Logback configuration file. Every TCP appender requires you to configure an
    encoder. You may choose between `LogstashEncoder` and `LoggingEventCompositeJsonEncoder`.
    `LoggingEventCompositeJsonEncoder` gives you more flexibility. It is composed
    of one or more JSON providers that are mapped to the JSON output. By default,
    there are no providers configured. It doesn''t work that way with `LogstashTCPAppender`.
    By default, it includes several standard fields, such as timestamp, version, logger
    name, and stack trace. It also adds all entries from the **m****apped diagnostic
    context** (**MDC**) and the context, unless you disable it by setting one of the
    `includeMdc` or `includeContext` properties to `false`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是在Logback配置文件中定义带有`LogstashTCPAppender`类的追加器。每个TCP追加器都需要你配置一个编码器。你可以选择`LogstashEncoder`和`LoggingEventCompositeJsonEncoder`之间。`LoggingEventCompositeJsonEncoder`给你更多的灵活性。它由一个或多个映射到JSON输出的JSON提供者组成。默认情况下，没有提供者被配置。`LogstashTCPAppender`不是这样。默认情况下，它包括几个标准字段，如时间戳、版本、日志器名称和堆栈跟踪。它还添加了来自**映射诊断上下文**（**MDC**）和上下文的所有条目，除非你通过将`includeMdc`或`includeContext`属性设置为`false`来禁用它：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, I would like to come back for a moment to our sample system. We are still
    in the same Git repository ([https://github.com/piomin/sample-spring-cloud-comm.git](https://github.com/piomin/sample-spring-cloud-comm.git))
    and `feign_with_discovery` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery](https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery)).
    I have added some logging entries in the source code in accordance with the recommendations
    described in the *Best logging practices for microservices* section. Here''s the
    current version of the `POST` method inside `order-service`. I have used Logback
    over SLF4J as a logger by calling the `getLogger` method from `org.slf4j.LoggerFactory`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想回到我们的示例系统片刻。我们仍然在同一个Git仓库([https://github.com/piomin/sample-spring-cloud-comm.git](https://github.com/piomin/sample-spring-cloud-comm.git))的`feign_with_discovery`分支([https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery](https://github.com/piomin/sample-spring-cloud-comm/tree/feign_with_discovery))。我在源代码中添加了一些日志条目，按照*微服务最佳日志实践*部分描述的建议。以下是`order-service`内部的`POST`方法的当前版本。我通过从`org.slf4j.LoggerFactory`调用`getLogger`方法，使用Logback
    over SLF4J作为日志记录器：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s take a look at the Kibana dashboard. It is available at `http://192.168.99.100:5601`.
    The application logs may be easily discovered and analyzed there. You can select
    the required index name in the menu on the left side of the page (labeled **1**
    in the following screenshot). Log statistics are presented on the timeline graph
    (**2**). You can narrow down the time taken as search parameter by clicking a
    concrete bar or choosing a group of bars. All logs for a given period of time
    are displayed on the panel present below the graph (**3**):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Kibana仪表板。它可通过`http://192.168.99.100:5601`访问。应用程序日志在那里可以轻松发现和分析。你可以在页面左侧的菜单中选择所需的索引名称（在以下屏幕快照中标记为**1**）。日志统计信息以时间线图的形式展示（**2**）。你可以通过点击具体柱状图或选择一组柱状图来缩小搜索参数所花费的时间。给定时间段内的所有日志都显示在图表下方的面板中（**3**）：
- en: '![](img/3e56b172-f8c5-4a8a-84c3-0bece490c7a5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e56b172-f8c5-4a8a-84c3-0bece490c7a5.png)'
- en: 'Each entry can be expanded to look at its details. In the detailed table view,
    we can see, for example, the name of the Elasticsearch index (`_index`) and the
    level or name of the microservice (`appName`). Most of those fields have been
    set by `LoggingEventCompositeJsonEncoder`. I have only defined one application-specific
    field, `appName`:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每个条目都可以扩展以查看其详细信息。在详细表格视图中，我们可以看到，例如，Elasticsearch索引的名称（`_index`）和微服务的级别或名称（`appName`）。大多数这些字段都是由`LoggingEventCompositeJsonEncoder`设置的。我只定义了一个应用程序特定的字段，`appName`：
- en: '![](img/032646e3-4508-404b-b1cc-9ad63012db02.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/032646e3-4508-404b-b1cc-9ad63012db02.png)'
- en: 'Kibana gives us a great ability to search for particular entries. We may define
    filters just by clicking on the selected entries in order to define a set of search
    criteria. In the preceding screenshot, you can see how I filtered out all the
    entries with incoming HTTP requests. As you probably remember, the `org.springframework.web.filter.CommonsRequestLoggingFilter`
    class is responsible for logging them. I have just defined the filter whose name
    is equal to a fully-qualified logger class name. Here''s the screen from my Kibana
    dashboard, which displays the logs generated only by `CommonsRequestLoggingFilter`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana赋予我们搜索特定条目的强大能力。我们只需点击选中的条目即可定义过滤器，以定义一组搜索条件。在前面的屏幕快照中，你可以看到我过滤掉了所有进入HTTP请求的条目。正如你可能记得的，`org.springframework.web.filter.CommonsRequestLoggingFilter`类负责记录它们。我只是定义了一个名称与完全限定日志类名相等的过滤器。以下是我Kibana仪表板上的屏幕截图，它只显示由`CommonsRequestLoggingFilter`生成的日志：
- en: '![](img/f1b536fc-7f08-4f9a-b12a-cc92d8fe5ebc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1b536fc-7f08-4f9a-b12a-cc92d8fe5ebc.png)'
- en: Using AMQP appender and a message broker
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AMQP appender和消息代理
- en: 'The configuration with the Spring AMQP appender and message broker is a little
    bit more complicated than the method that uses the simple TCP appender. First,
    you need to launch a message broker on your local machine. I have described this
    process in [Chapter 5](37142825-02d0-48a0-99df-1a1a88a1bbd4.xhtml), *Distributed
    Configuration with Spring Cloud Config*, where I introduced RabbitMQ for dynamic
    configuration reloading with Spring Cloud Bus. Assuming you have started an instance
    of RabbitMQ locally or as a Docker container, you can proceed to configuration.
    We have to create a queue for publishing incoming events and then bind it to the
    exchange. To achieve this, you should log in to the Rabbit management console
    and then go to the Queues section. I have created the queue with the name `q_logstash`.
    I defined the new exchange with the name `ex_logstash`, which is visible in the
    following screenshot. The queue has been bound to the exchange with routing keys
    for all the example microservices:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spring AMQP appender和消息代理的配置比使用简单的TCP appender的方法要复杂一些。首先，你需要在你的本地机器上启动一个消息代理。我在[第5章](37142825-02d0-48a0-99df-1a1a88a1bbd4.xhtml)，*与Spring
    Cloud Config的分布式配置*中描述了这一过程，其中我介绍了使用Spring Cloud Bus的RabbitMQ进行动态配置重载。假设你已经在本地下启动了一个RabbitMQ实例或作为Docker容器启动，你可以继续进行配置。我们必须为发布传入事件创建一个队列，然后将其绑定到交换机。为此，你应该登录到Rabbit管理控制台，然后单击队列部分。我创建了一个名为`q_logstash`的队列。我定义了一个名为`ex_logstash`的新交换机，如下面的屏幕截图所示。该队列已使用所有示例微服务的路由键绑定到交换机：
- en: '![](img/6af1e4a9-b02d-4c4f-9e24-dea67478f24b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6af1e4a9-b02d-4c4f-9e24-dea67478f24b.png)'
- en: 'After we have launched and configured the instance of RabbitMQ, we may start
    integrating on the application side. First, you have to include `spring-boot-starter-amqp` in
    the project dependencies to provide implementations of the AMQP client and AMQP
    appender:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们启动和配置了RabbitMQ实例之后，我们可以在应用程序方面开始集成。首先，你必须将`spring-boot-starter-amqp`包含在项目依赖项中，以提供AMQP客户端和AMQP
    appender的实现：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, the only thing you have to do is to define the appender with the `org.springframework.amqp.rabbit.logback.AmqpAppender` class in
    the Logback configuration file. The most important properties that need to be
    set are the RabbitMQ network address (`host`, `port`), the name of the declared
    exchange (`exchangeName`), and the routing key (`routingKeyPattern`), which has
    to match one of the keys declared for the exchange bindings. In comparison with
    the TCP appender, a disadvantage of this approach is the need to prepare a JSON
    message sent to Logstash by yourself. Here''s a fragment of the Logback configuration
    for `order-service`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你唯一需要做的是在Logback配置文件中定义具有`org.springframework.amqp.rabbit.logback.AmqpAppender`类的appender。需要设置的最重要属性是RabbitMQ网络地址（`host`，`port`），声明的交换机名称（`exchangeName`）和路由键（`routingKeyPattern`），它必须与为交换机绑定声明的其中一个键匹配。与TCP
    appender相比，这种方法的缺点是需要自己准备发送给Logstash的JSON消息。以下是`order-service`的Logback配置片段：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Logstash may be easily integrated with RabbitMQ by declaring the `rabbitmq`
    (`logstash-input-rabbitmq`) input:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过声明`rabbitmq`（`logstash-input-rabbitmq`）输入，Logstash可以轻松集成RabbitMQ：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Spring Cloud Sleuth
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spring Cloud Sleuth
- en: Spring Cloud Sleuth is a rather small, simple project, which nevertheless provides
    some useful features for logging and tracing. If you refer to the example discussed
    in the *Using LogstashTCPAppender* section, you can easily see that there is no
    possibility to filter all the logs related to single request. In a microservices-based
    environment, it is also very important to correlate messages exchanged by the
    applications when handling requests that are coming into the system. This is the
    main motivation in creating the Spring Cloud Sleuth project.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Sleuth是一个相当小型的、简单的项目，但它提供了一些对日志记录和跟踪有用的功能。如果你参考*使用LogstashTCPAppender*部分中讨论的示例，你可以很容易地看出，没有可能过滤出与单个请求相关的所有日志。在基于微服务的环境中，关联应用程序在处理进入系统的请求时交换的消息也非常重要。这是创建Spring
    Cloud Sleuth项目的主要动机。
- en: If Spring Cloud Sleuth is enabled for the application, it adds some HTTP headers
    to the requests, which allows you to link requests with the responses and the
    messages exchanged by independent applications, for example, through RESTful API.
    It defines two basic units of work—span and trace. Each of these is identified
    by a unique 64 bit ID. The value of the trace ID is equal to the initial value
    of the span ID. Span refers to a single exchange, where the response is sent as
    a reaction to the request. Trace is something that is usually called **correlation
    IT**, and it helps us to link all the logs from different applications generated
    during the processing of requests coming into the system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果为应用程序启用了Spring Cloud Sleuth，它会向请求中添加一些HTTP头，这允许您将请求与响应以及独立应用程序之间交换的消息链接起来，例如，通过RESTful
    API。它定义了两个基本工作单位——跨度（span）和跟踪（trace）。每个都有一个独特的64位ID。跟踪ID的值等于跨度ID的初始值。跨度指的是一个单独的交换，其中响应是作为对请求的反应发送的。跟踪通常被称为**上下文关联**（correlation
    IT），它帮助我们链接系统处理传入请求时不同应用程序生成的所有日志。
- en: 'Every trace and span ID is added to the Slf4J **MDC** (**mapped diagnostic
    context**), so you will able to extract all the logs with a given trace or span
    in a log aggregator. MDC is just a map that stores the context data of the current
    thread. Every client request coming to the server is handled by a different thread.
    Thanks to this, each thread can have access to the values of its MDC within the
    thread lifecycle. As well as `spanId` and `traceId`, Spring Cloud Sleuth also
    adds the following two spans to the MDC:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每个跟踪和跨度ID都添加到Slf4J **MDC**（**映射诊断上下文**）中，因此您将能够在日志聚合器中提取具有给定跟踪或跨度的所有日志。MDC只是一个存储当前线程上下文数据的映射。每个到达服务器的客户端请求都是由不同的线程处理的。得益于这一点，每个线程在其线程生命周期内都可以访问其MDC的值。除了`spanId`和`traceId`之外，Spring
    Cloud Sleuth还将在MDC中添加以下两个跨度：
- en: '`appName`: The name of the application that has generated the log entry'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`appName`：生成日志条目的应用程序名称'
- en: '`exportable`: This specifies whether the log should be exported to Zipkin or
    not'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exportable`：这指定了日志是否应导出到Zipkin'
- en: 'In addition to the preceding features, Spring Cloud Sleuth also provides:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面的特性外，Spring Cloud Sleuth还提供了：
- en: An abstraction over common distributed tracing data models, which allows integrating
    with Zipkin.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种对常见分布式跟踪数据模型的抽象，允许与Zipkin集成。
- en: Records timing information in order to aid it in latency analysis. It also includes
    different sampling policies to manage the volume of data exported to Zipkin.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录时间信息以帮助进行延迟分析。它还包括不同的抽样策略来管理导出到Zipkin的数据量。
- en: Integrates with common Spring components taking part in communication like servlet
    filter, asynchronous endpoints, RestTemplate, message channels, Zuul filters and
    Feign client.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与参与通信的常见Spring组件集成，如servlet过滤器、异步端点、RestTemplate、消息通道、Zuul过滤器和Feign客户端。
- en: Integrating Sleuth with an application
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Sleuth集成到应用程序中
- en: 'In order to enable Spring Cloud Sleuth features for the application, just add
    the `spring-cloud-starter-sleuth` starter to the dependencies:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在应用程序中启用Spring Cloud Sleuth功能，只需将`spring-cloud-starter-sleuth`启动器添加到依赖项中：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After including this dependency, the format of the log entries generated by
    the application has been changed. You can see this as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 包含此依赖项后，应用程序生成的日志条目的格式已更改。您可以通过以下方式看到这一点：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Searching events using Kibana
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kibana搜索事件
- en: 'Spring Cloud Sleuth automatically adds HTTP headers `X-B3-SpanId` and `X-B3-TraceId`
    to all the requests and responses. These fields are also included to the MDC as
    `spanId` and `traceId`. But before moving to the Kibana dashboard, I would like
    you to take a look at the following figure. This is a sequence diagram that illustrates
    the communication flow between sample microservices:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Sleuth自动向所有请求和响应添加HTTP头`X-B3-SpanId`和`X-B3-TraceId`。这些字段也包括在MDC中作为`spanId`和`traceId`。但在移到Kibana仪表板之前，我想让您看一下下面的图表。这是一个顺序图，展示了样本微服务之间的通信流程：
- en: '![](img/d8eb077a-8417-4960-9e12-f67cb2cf2070.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8eb077a-8417-4960-9e12-f67cb2cf2070.png)'
- en: There are two available methods that are exposed by `order-service`. The first
    is for creating a new order and the second is for confirming it. The first `POST
    /` method, in fact, calls endpoints from all other services directly from `customer-service`,
    `product-service`, and `account-service` through `customer-service`. The second
    `PUT /{id}` method integrates with only one endpoint from `account-service`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`order-service`暴露了两种可用方法。第一种是创建新订单，第二种是确认它。第一个`POST /`方法，实际上，直接从`customer-service`、`product-service`和`account-service`通过`customer-service`调用所有其他服务的端点。第二个`PUT
    /{id}`方法只与`account-service`的一个端点集成。'
- en: 'The flow described previously may now be mapped by the log entries stored in
    ELK Stack. When using Kibana as a log aggregator, together with fields generated
    by Spring Cloud Sleuth, we may easily find entries by filtering them using trace
    or span IDs. Here''s an example, where we have discovered all the events related
    to a call of the `POST /` endpoint from `order-service` with the `X-B3-TraceId` field equal
    to `103ec949877519c2`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '前述流程现在可以通过存储在ELK Stack中的日志条目进行映射。当使用Kibana作为日志聚合器，结合由Spring Cloud Sleuth生成的字段时，我们可以通过使用trace或span
    ID过滤它们来轻松找到条目。这是一个例子，我们发现所有与从`order-service`调用`POST /`端点有关的事件，其`X-B3-TraceId`字段等于`103ec949877519c2`:'
- en: '![](img/c108158a-5db0-419a-88dd-a10b8f87d796.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c108158a-5db0-419a-88dd-a10b8f87d796.png)'
- en: 'Here''s an example similar to the previous one, but where all events stored
    during the processing request are sent to the `PUT /{id}` endpoint. These entries
    have been also filtered out by the `X-B3-TraceId` field, the value of which is
    equal to `7070b90bfb36c961`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '下面是一个与前一个例子类似的例子，但是在这个例子中，所有在处理请求期间存储的事件都被发送到`PUT /{id}`端点。这些条目也通过`X-B3-TraceId`字段过滤出来，该字段的值等于`7070b90bfb36c961`:'
- en: '![](img/4587b4f2-9b08-4ba3-8f56-662bdf8ee49a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4587b4f2-9b08-4ba3-8f56-662bdf8ee49a.png)'
- en: 'Here, you can see the full list of fields, which has been sent to Logstash
    by the microservice application. The fields with the `X-` prefix have been included
    in the message by the Spring Cloud Sleuth library:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到已经发送到Logstash的微服务应用程序的完整字段列表。带有`X-`前缀的字段已经被Spring Cloud Sleuth库包含在消息中：
- en: '![](img/dda5a4be-8e5e-4cc0-aa93-eb13883dbcbb.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dda5a4be-8e5e-4cc0-aa93-eb13883dbcbb.png)'
- en: Integrating Sleuth with Zipkin
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Sleuth与Zipkin集成
- en: Zipkin is a popular, open source, distributed tracing system, which helps in
    gathering timing data needed to analyze latency problems in microservices-based
    architecture. It is able to collect, look up, and visualize data using a UI web
    console. The Zipkin UI provides a dependency diagram showing how many traced requests
    were processed by all applications within the system. Zipkin consists of four
    elements. I have already mentioned one of them, Web UI. The second one is Zipkin
    collector, which is responsible for validating, storing, and indexing all incoming
    trace data. Zipkin uses Cassandra as a default backend store. It also natively
    supports Elasticsearch and MySQL. The last element is query service, which provides
    a simple JSON API for finding and retrieving traces. It is mostly consumed by
    Web UI.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin是一个流行的、开源的分布式追踪系统，它帮助收集分析微服务架构中延迟问题的所需时序数据。它能够使用UI web控制台收集、查询和可视化数据。Zipkin
    UI提供了一个依赖关系图，显示了系统内所有应用程序处理了多少追踪请求。Zipkin由四个元素组成。我已经提到了其中一个，Web UI。第二个是Zipkin收集器，负责验证、存储和索引所有传入的追踪数据。Zipkin使用Cassandra作为默认的后端存储。它还原生支持Elasticsearch和MySQL。最后一个元素是查询服务，它为查找和检索追踪提供了简单的JSON
    API。它主要由Web UI消费。
- en: Running the Zipkin server
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Zipkin服务器
- en: 'We may run the Zipkin server locally in several ways. One of these ways involves
    using a Docker container. The following command launches an in-memory server instance:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种方式在本地运行Zipkin服务器。其中一种方式是使用Docker容器。以下命令启动一个内存中的服务器实例：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After running the Docker container, the Zipkin API is available at `http://192.168.99.100:9411`.
    Alternatively, you can start it using Java libraries and the Spring Boot application.
    To enable Zipkin for your application, you should include the following dependencies
    to your Maven `pom.xml` file, as shown in the following code fragment. The default
    versions are managed by `spring-cloud-dependencies`. For our example application,
    I have used `Edgware.RELEASE` Spring Cloud Release Train:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '在运行Docker容器之后，Zipkin API在`http://192.168.99.100:9411`可用。或者，你可以使用Java库和Spring
    Boot应用程序来启动它。为了启用Zipkin，你应该在你的Maven `pom.xml`文件中包含以下依赖项，如下面的代码片段所示。默认版本由`spring-cloud-dependencies`管理。在我们的示例应用程序中，我使用了`Edgware.RELEASE`
    Spring Cloud Release Train:'
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'I have added a new `zipkin-service` module to our example system. It is really
    simple. The only thing that has to be implemented is the application main class,
    which is annotated with `@EnableZipkinServer`. Thanks to this, the Zipkin instance
    is embedded in the Spring Boot application:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我们的示例系统中增加了一个新的`zipkin-service`模块。它非常简单。必须实现的唯一事情是应用的主类，它用`@EnableZipkinServer`注解标记。得益于这一点，Zipkin实例被嵌入到Spring
    Boot应用程序中：
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In order to launch the Zipkin instance on its default port, we have to override
    the default server port in the `application.yml` file. After launching the application,
    the Zipkin API is available at `http://localhost:9411`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在默认端口上启动Zipkin实例，我们必须在`application.yml`文件中覆盖默认服务器端口。启动应用程序后，Zipkin API在`http://localhost:9411`处可用：
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Building the client application
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建客户端应用程序
- en: 'If you would like to use both Spring Cloud Sleuth and Zipkin in your project,
    just add starter `spring-cloud-starter-zipkin` to the dependencies. It enables
    integration with Zipkin via the HTTP API. If you have started the Zipkin server
    as an embedded instance inside the Spring Boot application, you don''t have to
    provide any additional configuration containing connection address. If you use
    the Docker container, you should override the default URL in `application.yml`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在项目中同时使用Spring Cloud Sleuth和Zipkin，只需在依赖项中添加`spring-cloud-starter-zipkin`启动器。它通过HTTP
    API实现了与Zipkin的集成。如果你已经在Spring Boot应用程序内部以内嵌实例启动了Zipkin服务器，你不需要提供包含连接地址的任何附加配置。如果你使用Docker容器，你应该在`application.yml`中覆盖默认URL：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can always take advantage of integration with service discovery. If you
    have the discovery client enabled through `@EnableDiscoveryClient` for your application
    with the embedded Zipkin server, you may just set the property `spring.zipkin.locator.discovery.enabled`
    to `true`. In that case, even if it is not available under the default port, all
    applications will be able to localize it through the registered name. You should
    also override the default Zipkin application name with the `spring.zipkin.baseUrl` property:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你总是可以利用与服务发现的集成。如果你通过`@EnableDiscoveryClient`为带有内嵌Zipkin服务器的应用程序启用了发现客户端，你只需将属性`spring.zipkin.locator.discovery.enabled`设置为`true`即可。在这种情况下，即使它不在默认端口上可用，所有应用程序都可以通过注册名称来定位它。你还应该用`spring.zipkin.baseUrl`属性覆盖默认的Zipkin应用程序名称：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'By default, Spring Cloud Sleuth sends only a few selected incoming requests.
    It is determined by the property `spring.sleuth.sampler.percentage`, the value
    of which needs to be a double between 0.0 and 1.0\. The sampling solution has
    been implemented because data volumes exchanged between distributed systems can
    be sometimes very high. Spring Cloud Sleuth provides sampler interface that can
    be implemented to take control over the sampling algorithm. The default implementation
    is available in class `PercentageBasedSampler`. If you would like to trace all
    the requests exchanged by your applications, just declare `AlwaysSampler` bean.
    It may be useful for the test purposes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Spring Cloud Sleuth只发送一些选定的传入请求。这是由属性`spring.sleuth.sampler.percentage`决定的，其值必须是一个在0.0和1.0之间的双精度值。采样解决方案已经实现，因为分布式系统之间交换的数据量有时可能非常高。Spring
    Cloud Sleuth提供了采样器接口，可以实现来控制采样算法。默认实现位于`PercentageBasedSampler`类中。如果你想追踪你应用程序之间交换的所有请求，只需声明`AlwaysSampler`bean。这对于测试目的可能是有用的：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Analyze data with the Zipkin UI
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zipkin UI分析数据
- en: Let's go back for a moment to our example system. As I have mentioned before,
    the new `zipkin-service` module has been added. I have also enabled Zipkin tracing
    for all the microservices, including `gateway-service`.  By default, Sleuth takes
    the value `spring.application.name` as a span's service name. You may override
    that name with the `spring.zipkin.service.name` property.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的示例系统一会儿。如我之前提到的，新的`zipkin-service`模块已经增加。我还为所有微服务（包括`gateway-service`）启用了Zipkin跟踪。默认情况下，Sleuth将`spring.application.name`的值作为跨度服务名称。你可以用`spring.zipkin.service.name`属性覆盖那个名称。
- en: To successfully test our system with Zipkin, we have to start the microservices,
    gateway, discovery, and Zipkin servers. To generate and send some test data, you
    could just run the JUnit test implemented by the `pl.piomin.services.gateway.GatewayControllerTest` class.
    It sends 100 messages to `order-service` via `gateway-service`, available at `http://localhost:8080/api/order/**`.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功使用Zipkin测试我们的系统，我们必须启动微服务、网关、发现和Zipkin服务器。为了生成并发送一些测试数据，你可以运行由`pl.piomin.services.gateway.GatewayControllerTest`类实现的JUnit测试。它通过`gateway-service`向`order-service`发送100条消息，`gateway-service`可通过`http://localhost:8080/api/order/**`访问。
- en: 'Let''s analyze the data collected from all the services by Zipkin. You may
    easily check it out using its UI web console. All the traces are tagged with the
    service''s name spans. If there are five spans for the entry, it means that the
    request coming into the system has been processed by five different services.
    You can see this in the following screenshot:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析Zipkin从所有服务收集的数据。你可以通过其Web控制台UI轻松查看。所有跟踪都被标记为服务的名称跨度。如果一个条目有五个跨度，这意味着进入系统的请求被五个不同的服务处理。你可以在以下屏幕截图中看到这一点：
- en: '![](img/22f22dfd-2f85-4951-a595-58f9d9f2e542.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22f22dfd-2f85-4951-a595-58f9d9f2e542.png)'
- en: 'You may filter the entries with different criteria, such as the service name,
    span name, trace ID, request time, or duration. Zipkin also visualizes failed
    requests and sorts them by duration, in descending or ascending order:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用不同的标准过滤条目，比如服务名称、跨度名称、跟踪ID、请求时间或持续时间。Zipkin还可视化失败的请求并按持续时间降序或升序排序：
- en: '![](img/838af6f7-c7cb-4894-8a9e-1f13a49baa51.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/838af6f7-c7cb-4894-8a9e-1f13a49baa51.png)'
- en: 'You can take a look at the details of every entry. Zipkin visualizes the flow
    between all the microservices taking part in communication. It is considering
    timing the data of every incoming request. You may uncover the reasons for latency
    in your system:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看每个条目的详细信息。Zipkin可视化了所有参与通信的微服务之间的流程。它考虑了每个传入请求的时间数据。你可以揭示系统延迟的原因：
- en: '![](img/c8eef8c1-15f9-4bd9-8ea3-41cc319da73a.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8eef8c1-15f9-4bd9-8ea3-41cc319da73a.png)'
- en: 'Zipkin provides some additional interesting features. One of these is the ability
    to visualize dependencies between applications. The following screenshot illustrates
    the communication flow of our sample system:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin提供了一些额外有趣的功能。其中之一是能够可视化应用程序之间的依赖关系。以下屏幕截图说明了我们的示例系统的通信流程：
- en: '![](img/7ac3b51e-bbae-409f-8fd2-515abf55c6ea.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ac3b51e-bbae-409f-8fd2-515abf55c6ea.png)'
- en: 'You may check out how many messages have been exchanged between services just
    by clicking on the relevant element:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击相关元素来查看服务之间交换了多少消息：
- en: '![](img/6d479575-78db-4869-8be4-77bc371c36e3.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d479575-78db-4869-8be4-77bc371c36e3.png)'
- en: Integration via message broker
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过消息代理进行集成
- en: 'Integration with Zipkin via HTTP is not the only option. As is usual with Spring
    Cloud, we may use a message broker as a proxy. There are two available brokers—RabbitMQ
    and Kafka. The first of these can be included in the project by using the `spring-rabbit`
    dependency, while the second can be included with `spring-kafka`. The default
    destination name for both of these brokers is `zipkin`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP集成Zipkin并不是唯一选项。正如Spring Cloud通常所做的那样，我们可以使用消息代理作为代理。有两个可用的代理商—RabbitMQ和Kafka。第一个可以通过使用`spring-rabbit`依赖项包含在项目中，而第二个可以通过`spring-kafka`包含。这两个代理商的默认目的地名称都是`zipkin`：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This feature also requires changes on the Zipkin server side. We have configured
    a consumer that is listening for the data coming into the RabbitMQ or Kafka queue.
    To achieve this, just include the following dependencies in your project. You
    still need to have the `zipkin-server` and `zipkin-autoconfigure-ui` artifacts
    in the classpath:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能还要求Zipkin服务器端进行更改。我们配置了一个消费者，它正在监听来自RabbitMQ或Kafka队列的数据。为了实现这一点，只需在你的项目中包含以下依赖项。你仍然需要将`zipkin-server`和`zipkin-autoconfigure-ui`工件包含在类路径中：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should annotate the main application class with `@EnableZipkinStreamServer`
    instead of `@EnableZipkinServer`. Fortunately, `@EnableZipkinStreamServer` is
    also annotated with `@EnableZipkinServer`, which means that you may also use the
    standard Zipkin server endpoints for collecting spans over HTTP, and for searching
    them with the UI web console:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该用`@EnableZipkinStreamServer`而不是`@EnableZipkinServer`注解主应用类。幸运的是，`@EnableZipkinStreamServer`也注解有`@EnableZipkinServer`，这意味着你也可以使用标准的Zipkin服务器端点通过HTTP收集跨度，以及使用Web控制台查找它们：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Logging and tracing are usually not very important during development, but these
    are the key features that are used in the maintenance of the system. In this chapter,
    I have placed emphasis on the fields of development and operations. I have shown
    you how to integrate a Spring Boot microservice application with Logstash and
    Zipkin in several ways. I have also shown you some examples to illustrate how
    to enable Spring Cloud Sleuth features for an application in order to make it
    easier to monitor calls between many microservices. After reading this chapter,
    you should also be able to effectively use Kibana as a log aggregator tool and
    Zipkin as a tracing tool for discovering bottlenecks in communication inside your
    system.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，日志记录和跟踪通常并不是非常重要，但这些是系统维护中的关键特性。在本章中，我重点介绍了开发和运维领域。我向您展示了如何以几种不同的方式将
    Spring Boot 微服务应用程序与 Logstash 和 Zipkin 集成。我还向您展示了如何启用 Spring Cloud Sleuth 功能的一些示例，以便更容易监视许多微服务之间的调用。阅读完本章后，您还应该能够有效地使用
    Kibana 作为日志聚合工具，以及使用 Zipkin 作为跟踪工具，发现系统内部通信的瓶颈。
- en: Spring Cloud Sleuth, in conjunction with Elastic Stack and Zipkin, seems to
    be a very powerful ecosystem, which removes any doubts you might have about problems
    with monitoring systems that consist of many independent microservices.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Sleuth 与 Elastic Stack 和 Zipkin 结合使用，似乎是一个非常强大的生态系统，它消除了您可能对由许多独立微服务组成的监控系统存在问题的任何疑虑。
