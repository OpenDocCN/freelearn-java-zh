- en: Chapter 8. Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章。集群
- en: This chapter will cover WildFly's clustering capabilities. The term cluster
    is used to describe a system split over several machines. Having the components
    of a system synchronize over multiple machines generally improves performance
    and availability.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍 WildFly 的集群功能。集群一词用于描述跨越多个机器的系统。系统组件在多台机器上同步通常可以提高性能和可用性。
- en: Clustering serves as an essential component to providing scalability and high
    availability to your applications. One major benefit of using clustering is that
    you can spread the traffic load across several AS instances via **load balancing**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 集群是提供应用程序可伸缩性和高可用性的基本组件。使用集群的一个主要好处是，您可以通过**负载均衡**将流量负载分散到多个 AS 实例。
- en: 'Load balancing is an orthogonal aspect of your enterprise application and is
    generally achieved by using a properly configured web server in front of the application
    server. For this reason, load balancing is discussed in the next chapter while,
    in this chapter, we will discuss the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是企业应用程序的一个正交方面，通常通过在应用程序服务器前面使用配置正确的 Web 服务器来实现。因此，负载均衡将在下一章中讨论，而本章我们将讨论以下主题：
- en: All available options to set up a WildFly cluster either using a standalone
    configuration or a domain of servers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有可用于设置 WildFly 集群的选项，无论是使用独立配置还是服务器域
- en: How to effectively configure the various components required for clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何有效地配置集群所需的各个组件
- en: The **JGroups** subsystem, which is used for the underlying communication between
    nodes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JGroups** 子系统，用于节点之间的底层通信'
- en: The **Infinispan** subsystem, which handles the cluster consistency using its
    advanced data grid platform
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Infinispan** 子系统，它使用其高级数据网格平台处理集群一致性'
- en: The **Messaging** subsystem, which uses the HornetQ clusterable implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息传递**子系统，它使用 HornetQ 可集群实现'
- en: Setting up a WildFly cluster
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 WildFly 集群
- en: For the benefit of impatient readers, we will immediately show you how to get
    a cluster of WildFly nodes up and running.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足那些不耐烦的读者的需求，我们将立即向您展示如何快速设置并运行 WildFly 节点集群。
- en: All you have to do to shape a new server profile is create a new XML configuration
    file. As the standalone server holds just a single profile, you will likely want
    to use either the configuration file named `standalone-ha.xml` or `standalone-full-ha.xml`.
    Both of these ship with WildFly. This configuration file contains all the clustering
    subsystems.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您要塑造一个新的服务器配置文件只需创建一个新的 XML 配置文件。由于独立服务器只包含一个配置文件，您可能希望使用名为 `standalone-ha.xml`
    或 `standalone-full-ha.xml` 的配置文件。这两个配置文件都包含在 WildFly 中。此配置文件包含所有集群子系统。
- en: On the other hand, a domain server is able to store multiple profiles in the
    core `domain.xml` configuration file, hence you can use this file both for clustered
    domains and for nonclustered domain servers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，域服务器能够存储多个配置文件在核心 `domain.xml` 配置文件中，因此您可以使用此文件既用于集群域，也用于非集群域服务器。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Clustering and domains are two separate concepts, the functionality of each
    does not overlap. While the aim of clustering is to provide scalability, load
    balancing, and high availability, a domain is a logical grouping of servers that
    share a centralized domain configuration and can be managed as a single unit.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 集群和域是两个不同的概念，每个的功能不重叠。虽然集群的目的是提供可伸缩性、负载均衡和高可用性，但域是服务器的一个逻辑分组，这些服务器共享集中式域配置，并且可以作为一个单一单元进行管理。
- en: We will now describe the different ways to assemble and start a cluster of standalone
    servers and domain servers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将描述组装和启动独立服务器和域服务器集群的不同方法。
- en: Setting up a cluster of standalone servers
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置独立服务器集群
- en: 'Configuring WildFly clusters for standalone servers can be broken down into
    two main possibilities:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将 WildFly 集群配置为独立服务器可以分为两种主要可能性：
- en: A cluster of WildFly nodes running on different machines
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同机器上运行的 WildFly 节点集群
- en: A cluster of WildFly nodes running on the same machine
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一机器上运行的 WildFly 节点集群
- en: We will look at each of these in turn.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次查看这些内容。
- en: A cluster of nodes running on different machines
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在不同机器上运行的节点集群
- en: 'If you decide to install each WildFly server on a dedicated machine, you are
    *horizontally scaling* your cluster. In terms of configuration, this requires
    the least effort—all you have to do is bind the server to its IP address in the
    configuration file, and start the server using the `standalone-ha.xml` configuration.
    Let''s build an example with a simple, two-node cluster as illustrated in the
    following figure:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定在每个专用机器上安装WildFly服务器，你正在水平扩展你的集群。在配置方面，这需要最少的努力——你所要做的就是将服务器绑定到配置文件中的IP地址，并使用`standalone-ha.xml`配置启动服务器。让我们通过以下图示的简单、双节点集群构建一个示例：
- en: '![A cluster of nodes running on different machines](img/6232OS_08_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![运行在不同机器上的节点集群](img/6232OS_08_01.jpg)'
- en: 'Open the `standalone-ha.xml` file on each WildFly distribution, and navigate
    to the `interfaces` section. Within the nested interface element, insert the IP
    address of the standalone server. For the first machine (`192.168.10.1`), we will
    define the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个WildFly发行版的`standalone-ha.xml`文件上打开，并导航到`interfaces`部分。在嵌套的接口元素中，插入独立服务器的IP地址。对于第一台机器（`192.168.10.1`），我们将定义以下内容：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'On the second machine (`192.168.10.2`), we will bind to the other IP address:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二台机器（`192.168.10.2`）上，我们将绑定到另一个IP地址：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is the only thing you need to change in your configuration. To start the
    cluster, you have to start your standalone server using the `standalone-ha.xml`
    configuration file as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你需要更改配置的唯一内容。要启动集群，你必须使用`standalone-ha.xml`配置文件启动你的独立服务器，如下所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Rather than updating the `standalone-ha.xml` file with the IP address of each
    server, you can use the `-b` option, which allows you to provide the binding IP
    address on server startup. In addition, you can use the `-bmanagement` flag to
    specify the management-interface address. Using these options, the previous configuration
    for the first server can be rewritten as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是在`standalone-ha.xml`文件中更新每个服务器的IP地址，你可以使用`-b`选项，它允许你在服务器启动时提供绑定IP地址。此外，你可以使用`-bmanagement`标志来指定管理接口地址。使用这些选项，第一个服务器的先前配置可以重写为：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For the second server, it can be rewritten as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个服务器，它可以重写为：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Within a few seconds, your servers will be running; however, we have not mentioned
    any details relating to clustering nodes in the console. This is because, in WildFly,
    the core services are only started on demand. This means the clustering services
    are started only when the server detects that they are required and are stopped
    when no longer required. Hence, simply starting the server with a configuration
    that includes the clustering subsystems will not initiate the clustering services.
    To do this, we will need to deploy a cluster-enabled application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟内，你的服务器将开始运行；然而，我们没有提到与控制台中的集群节点相关的任何细节。这是因为，在WildFly中，核心服务仅在需要时启动。这意味着集群服务仅在服务器检测到需要时启动，并在不再需要时停止。因此，仅使用包含集群子系统的配置启动服务器不会启动集群服务。为此，我们需要部署一个集群启用的应用程序。
- en: 'So, in order to verify our installation, we will deploy a bare-bones, cluster-enabled,
    web application named `Example.war`. To enable clustering of your web applications,
    you must mark them as *distributable* in the `web.xml` descriptor:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了验证我们的安装，我们将部署一个基本的、集群启用的、名为`Example.war`的Web应用程序。要启用你的Web应用的集群，你必须将它们标记为`*distributable*`在`web.xml`描述符中：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When you have deployed the application to both machines, you will see that
    the clustering services are now started and that each machine is able to find
    other members within the cluster, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将应用程序部署到两台机器上时，你会看到集群服务现在已启动，并且每台机器都能在集群中找到其他成员，如下所示：
- en: '![A cluster of nodes running on different machines](img/6232OS_08_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![运行在不同机器上的节点集群](img/6232OS_08_02.jpg)'
- en: A cluster of nodes running on the same machine
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行在同一台机器上的节点集群
- en: The second variant of the standalone configuration comes into play when your
    server nodes are located (all or some of them) on the same machine. This scenario
    generally applies when you are scaling your architecture *vertically* by adding
    more hardware resources to your computer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的服务器节点（全部或部分）位于同一台机器上时，独立配置的第二种变体开始发挥作用。这种情况通常适用于你通过向计算机添加更多硬件资源来垂直扩展你的架构时。
- en: 'Configuring server nodes on the same machine obviously requires duplicating
    your WildFly distribution on your filesystem. In order to avoid port conflicts
    between server distributions, you have to choose between the following two options:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一台机器上配置服务器节点显然需要你在文件系统中复制你的 WildFly 发行版。为了避免服务器发行版之间的端口冲突，你必须在这两个选项之间进行选择：
- en: Define multiple IP address on the same machine
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一台机器上定义多个 IP 地址
- en: Define a port offset for each server distribution
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个服务器发行版定义端口偏移量
- en: Setting up a cluster on the same machine using multiple IP addresses
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在同一台机器上使用多个 IP 地址设置集群
- en: This is also known as **multihoming** and requires a small amount of configuration
    to get working. Each operating system uses a different approach to achieve this.
    Illustrating the possible ways to configure multihoming is outside the scope of
    this book but, if you are interested in multihoming, we have provided links with
    detailed instructions on how to set up multihoming on Linux and Windows.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为 **多宿主**，需要少量配置才能工作。每个操作系统都有不同的方法来实现这一点。本书不涉及配置多宿主的可能方法，但如果你对多宿主感兴趣，我们提供了有关如何在
    Linux 和 Windows 上设置多宿主的详细说明链接。
- en: 'If you are using Linux, this tutorial describes in detail how to assign multiple
    IPs to a single network interface, also known as **IP aliasing**:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Linux，这篇教程详细介绍了如何将多个 IP 地址分配给单个网络接口，也称为 **IP 别名**：
- en: '[http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/](http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/](http://www.tecmint.com/create-multiple-ip-addresses-to-one-single-network-interface/)'
- en: 'Windows users can refer to the following blog that details how to set up multihoming
    in Windows 7:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 用户可以参考以下博客，了解如何在 Windows 7 中设置多宿主：
- en: '[http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html](http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html](http://shaheerart.blogspot.com/2011/05/how-to-configure-multihomed-server-in.html)'
- en: 'Once you have set up your network interface correctly, you will need to update
    your `standalone-ha.xml` file. You need to bind each IP to a different WildFly
    instance, just as we did when setting up the multiple-host cluster. Within the
    configuration file, navigate to the `interfaces` section and, within the nested
    `interface` element, insert the IP address to be bound to that standalone server:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你正确设置了网络接口，你将需要更新你的 `standalone-ha.xml` 文件。你需要将每个 IP 绑定到不同的 WildFly 实例，就像我们在设置多主机集群时做的那样。在配置文件中，导航到
    `interfaces` 部分，并在嵌套的 `interface` 元素中插入要绑定到该独立服务器的 IP 地址：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, the first server distribution is bound to the IP Address `192.168.10.1`
    and the second one to `192.168.10.2`. (remember that you can also use the `-b`
    and `-bmanagement` switches described earlier).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第一个服务器分配绑定到 IP 地址 `192.168.10.1`，第二个绑定到 `192.168.10.2`。（记住，你还可以使用前面描述的
    `-b` 和 `-bmanagement` 开关）。
- en: 'The following figure depicts this scenario:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这个场景：
- en: '![Setting up a cluster on the same machine using multiple IP addresses](img/6232OS_08_06.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![在同一台机器上使用多个 IP 地址设置集群](img/6232OS_08_06.jpg)'
- en: Setting up a cluster on the same machine using port offset
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在同一台机器上使用端口偏移设置集群
- en: Configuring multihoming is not always a viable choice, as it requires a relative
    amount of network administration experience. A simpler and more straightforward
    option is to define a port offset for each of your cluster members. By defining
    a port offset for each server, all the default-server binding interfaces will
    shift by a fixed number, hence you will not have two servers running on the same
    ports, causing port conflicts.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 配置多宿主并不总是可行的选择，因为它需要一定量的网络管理经验。一个更简单、更直接的选择是为每个集群成员定义一个端口偏移量。通过为每个服务器定义端口偏移量，所有默认服务器绑定接口都将按固定数值移动，因此你不会有两个服务器在相同的端口上运行，从而避免端口冲突。
- en: 'When using port offset, you will bind each server to the same IP address. So,
    for all your server distributions, you will configure the `standalone-ha.xml`
    file as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用端口偏移时，你将每个服务器绑定到相同的 IP 地址。因此，对于所有你的服务器发行版，你将按照以下方式配置 `standalone-ha.xml`
    文件：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will then leave the first server configuration unchanged. It will use the
    default socket-binding ports:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将第一个服务器配置保持不变。它将使用默认的套接字绑定端口：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For the second server configuration, you will specify a `port-offset` value
    of `150`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个服务器配置，你将指定 `port-offset` 值为 `150`：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Your cluster configuration is now complete. You can verify this by starting
    each server distribution by passing it as an argument to the configuration file
    as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您的集群配置现在已完成。您可以通过将配置文件作为参数传递给每个服务器分布来验证此操作：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From the following screenshot, you can see that a port offset of 150 has been
    applied:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下屏幕截图可以看出，已应用150的端口偏移：
- en: '![Setting up a cluster on the same machine using port offset](img/6232OS_08_07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![在同一台机器上使用端口偏移设置集群](img/6232OS_08_07.jpg)'
- en: Setting up a cluster of domain servers
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置域服务器的集群
- en: When you are configuring a domain cluster, you will find that the clustering
    subsystems are already included within the main configuration file `domain.xml`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当您配置域集群时，您会发现聚类子系统已经包含在主配置文件`domain.xml`中。
- en: 'As a matter of fact, the WildFly domain deals with clustering just as another
    profile used by the application server. Opening the `domain.xml` file, you will
    see that the application server ships with the following four profiles:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，WildFly域将聚类处理得就像应用程序服务器使用的另一个配置文件一样。打开`domain.xml`文件，您会看到应用程序服务器附带以下四个配置文件：
- en: The `default` profile for nonclustered environments
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对非集群环境的`default`配置文件
- en: The `ha` profile for clustered environments
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对集群环境的`ha`配置文件
- en: The `full` profile with all the subsystems for nonclustered environments
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对非集群环境的`full`配置文件，包含所有子系统
- en: The `full-ha` profile with all the subsystems for clustered environments
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对集群环境的`full-ha`配置文件，包含所有子系统
- en: So, in order to use clustering on a domain, you have to first configure your
    server groups to point to one of the `ha` profiles.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了在域上使用聚类，您必须首先配置您的服务器组以指向其中一个`ha`配置文件。
- en: 'Let''s look at an example configuration that uses two server groups. The following
    code snippet is from `domain.xml`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个使用两个服务器组的示例配置。以下代码片段来自`domain.xml`：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As highlighted in the `socket-binding-group` element, we are referencing the
    `ha-sockets` group, which contains all socket bindings used for the cluster. Have
    a look at the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如`socket-binding-group`元素所强调的，我们正在引用包含集群中所有套接字绑定的`ha-sockets`组。请看以下代码：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we need to define the servers that are part of the domain (and of the
    cluster). To keep things simple, we will reuse the domain server list that is
    found in the default `host.xml` file, as shown in the following code snippet:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义属于域（和集群）的服务器。为了使事情简单，我们将重用默认`host.xml`文件中找到的域服务器列表，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We do not need to specify the socket-binding group for each server, as this
    was configured in the `domain.xml` file. If we want to override the socket-binding
    group, then we can add the following to the `host.xml` file:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要为每个服务器指定套接字绑定组，因为这在`domain.xml`文件中已经配置好了。如果我们想覆盖套接字绑定组，则可以在`host.xml`文件中添加以下内容：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following figure shows an overview of this configuration:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了此配置的概述：
- en: '![Setting up a cluster of domain servers](img/6232OS_08_08.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![设置域服务器的集群](img/6232OS_08_08.jpg)'
- en: Your clustered domain can now be started using the standard batch script (`domain.sh`
    or `domain.bat`). The server groups will now point to the `ha` profile and form
    a cluster of two nodes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用标准批处理脚本（`domain.sh`或`domain.bat`）启动您的集群域。服务器组现在将指向`ha`配置文件，并形成一个由两个节点组成的集群。
- en: Troubleshooting the cluster
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚集故障排除
- en: Communication via nodes in a cluster is achieved via UDP and multicasts.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集群中的节点进行通信是通过UDP和多播实现的。
- en: Note
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A multicast is a protocol by which data is transmitted simultaneously to all
    hosts that are part of the multicast group. You can think about multicast as a
    radio channel where only those tuned to a particular frequency receive the data.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多播是一种协议，通过该协议数据同时传输到所有属于多播组的宿主。您可以将多播想象成一个只有调谐到特定频率的接收器才能接收数据的无线电频道。
- en: 'If you are having problems, typically it is due to one of the following reasons:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到问题，通常是由于以下原因之一：
- en: The nodes are behind a firewall. If your nodes are on different machines, then
    it is possible that the firewall is blocking the multicasts. you can test this
    by disabling the firewall for each node or adding the appropriate rules.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点位于防火墙之后。如果您的节点在不同的机器上，那么防火墙可能正在阻止多播。您可以通过为每个节点禁用防火墙或添加适当的规则来测试这一点。
- en: You are using a home network or are behind a gateway. Typically, home networks
    will redirect any UDP traffic to the **Internet Service Provider** (**ISP**),
    which is then either dropped by the ISP or just lost. To fix this, you will need
    to add a route to the firewall/gateway that will redirect any multicast traffic
    back on to the local network instead.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您正在使用家庭网络或位于网关之后。通常，家庭网络会将任何 UDP 流量重定向到 **互联网服务提供商**（**ISP**），然后 ISP 要么丢弃它，要么它就丢失了。为了解决这个问题，您需要在防火墙/网关中添加一个路由，将任何多播流量重定向回本地网络。
- en: Tip
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Mac OS X**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**Mac OS X**'
- en: 'If you are using a Mac, you may get a **java.io.IOException: Network is unreachable**
    error when trying to start a domain in the `ha` mode. To get around this, you
    will need to create a proper network route to use UDP as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您使用的是 Mac，当尝试以 `ha` 模式启动域时，可能会遇到 **java.io.IOException: Network is unreachable**
    错误。为了解决这个问题，您需要创建一个合适的网络路由来使用 UDP，如下所示：'
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To allow you to check whether your machine is set up correctly for multicast,
    JGroups ships with two test applications that can be used to test IP multicast
    communication. The test classes are `McastReceiverTest` and `McastSenderTest`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您检查您的机器是否已正确设置以进行多播，JGroups 随带两个测试应用程序，可用于测试 IP 多播通信。测试类是 `McastReceiverTest`
    和 `McastSenderTest`。
- en: 'In order to test multicast communication on your server, you should first navigate
    to the location of the `jgroups` JAR within the `modules` directory, shown here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试服务器上的多播通信，您首先需要导航到 `modules` 目录中 `jgroups` JAR 文件的位置，如下所示：
- en: '`JBOSS_HOME/modules/system/layers/base/org/jgroups/main`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`JBOSS_HOME/modules/system/layers/base/org/jgroups/main`'
- en: Within this directory, you will find `jgroups-3.4.3.Final.jar`, which contains
    the test programs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在此目录中，您将找到包含测试程序的 `jgroups-3.4.3.Final.jar` 文件。
- en: 'Now, run the `McastReceiverTest` by running the following command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过运行以下命令来运行 `McastReceiverTest`：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'On the same machine, but in a different terminal, run the `McastSenderTest`
    command, as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一台机器上，但在不同的终端中，运行 `McastSenderTest` 命令，如下所示：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If multicast works correctly, you should be able to type in the `McastSenderTest`
    window and see the output in the `McastReceiverTest` window, as shown in the following
    screenshot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多播工作正常，您应该在 `McastSenderTest` 窗口中输入，并在 `McastReceiverTest` 窗口中看到输出，如下面的截图所示：
- en: '![Troubleshooting the cluster](img/6232OS_08_09.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![集群故障排除](img/6232OS_08_09.jpg)'
- en: You should perform this test on each machine in the cluster. Once you have done
    this, you need to ensure that UDP communication works between each machine in
    the cluster by running `McastSenderTest` on one machine and `McastReceiverTest`
    on the other.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在集群中的每台机器上执行此测试。一旦完成，您需要确保通过在一台机器上运行 `McastSenderTest` 和在另一台机器上运行 `McastReceiverTest`
    来确保集群中每台机器之间的 UDP 通信正常工作。
- en: 'Finally, if you are experiencing issues with the default multicast address
    or port, you can change it by modifying the `jgroups-udp` socket-binding group
    within the `domain.xml` file:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您在使用默认的多播地址或端口时遇到问题，您可以通过修改 `domain.xml` 文件中的 `jgroups-udp` socket-binding
    group 来更改它：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Configuring the WildFly cluster
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 WildFly 集群
- en: 'WildFly supports clustering out of the box. There are several libraries that
    work together to provide support for clustering. The following figure shows the
    basic clustering architecture adopted by WildFly:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: WildFly 支持开箱即用的集群功能。有几个库协同工作以提供对集群的支持。以下图显示了 WildFly 采纳的基本集群架构：
- en: '![Configuring the WildFly cluster](img/6232OS_08_10.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![配置 WildFly 集群](img/6232OS_08_10.jpg)'
- en: The JGroups library is core to WildFly clustering. It provides the communication
    channels between nodes of the cluster using a multicast transmission. These channels
    are created upon deployment of a clustered application and are used to transmit
    sessions and contexts around the cluster.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: JGroups 库是 WildFly 集群的核心。它通过多播传输提供集群节点之间的通信通道。这些通道在部署集群应用程序时创建，并用于在集群中传输会话和上下文。
- en: Another important component of clustering is **Infinispan**. Infinispan handles
    the replication of your application data across the cluster by means of a distributed
    cache.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的另一个重要组件是 **Infinispan**。Infinispan 通过分布式缓存的方式处理您的应用程序数据在集群中的复制。
- en: Configuring the JGroups subsystem
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 JGroups 子系统
- en: Within the realm of JGroups, nodes are commonly referred to as **members**,
    and clusters are referred to as **groups**.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JGroups 的范围内，节点通常被称为 **成员**，而集群被称为 **组**。
- en: A node is a process running on a host. JGroups keeps track of all processes
    within a group. When a node joins a group, the system sends a message to all existing
    members of that group. Likewise, when a node leaves or crashes, all the other
    nodes of that group are notified.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 节点是在主机上运行的一个进程。JGroups跟踪组内所有进程。当一个节点加入一个组时，系统会向该组的所有现有成员发送消息。同样，当一个节点离开或崩溃时，该组的所有其他节点都会收到通知。
- en: 'As we outlined earlier in the chapter, the processes (nodes) of a group can
    be located on the same host, or on different machines on a network. A member can
    also be part of multiple groups. The following figure illustrates a detailed view
    of JGroups architecture:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本章前面概述的，一个组的进程（节点）可以位于同一主机上，也可以位于网络上的不同机器上。一个成员也可以是多个组的成员。以下图展示了JGroups架构的详细视图：
- en: '![Configuring the JGroups subsystem](img/6232OS_08_11.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![配置JGroups子系统](img/6232OS_08_11.jpg)'
- en: A JGroups process broadly consists of three parts, namely a **Channel**, the
    **Building Blocks**, and the **Protocol Stack**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: JGroups进程大致由三部分组成，即**通道**、**构建块**和**协议栈**。
- en: A **Channel** is a simple socket-like interface used by application programmers
    to build reliable group communication applications.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通道**是一个类似于套接字的简单接口，应用程序程序员使用它来构建可靠的群组通信应用程序。'
- en: The **Building Blocks** collectively form an abstraction interface layered on
    top of channels, which can be used instead of channels whenever a higher level
    interface is required.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建块**共同形成了一个抽象接口，该接口位于通道之上，可以在需要高级接口时替代通道使用。'
- en: The **Protocol Stack** contains a number of protocol layers in a bidirectional
    list. All messages sent have to pass through all the protocols. A layer does not
    *necessarily* correspond to a transport protocol. For example, a fragmentation
    layer might break up a message into several smaller messages, adding a header
    with an ID to each fragment, and reassemble the fragments on the receiver's side.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协议栈**包含一个双向列表中的多个协议层。所有发送的消息都必须通过所有协议。一个层不一定对应一个传输协议。例如，一个分片层可能会将一个消息分成几个较小的消息，为每个片段添加一个带有ID的头部，并在接收方重新组装这些片段。'
- en: In the previous figure, when sending a message, the `PING` protocol is executed
    first, then `MERGE2`, followed by `FD_SOCK`, and finally, the `FD` protocol. When
    the message is received, this order would be reversed, which means that it would
    meet the `FD` protocol first, then `FD_SOCK`, followed by `MERGE2`, and finally
    up to `PING`. In WildFly, the JGroups configuration is found within the JGroups
    subsystem in the main `standalone-ha.xml/domain.xml` configuration file.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个图中，当发送消息时，首先执行`PING`协议，然后是`MERGE2`，接着是`FD_SOCK`，最后是`FD`协议。当收到消息时，这个顺序会被颠倒，这意味着它首先会遇到`FD`协议，然后是`FD_SOCK`，接着是`MERGE2`，最后是`PING`。在WildFly中，JGroups配置位于主`standalone-ha.xml/domain.xml`配置文件中的JGroups子系统内。
- en: 'Within the JGroups subsystem, you can find the list of configured transport
    stacks. The following code snippet shows the default UDP stack used for communication
    between nodes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在JGroups子系统中，你可以找到配置的传输堆栈列表。以下代码片段显示了节点之间通信使用的默认UDP堆栈：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: UDP is the default protocol for JGroups and uses multicast (or, if not available,
    multiple unicast messages) to send and receive messages. A multicast UDP socket
    can send and receive datagrams from multiple clients. Another feature of multicast
    is that a client can contact multiple servers with a single packet, without knowing
    the specific IP address of any of the hosts.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: UDP是JGroups的默认协议，使用多播（如果不可用，则使用多个单播消息）来发送和接收消息。一个多播UDP套接字可以向多个客户端发送和接收数据报。多播的另一个特性是，客户端可以使用单个数据包联系多个服务器，而不需要知道任何主机的特定IP地址。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Switching to the TCP protocol is as easy as changing the `default-stack` attribute:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 切换到TCP协议就像更改`default-stack`属性一样简单：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: TCP stacks are typically used when IP multicasting cannot be used for some reason.
    For example, when you want to create a network over a WAN. We will cover TCP configuration
    later in this chapter.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当IP多播由于某些原因无法使用时，通常使用TCP堆栈。例如，当你想在WAN上创建一个网络时。我们将在本章后面介绍TCP配置。
- en: A detailed description of all JGroups protocols is beyond the scope of this
    book but, for convenience, you can find a short description of each in the following
    table. To find out more about these protocols, or about JGroups, you can refer
    to the JGroups site at [http://jgroups.org/manual/html/index.html](http://jgroups.org/manual/html/index.html).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所有JGroups协议的详细描述超出了本书的范围，但为了方便，你可以在以下表中找到每个协议的简要描述。要了解更多关于这些协议或关于JGroups的信息，你可以参考JGroups网站[http://jgroups.org/manual/html/index.html](http://jgroups.org/manual/html/index.html)。
- en: '| Category | Usage | Protocols |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 用途 | 协议 |'
- en: '| --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Transport | This is responsible for sending and receiving messages across
    the network | `IDP`, `TCP`, and `TUNNEL` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 传输 | 这负责在网络中发送和接收消息 | `IDP`, `TCP`, 和 `TUNNEL` |'
- en: '| Discovery | This is used to discover active nodes in the cluster and determine
    who the coordinator is | `PING`, `MPING`, `TCPPING`, and `TCPGOSSIP` |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 发现 | 这用于发现集群中的活动节点并确定协调者是谁 | `PING`, `MPING`, `TCPPING`, 和 `TCPGOSSIP` |'
- en: '| Failure detection | This one is used to poll cluster nodes to detect node
    failures | `FD`, `FD_SIMPLE`, `FD_PING`, `FD_ICMP`, `FD_SOCK`, and `VERIFY_SUSPECT`
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 故障检测 | 此功能用于轮询集群节点以检测节点故障 | `FD`, `FD_SIMPLE`, `FD_PING`, `FD_ICMP`, `FD_SOCK`,
    和 `VERIFY_SUSPECT` |'
- en: '| Reliable delivery | This ensures that messages are actually delivered in
    the right order (FIFO) to the destination node | `CAUSAL`, `NAKACK`, `pbcast`.`NAKACK`,
    `SMACK`, `UNICAST`, and `PBCAST` |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 可靠投递 | 这确保消息确实以正确的顺序（FIFO）发送到目标节点 | `CAUSAL`, `NAKACK`, `pbcast`.`NAKACK`,
    `SMACK`, `UNICAST`, 和 `PBCAST` |'
- en: '| Group membership | This is used to notify the cluster when a node joins,
    leaves, or crashes | `pbcast`.`GMS`, `MERGE`, `MERGE2`, and `VIEW_SYNC` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 组成员 | 这用于在节点加入、离开或崩溃时通知集群 | `pbcast`.`GMS`, `MERGE`, `MERGE2`, 和 `VIEW_SYNC`
    |'
- en: '| Flow control | This is used to adapt the data-sending rate to the data-receipt
    rate among the nodes | `FC` |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 流控制 | 这用于调整节点间数据发送速率以适应数据接收速率 | `FC` |'
- en: '| Fragmentation | This fragments messages larger than a certain size and unfragments
    them at the receiver''s side | `FRAG2` |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 分片 | 这会将大于特定大小的消息分片，并在接收方处解分片 | `FRAG2` |'
- en: '| State transfer | This one synchronizes the application state (serialized
    as a byte array) from an existing node with a newly-joining node | `pbcast`.`STATE_TRANSFER`
    and `pbcast`.`STREAMING_STATE_TRANSFER` |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 状态转移 | 此功能将现有节点上的应用程序状态（序列化为字节数组）与新加入的节点同步 | `pbcast`.`STATE_TRANSFER` 和
    `pbcast`.`STREAMING_STATE_TRANSFER` |'
- en: Customizing the protocol stack
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义协议栈
- en: 'If you want to customize your transport configuration at a lower level, then
    you can override the default properties used by JGroups or even the single protocol
    properties. For example, the following configuration can be used to change the
    default send or receive buffer used by the JGroups UDP stack:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在较低级别自定义传输配置，那么你可以覆盖JGroups使用的默认属性，甚至可以覆盖单个协议属性。例如，以下配置可以用来更改JGroups UDP堆栈使用的默认发送或接收缓冲区：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you want to have a look at all the properties available within the JGroups
    subsystem, either at transport level or at the protocol level, you can consult
    the JGroups XSD file, `jboss-as-jgroups_2_0.xsd`, found in the `JBOSS_HOME/docs/schema`
    folder of your server distribution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看JGroups子系统内可用的所有属性，无论是在传输层还是在协议层，你可以查阅你的服务器发行版`JBOSS_HOME/docs/schema`文件夹中的JGroups
    XSD文件，`jboss-as-jgroups_2_0.xsd`。
- en: Configuring the Infinispan subsystem
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Infinispan子系统
- en: One of the requirements of a cluster is that data is synchronized across its
    members. This is because, should there be a failure of a node, the application
    and its session can continue on other members of the cluster. This is known as
    **High Availability**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 集群的一个要求是数据在其成员之间同步。这是因为，如果某个节点发生故障，应用程序及其会话可以在集群的其他成员上继续运行。这被称为**高可用性**。
- en: WildFly uses Infinispan as the distributed caching solution behind its clustering
    functionality. Although Infinispan is embedded within the application server,
    it can also be used as a standalone data-grid platform.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: WildFly使用Infinispan作为其集群功能背后的分布式缓存解决方案。尽管Infinispan嵌入在应用程序服务器中，但它也可以作为一个独立的数据网格平台。
- en: We will now quickly look at Infinispan's configuration, which is found in the
    Infinispan subsystem within the main `standalone-ha.xml` or `domain.xml` configuration
    file.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将快速查看Infinispan的配置，该配置位于主`standalone-ha.xml`或`domain.xml`配置文件中的Infinispan子系统内。
- en: 'The following is the backbone of the Infinispan configuration:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Infinispan配置的核心：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: One of the key differences between the standalone Infinispan configuration and
    the Infinispan subsystem within WildFly is that the WildFly configuration exposes
    multiple `cache-container` elements, while the native configuration file contains
    configurations for a single cache container.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 独立Infinispan配置与WildFly内部的Infinispan子系统之间的一个关键区别是，WildFly配置公开了多个`cache-container`元素，而本地配置文件包含单个缓存容器的配置。
- en: 'Each `cache-container` element contains one or more caching policies, which
    define how data is synchronized for that specific cache container. The following
    caching strategies can be used by cache containers:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`cache-container`元素包含一个或多个缓存策略，这些策略定义了特定缓存容器中数据的同步方式。缓存容器可以使用以下缓存策略：
- en: '**Local**: In this caching mode, the entries are stored on the local node only,
    regardless of whether a cluster has formed. Infinispan typically operates as a
    local cache.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地**: 在这种缓存模式下，条目仅存储在本地节点上，无论是否形成了集群。Infinispan通常作为本地缓存运行。'
- en: '**Replication**: In this caching mode, all entries are replicated to all nodes.
    Infinispan typically operates as a temporary data store and doesn''t offer increased
    heap space.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制**: 在这种缓存模式下，所有条目都复制到所有节点。Infinispan通常作为临时数据存储运行，并不提供增加的堆空间。'
- en: '**Distribution**: In this caching mode, the entries are distributed to a subset
    of the nodes only. Infinispan typically operates as a data grid providing increased
    heap space.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布**: 在这种缓存模式下，条目仅分布到节点子集。Infinispan通常作为提供增加堆空间的数据网格运行。'
- en: '**Invalidation**: In this caching mode, the entries are stored in a cache store
    only (such as a database) and invalidated from all nodes. When a node needs the
    entry, it will load it from a cache store. In this mode, Infinispan operates as
    a distributed cache, backed by a canonical data store, such as a database.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**失效**: 在这种缓存模式下，条目仅存储在缓存存储中（例如数据库）并从所有节点中失效。当一个节点需要条目时，它将从缓存存储中加载它。在这种模式下，Infinispan作为由规范数据存储（如数据库）支持的分布式缓存运行。'
- en: In the following sections, we will have a more detailed look at some of the
    cache configurations, such as `session` caches (the `web` cache and the `SFSB`
    cache) and the `hibernate` cache. Understanding these is essential if you are
    to configure your clustered applications properly.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地查看一些缓存配置，例如`session`缓存（`web`缓存和`SFSB`缓存）以及`hibernate`缓存。如果您要正确配置您的集群应用程序，理解这些配置是至关重要的。
- en: Configuring session cache containers
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置会话缓存容器
- en: 'In this section, we will look at the caching configuration for the HTTP session
    and for stateful and singleton-session beans. The way the caches are configured
    for these three is very similar. For this reason, we will discuss them together
    and show the similarities between them. So, here is the `cache-container` configuration
    for the `web` cache, the `ejb` cache, and the `server` cache. The `web` cache
    refers to the HTTP session cache, the `ejb` cache relates to stateful session
    beans (SFSBs), and the `server` cache relates to the singleton-session beans:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看HTTP会话、有状态和单例会话bean的缓存配置。这三个缓存配置的方式非常相似。因此，我们将一起讨论它们，并展示它们之间的相似性。因此，以下是`web`缓存、`ejb`缓存和`server`缓存的`cache-container`配置。`web`缓存指的是HTTP会话缓存，`ejb`缓存与有状态会话bean（SFSBs）相关，`server`缓存与单例会话bean相关：
- en: '[PRE23]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The configuration for each container can contain one or more caching strategy
    elements. These elements are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每个容器的配置可以包含一个或多个缓存策略元素。这些元素如下：
- en: '`replicated-cache`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`复制缓存`'
- en: '`distributed-cache`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`分布式缓存`'
- en: '`invalidation-cache`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`失效缓存`'
- en: '`local-cache`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`本地缓存`'
- en: 'Each of these cache elements can be defined zero or more times. To specify
    which cache element to use for the cache container, simply reference the name
    of the cache as the property of the `default-cache` attribute. In the next section,
    we will explore in detail the differences between these cache modes. Within each
    cache definition, you may have noticed the `locking` attribute that corresponds
    to the equivalent database isolation levels. Infinispan supports the following
    isolation levels:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个这些缓存元素都可以定义零次或多次。要指定用于缓存容器的缓存元素，只需将缓存名称作为`default-cache`属性的属性引用。在下一节中，我们将详细探讨这些缓存模式之间的差异。在每个缓存定义中，您可能已经注意到与等效数据库隔离级别相对应的`locking`属性。Infinispan支持以下隔离级别：
- en: '`NONE`: No isolation level means no transactional support.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NONE`: 没有隔离级别意味着没有事务支持。'
- en: '`READ_UNCOMMITTED`: The lowest isolation level, dirty reads are allowed, which
    means one transaction may see uncommitted data from another transaction. Rows
    are only locked during the writing of data, not for reads.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`READ_UNCOMMITTED`：最低的隔离级别，允许脏读，这意味着一个事务可能看到来自另一个事务未提交的数据。行仅在写入数据时锁定，而不是在读取时锁定。'
- en: '`READ_COMMITTED`: The transaction acquires read and write locks on all retrieved
    data. Write locks are released at the end of the transaction, and read locks are
    released as soon as the data is selected.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`READ_COMMITTED`：事务在检索的所有数据上获取读和写锁。写锁在事务结束时释放，读锁在数据被选中时立即释放。'
- en: '`REPEATABLE_READ`: This is the default isolation level used by Infinispan.
    The transaction acquires read and write locks on all retrieved data and is kept
    until the end of the transaction. Phantom reads can occur. Phantom reads are when
    you execute the same query in the same transaction and get a different number
    of results.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`REPEATABLE_READ`：这是Infinispan使用的默认隔离级别。事务在检索的所有数据上获取读和写锁，并保持到事务结束。可能会发生幻读。幻读是指你在同一个事务中执行相同的查询，但得到的结果数量不同。'
- en: '`SERIALIZABLE`: The strictest isolation level. All transactions occur in an
    isolated fashion as if they are being executed serially (one after the other),
    as opposed to concurrently.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SERIALIZABLE`：最严格的隔离级别。所有事务都以隔离方式发生，就像它们是按顺序（一个接一个）执行一样，而不是并发执行。'
- en: Another element nested within the cache configuration is `file-store`. This
    element configures the path in which to store the cached data. The default data
    is written in the `jboss.server.data.dir` directory under a folder with the same
    name as the cache container.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存配置中嵌套的另一个元素是`file-store`。此元素配置了存储缓存数据的路径。默认数据写入与缓存容器同名的文件夹下的`jboss.server.data.dir`目录。
- en: 'For example, the following figure shows the default `file-store` path for the
    standalone `web` cache container:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下图显示了独立`web`缓存容器的默认`file-store`路径：
- en: '![Configuring session cache containers](img/6232OS_08_12.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![配置会话缓存容器](img/6232OS_08_12.jpg)'
- en: 'If you wish, you can customize the `file-store` path using the `relative-to`
    and `path` elements, just as we did in [Chapter 2](ch02.html "Chapter 2. Configuring
    the Core WildFly Subsystems"), *Configuring the Core WildFly Subsystems*, for
    the path element:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，可以使用`relative-to`和`path`元素自定义`file-store`路径，就像我们在[第2章](ch02.html "第2章。配置核心WildFly子系统")中做的那样，*配置核心WildFly子系统*，对于路径元素：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Before moving on, let's briefly look at the way messages are sent between each
    node.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们简要地看看消息如何在各个节点之间发送。
- en: 'Data synchronization across members can be done via synchronous messages (`SYNC`)
    or asynchronous messages (`ASYNC`), which are defined as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 成员之间的数据同步可以通过同步消息（`SYNC`）或异步消息（`ASYNC`）来完成，具体定义如下：
- en: '**Synchronous** messaging is the least efficient of the two, as each node needs
    to wait for a message acknowledgement from other cluster members. However, synchronous
    mode is useful if you have a need for high consistency.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步**消息是两者中效率最低的，因为每个节点都需要等待来自其他集群成员的消息确认。然而，如果需要高一致性，同步模式是有用的。'
- en: '**Asynchronous** messaging is the quicker of the two, the flip side being that
    consistency suffers. Asynchronous messaging is particularly useful when HTTP session
    replication and sticky sessions are enabled. In this scenario, a session is always
    accessed from the same cluster node. Only when a node fails is the data accessed
    from a different node.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步**消息是两者中较快的，但缺点是一致性会受到影响。异步消息在启用HTTP会话复制和粘性会话时特别有用。在这种情况下，会话总是从同一个集群节点访问。只有当节点失败时，数据才会从不同的节点访问。'
- en: 'The SYNC and ASYNC properties are set in the `mode` attribute of the cache
    element:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 同步（SYNC）和异步（ASYNC）属性是在缓存元素的`mode`属性中设置的：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Choosing between replication and distribution
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择复制和分布之间的选择
- en: 'When using **replicated** caching, Infinispan will store every entry on every
    node in the cluster grid. This means that entries added to any one of these cache
    instances will be replicated to all other cache instances in the cluster, and
    any entry can be retrieved from any cache. The arrows indicate the direction in
    which data is being replicated. In the following figure, you can see that session
    data from Node 1 is being copied to Nodes 2, 3, and 4:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用**复制**缓存时，Infinispan将在集群网格中的每个节点上存储每个条目。这意味着添加到这些缓存实例中的任何条目都将复制到集群中的所有其他缓存实例，并且可以从任何缓存中检索任何条目。箭头指示数据复制的方向。在下面的图中，您可以看到来自节点1的会话数据正在被复制到节点2、3和4：
- en: '![Choosing between replication and distribution](img/6232OS_08_13.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![选择复制和分布](img/6232OS_08_13.jpg)'
- en: The scalability of replication is a function of cluster size and average data
    size. If we have many nodes and/or large data sets, we hit a scalability ceiling.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 复制的可伸缩性是集群大小和平均数据大小的函数。如果我们有多个节点和/或大数据集，我们将遇到可伸缩性的上限。
- en: Note
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If `DATA_SIZE * NUMBER_OF_HOSTS` is smaller than the memory available to each
    host, then replication is a viable choice.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`DATA_SIZE * NUMBER_OF_HOSTS`小于每个主机可用的内存，那么复制是一个可行的选择。
- en: On the other hand, when using **distributed** caching, Infinispan will store
    every cluster entry on a subset of the nodes in the grid.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当使用**分布式**缓存时，Infinispan将在网格的节点子集上存储每个集群条目。
- en: Distribution makes use of a consistent-hash algorithm to determine where entries
    should be stored within the cluster. You can configure how many copies of a cache
    entry are maintained across the cluster. The value you choose here is a balance
    between performance and durability of data. The more copies you maintain, the
    lower the performance, but the lower the risk of losing data due to server outages.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式缓存利用一致性哈希算法来确定条目应在集群中存储的位置。您可以配置跨集群维护的缓存条目副本数量。您在这里选择的值是在性能和数据持久性之间的平衡。您维护的副本越多，性能越低，但服务器故障导致数据丢失的风险越低。
- en: Note
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can use the `owners` parameter (with a default value of `2`) to define
    the number of cluster-wide copies for each cache entry:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`owners`参数（默认值为`2`）来定义每个缓存条目的集群范围副本数：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following figure shows how the session data will be replicated across the
    nodes when the `owners` parameter is set to `2`. Each node replicates its session
    data to two other nodes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了当`owners`参数设置为`2`时，会话数据如何在节点之间进行复制。每个节点将其会话数据复制到另外两个节点：
- en: '![Choosing between replication and distribution](img/6232OS_08_14.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![选择复制和分布](img/6232OS_08_14.jpg)'
- en: The choice between replication and distribution depends largely on the cluster
    size. For example, replication provides a quick and easy way to share states across
    a cluster; however, it only performs well in small clusters (fewer than ten servers).
    This is due to the increased number of replication messages that need to be sent
    as cluster size increases. In a distributed cache, several copies of an entry
    are maintained across nodes in order to provide redundancy and fault tolerance.
    The number of copies saved is typically far fewer than the number of nodes in
    the cluster. This means a distributed cache provides a far greater degree of scalability
    than a replicated cache.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 复制和分布之间的选择在很大程度上取决于集群大小。例如，复制提供了一种快速简单的方法在集群之间共享状态；然而，它仅在小型集群（少于十个服务器）中表现良好。这是因为随着集群大小的增加，需要发送的复制消息数量增加。在分布式缓存中，为了提供冗余和容错，节点间维护了条目的多个副本。保存的副本数量通常远少于集群中的节点数量。这意味着分布式缓存比复制缓存提供了更大的可伸缩性。
- en: Configuring the hibernate cache
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Hibernate缓存
- en: The hibernate cache container is a key part of your configuration as it handles
    the caching of your data tier. WildFly uses hibernate as the default JPA implementation,
    so the concepts described in this chapter apply both to hibernate applications
    and to JPA-based applications. Hibernate caches are conceptually different from
    session-based caches. They are based on the assumption that you have a permanent
    storage for your data (the database) This means that it is not necessary to replicate
    or distribute copies of the entities across the cluster in order to achieve high
    availability. You just need to inform your nodes when data has been modified so
    that the entry in the cache can be invalidated. If a cache is configured for invalidation
    rather than replication, every time data is changed in a cache, other caches in
    the cluster receive a message that their data is now stale and should be evicted
    from memory.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Hibernate 缓存容器是您配置的关键部分，因为它处理数据层的缓存。WildFly 使用 Hibernate 作为默认的 JPA 实现，因此本章中描述的概念既适用于
    Hibernate 应用程序，也适用于基于 JPA 的应用程序。Hibernate 缓存在概念上与基于会话的缓存不同。它们基于这样的假设，即您有一个永久存储数据的地方（数据库）。这意味着为了实现高可用性，不需要在集群中复制或分发实体的副本。您只需在数据被修改时通知您的节点，以便缓存中的条目可以被失效。如果缓存配置为失效而不是复制，每次缓存中的数据发生变化时，集群中的其他缓存都会收到一条消息，告知它们的数据现在已过时，应该从内存中移除。
- en: 'The benefit of this is twofold. First, network traffic is minimized as invalidation
    messages are very small compared to replicating updated data. Second, caches in
    the cluster will only need to do database lookups when data is stale. Whenever
    a new entity or collection is read from database, it''s only cached **locally**
    in order to reduce traffic between nodes:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是双重的。首先，由于失效消息与复制更新数据相比非常小，因此网络流量最小化。其次，集群中的缓存只有在数据过时时才需要执行数据库查找。每次从数据库读取新的实体或集合时，它只**本地**缓存，以减少节点之间的流量：
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `local-query` cache is configured by default to store up to 10,000 entries
    in an LRU vector. Each entry will be evicted from the cache automatically if it
    has been idle for 100,000 milliseconds, as per the `max-idle` attribute.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`local-query` 缓存配置为在 LRU 向量中存储最多 10,000 个条目。如果条目在 100,000 毫秒内处于空闲状态，则根据
    `max-idle` 属性，它将自动从缓存中驱逐。
- en: 'The following is a summary of the eviction strategies supported by Infinispan:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 Infinispan 支持的驱逐策略的总结：
- en: '`NONE`: This value disables the eviction thread'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NONE`: 此值禁用驱逐线程'
- en: '`UNORDERED`: This is now deprecated. Using this value will cause the LRU to
    be used'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UNORDERED`: 现在已弃用。使用此值将导致使用 LRU'
- en: '`LRU`: This value causes evictions to occur based on a **least-recently-used**
    pattern'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LRU`: 此值会导致基于**最近最少使用**模式的驱逐发生'
- en: '`LIRS`: This value addresses shortcomings of LRU. Eviction relies on inter-reference
    recency of cache entries'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LIRS`: 此值解决了 LRU 的不足。驱逐依赖于缓存条目的互引用最近性'
- en: Tip
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: To read more about how LIRS works, see the Infinispan documentation at [http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies](http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 LIRS 如何工作的信息，请参阅 Infinispan 文档[http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies](http://infinispan.org/docs/6.0.x/user_guide/user_guide.html#_eviction_strategies)。
- en: 'Once the local cache entity is updated, the cache will send a message to other
    members of the cluster, telling them that the entity has been modified. This is
    when the `invalidation-cache` comes into play. Take a look at the following code:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦本地缓存实体被更新，缓存将向集群中的其他成员发送消息，告知它们实体已被修改。这就是`invalidation-cache`发挥作用的时候。看看以下代码：
- en: '[PRE28]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The default configuration for the invalidation cache uses the same eviction
    and expiration settings as for local query cache. The maximum number of entries
    is set to 10,000 and the idle time before expiration to 100,000 milliseconds.
    The invalidation cache can also be configured to be synchronous (`SYNC`) or asynchronous
    (`ASYNC`). If you configure your invalidation cache to be synchronous, then your
    cache will be blocked until all caches in the cluster receive responses to invalidation
    messages. On the other hand, an asynchronous invalidation cache does not block
    and wait for a response, which results in increased performance. By default, hibernate
    is configured to use `REPEATABLE_READ` as the cache isolation level. For most
    cases, the default isolation level of `REPEATABLE_READ` will suffice. If you want
    to update it to, say, `READ_COMMITTED`, then you will need to add the following
    to your configuration:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 无效化缓存的默认配置使用与本地查询缓存相同的驱逐和过期设置。最大条目数设置为10,000，过期前的空闲时间为100,000毫秒。无效化缓存也可以配置为同步（`SYNC`）或异步（`ASYNC`）。如果您将无效化缓存配置为同步，则您的缓存将阻塞，直到集群中的所有缓存都收到无效化消息的响应。另一方面，异步无效化缓存不会阻塞并等待响应，这会导致性能提高。默认情况下，hibernate配置为使用`REPEATABLE_READ`作为缓存隔离级别。对于大多数情况，默认的隔离级别`REPEATABLE_READ`将足够。如果您想将其更新为，例如，`READ_COMMITTED`，那么您需要将以下内容添加到您的配置中：
- en: '[PRE29]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The last bit of configuration we are going to look at within the Infinispan
    subsystem is the `timestamp` cache. The `timestamp` cache keeps track of the last
    time each database table was updated.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Infinispan子系统中查看的最后一点配置是`timestamp`缓存。`timestamp`缓存跟踪每个数据库表最后更新的时间。
- en: 'The `timestamp` cache is strictly related to the query cache. It is used to
    store the result set of a query run against the database. If the query cache is
    enabled, before a query run, the query cache is checked. If the timestamp of the
    last update on a table is greater than the time the query results were cached,
    the entry is evicted from the cache and a fresh database lookup is made. This
    is referred to as a cache miss. Have a look at the following code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`timestamp`缓存严格相关于查询缓存。它用于存储对数据库运行的查询的结果集。如果启用了查询缓存，在查询运行之前，会检查查询缓存。如果表上最后更新的时间戳大于查询结果缓存的時間，则条目将从缓存中删除，并执行新的数据库查找。这被称为缓存未命中。请看以下代码：'
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: By default, the `timestamps` cache is configured with asynchronous replication
    as the clustering mode. Since all cluster nodes must store all timestamps, local
    or invalidated cache types are not allowed, and no eviction/expiration is allowed
    either.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`timestamps`缓存配置为异步复制作为集群模式。由于所有集群节点都必须存储所有时间戳，因此不允许本地或失效的缓存类型，也不允许驱逐/过期。
- en: Using replication for the hibernate cache
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用复制为Hibernate缓存
- en: 'There may be situations when you want to replicate your entity cache across
    other cluster nodes, instead of using local caches and invalidation. This may
    be the case when:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在您想要在其他集群节点上复制实体缓存，而不是使用本地缓存和无效化的情况。这可能是在以下情况下：
- en: Queries executed are quite expensive
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行的查询相当昂贵
- en: Queries are likely to be repeated in different cluster nodes
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询很可能在不同的集群节点上重复
- en: Queries are unlikely to be invalidated out of the cache (Hibernate invalidates
    query results from the cache when one of the entity classes involved in the query's
    `WHERE` clause changes)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询不太可能从缓存中失效（当查询的`WHERE`子句中涉及的实体类之一发生变化时，Hibernate会从缓存中使查询结果失效）
- en: 'In order to switch to a replicated cache, you have to configure your `default-cache`
    attribute, as shown in the following code snippet, as well as add the relevant
    `replicated-cache` configuration:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了切换到复制缓存，您必须配置您的`default-cache`属性，如下面的代码片段所示，以及添加相关的`replicated-cache`配置：
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Advanced Infinispan configuration
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级Infinispan配置
- en: Until now, we looked at the essential components required to get working with
    a clustered application. Infinispan has a wealth of options available to further
    customize your cache.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已查看使用集群应用程序所需的基本组件。Infinispan提供了丰富的选项来进一步自定义您的缓存。
- en: Tip
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: For more information on advanced configuration of Infinispan via the Infinispan
    subsystem, you can check out the documentation at [http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html](http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关通过Infinispan子系统的高级配置的更多信息，你可以查看以下文档：[http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html](http://infinispan.org/docs/6.0.x/infinispan_server_guide/infinispan_server_guide.html)
- en: Configuring the Infinispan transport
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Infinispan传输
- en: 'The Infinispan subsystem relies on the JGroups subsystem to transport cached
    data between nodes. JGroups uses UDP as the default transport protocol as defined
    by the `default-stack` attribute in the JGroups subsystem:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Infinispan子系统依赖于JGroups子系统在节点之间传输缓存数据。JGroups使用UDP作为默认传输协议，这是由JGroups子系统中的`default-stack`属性定义的：
- en: '[PRE32]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can, however, configure a different transport for each cache container.
    If you want to use TCP as the transport protocol for the web cache container,
    then you can add the `stack` attribute and set it to `tcp`:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以为每个缓存容器配置不同的传输。如果你想将TCP作为Web缓存容器的传输协议，则可以添加`stack`属性并将其设置为`tcp`：
- en: '[PRE33]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The default UDP transport is usually suitable for large clusters. It may also
    be suitable if you are using replication or invalidation, as it minimizes opening
    too many sockets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的UDP传输通常适用于大型集群。如果你正在使用复制或失效，它也可能适用，因为它可以最小化打开过多的套接字。
- en: To learn about the differences between TCP and UDP, please refer to this external
    link at [http://www.skullbox.net/tcpudp.php](http://www.skullbox.net/tcpudp.php).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解TCP和UDP之间的区别，请参阅此外部链接：[http://www.skullbox.net/tcpudp.php](http://www.skullbox.net/tcpudp.php)。
- en: Configuring the Infinispan threads
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置Infinispan线程
- en: 'It is important to note that the thread-pool subsystem has been deprecated
    in WildFly 8\. It is quite likely that it will be removed completely in WildFly
    9\. The configuration in this section can still be used in WildFly 8, but you
    will need to add the threads subsystem to your configuration file. Take a look
    at the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在WildFly 8中，线程池子系统已被弃用。在WildFly 9中，它很可能将被完全移除。本节中的配置仍可用于WildFly 8，但你需要将线程子系统添加到你的配置文件中。请看以下代码：
- en: '[PRE34]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Just as you can for JGroups transport, you can externalize your Infinispan
    thread configuration, moving it into the thread-pool subsystem. The following
    thread pools can be configured per cache-container:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你可以为JGroups传输外部化你的Infinispan线程配置一样，你可以将其移动到线程池子系统。以下线程池可以按缓存容器配置：
- en: '| Thread pool | Description |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Thread pool | 描述 |'
- en: '| --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| transport | This gives the size of the bounded thread pool whose threads
    are responsible for transporting data across the network |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| transport | 这给出了负责在网络中传输数据的有限线程池的大小 |'
- en: '| listener-executor | This gives the size of the thread pool used for registering
    and getting notified when some cache events take place |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| listener-executor | 这给出了用于注册和接收某些缓存事件发生时的通知的线程池的大小 |'
- en: '| replication-queue-executor | This gives the size of the scheduled replication
    executor used for replicating cache data |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| replication-queue-executor | 这给出了用于复制缓存数据的预定复制执行器的大小 |'
- en: '| eviction-executor | This gives the size of the scheduled executor service
    used to periodically run eviction cleanup tasks |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| eviction-executor | 这给出了用于定期运行驱逐清理任务的预定执行器服务的大小 |'
- en: 'Customizing the thread pool may be required in some cases, for example, you
    may want to apply a cache replication algorithm. You may then need to choose the
    number of threads used for replicating data. In the following example, we are
    externalizing the thread pools of the web `cache-container` by defining a maximum
    of 25 threads for the bounded-queue-thread-pool and five threads for replicating
    data:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下可能需要自定义线程池，例如，你可能想应用缓存复制算法。你可能需要选择用于复制数据线程的数量。在以下示例中，我们通过定义最大25个线程用于有界队列线程池和5个线程用于复制数据来外部化`cache-container`的线程池：
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Clustering the messaging subsystem
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群消息子系统
- en: We will conclude this chapter by discussing the messaging subsystem.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过讨论消息子系统来结束本章。
- en: The JMS provider used in WildFly is **HornetQ**. In order to share the message
    processing load, HornetQ servers can be grouped together in a cluster. Each active
    node in the cluster contains an active HornetQ server. HornetQ manages its own
    messages and handles its own connections. Behind the scenes, when a node forms
    a cluster connection to another node, a core bridge connection is created between
    them. Once the connection has been established, messages can flow between each
    of the nodes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: WildFly 中使用的 JMS 提供者是 **HornetQ**。为了共享消息处理负载，HornetQ 服务器可以一起组成一个集群。集群中的每个活动节点都包含一个活动的
    HornetQ 服务器。HornetQ 管理自己的消息并处理自己的连接。在幕后，当一个节点与其他节点建立集群连接时，它们之间会创建一个核心桥接连接。一旦建立连接，消息就可以在各个节点之间流动。
- en: 'Clustering is automatically enabled in HornetQ if one or more `cluster-connection`
    elements are defined. The following example is taken from the default `full-ha`
    profile:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 HornetQ 中定义了一个或多个 `cluster-connection` 元素，则会自动启用集群。以下示例取自默认的 `full-ha` 配置文件：
- en: '[PRE36]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, let's look at how to configure the `cluster-connection`. The following
    is a typical cluster connection configuration. You can either update the default
    `cluster-connection`, or you can add your own `cluster-connection` element within
    your `<hornetq-server>` definition.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何配置 `cluster-connection`。以下是一个典型的集群连接配置。您可以选择更新默认的 `cluster-connection`，或者您可以在
    `<hornetq-server>` 定义中添加自己的 `cluster-connection` 元素。
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `cluster-connection` `name` attribute obviously defines the cluster connection
    name, which we are going to configure. There can be zero or more cluster connections
    configured in your messaging subsystem.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster-connection` 的 `name` 属性显然定义了集群连接的名称，这是我们将要配置的。在您的消息子系统中可以配置零个或多个集群连接。'
- en: The `address` element is a mandatory parameter and determines how messages are
    distributed across the cluster. In this example, the cluster connection will only
    load balance the messages that are sent to addresses that start with `jms`. This
    cluster connection will, in effect, apply to all JMS queue and topic subscriptions.
    This is because they map to core queues that start with the substring `jms`.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`address` 元素是一个必填参数，它确定消息如何在集群中分配。在这个例子中，集群连接将只负载均衡发送到以 `jms` 开头的地址的消息。实际上，这个集群连接将应用于所有
    JMS 队列和主题订阅。这是因为它们映射到以子字符串 `jms` 开头的核心队列。'
- en: The `connector-ref` element references the connector, which has been defined
    in the `connectors` section of the messaging subsystem. In this case, we are using
    the http connector (see [Chapter 3](ch03.html "Chapter 3. Configuring Enterprise
    Services"), *Configuring Enterprise Services*, for more information about the
    available connectors).The `retry-interval` element determines the interval in
    milliseconds between the message retry attempts. If a cluster connection is attempted
    and the target node has not been started, or is in the process of being rebooted,
    then connection attempts from other nodes commence only once the time period defined
    in the `retry-interval` has elapsed.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`connector-ref` 元素引用了在消息子系统的 `connectors` 部分中定义的连接器。在这种情况下，我们使用的是 http 连接器（有关可用连接器的更多信息，请参阅[第
    3 章](ch03.html "第 3 章。配置企业服务"），*配置企业服务*）。`retry-interval` 元素确定消息重试尝试之间的时间间隔（以毫秒为单位）。如果尝试建立集群连接，而目标节点尚未启动，或者正在重启过程中，那么来自其他节点的连接尝试将仅在
    `retry-interval` 中定义的时间间隔过去后开始。'
- en: The `forward-when-no-consumers` element, when set to `true`, will ensure that
    each incoming message is distributed round robin even though there may not be
    a consumer on the receiving node.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `forward-when-no-consumers` 元素设置为 `true` 时，将确保即使接收节点上可能没有消费者，每个传入的消息也会被循环分配。
- en: Note
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can actually specify the connection load-balancing policy within the `connection-factory`
    element. The out-of-the-box policies are **Round-Robin** (`org.hornetq.api.core.client.loadbalance.RoundRobinConnectionLoadBalancingPolicy`)
    and **Random** (`org.hornetq.api.core.client.loadbalance.RandomConnectionLoadBalancingPolicy`).
    You can also add your own policy by implementing the `org.hornetq.api.core.client.loadbalance.ConnectionLoadBalancingPolicy`
    interface.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 您实际上可以在 `connection-factory` 元素内指定连接负载均衡策略。开箱即用的策略是 **轮询** (`org.hornetq.api.core.client.loadbalance.RoundRobinConnectionLoadBalancingPolicy`)
    和 **随机** (`org.hornetq.api.core.client.loadbalance.RandomConnectionLoadBalancingPolicy`)。您还可以通过实现
    `org.hornetq.api.core.client.loadbalance.ConnectionLoadBalancingPolicy` 接口添加自己的策略。
- en: 'The following example shows how to use the random policy for a connection factory:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例显示了如何为连接工厂使用随机策略：
- en: '[PRE38]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Finally, the optional `max-hops` value is set to `1` (default), which is the
    maximum number of times a message can be forwarded between nodes. A value of `1`
    means messages are only load balanced to other HornetQ serves, which are directly
    connected to this server. HornetQ can also be configured to load-balance messages
    to nodes that are indirectly connected to it, that is, the other HornetQ servers
    are intermediaries in a chain.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可选的`max-hops`值设置为`1`（默认），这是消息在节点之间可以转发最大次数。值为`1`表示消息仅被负载均衡到直接连接到此服务器的其他HornetQ服务器。HornetQ还可以配置为将消息负载均衡到间接连接到它的节点，即其他HornetQ服务器是链中的中介。
- en: Tip
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also refer to `jboss-as-messaging_2_0.xsd` for the full list of available
    parameters. This can be found in the `JBOSS_HOME/docs/schema` folder of your server
    distribution.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以参考`jboss-as-messaging_2_0.xsd`以获取可用参数的完整列表。这可以在您的服务器发行版的`JBOSS_HOME/docs/schema`文件夹中找到。
- en: Configuring messaging credentials
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置消息凭据
- en: 'If you try to start a cluster where nodes use the `full-ha` profile, you will
    get an error logged to the console, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尝试启动使用`full-ha`配置文件的集群，您将在控制台看到以下错误日志：
- en: '[PRE39]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This is because, when attempting to create connections between nodes, HornetQ
    uses a cluster user and cluster password. As you can see in the default configuration,
    you are required to update the password value:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，当尝试在节点之间建立连接时，HornetQ使用集群用户和集群密码。正如您在默认配置中看到的，您需要更新密码值：
- en: '[PRE40]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once you have changed this password, start your cluster, and you should see
    a successful bridge between nodes:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您更改了此密码，启动您的集群，您应该会看到节点之间成功建立桥接：
- en: '![Configuring messaging credentials](img/6232OS_08_15.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![配置消息凭据](img/6232OS_08_15.jpg)'
- en: Configuring clustering in your applications
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在您的应用程序中配置集群
- en: 'We will now complete our journey through the clustering system by looking at
    how to cluster the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过查看如何对以下内容进行集群来完成我们对集群系统的探索：
- en: Session beans
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会话Bean
- en: Entities
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实体
- en: Web applications
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web应用程序
- en: Clustering session beans
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群会话Bean
- en: In [Chapter 3](ch03.html "Chapter 3. Configuring Enterprise Services"), *Configuring
    Enterprise Services*, we discussed the difference between **Stateless Session
    Beans** (**SLSBs**), **Stateful Session Beans** (**SFSBs**), and **Singleton Session
    Beans**.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html "第3章。配置企业服务") *配置企业服务* 中，我们讨论了**无状态会话Bean**（**SLSB**）、**有状态会话Bean**（**SFSB**）和**单例会话Bean**之间的区别。
- en: 'SLSBs are not able to retain state between invocations, so the main benefit
    of clustering an SLSB is to balance the load between an array of servers:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: SLSB无法在调用之间保留状态，因此集群SLSB的主要好处是在服务器数组之间平衡负载：
- en: '[PRE41]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'If you want to further specialize your SLSB, then you can choose the load-balancing
    algorithm used to distribute the load between your EJBs. The following are the
    available load-balancing policies for your SLSB:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想进一步专业化您的SLSB，那么您可以选择用于在您的EJB之间分配负载的负载均衡算法。以下是为您的SLSB提供的可用负载均衡策略：
- en: '| Load-balancing policy | Description |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 负载均衡策略 | 描述 |'
- en: '| --- | --- |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `RoundRobin` | It is the default load-balancing policy. The smart proxy cycles
    through a list of WildFly Server instances in a fixed order. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| `RoundRobin` | 这是默认的负载均衡策略。智能代理以固定顺序遍历WildFly服务器实例列表。|'
- en: '| `RandomRobin` | Under this policy, each request is redirected by the smart
    proxy to a random node in the cluster. |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| `RandomRobin` | 在此策略下，每个请求都由智能代理重定向到集群中的随机节点。|'
- en: '| `FirstAvailable` | It implies a random selection of the node, but subsequent
    calls will stick to that node until the node fails. The next node will again be
    selected randomly. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| `FirstAvailable` | 它意味着对节点的随机选择，但后续调用将坚持使用该节点，直到节点失败。下一个节点将再次随机选择。|'
- en: '| `FirstAvailableIdenticalAllProxies` | This is the same as `FirstAvailable`,
    except that the random node selection will then be shared by all dynamic proxies.
    |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| `FirstAvailableIdenticalAllProxies` | 这与`FirstAvailable`相同，但随机节点选择将由所有动态代理共享。|'
- en: 'Then, you can apply the load-balancing policy as in the following example:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以根据以下示例应用负载均衡策略：
- en: '[PRE42]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In JBoss AS 7, you were required to annotate your SFSB with `@Clustered` in
    order to replicate the state of the SFSB. In WildFly, this is not the case, as
    SFSB are configured to have passivation enabled by default. This means that as
    long as you annotate your bean with `@Stateful`, and you are using a server profile
    that supports high availability, your SFSB will have its state replicated across
    servers. Have a look at the following code:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在JBoss AS 7中，您需要使用`@Clustered`注解您的SFSB以复制SFSB的状态。在WildFly中，情况并非如此，因为SFSB默认配置为启用钝化。这意味着只要您使用`@Stateful`注解您的bean，并且您使用的是支持高可用性的服务器配置文件，您的SFSB的状态将在服务器之间进行复制。请看以下代码：
- en: '[PRE43]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To disable passivation/replication, you can simply set `passivationCapable`
    to `false`, as shown here:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要禁用钝化/复制，您只需将`passivationCapable`设置为`false`，如下所示：
- en: '[PRE44]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'By default, SFSBs use the cache container named `ejb`, which replicates sessions
    across all nodes. Should your application server node fail while sessions are
    running, the EJB proxy will detect it and choose another node where session data
    has been replicated. You can, however, reference a custom cache container used
    by your SFSB with the `@org.jboss.ejb3.annotation.CacheConfig` annotation. Have
    a look at the following code:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SFSBs使用名为`ejb`的缓存容器，该容器会在所有节点之间复制会话。如果在会话运行期间应用程序服务器节点失败，EJB代理将检测到这一点，并选择另一个已复制会话数据的节点。然而，您可以使用`@org.jboss.ejb3.annotation.CacheConfig`注解引用您的SFSB使用的自定义缓存容器。请看以下代码：
- en: '[PRE45]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following is the corresponding cache container that uses a distributed
    cache:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用分布式缓存的对应缓存容器：
- en: '[PRE46]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Clustering entities
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实体聚类
- en: As entities sit deep in the backend, they do not need to be considered with
    regard to load-balancing logic or session replication. However, it is useful to
    cache your entities to avoid roundtrips to the database. The EJB3 persistence
    layer implementation in WildFly is Hibernate 4.3.5\. The Hibernate framework includes
    a complex cache mechanism, which is implemented both at the Session level and
    at the SessionFactory level.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实体位于后端深处，因此不需要考虑负载均衡逻辑或会话复制。然而，缓存您的实体以避免往返数据库是有用的。WildFly中的EJB3持久层实现是Hibernate
    4.3.5。Hibernate框架包括一个复杂的缓存机制，该机制在Session级别和SessionFactory级别都得到了实现。
- en: The cache used in the Session level is called the first-level cache and only
    has session scope. This cache is cleared as soon as the Hibernate session using
    it is closed. Hibernate uses second-level caching to store entities or collections
    retrieved from the database. It can also store the results of recent queries.
    It is the second-level cache that we need to cluster, as this cache is used across
    sessions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在Session级别使用的缓存称为一级缓存，并且只有会话作用域。此缓存在关闭使用它的Hibernate会话时立即清除。Hibernate使用二级缓存来存储从数据库检索的实体或集合。它还可以存储最近查询的结果。正是这个二级缓存需要我们进行集群，因为这个缓存被跨会话使用。
- en: 'Enabling the second-level cache for your enterprise applications is relatively
    straightforward. If you are using JPA, then all you need to do to enable the second-level
    cache is add the following to your `persistence.xml` configuration file:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 为您的企业应用程序启用二级缓存相对简单。如果您正在使用JPA，那么您只需将以下内容添加到您的`persistence.xml`配置文件中即可启用二级缓存：
- en: '[PRE47]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The first element, `shared-cache-mode`, is the JPA 2.x way of specifying whether
    the entities and entity-related state of a persistence unit will be cached. The
    `shared-cache-mode` element has five possible values, as indicated in the following
    table:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个元素`shared-cache-mode`是JPA 2.x指定持久单元的实体及其相关状态是否将被缓存的方式。`shared-cache-mode`元素有五个可能的值，如下表所示：
- en: '| Shared Cache mode | Description |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 共享缓存模式 | 描述 |'
- en: '| --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `ALL` | This value causes all entities and entity-related states and data
    to be cached. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| `ALL` | 此值导致所有实体及其相关状态和数据都被缓存。 |'
- en: '| `NONE` | This value causes caching to be disabled for the persistence unit.
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| `NONE` | 此值导致持久单元的缓存被禁用。 |'
- en: '| `ENABLE_SELECTIVE` | This value allows caching if the `@Cacheable` annotation
    is specified on the entity class. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| `ENABLE_SELECTIVE` | 此值允许在实体类上指定`@Cacheable`注解时进行缓存。 |'
- en: '| `DISABLE_SELECTIVE` | This value enables the cache and causes all entities
    to be cached except those for which `@Cacheable(false)` is specified. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| `DISABLE_SELECTIVE` | 此值启用缓存，并导致除了那些指定了`@Cacheable(false)`的实体之外的所有实体都被缓存。
    |'
- en: The property named `hibernate.cache.use_minimal_puts` performs some optimization
    on the second-level cache by reducing the amount of writes in the caches at the
    cost of some additional reads. This is beneficial when clustering your entities,
    as the put operation is very expensive as it activates cache replication listeners.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 命名为 `hibernate.cache.use_minimal_puts` 的属性通过减少缓存中的写入量来对二级缓存进行一些优化，这以增加额外的读取为代价。当对实体进行集群时，这很有益处，因为
    put 操作非常昂贵，因为它激活了缓存复制监听器。
- en: 'In addition, if you plan to use the Hibernate query cache in your applications,
    you need to activate it with a separate property, as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你计划在你的应用程序中使用 Hibernate 查询缓存，你需要通过一个单独的属性来激活它，如下所示：
- en: '[PRE48]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'For the sake of completeness, we will also include the configuration needed
    to use Infinispan as a caching provider for native Hibernate applications. This
    is the list of properties you have to add to your `hibernate.cfg.xml`:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还将包括使用 Infinispan 作为原生 Hibernate 应用程序缓存提供者的配置。这是你必须添加到你的 `hibernate.cfg.xml`
    中的属性列表：
- en: '[PRE49]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As you can see, the configuration is more verbose because you have to tell Hibernate
    to use Infinispan as a caching provider. This requires setting the correct Hibernate
    transaction factory using the `hibernate.transaction.factory_class` property.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，配置更加详细，因为你必须告诉 Hibernate 使用 Infinispan 作为缓存提供者。这需要使用 `hibernate.transaction.factory_class`
    属性设置正确的 Hibernate 事务工厂。
- en: The `hibernate.cache.infinispan.cachemanager` property exposes the cache manager
    used by Infinispan. By default, Infinispan binds the cache manager responsible
    for the second-level cache to the JNDI name `java:CacheManager/entity`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`hibernate.cache.infinispan.cachemanager` 属性暴露了 Infinispan 使用的缓存管理器。默认情况下，Infinispan
    将负责二级缓存的缓存管理器绑定到 JNDI 名称 `java:CacheManager/entity`。'
- en: Finally, the `hibernate.cache.region.factory_class` property tells Hibernate
    to use Infinispan's second-level caching integration, which uses `CacheManager`,
    as defined previously, as the source for the Infinispan cache's instances.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`hibernate.cache.region.factory_class` 属性告诉 Hibernate 使用 Infinispan 的二级缓存集成，该集成使用
    `CacheManager`，如之前定义的，作为 Infinispan 缓存实例的来源。
- en: Caching entities
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存实体
- en: 'Unless you have set `shared-cache-mode` to `ALL`, Hibernate will not automatically
    cache your entities. You have to select which entities or queries need to be cached.
    This is definitely the safest option since indiscriminate caching can hurt performance.
    The following example shows how to do this for JPA entities using annotations:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你已将 `shared-cache-mode` 设置为 `ALL`，否则 Hibernate 不会自动缓存你的实体。你必须选择哪些实体或查询需要被缓存。这绝对是最安全的选项，因为无差别的缓存可能会损害性能。以下示例展示了如何使用注解为
    JPA 实体进行此操作：
- en: '[PRE50]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Using JPA annotations
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 JPA 注解
- en: The `@javax.persistence.Cacheable` annotation dictates whether this entity class
    should be cached in the second-level cache. This is only applicable when the `shared-cache-mode`
    is not set to `ALL`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '`@javax.persistence.Cacheable` 注解指定此实体类是否应该缓存在二级缓存中。这仅在 `shared-cache-mode`
    未设置为 `ALL` 时适用。'
- en: Using Hibernate annotations
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Hibernate 注解
- en: The `@org.hibernate.annotations.Cache` annotation is the older annotation used
    to achieve the same purpose as `@Cacheable`. You can still use it to define which
    strategy Hibernate should use to control concurrent access to cache contents.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`@org.hibernate.annotations.Cache` 注解是用于实现与 `@Cacheable` 相同目的的较旧注解。你仍然可以使用它来定义
    Hibernate 应该使用哪种策略来控制缓存内容的并发访问。'
- en: The `CacheConcurrencyStrategy.TRANSACTIONAL` property provides support for Infinispan's
    fully-transactional JTA environment.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`CacheConcurrencyStrategy.TRANSACTIONAL` 属性为 Infinispan 的完全事务性 JTA 环境提供支持。'
- en: 'If there is a chance that your application data is read but never modified,
    you can apply the `CacheConcurrencyStrategy.READ_ONLY` property that does not
    evict data from the cache (unless performed programmatically):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的应用程序数据有可能只读不修改，你可以应用 `CacheConcurrencyStrategy.READ_ONLY` 属性，该属性不会从缓存中驱逐数据（除非程序化执行）：
- en: '[PRE51]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Finally, the last attribute is the caching region that defines where entities
    are placed. If you do not specify a cache region for an entity class, all instances
    of this class will be cached in the `_default` region. Defining a caching region
    can be useful if you want to perform a fine-grained management of caching areas.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一个属性是缓存区域，它定义了实体放置的位置。如果你没有为实体类指定缓存区域，则此类的所有实例都将缓存在 `_default` 区域中。定义缓存区域可能有助于你想要执行细粒度的缓存区域管理。
- en: Caching queries
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存查询
- en: The query cache can be used to cache the result set of a query. This means that
    if the same query is issued again, it will not hit the database but return the
    cached value.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 查询缓存可用于缓存查询的结果集。这意味着如果再次发出相同的查询，它将不会击中数据库，而是返回缓存的值。
- en: Note
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The query cache does not cache the state of the actual entities in the result
    set; it caches only the identifier values and results of the value type.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 查询缓存不会缓存结果集中实际实体的状态；它只缓存标识值和值类型的结果。
- en: 'In the following example, the query result set named `listUsers` is configured
    to be cached using the `@QueryHint` annotation inside a `@NamedQuery` annotation:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，名为`listUsers`的查询结果集被配置为使用`@NamedQuery`注解内的`@QueryHint`注解进行缓存：
- en: '[PRE52]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Overuse of the query cache may reduce your application's performance, so use
    it wisely. First, the query cache will increase the memory requirements if your
    queries (stored as key in the query cache map) are made up of hundreds of characters.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 过度使用查询缓存可能会降低应用程序的性能，因此请明智地使用它。首先，如果您的查询（作为查询缓存映射中的键存储）由数百个字符组成，查询缓存将增加内存需求。
- en: Second, and more important, the result of the query cache is invalidated each
    time there's a change in one of the tables you are querying. This can lead to
    a very poor hit ratio of the query cache. Therefore, it is advisable to turn off
    the query cache unless you are querying a table that is seldom updated.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，更重要的是，每次查询的表中发生更改时，查询缓存的结果都会被无效化。这可能导致查询缓存命中率非常低。因此，除非您查询的表很少更新，否则建议关闭查询缓存。
- en: Clustering web applications
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群Web应用程序
- en: 'Clustering web applications requires the least effort. As we touched upon earlier,
    all you need to do to switch on clustering in a web application is add the following
    directive in the `web.xml`:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 集群Web应用程序需要最少的努力。正如我们之前提到的，要在Web应用程序中启用集群，您只需在`web.xml`中添加以下指令：
- en: '[PRE53]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'By default, clustered web applications will use the web cache contained in
    the Infinispan configuration. You also have the option of setting up a specific
    cache per deployment unit. This can be achieved by adding the `replication-config`
    directive to the `jboss-web.xml` file and specifying the cache name to use:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，集群Web应用程序将使用Infinispan配置中包含的Web缓存。您还可以为每个部署单元设置特定的缓存。这可以通过向`jboss-web.xml`文件添加`replication-config`指令并指定要使用的缓存名称来实现：
- en: '[PRE54]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The previous configuration should obviously reference a cache defined in the
    main configuration file:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配置显然应该引用主配置文件中定义的缓存：
- en: '[PRE55]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at a lot of configuration options around clustering.
    There was a lot of information to take in but, in summary, we will mention the
    following key points.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了围绕集群的大量配置选项。有很多信息需要吸收，但总的来说，我们将提到以下关键点。
- en: A WildFly cluster can be composed of either standalone nodes or as part of a
    domain of servers .The clustering subsystem is defined in the `standalone-ha.xml`
    and `standalone-full-ha.xml` configurations files.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: WildFly集群可以由独立节点组成，或者作为服务器域的一部分。集群子系统在`standalone-ha.xml`和`standalone-full-ha.xml`配置文件中定义。
- en: 'There are three main components required for clustering: JGroups, Infinispan,
    and messaging. JGroups provides communication between nodes in a cluster. By default,
    JGroups uses UDP multicast messages to handle the cluster life cycle events.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 集群需要三个主要组件：JGroups、Infinispan和消息传递。JGroups提供集群节点之间的通信。默认情况下，JGroups使用UDP多播消息来处理集群生命周期事件。
- en: Within enterprise applications, there are several caches that need to be configured
    in order to achieve consistency of data. There are four cache containers configured
    by default in WildFly. These are the singleton session bean cluster `cache-container`,
    the SLSB `cache-container`, the web `cache-container`, and the Hibernate `cache-container`.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业应用程序中，需要配置多个缓存以实现数据的一致性。WildFly默认配置了四个缓存容器。这些是单例会话Bean集群`cache-container`、SLSB
    `cache-container`、Web `cache-container`和Hibernate `cache-container`。
- en: The singleton cluster (server) `cache-container` is configured to replicate
    singleton session bean data across nodes in the cluster. The SFSB's (ejb) `cache-container`
    is configured to replicate stateful session bean data across nodes in the cluster.
    The web `cache-container` is configured to replicate HTTP session data across
    nodes in the cluster. The Hibernate `cache-container` uses a more complex approach
    by defining a `local-query` strategy to handle local entities. An `invalidation-cache`
    is used when data is updated and other cluster nodes need to be informed. Finally,
    a `replicated-cache` is used to replicate the query timestamps.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 单例集群（服务器）`cache-container`被配置为在集群节点间复制单例会话bean数据。SFSB的（ejb）`cache-container`被配置为在集群节点间复制状态化会话bean数据。Web
    `cache-container`被配置为在集群节点间复制HTTP会话数据。Hibernate `cache-container`通过定义一个`local-query`策略来处理本地实体，采用了一种更复杂的方法。当数据更新且其他集群节点需要被通知时，会使用`invalidation-cache`。最后，使用`replicated-cache`来复制查询时间戳。
- en: Lastly, we looked at the messaging subsystem, which can be easily clustered
    by defining one `cluster-connection` element. This will cause messages to be transparently
    load-balanced across your JMS servers.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们查看了一下消息子系统，该子系统可以通过定义一个`cluster-connection`元素轻松地进行集群化。这将导致消息在您的JMS服务器之间透明地负载均衡。
- en: In the next chapter we will look at load balancing, the other half of the story
    when it comes to configuring high availability.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨负载均衡，这是配置高可用性的故事的一半。
