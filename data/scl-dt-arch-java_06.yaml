- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Architecting a Real-Time Processing Pipeline
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构实时处理管道
- en: In the previous chapter, we learned how to architect a big data solution for
    a high-volume batch-based data engineering problem. Then, we learned how big data
    can be profiled using Glue DataBrew. Finally, we learned how to logically choose
    between various technologies to build a Spark-based complete big data solution
    in the cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何为高吞吐量的基于批处理的数据工程问题架构大数据解决方案。然后，我们学习了如何使用Glue DataBrew对大数据进行概要分析。最后，我们学习了如何逻辑上选择各种技术，在云中构建基于Spark的完整大数据解决方案。
- en: In this chapter, we will discuss how to analyze, design, and implement a real-time
    data analytics solution to solve a business problem. We will learn how the reliability
    and speed of processing can be achieved with the help of distributed messaging
    systems such as Apache Kafka to stream and process the data. Here, we will discuss
    how to write a Kafka Streams application to process and analyze streamed data
    and store the results of a real-time processing engine in a NoSQL database such
    as MongoDB, DynamoDB, or DocumentDB using Kafka connectors.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何分析、设计和实现一个实时数据分析解决方案来解决业务问题。我们将学习如何借助分布式消息系统（如Apache Kafka）实现数据的流式传输和处理，从而实现处理速度和可靠性的提升。在这里，我们将讨论如何编写Kafka
    Streams应用程序来处理和分析流式数据，并使用Kafka连接器将实时处理引擎的结果存储到NoSQL数据库（如MongoDB、DynamoDB或DocumentDB）中。
- en: By the end of this chapter, you will know how to build a real-time streaming
    solution to predict the risk category of a loan application using Java and Kafka-related
    technologies. You will also know how a real-time data analytics problem is designed
    and architected. Throughout this journey, you will learn how to publish events
    to Kafka, analyze that data using Kafka Streams, and store the result of the analytics
    in MongoDB in real time. By doing so, you will know how to approach a real-time
    data engineering problem and build an effective streaming solution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将了解如何使用Java和Kafka相关技术构建一个实时流式解决方案来预测贷款申请的风险类别。你还将了解实时数据分析问题的设计和架构。在整个过程中，你将学习如何将事件发布到Kafka，使用Kafka
    Streams分析数据，并将分析结果实时存储到MongoDB中。通过这样做，你将了解如何处理实时数据工程问题并构建有效的流式解决方案。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding and analyzing the streaming problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和分析流式问题
- en: Architecting the solution
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构解决方案
- en: Implementing and verifying the design
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施和验证设计
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you’ll need the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，你需要以下内容：
- en: Prior knowledge of Java
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉Java
- en: Java 1.8 or above, Maven, Apache Kafka, and PostgreSQL installed on your local
    system
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地系统上安装Java 1.8或更高版本、Maven、Apache Kafka和PostgreSQL
- en: A MongoDB Atlas subscription in the cloud
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中拥有MongoDB Atlas订阅
- en: IntelliJ IDEA Community or Ultimate edition installed on your local system
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地系统上安装IntelliJ IDEA Community或Ultimate版本
- en: Understanding and analyzing the streaming problem
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和分析流式问题
- en: So far, we have looked at data engineering problems that involve ingesting,
    storing, or analyzing the stored data. However, in today’s competitive business
    world, online web apps and mobile applications have made consumers more demanding
    and less patient. As a result, businesses must adapt and make decisions in real
    time. We will be trying to solve such a real-time decision-making problem in this
    chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了涉及数据摄取、存储或分析的数据工程问题。然而，在当今竞争激烈的市场环境中，在线Web应用和移动应用使得消费者更加苛刻且缺乏耐心。因此，企业必须适应并实时做出决策。在本章中，我们将尝试解决这样一个实时决策问题。
- en: Problem statement
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: A financial firm, XYZ, that offers credit cards, has a credit card application
    that works in real time and uses various user interfaces such as mobile and online
    web applications. Since customers have multiple options and are less patient,
    XYZ wants to make sure that the credit loan officer can decide on credit card
    approval in a split second or in real time. To do that, the application needs
    to be analyzed and a credit risk score needs to be generated for each application.
    This risk score, along with the necessary application parameters, will help the
    credit loan officer decide quickly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一家名为XYZ的金融公司，提供信用卡服务，其信用卡申请是实时工作并使用各种用户界面，如移动和在线网络应用程序。由于客户有多种选择并且不太有耐心，XYZ想确保信贷贷款官员可以在瞬间或实时决定信用卡批准。为此，需要分析应用程序并为每个申请生成一个信用风险评分。这个风险评分，连同必要的申请参数，将帮助信贷贷款官员快速做出决定。
- en: Analyzing the problem
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析问题
- en: Let’s analyze the given problem. First, let’s analyze the requirements in terms
    of the four dimensions of data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析给定的问题。首先，让我们从数据的四个维度来分析需求。
- en: First, we will try to answer the question, *what is the velocity of the data?*
    This is the most important factor of this problem. As evident from the problem
    statement, unlike our previous problems, source data is being received in real
    time and the data analysis also needs to happen in real time. This kind of problem
    is well suited for a real-time streaming solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将尝试回答*数据速度是多少？*这是这个问题的关键因素。正如问题陈述所示，与我们的先前问题不同，源数据是实时接收的，数据分析也需要实时进行。这类问题非常适合实时流解决方案。
- en: Now, the next dimension that we need to discuss is *volume*. However, since
    our problem involves streaming data, it doesn’t make sense to discuss the total
    volume of the data. Rather, we should be answering questions such as, *how many
    applications are submitted every minute or every hour on average, as well as at
    peak times? Will this volume increase in the future? If it does, how many times
    and how frequently it is likely to increase?* We should go back to the client
    with these questions. Often, in a business, these answers are not readily available
    if the client is creating a real-time pipeline for the first time. In such a scenario,
    we should ask for the most granular average data velocity information (in this
    case, the number of applications filed) available with the client – for each day,
    week, or month and then calculate the average expected volume in a minute. Also,
    to understand the increase in volume, we can ask about the target projections
    as far as sales are concerned over a year and try to predict the volume increase.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要讨论的下一个维度是*体积*。然而，由于我们的问题涉及流数据，讨论数据的总体体积是没有意义的。相反，我们应该回答诸如*平均每分钟或每小时提交多少应用程序？在高峰时段会怎样？这个体积在未来会增加吗？如果会增加，它可能会增加多少次以及频率如何？*等问题。我们应该带着这些问题回到客户那里。通常情况下，如果客户是第一次创建实时管道，这些答案可能并不容易获得。在这种情况下，我们应该要求客户提供最细粒度的平均数据速度信息（在这种情况下，指的是每天、每周或每月提交的申请数量），然后计算每分钟的平均预期体积。此外，为了了解体积的增加，我们可以询问关于一年内销售方面的目标预测，并尝试预测体积增加。
- en: Let’s suppose that the client is getting one million applications per day and
    that their target is to increase sales by 50% over the next 2 years. Considering
    that the usual approval rate is 50%, we can expect a two times increase in the
    application submission rate. This would mean that we could expect a volume of
    2 million applications per day in the future.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设客户每天接收一百万个申请，并且他们的目标是未来两年内将销售额增加50%。考虑到通常的批准率为50%，我们可以预期申请提交率将增加两倍。这意味着我们可能会在未来每天预期到两百万个申请的体积。
- en: 'Since our solution needs to be real-time, must process more than a million
    records, and the volume is likely to increase in the future, the following characteristics
    are essential for our streaming solution:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的解决方案需要是实时的，必须处理超过一百万条记录，并且预计体积在未来可能会增加，因此以下特性对于我们的流解决方案是必不可少的：
- en: Should be robust
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该健壮
- en: Should support asynchronous communication between various systems within the
    solution and external source/sink
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应支持解决方案内部各种系统以及外部源/汇之间的异步通信
- en: Should ensure zero data loss
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应确保零数据丢失
- en: Should be fault tolerant as we are processing data in real time
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该具有容错性，因为我们正在实时处理数据
- en: Should be scalable
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该可扩展
- en: Should give great performance, even if the volume increases
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数据量增加，也应提供出色的性能
- en: Keeping all these factors in mind, we should choose a pub-sub messaging system
    as this can ensure scalability, fault tolerance, higher parallelism, and message
    delivery guarantees. Distributed messaging/streaming platforms such as Apache
    Kafka, AWS Kinesis, and Apache Pulsar are best suited to solve our problem.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，我们应该选择一个发布/订阅消息系统，因为这可以确保可伸缩性、容错性、更高的并行性和消息投递保证。分布式消息/流平台，如Apache
    Kafka、AWS Kinesis和Apache Pulsar，最适合解决我们的问题。
- en: Next, we will be focusing on the *variety* of the data. In a typical streaming
    platform, we receive data as events. Each event generally contains one record,
    though sometimes, it may contain multiple records. Usually, these events are transmitted
    in platform-independent data formats such as JSON and Avro. In our use case, we
    will receive the data in JSON format. In an actual production scenario, there’s
    a chance that the data may be in Avro format.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将关注数据的**多样性**。在一个典型的流平台中，我们以事件的形式接收数据。每个事件通常包含一条记录，尽管有时可能包含多条记录。通常，这些事件以平台无关的数据格式（如JSON和Avro）传输。在我们的用例中，我们将以JSON格式接收数据。在实际的生产场景中，数据可能是Avro格式。
- en: One of the challenges that real-time streaming solutions face is the *veracity*
    of the data. Often, veracity is determined based on the various possibilities
    of noise that can come from the data. However, accurate analysis of the veracity
    happens as a real-time project gets implemented and tests are run with real data.
    As with many software engineering solutions, real-time data engineering solutions
    mature over time to handle noise and exceptions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实时流解决方案面临的一个挑战是数据的**真实性**。通常，真实性是基于数据可能出现的各种噪声可能性来确定的。然而，真实性的准确分析是在实时项目实施和用真实数据进行测试时发生的。与许多软件工程解决方案一样，实时数据工程解决方案会随着时间的推移而成熟，以处理噪声和异常。
- en: For the sake of simplicity, we will assume that our data that is getting published
    in the input topic is already clean, so we won’t discuss veracity in our current
    use case. However, in the real world, the data that is received over the input
    topic contains anomalies and noise, which needs to be taken care of. In such cases,
    we can write a Kafka Streams application to clean and format the data and put
    it in a processing topic. Also, erroneous records are moved to the error topic
    from the input topic; they are not sent to the processing topic. Then, the streaming
    app for data analytics consumes the data from the processing topic (which contains
    clean data only).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们假设输入主题中发布的数据已经是干净的，因此我们不会在我们的当前用例中讨论真实性。然而，在现实世界中，通过输入主题接收到的数据可能包含异常和噪声，这需要我们注意。在这种情况下，我们可以编写一个Kafka
    Streams应用程序来清理和格式化数据，并将其放入处理主题。此外，从输入主题中移动错误记录到错误主题；它们不会被发送到处理主题。然后，数据分析流应用程序从处理主题（仅包含干净数据）中消费数据。
- en: Now that we have analyzed the dimensions of data for this problem and have concluded
    that we need to build a real-time streaming pipeline, our next question will be,
    *which platform? Cloud or on-premise?*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分析了该问题的数据维度，并得出结论，我们需要构建一个实时流管道，我们的下一个问题将是，*哪个平台？云还是本地？*
- en: To answer these questions, let’s look at any constraints that we have. To analyze
    the streaming data, we must pull and read each customer’s credit history record.
    However, since the credit history of a customer is sensitive information, we would
    prefer to use that information from an on-premise application. However, the company’s
    mobile or web backend systems are deployed on the cloud. So, it makes sense to
    store the analyzed data on the cloud since it will take less time for the mobile
    or other web applications to fetch the data from the cloud than from on-premise.
    So, in this case, we will go with a hybrid approach, in which credit history data
    will be stored on-premise and the data will be analyzed and processed on-premise,
    but the resultant data will be stored in the cloud so that it can easily be retrieved
    from mobile and web backend systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这些问题，让我们看看我们有哪些限制。为了分析流数据，我们必须提取并读取每位客户的信用历史记录。然而，由于客户的信用历史是敏感信息，我们更愿意从本地应用程序中使用这些信息。然而，公司的移动或Web后端系统部署在云上。因此，将分析数据存储在云上是有意义的，因为移动或其他Web应用程序从云上获取数据比从本地获取数据所需的时间更少。因此，在这种情况下，我们将采用混合方法，其中信用历史数据将存储在本地，数据将在本地进行分析和处理，但结果数据将存储在云上，以便可以轻松地从移动和Web后端系统检索。
- en: In this section, we analyzed the data engineering problem and realized that
    this is a real-time stream processing problem, where the processing will happen
    on-premise. In the next section, we will use the result of this analysis and connect
    the dots to design the data pipeline and choose the correct technology stack.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了数据工程问题，并意识到这是一个实时流处理问题，处理将在本地进行。在下节中，我们将使用这次分析的结果，连接点来设计数据管道和选择正确的技术堆栈。
- en: Architecting the solution
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计解决方案
- en: 'To architect the solution, let’s summarize the analysis we discussed in the
    previous section. Here are the conclusions we can make:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设计解决方案，让我们总结一下上一节中讨论的分析。以下是我们可以得出的结论：
- en: This is a real-time data engineering problem
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个实时数据工程问题。
- en: This problem can be solved using a streaming platform such as Kafka or Kinesis
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此问题可以使用Kafka或Kinesis等流平台解决。
- en: 1 million events will be published daily, with a chance of the volume of events
    increasing over time
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每天将发布100万条事件，事件量有可能随时间增加。
- en: The solution should be hosted on a hybrid platform, where data processing and
    analysis are done on-premise and the results are stored in the cloud for easy
    retrieval
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决方案应托管在混合平台上，数据处理和分析在本地进行，结果存储在云中以方便检索。
- en: Since our streaming platform is on-premise and can be maintained on on-premise
    servers, Apache Kafka is a great choice. It supports a distributed, fault-tolerant,
    robust, and reliable architecture. It can be easily scaled by increasing the number
    of partitions and provides an at-least-once delivery guarantee (which ensures
    that at least one copy of all events will be delivered without event drops).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的流媒体平台是本地部署的，并且可以在本地服务器上维护，因此Apache Kafka是一个很好的选择。它支持分布式、容错、健壮和可靠的架构。它可以通过增加分区数量来轻松扩展，并提供至少一次投递保证（这确保了所有事件至少会投递一次，不会发生事件丢失）。
- en: Now, let’s see how we will determine how the results and other information will
    be stored. In this use case, the credit history of an individual has a structured
    format and should be stored on-premise. RDBMS is a great option for such data
    storage. Here, we will be using PostgreSQL for this because PostgreSQL is open
    source, enterprise-ready, robust, reliable, and high performing (and also because
    we used it as an RDBMS option in [*Chapter 4*](B17084_04.xhtml#_idTextAnchor062),
    *ETL Data Load – A Batch-Based Solution to Ingesting Data in a Data Warehouse*).
    Unlike credit history, the applications need to be accessed by mobile and web
    backends running on AWS, so the data storage should be on the cloud.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们将如何确定结果和其他信息将如何存储。在本用例中，个人的信用历史具有结构化格式，应存储在本地。关系数据库管理系统（RDBMS）是此类数据存储的绝佳选择。在这里，我们将使用PostgreSQL，因为PostgreSQL是开源的、企业级的、健壮的、可靠的、高性能的（而且因为我们将其用作RDBMS选项在[*第4章*](B17084_04.xhtml#_idTextAnchor062)，*ETL数据加载
    - 数据仓库中数据摄入的基于批处理解决方案*））。与信用历史不同，应用程序需要由运行在AWS上的移动和Web后端访问，因此数据存储应在云上。
- en: Also, let’s consider that this data will primarily be consumed by mobile and
    web backend applications. So, would it be worth storing the data in a document
    format that can be readily pulled and used by the web and mobile backends? MongoDB
    Atlas on AWS cloud is a great option for storing documents in a scalable way and
    has a pay-as-you-go model. We will use MongoDB Atlas on AWS as the sink of the
    resultant data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们考虑这些数据将主要被移动和Web后端应用程序消费。那么，将数据存储在可以由Web和移动后端轻松拉取和使用的文档格式中是否值得？MongoDB
    Atlas在AWS云上是一个以可扩展方式存储文档的绝佳选择，并且具有按使用付费的模式。我们将使用AWS上的MongoDB Atlas作为结果数据的汇点。
- en: Now, let’s discuss how we will process the data in real time. The data will
    be sent as events to a Kafka topic. We will write a streaming application to process
    and write the result event on an output topic. The resulting record will contain
    the risk score as well. To dump the data from Kafka to any other data store or
    database, we can either write a consumer application or use Kafka Sink connectors.
    Writing a Kafka consumer app requires development and maintenance effort. However,
    if we choose to use Kafka Connect, we have to just configure it to get the benefits
    of a Kafka consumer. Kafka Connect is faster to deliver, easier to maintain, and
    more robust as all exception handling and edge cases are already taken care of
    and well-documented. So, we will use a Kafka Sink connector to save the result
    events from the output topic to the MongoDB Atlas database.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论我们将如何实时处理数据。数据将以事件的形式发送到Kafka主题。我们将编写一个流应用程序来处理并将结果事件写入输出主题。生成的记录将包含风险评分。要将数据从Kafka导出到任何其他数据存储或数据库，我们可以编写一个消费者应用程序或使用Kafka
    Sink连接器。编写Kafka消费者应用程序需要开发和维护工作。然而，如果我们选择使用Kafka Connect，我们只需配置它即可获得Kafka消费者的好处。Kafka
    Connect交付更快，维护更简单，并且更健壮，因为所有异常处理和边缘情况都已妥善处理，并有良好的文档记录。因此，我们将使用Kafka Sink连接器将输出主题的结果事件保存到MongoDB
    Atlas数据库。
- en: 'The following diagram describes the solution architecture:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了解决方案架构：
- en: '![Figure 6.1 – Solution architecture for our real-time credit risk analyzer
    ](img/B17084_06_001.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 我们实时信用风险分析器的解决方案架构](img/B17084_06_001.jpg)'
- en: Figure 6.1 – Solution architecture for our real-time credit risk analyzer
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 我们实时信用风险分析器的解决方案架构
- en: 'As shown in the preceding diagram, our solution architecture is as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们的解决方案架构如下：
- en: A new application event gets published in the input Kafka topic
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个新的应用程序事件被发布到输入Kafka主题
- en: The Kafka Streams application – the Risk Calculator app – reads the application
    event and fetches the corresponding credit history of the applicant from the credit
    history database
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Streams应用程序（风险计算器应用程序）读取应用程序事件并从信用历史数据库获取申请人的相应信用历史
- en: The Risk Calculator app creates and sends an HTTP request to the Risk Score
    Generator app with all the required parameters
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险计算器应用程序创建并发送一个包含所有必需参数的HTTP请求到风险评分生成器应用程序
- en: The Risk Score Generator app uses the already trained ML models to calculate
    the risk score of the application and returns the result to the Risk Calculator
    app
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险评分生成器应用程序使用已经训练好的机器学习模型来计算应用程序的风险评分，并将结果返回给风险计算器应用程序
- en: The Risk Calculator app generates the enriched application event and writes
    the resultant event in the output topic
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险计算器应用程序生成丰富的应用程序事件，并将结果事件写入输出主题
- en: A Kafka Sink connector, which is configured on the output topic, is responsible
    for consuming and writing the data to the MongoDB Atlas cloud database
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置在输出主题上的Kafka Sink连接器负责消费并将数据写入MongoDB Atlas云数据库
- en: If there is a processing error during Kafka streaming, an error message, along
    with the input event, will be written in the error database
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在Kafka流处理过程中出现处理错误，将错误消息以及输入事件写入错误数据库
- en: Now that we have learned how to architect a solution for our real-time data
    analysis needs, let’s learn how to implement the architecture.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何为我们的实时数据分析需求构建解决方案，让我们学习如何实现架构。
- en: Implementing and verifying the design
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施和验证设计
- en: The first step in a real-time implementation like this is to set up the streaming
    platform. To implement our architecture, we need to install Apache Kafka and create
    the necessary topics on our local machine.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种实时实现中的第一步是设置流平台。为了实现我们的架构，我们需要在本地机器上安装Apache Kafka并创建必要的主题。
- en: Setting up Apache Kafka on your local machine
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的本地机器上设置Apache Kafka
- en: 'In this section, you will learn how to set up an Apache Kafka cluster, run
    it, and create and list topics. Follow these steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何设置Apache Kafka集群，运行它，以及创建和列出主题。按照以下步骤操作：
- en: Download Apache Kafka version 2.8.1 from [https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz](https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz).
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz](https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz)下载Apache
    Kafka版本2.8.1。
- en: 'Extract the `kafka_2.12-2.8.1.tgz` archive file. The following command will
    help you do the same on Linux or macOS:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取`kafka_2.12-2.8.1.tgz`存档文件。以下命令将帮助您在Linux或macOS上执行相同的操作：
- en: '[PRE0]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Navigate to the Kafka installation root directory and start zookeeper using
    the following command:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到Kafka安装根目录，并使用以下命令启动zookeeper：
- en: '[PRE1]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, run the Kafka server using the following command:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令运行Kafka服务器：
- en: '[PRE2]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, create the topics using the following commands:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令创建主题：
- en: '[PRE3]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For the sake of simplicity, we have defined one partition and set the replication
    factor as 1\. But in a real production environment, the replication factor should
    be three or more. The number of partitions is based on the volume and velocity
    of data that needs to be processed and the optimum speed at which they should
    be processed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们定义了一个分区，并将副本因子设置为1。但在实际的生产环境中，副本因子应该是三个或更多。分区的数量基于需要处理的数据量及其速度，以及它们应该以何种最佳速度进行处理。
- en: 'We can list the topics that we created in the cluster using the following command:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令列出在集群中创建的主题：
- en: '[PRE4]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have installed Apache Kafka and created the topics that we need
    for the solution, you can focus on creating the credit records table and error
    table in a PostgreSQL instance installed on your local machine. The DDL and DML
    statements for these tables are available at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Apache Kafka并创建了所需的主题，您可以将注意力集中在在本地机器上安装的PostgreSQL实例中创建信用记录表和错误表。这些表的DDL和DML语句可在[https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/SQL)找到。
- en: Reference notes
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参考笔记
- en: 'If you are new to Kafka, I recommend learning the basics by reading the official
    Kafka documentation: [https://kafka.apache.org/documentation/#gettingStarted](https://kafka.apache.org/documentation/#gettingStarted).
    Alternatively, you can refer to the book *Kafka, The Definitive Guide*, by *Neha
    Narkhede*, *Gwen Sharipa*, and *Todd Palino*.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是Kafka的新手，我建议通过阅读官方Kafka文档来学习基础知识：[https://kafka.apache.org/documentation/#gettingStarted](https://kafka.apache.org/documentation/#gettingStarted)。或者，您可以参考由*Neha
    Narkhede*、*Gwen Sharipa*和*Todd Palino*合著的书籍《Kafka，权威指南》。
- en: In this section, we set up the Kafka streaming platform and the credit record
    database. In the next section, we will learn how to implement the Kafka streaming
    application to process the application event that reaches *landingTopic1* in real
    time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设置Kafka流平台和信用记录数据库。在下一节中，我们将学习如何实现Kafka流应用程序，以实时处理到达*landingTopic1*的应用程序事件。
- en: Developing the Kafka streaming application
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发Kafka流应用程序
- en: Before we implement the solution, let’s explore and understand a few basic concepts
    about Kafka Streams. Kafka Streams provides a client library for processing and
    analyzing data on the fly and sending the processed result into a sink (preferably
    an output topic).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现解决方案之前，让我们探索并理解一些关于Kafka Streams的基本概念。Kafka Streams提供了一个客户端库，用于实时处理和分析数据，并将处理后的结果发送到接收器（最好是输出主题）。
- en: 'A stream is an abstraction that represents unbound, continuously updating data
    in Kafka Streams. A stream processing application is a program written using the
    Kafka Streams library to process data that is present in the stream. It defines
    processing logic using a topology. A Kafka Streams topology is a graph that consists
    of stream processors as nodes and streams as edges. The following diagram shows
    an example topology for Kafka Streams:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 流是一个抽象，表示Kafka Streams中无界、持续更新的数据。流处理应用程序是使用Kafka Streams库编写的程序，用于处理流中的数据。它使用拓扑定义处理逻辑。Kafka
    Streams拓扑是一个由流处理器作为节点和流作为边的图。以下图显示了Kafka Streams的一个示例拓扑：
- en: '![Figure 6.2 – Sample Kafka Streams topology ](img/B17084_06_002.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – Kafka Streams拓扑示例](img/B17084_06_002.jpg)'
- en: Figure 6.2 – Sample Kafka Streams topology
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – Kafka Streams拓扑示例
- en: 'As you can see, a topology consists of **Stream Processors** – these are nodes
    and edges that represent streams. There can be two kinds of special stream processor
    nodes, as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一个拓扑由**流处理器**组成——这些节点和边代表流。可以有两种特殊流处理器节点，如下所示：
- en: '**Source Processor**: This is a special stream processing node that produces
    an input stream of data from consuming messages from one or multiple Kafka topics'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源处理器**：这是一个特殊的流处理节点，它从消费一个或多个Kafka主题的消息中产生数据输入流'
- en: '**Sink Processor**: As the name suggests, a sink processor consumes data from
    upstream and writes it to a sink or target topic'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汇处理器**：正如其名所示，汇处理器从上游消耗数据并将其写入汇点或目标主题'
- en: A topology in a Kafka streaming application can be built using a low-level Processor
    API or using high-level **Domain-Specific Language** (**DSL**) APIs. When an event
    is published to a source Kafka topic, the topology gets triggered, which processes
    the event using the topology definition and publishes the processed event to the
    Sink topic. Once a topology is successfully invoked and completed on a source
    event, the event offset is committed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka流应用程序中，可以使用低级处理器API或使用高级**领域特定语言**（**DSL**）API构建拓扑。当一个事件发布到源Kafka主题时，拓扑被触发，使用拓扑定义处理该事件，并将处理后的事件发布到汇点主题。一旦拓扑在源事件上成功调用并完成，事件偏移量将被提交。
- en: 'In our use case, the Kafka Streams application will do the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，Kafka Streams应用程序将执行以下操作：
- en: For the application event received, find the credit history from the credit
    record database.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于接收到的应用程序事件，从信用记录数据库中查找信用历史。
- en: Create the ML request body using the data received from Kafka and the data pulled
    out from the credit record database
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用从Kafka接收的数据和从信用记录数据库中提取的数据创建ML请求体
- en: Make a REST call to the Risk Score Generator application
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向风险评分生成器应用程序发出REST调用
- en: Form the final output record
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形成最终输出记录
- en: Send the final output record to a sink topic using a Sink processor
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用汇处理器将最终输出记录发送到汇点主题
- en: 'First and foremost, we need to create a Spring Boot Maven project and add the
    required Maven dependencies. The following Spring Maven dependencies should be
    added to the `pom.xml` file, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个Spring Boot Maven项目并添加所需的Maven依赖项。以下Spring Maven依赖项应添加到`pom.xml`文件中，如下所示：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Apart from this, as we are planning to develop a Kafka streaming application,
    we also need to add Kafka-related Maven dependencies, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些之外，由于我们计划开发Kafka流应用程序，我们还需要添加Kafka相关的Maven依赖项，如下所示：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, let’s write the `main` class, where we will initialize the Kafka Spring
    Boot application. However, in our application, we must exclude `KafkaAutoConfiguration`
    (as we intend to use our own property names for Kafka-related fields and not Spring
    Boot’s default Kafka property names), as shown in the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写`main`类，我们将在这里初始化Kafka Spring Boot应用程序。然而，在我们的应用程序中，我们必须排除`KafkaAutoConfiguration`（因为我们打算使用自己的属性名来表示与Kafka相关的字段，而不是Spring
    Boot的默认Kafka属性名），如下所示：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After creating the `main` class, we will create the main `KafkaStreamConfiguration`
    class, where all streaming beans will be defined and instantiated. This is where
    we will use Kafka Streams DSL to build the topology. This class must be annotated
    with `@EnableKafka` and `@EnableKafkaStreams`, as shown in the following code
    snippet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建`main`类之后，我们将创建主要的`KafkaStreamConfiguration`类，其中将定义和实例化所有流化bean。这是我们使用Kafka
    Streams DSL构建拓扑的地方。此类必须使用以下代码片段中的`@EnableKafka`和`@EnableKafkaStreams`进行注解：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will create the `KafkaStreamsConfiguration` bean. The following code
    snippet shows the implementation of the `KafkaStreamsConfiguration` bean:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建`KafkaStreamsConfiguration` bean。以下代码片段显示了`KafkaStreamsConfiguration`
    bean的实现：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: While creating the `KafkaStreamsConfiguration` bean, we must pass all Kafka
    streaming-related properties. Here, it is mandatory to set `StreamsConfig.APPLICATION_ID`
    and `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG`. In this case, `StreamsConfig.APPLICATION_ID`
    corresponds to the consumer group ID of the Kafka Streams application, while `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG`
    corresponds to the Kafka broker address. Without these values, no Kafka streaming
    or consumer application can run or connect to the Kafka cluster. Kafka Streams
    applications can distribute the traffic coming from a topic within a consumer
    group among multiple consumers that share the same consumer group ID. By increasing
    the running instance of the streaming application while using the same ID, we
    can have more parallelism and better throughput. However, increasing the number
    of instances beyond the number of partitions in the Kafka topic will not have
    any effect on the throughput.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 `KafkaStreamsConfiguration` 对象时，我们必须传递所有与 Kafka 流相关的属性。在这里，设置 `StreamsConfig.APPLICATION_ID`
    和 `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` 是强制性的。在这种情况下，`StreamsConfig.APPLICATION_ID`
    对应于 Kafka Streams 应用程序的消费者组 ID，而 `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` 对应于
    Kafka 代理地址。没有这些值，无法运行或连接到 Kafka 集群的 Kafka 流或消费者应用程序。Kafka Streams 应用程序可以在具有相同消费者组
    ID 的多个消费者之间分配来自消费者组内主题的流量。通过使用相同的 ID 增加运行实例的数量，我们可以获得更多的并行性和更好的吞吐量。然而，将实例数量增加到
    Kafka 主题分区数量以上将不会对吞吐量产生任何影响。
- en: 'Now that we have created the `KafkaStreamsConfiguration` bean, let’s create
    `KStream`. While creating this `KStream` bean, we must define the topology. The
    following code creates the `KStream` bean:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了 `KafkaStreamsConfiguration` 对象，让我们创建 `KStream`。在创建此 `KStream` 对象时，我们必须定义拓扑。以下代码创建了
    `KStream` 对象：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Each message in a Kafka topic consists of a key and a value. The value contains
    the actual message, while the key helps determine the partition while the message
    is published. However, when we consume the message using streams, we must mention
    the type of key and value that we are expecting. In our case, we are expecting
    both the key and value to be `String`. So, the `KStream` bean is created as an
    instance of `KStream<String,String>`. First, we must create a stream using the
    `StreamsBuilder` class, which is part of the Kafka Streams API. In our use case,
    the topology is built as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 主题中的每条消息都由一个键和一个值组成。值包含实际的消息，而键有助于在消息发布时确定分区。然而，当我们使用流来消费消息时，我们必须提到我们期望的键和值的类型。在我们的情况下，我们期望键和值都是
    `String` 类型。因此，`KStream` 对象被创建为一个 `KStream<String,String>` 的实例。首先，我们必须使用 `StreamsBuilder`
    类创建一个流，它是 Kafka Streams API 的一部分。在我们的用例中，拓扑构建如下：
- en: First, using the `StreamsBuilder` API, input streams are created from `inputTopic`.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用 `StreamsBuilder` API，从 `inputTopic` 创建输入流。
- en: A transform processor is applied to the resultant input stream using the `transform()`
    DSL function.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `transform()` DSL 函数将转换处理器应用于结果输入流。
- en: A custom Transformer called `RiskCalculatorTransformer` is used to transform/process
    the data coming from the input stream.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个名为 `RiskCalculatorTransformer` 的自定义 Transformer 用于转换/处理来自输入流的 数据。
- en: The processed output event is written to `outputTopic`.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理后的输出事件被写入 `outputTopic`。
- en: 'Now, let’s learn how to write a custom Transformer for a Kafka Streams application.
    In our scenario, we have created `RiskCalculatorTransformer`. The following discussion
    explains how to develop a custom Transformer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何为 Kafka Streams 应用程序编写自定义 Transformer。在我们的场景中，我们创建了 `RiskCalculatorTransformer`。以下讨论解释了如何开发自定义
    Transformer：
- en: 'First, we must create a class that implements the `org.apache.kafka.streams.kstream.Transformer`
    interface. It has three methods – `init`, `transform`, and `close` – that need
    to be implemented. The following code shows the definition of the `Transformer`
    interface:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须创建一个实现 `org.apache.kafka.streams.kstream.Transformer` 接口的类。它有三个方法——`init`、`transform`
    和 `close`——需要实现。以下代码显示了 `Transformer` 接口的定义：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see, the `Transformer` interface expects three generic types – `K`,
    `V`, and `R`. `K` specifies the data type of the key of the message, `V` specifies
    the data type of the value of the message, and `R` specifies the data type of
    the result of the message. While `init` and `close` are only used when some pre
    or post-processing is needed before the message is processed, `transform` is a
    mandatory method that defines the actual transformation or processing logic.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`Transformer` 接口期望三个泛型类型 – `K`、`V` 和 `R`。`K` 指定消息键的数据类型，`V` 指定消息值的数据类型，`R`
    指定消息结果的数据类型。而 `init` 和 `close` 只在需要在进行消息处理之前进行预处理或后处理时使用，`transform` 是一个强制方法，它定义了实际的转换或处理逻辑。
- en: 'In our use case, we receive the value of the message as a JSON string, process
    it, add the risk score, and send out the resultant value as a JSON string. The
    data type of the key remains unchanged. Hence, we send out a `KeyValue` pair object
    as a result. Our final `Transformer` outline looks as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的用例中，我们以 JSON 字符串的形式接收消息的值，处理它，添加风险评分，并以 JSON 字符串的形式发送结果值。键的数据类型保持不变。因此，我们发送一个
    `KeyValue` 对象作为结果。我们的最终 `Transformer` 概述如下：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As shown in the preceding code, our Transformer is expecting the key and value
    of the message to be of the `String` type, and it returns a `KeyValue` pair where
    both the key and value are of the `String` type.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们的 Transformer 期望消息的键和值都是 `String` 类型，并返回一个键值对，其中键和值都是 `String` 类型。
- en: 'In our Transformer, we don’t need any pre or post-processing. So, let’s move
    on and discuss how to implement the `transform` method of our `Transformer`. The
    code of the `transform` method is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Transformer 中，我们不需要任何预处理或后处理。那么，让我们继续讨论如何实现我们的 `Transformer` 的 `transform`
    方法。`transform` 方法的代码如下：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the step-by-step guide for implementing our `transform` method:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现我们的 `transform` 方法的逐步指南：
- en: First, we deserialize the incoming value, which is a JSON string, into a POJO
    called `ApplicationEvent` using the Jackson `ObjectMapper` class.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用 Jackson 的 `ObjectMapper` 类将传入的值（一个 JSON 字符串）反序列化为一个名为 `ApplicationEvent`
    的 POJO。
- en: Then, we initiate a JDBC call to the credit record database using Spring’s `JdbcTemplate`.
    While forming the SQL, we use the application ID that was deserialized in the
    previous step. We get a list of the `CreditRecord` objects because of the JDBC
    call.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 Spring 的 `JdbcTemplate` 初始化一个对信用记录数据库的 JDBC 调用。在构建 SQL 时，我们使用之前步骤中反序列化的应用程序
    ID。由于 JDBC 调用，我们获得了一个 `CreditRecord` 对象列表。
- en: Next, we form the request body for the HTTP REST call that we are going to make
    to get the risk score. Here, we populate an `MLRequest` object using the `ApplicationEvent`
    object (deserialized earlier) and the list of `CreditRecord` objects we obtained
    in the previous step.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为即将进行的 HTTP REST 调用构建请求体。在这里，我们使用之前反序列化的 `ApplicationEvent` 对象和之前步骤中获得的
    `CreditRecord` 对象列表填充一个 `MLRequest` 对象。
- en: Then, we wrap the `MLRequest` object in an `HTTPEntity` object and make the
    REST call using the Spring `RestTemplate` API.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将 `MLRequest` 对象包装在一个 `HTTPEntity` 对象中，并使用 Spring 的 `RestTemplate` API
    进行 REST 调用。
- en: 'We deserialize the REST response to the `RiskScoreResponse` object. The model
    of the `RiskScoreResponse` object looks as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 REST 响应反序列化为 `RiskScoreResponse` 对象。`RiskScoreResponse` 对象的模型如下所示：
- en: '[PRE14]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If the REST response is `OK`, then we form the `EnrichedApplication` object
    using the `ApplicationEvent` and `RiskScoreResponse` objects.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 REST 响应是 `OK`，则使用 `ApplicationEvent` 和 `RiskScoreResponse` 对象构建 `EnrichedApplication`
    对象。
- en: Finally, we create and return a new `KeyValue` pair object, where the key is
    unchanged, but the value is the serialized string of the `EnrichedApplication`
    object we created in *step 6*.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建并返回一个新的 `KeyValue` 对象，其中键保持不变，但值是我们创建的 `EnrichedApplication` 对象的序列化字符串，该对象是在
    *步骤 6* 中创建的。
- en: For exception handling, we log any errors as well as send the error events to
    an error database for future analysis, reporting, and reconciliation. The reporting
    and reconciliation processes won’t be covered here and are usually done by some
    kind of batch programming.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于异常处理，我们将任何错误记录下来，并将错误事件发送到错误数据库以供未来分析、报告和调整。报告和调整过程在此不涉及，通常由某种批量编程完成。
- en: In this section, we learned how to develop a Kafka Streams application from
    scratch. However, we should be able to successfully unit test a streaming application
    to make sure that our intended functionalities are working fine. In the next section,
    we will learn how to unit test a Kafka Streams application.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何从头开始开发一个 Kafka Streams 应用程序。然而，我们应该能够成功地对流式应用程序进行单元测试，以确保我们的预期功能运行良好。在下一节中，我们将学习如何对
    Kafka Streams 应用程序进行单元测试。
- en: Unit testing a Kafka Streams application
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kafka Streams 应用程序的单元测试
- en: 'To unit test a Kafka Streams application, we must add the Kafka Streams test
    utility dependencies to the `pom.xml` file:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要对 Kafka Streams 应用程序进行单元测试，我们必须将 Kafka Streams 测试实用程序依赖项添加到 `pom.xml` 文件中：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Also, before we do a JUnit test, we need to refactor our code a little bit.
    We have to break the definition of the `KStream` bean into two methods, like so:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们进行 JUnit 测试之前，我们需要对我们的代码进行一点重构。我们必须将 `KStream` bean 的定义拆分为两个方法，如下所示：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As shown in the preceding code, we took out the `KStream` formation code, put
    it in a utility method in a singleton class called `StreamBuilder`, and used the
    `Bean` method as a wrapper on top of it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们将 `KStream` 形成代码提取出来，放入一个名为 `StreamBuilder` 的单例类中的工具方法中，并在其上方使用 `Bean`
    方法作为包装器。
- en: Now, let’s learn how to write the JUnit test case. First, our transformation
    requires a JDBC call and a REST call. To do so, we need to mock the JDBC call.
    To do that, we will use Mockito libraries.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何编写 JUnit 测试用例。首先，我们的转换需要 JDBC 调用和 REST 调用。要做到这一点，我们需要模拟 JDBC 调用。为此，我们将使用
    Mockito 库。
- en: 'We can mock our `JdbcTemplate` call like so:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样模拟我们的 `JdbcTemplate` 调用：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, we create a mock `JdbcTemplate` object using the `@Mock` annotation.
    Then, we use Mockito’s `when().thenReturn()` API to define a mock output for a
    call made using the mock `JdbcTemplate` object.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `@Mock` 注解创建一个模拟的 `JdbcTemplate` 对象。然后，我们使用 Mockito 的 `when().thenReturn()`
    API 定义一个模拟输出，用于通过模拟的 `JdbcTemplate` 对象进行的调用。
- en: 'A similar technique can be used to mock `RestTemplate`. The code for mocking
    `RestTemplate` is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似的技术来模拟 `RestTemplate`。模拟 `RestTemplate` 的代码如下：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, first, we mock `RestTemplate` using the `@Mock` annotation.
    Then, using Mockito APIs, we mock any `POST` call that returns a `RiskScoreResponse`
    object.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，首先，我们使用 `@Mock` 注解模拟 `RestTemplate`。然后，使用 Mockito API 模拟任何返回 `RiskScoreResponse`
    对象的 `POST` 调用。
- en: 'Now, let’s form the topology. You can use the following code to create the
    topology:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建拓扑。您可以使用以下代码创建拓扑：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we created an instance of the `org.apache.kafka.streams.StreamsBuilder`
    class. Using our `StreamBuilder` utility class, we defined the topology by calling
    the `getkStream` method. Finally, we built the topology by calling the `build()`
    method of the `org.apache.kafka.streams.StreamsBuilder` class.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个 `org.apache.kafka.streams.StreamsBuilder` 类的实例。使用我们的 `StreamBuilder`
    工具类，我们通过调用 `getkStream` 方法定义了拓扑。最后，我们通过调用 `org.apache.kafka.streams.StreamsBuilder`
    类的 `build()` 方法构建了拓扑。
- en: 'Kafka Stream’s test utils come with a `Utility` class called `TopologyTestDriver`.
    `TopologyTestDriver` is created by passing the topology and config details. Once
    `TopologyTestDriver` has been created, it helps to create `TestInputTopic` and
    `TestOutputTopic`. The following code describes how to instantiate a `TopologyTestDriver`
    and create `TestInputTopic` and `TestOutputTopic`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 的测试工具包含一个名为 `Utility` 的类，称为 `TopologyTestDriver`。`TopologyTestDriver`
    通过传递拓扑和配置详细信息来创建。一旦创建了 `TopologyTestDriver`，它有助于创建 `TestInputTopic` 和 `TestOutputTopic`。以下代码描述了如何实例化
    `TopologyTestDriver` 并创建 `TestInputTopic` 和 `TestOutputTopic`：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To create a `TestInputTopic`, we need to specify the name of the topic, as
    well as the key and value serializers. Similarly, `TestOutputTopic` requires key
    and value deserializers, along with the output topic name. We can push a test
    event to `TestInputTopic` using the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个 `TestInputTopic`，我们需要指定主题名称，以及键和值序列化器。同样，`TestOutputTopic` 需要键和值反序列化器，以及输出主题名称。我们可以使用以下代码将测试事件推送到
    `TestInputTopic`：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we can assert our expected result with the actual result using the
    `org.junit.Assert.assertEquals` static method, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `org.junit.Assert.assertEquals` 静态方法，通过以下方式对我们的预期结果和实际结果进行断言：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can run this JUnit test by right-clicking and running the `Test` class,
    as shown in the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过右键单击并运行 `Test` 类来运行此 JUnit 测试，如下面的截图所示：
- en: '![Figure 6.3 – Running a Kafka Streams JUnit test case ](img/B17084_06_003.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 运行 Kafka Streams JUnit 测试用例](img/B17084_06_003.jpg)'
- en: Figure 6.3 – Running a Kafka Streams JUnit test case
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 运行 Kafka Streams JUnit 测试用例
- en: 'Once you have run the JUnit test case, you will see the test result in the
    run window of IntelliJ IDE, as shown in the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你运行了 JUnit 测试用例，你将在 IntelliJ IDE 的运行窗口中看到测试结果，如下面的截图所示：
- en: '![Figure 6.4 – Verifying the JUnit test’s results ](img/B17084_06_004.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 验证 JUnit 测试的结果](img/B17084_06_004.jpg)'
- en: Figure 6.4 – Verifying the JUnit test’s results
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 验证 JUnit 测试的结果
- en: In this section, we learned how to write a JUnit test case for a Kafka streaming
    application and unit test our Streams application. In the next section, we will
    learn how to configure the streaming application and run the application on our
    local system.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何为 Kafka 流式应用程序编写 JUnit 测试用例并对我们的 Streams 应用程序进行单元测试。在下一节中，我们将学习如何配置流式应用程序并在我们的本地系统上运行应用程序。
- en: Configuring and running the application
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置和运行应用程序
- en: 'To run this application, we must configure the `application.yaml` file, which
    contains the following details:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此应用程序，我们必须配置 `application.yaml` 文件，其中包含以下详细信息：
- en: Application port number (as we will launch two Spring Boot applications on our
    local machine)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序端口号（因为我们将在本地机器上启动两个 Spring Boot 应用程序）
- en: Data source details
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源细节
- en: Kafka details such as bootstrap servers and topics
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka 细节，如引导服务器和主题
- en: The REST HTTP URL for the Risk Score Generator app
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 风险评分生成器应用程序的 REST HTTP URL
- en: 'Our sample `application.yaml` file will look as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例 `application.yaml` 文件将如下所示：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, we can run the application by running the main class, `CreditRiskCalculatorApp`,
    of the CreditRiskCalculator application. But before we start the CreditRiskCalculator
    app, we should run the RiskScoreGenerator app by running its main class – that
    is, `RiskScoreGenerator`. Both these applications are Spring Boot applications;
    please refer to the *Implementing and unit testing the solution* section of [*Chapter
    4*](B17084_04.xhtml#_idTextAnchor062), *ETL Data Load – A Batch-Based Solution
    to Ingesting Data in a Data Warehouse*, to learn how to run a Spring Boot application.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过运行 CreditRiskCalculator 应用程序的 main 类 `CreditRiskCalculatorApp` 来运行应用程序。但在我们启动
    CreditRiskCalculator 应用程序之前，我们应该通过运行其 main 类 – 即 `RiskScoreGenerator` 来运行 RiskScoreGenerator
    应用程序。这两个应用程序都是 Spring Boot 应用程序；请参阅 [*第 4 章*](B17084_04.xhtml#_idTextAnchor062)
    的 *实现和单元测试解决方案* 部分，*ETL 数据加载 – 数据仓库中数据摄取的基于批处理解决方案*，以了解如何运行 Spring Boot 应用程序。
- en: Troubleshooting tips
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除技巧
- en: If, while starting the CreditRiskCalculator application, you notice a warning
    message such as **Connection to node -1 (localhost/127.0.0.1:9092) could not be
    established. Broker may not be available.** in the logs, please ensure your Kafka
    server is reachable and running.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在启动 CreditRiskCalculator 应用程序时，你在日志中注意到类似于 **无法建立到节点 -1 (localhost/127.0.0.1:9092)
    的连接。代理可能不可用** 的警告消息，请确保你的 Kafka 服务器是可访问的并且正在运行。
- en: If you notice an exception such as `max.poll.interval.ms` or decrease the value
    of `max.poll.records`. This usually happens when the number of records polled
    takes more time to process than the maximum poll interval time configured.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到一个异常，例如 `max.poll.interval.ms` 或减少 `max.poll.records` 的值。这通常发生在检索的记录数量处理时间超过配置的最大检索间隔时间时。
- en: 'If you encounter an error such as **java.lang.IllegalArgumentException: Assigned
    partition**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你遇到一个错误，例如 **java.lang.IllegalArgumentException: 分区已分配**。'
- en: '`application.id`. Change your `application.id` to solve this problem.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`application.id`。将你的 `application.id` 更改以解决这个问题。'
- en: In this section, we learned how to create and unit test a Kafka Streams application.
    The source code for this application is available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何创建和单元测试 Kafka Streams 应用程序。该应用程序的源代码可在 GitHub 上找到，地址为 [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/CreditRiskCalculator)。
- en: Since implementing an ML-based Risk Score Generator app is outside the scope
    of this book, we have created a Spring Boot REST application that generates a
    dummy risk score between 1 to 100\. The code base of this dummy application is
    available on GitHub at [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现基于机器学习的风险分数生成器应用程序超出了本书的范围，我们已创建了一个 Spring Boot REST 应用程序，该应用程序生成介于 1 到
    100 之间的虚拟风险分数。此虚拟应用程序的代码库可在 GitHub 上找到，地址为 [https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator](https://github.com/PacktPublishing/Scalable-Data-Architecture-with-Java/tree/main/Chapter06/sourcecode/RiskScoreGenerator)。
- en: In a real-world scenario, ML-based applications are more likely to be written
    in Python than Java since Python has better support for AI/ML libraries. However,
    a Kafka Streams application will be able to make a REST call and get the generated
    risk score from that application, as shown earlier.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，基于机器学习的应用程序更有可能用 Python 编写而不是 Java，因为 Python 对 AI/ML 库的支持更好。然而，Kafka
    Streams 应用程序将能够发出 REST 调用并从该应用程序获取之前展示的生成风险分数，如图所示。
- en: So far, we have been receiving the event from an input Kafka topic, processing
    it on the fly, generating a risk score, and writing the enriched event to an output
    Kafka topic. In the next section, we will learn how to integrate Kafka with MongoDB
    and stream the events to MongoDB as soon as they are published to the output Kafka
    topic.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从输入 Kafka 主题接收事件，即时处理它，生成风险分数，并将丰富的事件写入输出 Kafka 主题。在下一节中，我们将学习如何将
    Kafka 与 MongoDB 集成，并将事件流式传输到 MongoDB，一旦它们在输出 Kafka 主题中发布。
- en: Creating a MongoDB Atlas cloud instance and database
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 MongoDB Atlas 云实例和数据库
- en: 'In this section, we will create a cloud-based instance using MongoDB Atlas.
    Follow these steps to set up the MongoDB cloud instance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 MongoDB Atlas 创建一个基于云的实例。按照以下步骤设置 MongoDB 云实例：
- en: Sign up for a MongoDB Atlas account if you haven’t done so already ([https://www.mongodb.com/atlas/database](https://www.mongodb.com/atlas/database)).
    While signing up, you will be asked for the type of subscription that you need.
    For this exercise, you can choose the shared subscription, which is free, and
    choose an AWS cluster as your preferred choice of cloud.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您尚未注册，请注册 MongoDB Atlas 账户（[https://www.mongodb.com/atlas/database](https://www.mongodb.com/atlas/database)）。在注册过程中，您将被要求选择所需的订阅类型。对于这个练习，您可以选择共享订阅，它是免费的，并选择
    AWS 集群作为您首选的云服务。
- en: 'You will see the following screen. Here, click the **Build a Database** button
    to create a new database instance:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到以下屏幕。在这里，点击**构建数据库**按钮以创建新的数据库实例：
- en: '![Figure 6.5 – MongoDB Atlas welcome screen ](img/B17084_06_005.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – MongoDB Atlas 欢迎屏幕](img/B17084_06_005.png)'
- en: Figure 6.5 – MongoDB Atlas welcome screen
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – MongoDB Atlas 欢迎屏幕
- en: 'To provision a new database, we will be asked to set a username and a password,
    as shown in the following screenshot:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要配置新的数据库，我们将被要求设置用户名和密码，如下面的截图所示：
- en: '![Figure 6.6 – Provisioning a new database instance ](img/B17084_06_006.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 配置新的数据库实例](img/B17084_06_006.jpg)'
- en: Figure 6.6 – Provisioning a new database instance
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 配置新的数据库实例
- en: 'Then, we will be asked to enter all the IP addresses that we want to grant
    access to the MongoDB instance. Here, since we will run our application from our
    local system, we will add our local IP address to the IP Access List. Then, click
    **Finish and Close**:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将被要求输入所有我们希望授予 MongoDB 实例访问权限的 IP 地址。在这里，由于我们将从我们的本地系统运行应用程序，我们将添加我们的本地
    IP 地址到 IP 访问列表。然后，点击**完成并关闭**：
- en: '![Figure 6.7 – Setting up an IP Access List during database provisioning ](img/B17084_06_007.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 在数据库配置期间设置 IP 访问列表](img/B17084_06_007.jpg)'
- en: Figure 6.7 – Setting up an IP Access List during database provisioning
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 在数据库配置期间设置 IP 访问列表
- en: 'Once the cluster has been created, we will see the cluster on the dashboard,
    as shown in the following screenshot. Now, click the **Browse Collections** button
    to see the collections and data in this database instance:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦集群创建完成，我们将在仪表板上看到集群，如下面的截图所示。现在，点击**浏览集合**按钮以查看此数据库实例中的集合和数据：
- en: '![Figure 6.8 – Cluster dashboard in MongoDB Atlas ](img/B17084_06_008.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – MongoDB Atlas 的集群仪表板](img/B17084_06_008.jpg)'
- en: Figure 6.8 – Cluster dashboard in MongoDB Atlas
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – MongoDB Atlas 的集群仪表板
- en: 'As shown in the following screenshot, currently, there are no collections or
    data. However, you can create collections or data manually while using this interface
    by clicking the **Add My Own Data** button:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如以下截图所示，目前没有任何集合或数据。然而，您可以通过点击**添加我的数据**按钮在此界面使用时手动创建集合或数据：
- en: '![Figure 6.9 – Exploring collections and data in the MongoDB database instance
    ](img/B17084_06_009.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 在MongoDB数据库实例中探索集合和数据](img/B17084_06_009.jpg)'
- en: Figure 6.9 – Exploring collections and data in the MongoDB database instance
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 在MongoDB数据库实例中探索集合和数据
- en: In this section, we learned how to create a cloud-based MongoDB instance using
    the online interface of MongoDB Atlas. In the next section, we will learn how
    to configure and deploy our MongoDB Kafka connectors to send the data from Kafka
    to MongoDB in real time.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用MongoDB Atlas的在线界面创建基于云的MongoDB实例。在下一节中，我们将学习如何配置和部署我们的MongoDB
    Kafka连接器，以实时将数据从Kafka发送到MongoDB。
- en: Configuring Kafka Connect to store the results in MongoDB
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Kafka Connect以将结果存储在MongoDB中
- en: Kafka Connect is an open source, pluggable data integration framework for Kafka.
    It enables data sources and data sinks to easily connect with Kafka. Instead of
    writing cumbersome code to publish a message from the data source or consume a
    message from Kafka to write into a data sink, Kafka Connect provides declarative
    configuration to connect to a data source or sink.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect是一个开源的、可插拔的数据集成框架，用于Kafka。它使得数据源和数据汇能够轻松地连接到Kafka。无需编写繁琐的代码来从数据源发布消息或从Kafka消费消息写入数据汇，Kafka
    Connect提供了声明性配置来连接到数据源或汇。
- en: A Kafka Connect cluster already ships with a few types of connectors, such as
    `FileSourceConnector`. However, we can install any available connectors by placing
    them in the `plugins` folder. For our use case, we will deploy the MongoDB connector
    plugin (discussed later in this chapter).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect集群已经预装了一些类型的连接器，例如`FileSourceConnector`。然而，我们可以通过将它们放置在`plugins`文件夹中来安装任何可用的连接器。对于我们的用例，我们将部署MongoDB连接器插件（本章后面将讨论）。
- en: A Kafka Connect instance can be deployed and run in either cluster or standalone
    mode. However, in production, it usually runs in cluster mode. When we run in
    cluster mode, we can register the Kafka connector configuration using Kafka Connects’
    REST API. In standalone mode, we can register a connector configuration while
    starting the Kafka Connect instance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect实例可以以集群模式或独立模式部署和运行。然而，在生产环境中，它通常以集群模式运行。当我们以集群模式运行时，我们可以使用Kafka
    Connect的REST API注册Kafka连接器配置。在独立模式下，我们可以在启动Kafka Connect实例时注册连接器配置。
- en: Since we are running our Kafka cluster on our local machine, we will deploy
    our Kafka Connect instance in standalone mode for this implementation. But remember,
    if you are implementing for production purposes, you should run Kafka, as well
    as Kafka Connect, in a clustered environment (this can be a physical cluster or
    a virtual cluster, such as a virtual machine, AWS ECS, or Docker container).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本地机器上运行我们的Kafka集群，因此我们将以独立模式部署我们的Kafka Connect实例进行此实现。但请记住，如果您是为了生产目的而实施，您应该以集群模式运行Kafka以及Kafka
    Connect（这可以是一个物理集群或虚拟集群，例如虚拟机、AWS ECS或Docker容器）。
- en: 'First, let’s set up the Kafka Connect cluster:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置Kafka Connect集群：
- en: 'First, create a new folder called `plugins` under the Kafka root installation
    folder, as shown in the following screenshot:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在Kafka根安装文件夹下创建一个名为`plugins`的新文件夹，如图所示：
- en: '![Figure 6.10 – Creating the plugins folder ](img/B17084_06_010.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 创建插件文件夹](img/B17084_06_010.jpg)'
- en: Figure 6.10 – Creating the plugins folder
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 创建插件文件夹
- en: 'Next, navigate to `connect-standalone.properties`, which is present in the
    `<Kafka-root>/config` folder. Add the following property to the `connect-standalone.properties`
    file:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导航到`connect-standalone.properties`，它位于`<Kafka-root>/config`文件夹中。将以下属性添加到`connect-standalone.properties`文件中：
- en: '[PRE24]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Then, download the MongoDB Kafka connector plugin from [https://www.confluent.io/hub/mongodb/kafka-connect-mongodb](https://www.confluent.io/hub/mongodb/kafka-connect-mongodb).
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从[https://www.confluent.io/hub/mongodb/kafka-connect-mongodb](https://www.confluent.io/hub/mongodb/kafka-connect-mongodb)下载MongoDB
    Kafka连接器插件。
- en: 'A ZIP file will be downloaded. Copy and extract the ZIP file in the `plugin`
    folder created under the Kafka root installation folder. At this point, the folder
    structure should look as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 将下载一个ZIP文件。在Kafka根安装文件夹下创建的`plugin`文件夹中复制并解压该ZIP文件。此时，文件夹结构应如下所示：
- en: '![Figure 6.11 – Kafka folder structure after deploying the mongo-kafka-connect
    plugin ](img/B17084_06_011.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 部署 mongo-kafka-connect 插件后的 Kafka 文件结构](img/B17084_06_011.jpg)'
- en: Figure 6.11 – Kafka folder structure after deploying the mongo-kafka-connect
    plugin
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 部署 mongo-kafka-connect 插件后的 Kafka 文件结构
- en: Now, let’s learn how to create and deploy a Kafka Connect worker configuration
    to create a pipeline between a Kafka topic and MongoDB sink.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何创建和部署 Kafka Connect 工作员配置，以在 Kafka 主题和 MongoDB 溢出之间创建管道。
- en: 'To write a Kafka Connect worker, we must understand the various types of declarative
    properties that Kafka Connect supports. The following diagram depicts various
    kinds of components that a Kafka Connect worker consists of:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写 Kafka Connect 工作员，我们必须了解 Kafka Connect 支持的各种声明性属性类型。以下图表描述了 Kafka Connect
    工作员由哪些类型的组件组成：
- en: '![Figure 6.12 – Kafka Connect components ](img/B17084_06_012.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – Kafka Connect 组件](img/B17084_06_012.jpg)'
- en: Figure 6.12 – Kafka Connect components
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – Kafka Connect 组件
- en: 'A Kafka connector consists of three types of components. They are as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 连接器由三种类型的组件组成。具体如下：
- en: '**Connector**: This interfaces Kafka with external data sources. It takes care
    of implementing whatever external protocol those data sources and sinks need to
    communicate with Kafka.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接器**：此接口将 Kafka 与外部数据源连接。它负责实现数据源和连接器与 Kafka 通信所需的外部协议。'
- en: '**Converter**: Converters are used to serialize and deserialize events.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：转换器用于序列化和反序列化事件。'
- en: '**Transformer**: This is an optional property. It is a stateless function that’s
    used to slightly transform the data so that it is in the right format for the
    destination.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：这是一个可选属性。它是一个无状态的函数，用于对数据进行轻微的转换，以便使其适合目标格式。'
- en: 'For our use case, we don’t need a transformer, but we do need to set all the
    properties related to the connector and converter. The following code is for the
    Kafka Sink Connect worker:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们不需要转换器，但我们需要设置所有与连接器和转换器相关的属性。以下代码是 Kafka Sink Connect 工作员的代码：
- en: '[PRE25]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As shown in the preceding configuration code, the connector properties such
    as `connector.class` and other properties specific to MongoDB are configured,
    and the converter properties such as `key.converter` and `value.converter` are
    set. Next, in the sink connector configuration, we define all the MongoDB connection
    properties, like so:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述配置代码所示，配置了连接器属性，如 `connector.class` 以及特定于 MongoDB 的其他属性，并设置了转换器属性，如 `key.converter`
    和 `value.converter`。接下来，在连接器配置中，我们定义了所有 MongoDB 连接属性，如下所示：
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we will set the `document.id` and `writemodel.strategy` properties in
    the sink connector configuration, as shown here:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在连接器配置中设置 `document.id` 和 `writemodel.strategy` 属性，如下所示：
- en: '[PRE27]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Save this configurations in a property file called `connect-riskcalc-mongodb-sink.properties`
    and place it in Kafka Connect’s `config` folder.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些配置保存到名为 `connect-riskcalc-mongodb-sink.properties` 的属性文件中，并将其放置在 Kafka Connect
    的 `config` 文件夹中。
- en: 'Now, we can run the Kafka Connect instance in standalone mode and start the
    `mongodb-sink` connector using the following command:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以以独立模式运行 Kafka Connect 实例，并使用以下命令启动 `mongodb-sink` 连接器：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, let’s learn how to troubleshoot possible issues that we may encounter.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何排查我们可能遇到的潜在问题。
- en: Troubleshooting the Kafka Sink connector
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kafka Sink 连接器的故障排除
- en: 'When a source or a sink connector runs on a Kafka Connect cluster, you may
    encounter multiple issues. The following list specifies a few common issues and
    how to resolve them:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 当源或连接器在 Kafka Connect 集群上运行时，你可能会遇到多个问题。以下列表指定了一些常见问题及其解决方法：
- en: 'If you encounter an error similar to the following, then please check whether
    the JSON message should contain a schema or not. If the JSON message should not
    contain schema, make sure that you set the `key.converter.schemas.enable` and
    `value.converter.schemas.enable` properties to `false`:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你遇到以下类似的错误，请检查 JSON 消息是否应该包含模式。如果 JSON 消息不应该包含模式，请确保将 `key.converter.schemas.enable`
    和 `value.converter.schemas.enable` 属性设置为 `false`：
- en: '[PRE29]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If you encounter an error such as `org.apache.kafka.common.errors.SerializationException:
    Error deserializing Avro message for id -1`, then check if the payload is Avro
    or JSON. If the message is a JSON payload instead of Avro, please change the value
    of the `value.converter` property in the connector to `org.apache.kafka.connect.json.JsonConverter`.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果您遇到类似`org.apache.kafka.common.errors.SerializationException: Error deserializing
    Avro message for id -1`的错误，请检查负载是否为Avro或JSON。如果消息是JSON负载而不是Avro，请将连接器中`value.converter`属性的值更改为`org.apache.kafka.connect.json.JsonConverter`。'
- en: 'You may encounter `BulkWriteExceptions` while writing to MongoDB. `BulkWriteExceptions`
    can be a `WriteError`, a `WriteConcernError`, or a `WriteSkippedError` (due to
    an earlier record failing in the ordered bulk write). Although we cannot prevent
    such errors, we can set the following parameters to move the rejected message
    to an error topic called `dead-letter-queue`:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在向MongoDB写入时，您可能会遇到`BulkWriteExceptions`。`BulkWriteExceptions`可能是一个`WriteError`、一个`WriteConcernError`或一个`WriteSkippedError`（由于在有序批量写入中先前的记录失败）。尽管我们无法防止此类错误，但我们可以设置以下参数将拒绝的消息移动到名为`dead-letter-queue`的错误主题：
- en: '[PRE30]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this section, we successfully created, deployed, and ran the MongoDB Kafka
    Sink connector. In the next section, we will discuss how to test the end-to-end
    solution.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功创建、部署并运行了MongoDB Kafka Sink连接器。在下一节中，我们将讨论如何测试端到端解决方案。
- en: Verifying the solution
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证解决方案
- en: To test the end-to-end pipeline, we must make sure that all the services, such
    as Kafka, PostgreSQL, Kafka Connect, and the MongoDB instance, are up and running.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试端到端管道，我们必须确保所有服务，如Kafka、PostgreSQL、Kafka Connect和MongoDB实例，都已启动并运行。
- en: Apart from that, the Kafka Streams application and the Risk Score Generator
    REST application should be up and running. We can start these applications by
    running the main Spring Boot application class.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Kafka Streams应用程序和风险评分生成器REST应用程序应该已经启动并运行。我们可以通过运行主Spring Boot应用程序类来启动这些应用程序。
- en: 'To test the application, open a new Terminal and navigate to the Kafka root
    installation folder. To start an instance of the Kafka console producer, use the
    following command:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试应用程序，打开一个新的终端并导航到Kafka根安装文件夹。要启动Kafka控制台生产者实例，请使用以下命令：
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we can publish input messages by using the console producer, as shown
    in the following screenshot:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用控制台生产者发布输入消息，如下面的截图所示：
- en: '![Figure 6.13 – Publishing messages using the Kafka console producer ](img/B17084_06_013.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 使用Kafka控制台生产者发布消息](img/B17084_06_013.jpg)'
- en: Figure 6.13 – Publishing messages using the Kafka console producer
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 使用Kafka控制台生产者发布消息
- en: 'As soon as we publish the message in the input topic, it gets processed, and
    the result is written to the MongoDB instance. You can verify the results in MongoDB
    like so:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在输入主题中发布消息，它就会被处理，并将结果写入MongoDB实例。您可以在MongoDB中这样验证结果：
- en: '![Figure 6.14 – Verifying the results ](img/B17084_06_015.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 验证结果](img/B17084_06_015.jpg)'
- en: Figure 6.14 – Verifying the results
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 验证结果
- en: In this section, we learned how to test the end-to-end solution for a real-time
    data processing problem and verify the result. Now, let’s summarize what we learned
    in this chapter.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何测试实时数据处理问题的端到端解决方案并验证结果。现在，让我们总结本章所学的内容。
- en: Summary
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to analyze a real-time data engineering problem,
    identify the streaming platform, and considered the basic characteristics that
    our solution must have to become an effective real-time solution. First, we learned
    how to choose a hybrid platform to suit legal needs as well as performance and
    cost-effectiveness.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何分析实时数据工程问题，确定流平台，并考虑我们的解决方案必须具备的基本特征以成为有效的实时解决方案。首先，我们学习了如何选择一个混合平台以满足法律需求以及性能和成本效益。
- en: Then, we learned how to use our conclusions from our problem analysis to build
    a robust, reliable, and effective real-time data engineering solution. After that,
    we learned how to install and run Apache Kafka on our local machine and create
    topics in that Kafka cluster. We also learned how to develop a Kafka Streams application
    to do stream processing and write the result to an output topic. Then, we learned
    how to unit test a Kafka Streams application to make the code more robust and
    defect-free. After that, we learned how to set up a MongoDB Atlas instance on
    the AWS cloud. Finally, we learned about Kafka Connect and how to configure and
    use a Kafka MongoDB Sink connector to send the processed event from the output
    topic to the MongoDB cluster. While doing so, we learned how to test and verify
    the real-time data engineering solution that we developed.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了如何利用我们的问题分析结论来构建一个稳健、可靠和有效的实时数据工程解决方案。之后，我们学习了如何在本地机器上安装和运行 Apache Kafka
    并在那个 Kafka 集群中创建主题。我们还学习了如何开发 Kafka Streams 应用程序来进行流处理并将结果写入输出主题。接着，我们学习了如何对 Kafka
    Streams 应用程序进行单元测试，以使代码更加稳健且无缺陷。之后，我们学习了如何在 AWS 云上设置 MongoDB Atlas 实例。最后，我们了解了
    Kafka Connect 以及如何配置和使用 Kafka MongoDB Sink 连接器，将处理后的事件从输出主题发送到 MongoDB 集群。在这个过程中，我们还学习了如何测试和验证我们所开发的实时数据工程解决方案。
- en: With that, we have learned how to develop optimized and cost-effective solutions
    for both batch-based and real-time data engineering problems. In the next chapter,
    we will learn about the various architectural patterns that are commonly used
    in data ingestion or analytics problems.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经学会了如何为基于批处理和实时数据工程问题开发优化且成本效益高的解决方案。在下一章中，我们将学习在数据摄取或分析问题中常用的一些架构模式。
