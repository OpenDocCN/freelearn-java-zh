- en: '*Chapter 9*: Distributed Logging, Tracing, and Monitoring'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 9 章*: 分布式日志记录、跟踪和监控'
- en: A microservice application often runs multiple microservices on a varied range
    of multiple hosts. For upstream consumers, the API gateway provides a one-stop-shop
    interface to access all the application endpoints. Any request to the API gateway
    is dispersed to one or more microservices. This distributed diffusion of request
    handling escalates challenges in maintaining microservices-based applications.
    If any anomaly or error occurs, it is hard to dig which microservice or distributed
    component is at fault. In addition, any effective microservices implementation
    must handle the maintenance challenges proactively.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务应用程序通常在多个主机上运行多个微服务。对于上游消费者，API 网关提供了一个一站式商店接口，用于访问所有应用程序端点。对 API 网关的任何请求都会分散到一个或多个微服务。这种分布式扩散的请求处理增加了维护基于微服务的应用程序的挑战。如果发生任何异常或错误，很难确定哪个微服务或分布式组件有故障。此外，任何有效的微服务实现都必须积极处理维护挑战。
- en: 'In this chapter, we will explore the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: '**Distributed logging**: How we can implement the log aggregation for distributed
    microservices so that application logs can be accessed and indexed in one place?'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式日志记录**: 我们如何实现分布式微服务的日志聚合，以便可以在一个地方访问和索引应用程序日志？'
- en: '**Distributed tracing**: How we can trace the execution of a user request that
    could be dispersed onto multiple microservices running on multiple host environments?'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式跟踪**: 我们如何跟踪可能分散到多个主机环境上运行的多个微服务的用户请求的执行？'
- en: '**Distributed monitoring**: How we can continuously monitor the key performance
    indicators for all the service components to get a holistic picture of the system''s
    health?'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式监控**: 我们如何持续监控所有服务组件的关键性能指标，以获得系统健康状况的整体视图？'
- en: By collecting these three different kinds of data – logging, tracing, and monitoring
    – we enhance the system observability. By accessing this telemetry data at any
    point in time, we can intuitively and precisely get a complete context of what
    and how a request was executed in the system. To learn more about observability,
    we will explore the microservices patterns on distributed logging, tracing, and
    monitoring with the hands-on `pet-clinic` application.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集这三种不同类型的数据——日志记录、跟踪和监控——我们增强了系统的可观察性。在任何时间点访问这些遥测数据，我们可以直观且精确地获得请求在系统中执行的整体上下文。要了解更多关于可观察性的信息，我们将通过实际的
    `pet-clinic` 应用程序探索分布式日志记录、跟踪和监控的微服务模式。
- en: By the end of this chapter, you will have good knowledge of implementing these
    observability patterns in the Micronaut framework.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将具备在 Micronaut 框架中实现这些可观察性模式的良好知识。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the commands and technical instructions in this chapter are run on Windows
    10 and macOS. Code examples covered in this chapter are available in the book's
    GitHub repository at [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有命令和技术说明都是在 Windows 10 和 macOS 上运行的。本章涵盖的代码示例可在本书的 GitHub 仓库中找到，网址为 [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09)。
- en: 'The following tools need to be installed and set up in the development environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下工具需要在开发环境中安装和设置：
- en: '**Java SDK**: Version 13 or above (we used Java 14).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java SDK**: 版本 13 或以上（我们使用了 Java 14）。'
- en: '**Maven**: This is optional and only required if you would like to use Maven
    as the build system. However, we recommend having Maven set up on any development
    machine. Instructions to download and install Maven can be found at [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Maven**: 这不是必需的，只有当您想使用 Maven 作为构建系统时才需要。然而，我们建议在任何开发机器上设置 Maven。有关下载和安装
    Maven 的说明，请参阅 [https://maven.apache.org/download.cgi](https://maven.apache.org/download.cgi)。'
- en: '**Development IDE**: Based on your preferences, any Java-based IDE can be used,
    but for the purpose of writing this chapter, IntelliJ was used.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发 IDE**: 根据您的偏好，可以使用任何基于 Java 的 IDE，但为了编写本章，我们使用了 IntelliJ。'
- en: '**Git**: Instructions to download and install Git can be found at [https://git-scm.com/downloads](https://git-scm.com/downloads).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git**: 有关下载和安装 Git 的说明，请参阅 [https://git-scm.com/downloads](https://git-scm.com/downloads)。'
- en: '**PostgreSQL**: Instructions to download and install PostgreSQL can be found
    at [https://www.postgresql.org/download/](https://www.postgresql.org/download/).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PostgreSQL**：有关下载和安装 PostgreSQL 的说明，请参阅 [https://www.postgresql.org/download/](https://www.postgresql.org/download/)。'
- en: '**MongoDB**: MongoDB Atlas provides a free online database-as-a-service with
    up to 512 MB storage. However, if a local database is preferred, then instructions
    to download and install can be found at [https://docs.mongodb.com/manual/administration/install-community/](https://docs.mongodb.com/manual/administration/install-community/).
    We used a local installation for writing this chapter.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MongoDB**：MongoDB Atlas 提供了一个免费的在线数据库即服务，存储空间高达 512 MB。然而，如果更喜欢本地数据库，则有关下载和安装的说明，请参阅
    [https://docs.mongodb.com/manual/administration/install-community/](https://docs.mongodb.com/manual/administration/install-community/)。我们为本章使用了本地安装。'
- en: '**REST client**: Any HTTP REST client can be used. We used the Advanced REST
    Client Chrome plugin.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**REST 客户端**：可以使用任何 HTTP REST 客户端。我们使用了 Advanced REST Client Chrome 插件。'
- en: '**Docker**: Instructions to download and install Docker can be found at https://docs.docker.com/get-docker/.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker**：有关下载和安装 Docker 的说明，请参阅 https://docs.docker.com/get-docker/。'
- en: Distributed logging in Micronaut microservices
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式日志记录在 Micronaut 微服务中
- en: As we discussed in the chapter introduction, in a microservices-based application,
    a user request is executed on multiple microservices running on different host
    environments. Therefore, the log messages are spread across multiple host machines.
    This brings a unique challenge to a developer or admin maintaining the application.
    If there's a failure, then it will be hard to zero down on the issue as you have
    to sign into multiple host machines/environments, grep the logs, and put them
    together to make sense.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在章节引言中讨论的，在基于微服务的应用程序中，用户请求在运行在不同主机环境上的多个微服务上执行。因此，日志消息会分散在多个主机机器上。这对维护应用程序的开发者或管理员来说是一个独特的挑战。如果出现故障，那么确定问题将变得很困难，因为你必须登录到多个主机机器/环境，grep
    日志，并将它们组合起来以便理解。
- en: In this section, we will dive into log aggregation for distributed logging in
    microservices.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入了解微服务中分布式日志记录的日志聚合。
- en: 'Log aggregation, as the name suggests, is combining the logs produced by various
    microservices and components in the application. Log aggregation typically involves
    the following components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，日志聚合是将各种微服务和应用程序组件产生的日志组合在一起。日志聚合通常涉及以下组件：
- en: '**Log producer**: This is any microservice or a distributed component that''s
    producing logs while executing the control flow.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志生产者**：这是在执行控制流时产生日志的任何微服务或分布式组件。'
- en: '**Log dispatcher**: The log dispatcher is responsible for collecting the logs
    produced by the log producer and dispatching them to the centralized storage.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志分发器**：日志分发器负责收集日志生产者产生的日志并将它们分发到集中存储。'
- en: '**Log storage**: The log storage persists and indexes the logs produced by
    all the application components and microservices.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志存储**：日志存储持久化和索引所有应用程序组件和微服务产生的日志。'
- en: '**Log visualizer**: The log visualizer provides a user interface for accessing,
    searching, and filtering the logs stored in the log storage.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志可视化器**：日志可视化器提供了一个用户界面，用于访问、搜索和过滤存储在日志存储中的日志。'
- en: 'In the `pet-clinic` application context, we will implement the **ELK** (short
    for **Elasticsearch**, **Logstash**, **Kibana**) Stack for distributed logging.
    Refer to the following figure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pet-clinic` 应用程序上下文中，我们将实现 **ELK** Stack（即 **Elasticsearch**、**Logstash**、**Kibana**）以实现分布式日志记录。请参考以下图示：
- en: '![Figure 9.1 – Distributed logging using the ELK Stack'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.1 – 使用 ELK Stack 进行分布式日志记录'
- en: '](img/Figure_9.1_B16585_Fixed_edited.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.1 – Distributed logging using the ELK Stack'
- en: Figure 9.1 – Distributed logging using the ELK Stack
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 9.1 – 使用 ELK Stack 进行分布式日志记录
- en: In the preceding diagram, the ELK Stack is used to implement distributed logging
    in the `pet-clinic` application. **Logstash** dispatches the logs into the **Elasticsearch**
    engine, which is then used by **Kibana** to provide a user interface.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，`pet-clinic` 应用程序中使用了 ELK Stack 来实现分布式日志记录。**Logstash** 将日志分发到 **Elasticsearch**
    引擎，然后由 **Kibana** 提供用户界面。
- en: In the next section, we will explore how we can set up an ELK Stack in a dockerized
    container.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何在 Docker 容器中设置 ELK Stack。
- en: Setting up ELK in Docker
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Docker 中设置 ELK
- en: 'To set up ELK in Docker, follow these instructions:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Docker 中设置 ELK，请按照以下说明操作：
- en: Check out `docker-elk` from [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-elk](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-elk](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro)
    检出 `docker-elk`。
- en: Open any Bash terminal (we used Git Bash).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何 Bash 终端（我们使用了 Git Bash）。
- en: Change directory to where you have checked out `docker-elk`.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录切换到您已检出 `docker-elk` 的位置。
- en: Run the `docker compose up –d` command.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `docker compose up –d` 命令。
- en: Wait for Docker to download the images and instantiate the ELK container.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待 Docker 下载镜像并实例化 ELK 容器。
- en: 'The preceding instructions will boot up an ELK app in Docker. You can verify
    the installation by going to the Docker Dashboard | **Containers / Apps**, as
    shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述指令将在 Docker 中启动一个 ELK 应用程序。您可以通过转到 Docker 仪表板 | **容器 / 应用** 来验证安装，如图所示：
- en: '![Figure 9.2 – Verifying ELK in the Docker Dashboard'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.2 – 在 Docker 仪表板中验证 ELK'
- en: '](img/Figure_9.2_B16585_Fixed.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.2_B16585_Fixed.jpg)'
- en: Figure 9.2 – Verifying ELK in the Docker Dashboard
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 9.2 – 在 Docker 仪表板中验证 ELK
- en: Here, you can verify ELK instantiation. By default, Elasticsearch runs on port
    `9200`, Logstash on `5000`, and Kibana on port `5601`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以验证 ELK 实例化。默认情况下，Elasticsearch 在端口 `9200` 上运行，Logstash 在 `5000`，Kibana
    在端口 `5601`。
- en: In the next section, we will modify our `pet-clinic` microservice to dispatch
    the logs to the Logstash instance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将修改我们的 `pet-clinic` 微服务，以便将日志发送到 Logstash 实例。
- en: Integrating Logstash with Micronaut microservices
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Logstash 与 Micronaut 微服务集成
- en: To integrate Logstash into the `pet-clinic` microservice, we will leverage Logback.
    We will introduce a new appender to Logback that can dispatch the logs to the
    previously created Logstash instance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Logstash 集成到 `pet-clinic` 微服务中，我们将利用 Logback。我们将向 Logback 引入一个新的追加器，可以将日志发送到之前创建的
    Logstash 实例。
- en: 'In the locally checked out `docker-elk` directory, you can verify that Logstash
    is configured with the following settings:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地检出的 `docker-elk` 目录中，您可以验证 Logstash 已配置以下设置：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In `logstash.config`, we have the following three sections:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `logstash.config` 中，我们有以下三个部分：
- en: '`input`: Logstash has the power to aggregate more than 50 different kinds of
    log sources. `input` configures Logstash for one or more input sources. In our
    configuration, we are enabling `tcp` input on port `5000`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：Logstash 有能力聚合超过 50 种不同的日志源。`input` 配置 Logstash 以配置一个或多个输入源。在我们的配置中，我们正在启用端口
    `5000` 上的 `tcp` 输入。'
- en: '`filter`: Logstash''s `filter` provides an easy way to transform the incoming
    logs into filter-defined log events. These events are then pushed to log storage.
    In the preceding configuration, we are using a `grok` filter along with `mutate`
    to add extra information (`app_name` and `app_port`) to the log events.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`：Logstash 的 `filter` 提供了一种简单的方法将传入的日志转换为过滤器定义的日志事件。然后，这些事件被推送到日志存储。在上述配置中，我们使用
    `grok` 过滤器与 `mutate` 一起添加额外的信息（`app_name` 和 `app_port`）到日志事件中。'
- en: '`output`: The `output` section configures the receiving sources so that Logstash
    can push the log events to the configured output sources. In the preceding configuration,
    we are configuring the standard output and Elasticsearch to receive the produced
    log events.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output`：`output` 部分配置接收源，以便 Logstash 可以将日志事件推送到配置的输出源。在上述配置中，我们正在配置标准输出和 Elasticsearch
    以接收生成的日志事件。'
- en: So far, we have booted up an ELK Docker instance with Logstash configured to
    receive, transform, and send log events into Elasticsearch. Next, we will make
    required amends in the `pet-clinic` microservices so that logs can be shipped
    to Logstash.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经启动了一个带有 Logstash 配置为接收、转换并将日志事件发送到 Elasticsearch 的 ELK Docker 实例。接下来，我们将对
    `pet-clinic` 微服务进行必要的修改，以便日志可以发送到 Logstash。
- en: Configuring microservices for distributed logging
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置微服务以进行分布式日志记录
- en: 'In order to make the `pet-clinic` microservice aggregate and ship logs to Logstash,
    we need to add the following `logstash-logback-encoder` dependency to all the
    microservice `pom.xml` files:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 `pet-clinic` 微服务能够聚合并将日志发送到 Logstash，我们需要将以下 `logstash-logback-encoder`
    依赖项添加到所有微服务的 `pom.xml` 文件中：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By importing `logstash-logback-encoder`, we can leverage the `net.logstash.logback.appender.LogstashTcpSocketAppender`
    class in `logback.xml`. This class provides the `logstash` appender, which can
    ship logs to the Logstash server from the microservice.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入 `logstash-logback-encoder`，我们可以在 `logback.xml` 中利用 `net.logstash.logback.appender.LogstashTcpSocketAppender`
    类。此类提供了 `logstash` 追加器，可以将微服务的日志发送到 Logstash 服务器。
- en: 'Modify `logback.xml` for all microservices by adding the Logstash appender
    as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下方式修改所有微服务的`logback.xml`，添加Logstash appender：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The Logstash appender will help in shipping the logs to `localhost:5000` and
    as we are running Logstash in a Docker container, we provide the address as `host.docker.internal`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash appender将帮助将日志发送到`localhost:5000`，因为我们正在Docker容器中运行Logstash，所以我们提供地址为`host.docker.internal`。
- en: Also, we need to add the appender to the root level by using `appender-ref`.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要通过使用`appender-ref`将appender添加到根级别。
- en: 'Furthermore, we need to define two properties for `app_name` and `app_port`.
    These are the filter configurations that will be used by Logstash to create the
    desired log events with app information. This is how we do it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要为`app_name`和`app_port`定义两个属性。这些是Logstash将用于创建带有应用程序信息的所需日志事件的过滤器配置。这是我们的做法：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding code snippet, we have added the required properties for the
    `pet-owner` microservice. We need to add similar properties in all the services
    so Logstash can generate service-specific log events.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们为`pet-owner`微服务添加了所需的属性。我们需要在所有服务中添加类似的属性，以便Logstash可以生成特定于服务的日志事件。
- en: Verifying the distributed logging in the pet-clinic application
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证`pet-clinic`应用程序中的分布式日志
- en: 'To verify that Logstash is receiving the logs from all the microservices in
    the `pet-clinic` application, we would need to re-build the Docker images and
    redeploy the `pet-clinic` application. Perform the following steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证Logstash是否正在从`pet-clinic`应用程序中的所有微服务接收日志，我们需要重新构建Docker镜像并重新部署`pet-clinic`应用程序。执行以下步骤：
- en: 'Open the terminal in the `pet-owner` microservice root directory:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pet-owner`微服务根目录中打开终端：
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 运行`jib`命令构建Docker镜像`mvn compile jib:dockerBuild`。
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 等待`jib`构建并将Docker镜像上传到本地Docker镜像仓库。
- en: 'Open the terminal in the `pet-clinic` microservice root directory:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pet-clinic`微服务根目录中打开终端：
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 运行`jib`命令构建Docker镜像`mvn compile jib:dockerBuild`。
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 等待`jib`构建并将Docker镜像上传到本地Docker镜像仓库。
- en: 'Open the terminal in the `pet-clinic-reviews` microservice root directory:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pet-clinic-reviews`微服务根目录中打开终端：
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 运行`jib`命令构建Docker镜像`mvn compile jib:dockerBuild`。
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 等待`jib`构建并将Docker镜像上传到本地Docker镜像仓库。
- en: 'Open the terminal in the `pet-clinic-concierge` microservice root directory:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pet-clinic-concierge`微服务根目录中打开终端：
- en: a. Run the `jib` command to build Docker `mvn compile jib:dockerBuild`.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 运行`jib`命令构建Docker镜像`mvn compile jib:dockerBuild`。
- en: b. Wait for `jib` to build and upload the Docker image to the local Docker images
    repository.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 等待`jib`构建并将Docker镜像上传到本地Docker镜像仓库。
- en: 'Open any Bash terminal and change the directory to where you have checked out
    the `pet-clinic` `docker-compose.yml` file:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何Bash终端并将目录更改为您已检出`pet-clinic` `docker-compose.yml`文件的位置：
- en: a. Run `docker compose up –d`.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 运行`docker compose up –d`。
- en: b. Wait for Docker to finish booting up the `pet-clinic` stack.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 等待Docker完成启动`pet-clinic`堆栈。
- en: 'Once the `pet-clinic` application is instantiated and running in Docker, we
    need to configure Kibana to index and show the logs. To index logs in Kibana,
    perform the following steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦`pet-clinic`应用程序在Docker中实例化并运行，我们需要配置Kibana以索引和显示日志。要在Kibana中索引日志，执行以下步骤：
- en: Navigate to Kibana at `http://localhost:5601` and log in using Elasticsearch
    credentials as mentioned in the `.env` file in the `docker-elk` directory.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`http://localhost:5601`上的Kibana并使用`docker-elk`目录中`.env`文件中提到的Elasticsearch凭据登录。
- en: Open the home page and click on the **Connect to your Elasticsearch** **index**
    hyperlink. After clicking on **Connect to your Elasticsearch** **index**, Kibana
    will provide a setup page to connect your index (see the following screenshot):![Figure
    9.3 – Connecting the Elasticsearch index in Kibana
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开主页并点击**连接到您的Elasticsearch索引**超链接。点击**连接到您的Elasticsearch索引**后，Kibana将提供一个设置页面来连接您的索引（见以下截图）：![图9.3
    – 在Kibana中连接Elasticsearch索引
- en: '](img/Figure_9.3_B16585_Fixed.jpg)'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_9.3_B16585_Fixed.jpg)'
- en: Figure 9.3 – Connecting the Elasticsearch index in Kibana
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.3 – 在Kibana中连接Elasticsearch索引
- en: Kibana provides an intuitive user interface to connect with your Elasticsearch
    index. Click on the highlighted portion in the screenshot and follow the steps
    as presented by Kibana.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kibana提供了一个直观的用户界面来连接到你的Elasticsearch索引。点击截图中的高亮部分，并按照Kibana展示的步骤进行操作。
- en: When the setup page loads, enter `logstash` in the **Index Patterns** textbox.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当设置页面加载时，在**索引模式**文本框中输入`logstash`。
- en: Click on the **Next step** button and select **@timestamp** in the configure
    settings.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮，并在配置设置中选择**@timestamp**。
- en: Then, click on **Create index pattern**.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，点击**创建索引模式**。
- en: 'After a successful index connection, you can go to the **Discover** page and
    view the application logs as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功建立索引连接后，你可以转到**Discover**页面，并按以下方式查看应用程序日志：
- en: '![Figure 9.4 – Viewing the application logs in Discover'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – 在Discover中查看应用程序日志'
- en: '](img/Figure_9.4_B16585_Fixed.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.4_B16585_Fixed.jpg)'
- en: Figure 9.4 – Viewing the application logs in Discover
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 在Discover中查看应用程序日志
- en: On the `app_name` and `app_port`, we can drill down on both the parameters to
    see the logs to a specific microservice.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在`app_name`和`app_port`上，我们可以深入查看这两个参数，以查看特定微服务的日志。
- en: So now, we have implemented an ELK Stack distributed logging that intuitively
    provides a common place to live access the microservices' logs. If there's any
    fault in any microservice, you can directly access Kibana and view/search the
    logs. As you add more microservice instances and components to your runtime topology,
    ELK will simplify the log management.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经实现了提供直观访问微服务日志的ELK Stack分布式日志记录。如果任何微服务出现故障，你可以直接访问Kibana并查看/搜索日志。随着你向运行时拓扑中添加更多的微服务实例和组件，ELK将简化日志管理。
- en: In the next section, we will dive into distributed tracing and how we can implement
    distributed tracing in the `pet-clinic` application.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入了解分布式追踪以及如何在`pet-clinic`应用程序中实现分布式追踪。
- en: Distributed tracing in Micronaut microservices
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Micronaut微服务中的分布式追踪
- en: Distributed tracing is the capability of the system to track and observe the
    execution flow of a request in distributed systems by collecting data as the request
    furthers from one service component to another. This trace data compiles metrics
    such as the time taken at each service along with end-to-end execution flow. Time
    metrics can help to zero down performance issues such as which service component
    is a bottleneck to the execution flow and why.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式追踪是系统追踪和观察分布式系统中请求执行流程的能力，通过收集数据来追踪请求从一个服务组件到另一个服务组件的过程。这些追踪数据会编译出每个服务所花费的时间以及端到端的执行流程。时间指标可以帮助定位性能问题，例如哪个服务组件是执行流程的瓶颈以及原因。
- en: 'A trace is a Gantt chart-like data structure that stores the trace information
    in spans. Each span will keep a trace for the execution flow in a particular service
    component. Furthermore, a span can have a reference to parent span and child spans.
    Refer to the following figure:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 追踪是一种类似于甘特图的数据结构，它将追踪信息存储在跨度中。每个跨度将保持特定服务组件中的执行流程的追踪。此外，跨度可以引用父跨度以及子跨度。参考以下图示：
- en: '![Figure 9.5 – Distributed tracing'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5 – 分布式追踪'
- en: '](img/Figure_9.5_B16585_Fixed.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.5_B16585_Fixed.jpg)'
- en: Figure 9.5 – Distributed tracing
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 分布式追踪
- en: In the preceding diagram, we can see the traces/spans for loading the `foo`
    page on the user interface app. It first calls `foo` object, which in turn calls
    `Bars` and `Bazs` for `foo`, respectively. The time taken for the whole execution
    will be the cumulative total of execution times at various service components.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到用户界面应用程序加载`foo`页面时的追踪/跨度。它首先调用`foo`对象，然后分别调用`Bars`和`Bazs`来处理`foo`。整个执行所花费的时间将是各个服务组件执行时间的累积总和。
- en: In the next section, we will implement a distributed tracing solution in the
    `pet-clinic` application.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现`pet-clinic`应用程序中的分布式追踪解决方案。
- en: Implementing distributed tracing in Micronaut
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Micronaut中实现分布式追踪
- en: In order to get hands-on with distributed tracing in Micronaut, we will implement
    Zipkin-based tracing in the `pet-clinic` application.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在Micronaut中实际操作分布式追踪，我们将在`pet-clinic`应用程序中实现基于Zipkin的追踪。
- en: 'We will run a Zipkin instance in Docker. To run Zipkin in Docker, perform the
    following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Docker中运行Zipkin实例。要在Docker中运行Zipkin，请执行以下步骤：
- en: Open any Bash terminal.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何Bash终端。
- en: Run the `docker run -d -p 9411:9411 openzipkin/zipkin` command.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`docker run -d -p 9411:9411 openzipkin/zipkin`命令。
- en: Wait for Docker to download and instantiate Zipkin on port `9411`. After successful
    instantiation, you can verify Zipkin by accessing `http://localhost:9411/zipkin`.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 等待Docker在端口`9411`下载并实例化Zipkin。实例化成功后，您可以通过访问`http://localhost:9411/zipkin`来验证Zipkin。
- en: 'Next, we will begin with the `pet-clinic-concierge` service, which is the API
    gateway. Add the following dependencies to the `pet-clinic-concierge` POM:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将从`pet-clinic-concierge`服务开始，这是API网关。向`pet-clinic-concierge` POM添加以下依赖项：
- en: '[PRE4]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: By importing the preceding dependencies, we can leverage Micronaut as well as
    third-party code artifacts for distributed tracing.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过导入前面的依赖项，我们可以利用Micronaut以及第三方代码工件进行分布式追踪。
- en: 'To enable the distributed tracing, we also need to amend `application.properties`.
    Add the following properties related to Zipkin:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启用分布式追踪，我们还需要修改`application.properties`文件。添加以下与Zipkin相关的属性：
- en: '[PRE5]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding application properties for Zipkin are added at the root level.
    In `url`, we specified a Docker instance of Zipkin running on localhost. Furthermore,
    in `sampler.probability`, we specify the value as `1` that will enable the tracing
    for all the user requests. This probability can be reduced to any value between
    0 and 1 wherein 0 means never sample and 1 means sample every request.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述Zipkin应用程序属性添加在根级别。在`url`中，我们指定了一个运行在本地的Docker实例的Zipkin。此外，在`sampler.probability`中，我们指定值为`1`，这将启用对所有用户请求的追踪。此概率可以降低到0到1之间的任何值，其中0表示从不采样，1表示对每个请求进行采样。
- en: 'Next, we need to tag the controller methods for spans. For managing the spans,
    we have the following two tags in Micronaut:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为控制器方法标记标签。对于管理跨度，Micronaut中有以下两个标签：
- en: 'a. `@NewSpan`: This will create a new span beginning from the method it''s
    tagged on.'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. `@NewSpan`：这将从它标记的方法开始创建一个新的跨度。
- en: 'b. `@ContinueSpan`: This will continue the previous span.'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. `@ContinueSpan`：这将继续前一个跨度。
- en: 'Since all the client controllers in `pet-clinic-concierge` are the interfacing
    points to any upstream consumers, we will use `@NewSpan` on these methods so that
    a new trace can begin. The following are the span-related changes in `OwnerResourceClientController`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`pet-clinic-concierge`中的所有客户端控制器都是任何上游消费者的接口点，因此我们将在这类方法上使用`@NewSpan`，以便开始新的追踪。以下是在`OwnerResourceClientController`中的跨度相关更改：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Similar changes to annotate client controller methods should be made in all
    the other clients for the `pet-owner`, `pet-clinic`, and `pet-clinic-reviews`
    microservices.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 应该在所有其他客户端的`pet-owner`、`pet-clinic`和`pet-clinic-reviews`微服务中对注释客户端控制器方法进行类似更改。
- en: Next, we need to modify the `pet-clinic` microservice for distributed tracing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要修改`pet-clinic`微服务以支持分布式追踪。
- en: Modifying the pet-clinic microservice for distributed tracing
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改`pet-clinic`微服务以支持分布式追踪
- en: Continuing with distributed tracing changes, we need to make the required amends
    in the `pet-owner`, `pet-clinic`, and `pet-clinic-reviews` microservices project
    POM and application properties as explained in the previous section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 继续分布式追踪的更改，我们需要在`pet-owner`、`pet-clinic`和`pet-clinic-reviews`微服务项目POM和应用程序属性中进行必要的修改，如前所述。
- en: 'Furthermore, to continue the tracing, we need to annotate controller methods
    with `@ContinueSpan` tags. Refer to the following code block:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了继续追踪，我们需要使用`@ContinueSpan`标签注释控制器方法。参考以下代码块：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`@ContinueSpan` must be annotated on all the controller methods in all the
    microservices (excluding `pet-clinic-concierge`, which is an API gateway). `@ContinueSpan`
    will continue the span/trace from the previous span/trace. In `pet-clinic-concierge`,
    we annotate the `createOwner()` method with `@NewSpan`, and in the `pet-owner`
    microservice, we use `@ContinueSpan`. Using these tags in tandem will trace the
    end-to-end execution flow.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`@ContinueSpan`必须注释在所有微服务中的所有控制器方法上（不包括作为API网关的`pet-clinic-concierge`）。`@ContinueSpan`将从前一个跨度/追踪中继续跨度/追踪。在`pet-clinic-concierge`中，我们使用`@NewSpan`注释`createOwner()`方法，在`pet-owner`微服务中，我们使用`@ContinueSpan`。使用这些标签可以一起追踪端到端的执行流程。'
- en: In the next section, we will verify the end-to-end trace for an HTTP request
    in the `pet-clinic` application.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将验证`pet-clinic`应用程序中HTTP请求的端到端追踪。
- en: Verifying the distributed tracing in the pet-clinic application
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证`pet-clinic`应用程序中的分布式追踪
- en: 'To verify the distributed tracing in the `pet-clinic` application, you must
    have the `pet-clinic` microservice running. We will fetch a list of owners via
    the API gateway. For this, perform the following steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证 `pet-clinic` 应用程序中的分布式跟踪，您必须确保 `pet-clinic` 微服务正在运行。我们将通过 API 网关获取所有者列表。为此，请执行以下步骤：
- en: Go to `http://localhost:32584/api/owners` in any browser tab or REST client.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在任何浏览器标签或 REST 客户端中转到 `http://localhost:32584/api/owners`。
- en: Navigate to Zipkin to verify the trace for the preceding HTTP `GET` call at
    `http://localhost:9411/zipkin`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 Zipkin，以验证在 `http://localhost:9411/zipkin` 的先前的 HTTP `GET` 调用的跟踪。
- en: Click on the **Run Query** button.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **运行查询** 按钮。
- en: Go to the `get /api/owners` request in the returned results and click **Show**.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在返回的结果中转到 `get /api/owners` 请求并点击 **显示**。
- en: 'After successfully performing these steps, you will see the following screen:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功执行这些步骤后，您将看到以下屏幕：
- en: '![Figure 9.6 – GET owners distributed tracing in Zipkin'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.6 – GET owners distributed tracing in Zipkin'
- en: '](img/Figure_9.6_B16585_Fixed.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.6_B16585_Fixed.jpg)'
- en: Figure 9.6 – GET owners distributed tracing in Zipkin
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – Zipkin 中的 GET owners 分布式跟踪
- en: Zipkin provides an intuitive user interface for accessing the request execution
    traces. You can see at first the request reaches `pet-clinic-concierge`, which
    is further passed on to the `pet-owner` microservice. In total it took approximately
    948 ms to complete the request with the majority of the time spent on the `pet-owner`
    microservice.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin 提供了一个直观的用户界面来访问请求执行跟踪。您可以看到请求首先到达 `pet-clinic-concierge`，然后进一步传递给 `pet-owner`
    微服务。总共花费了大约 948 毫秒来完成请求，其中大部分时间花在 `pet-owner` 微服务上。
- en: In the next section, we will focus on distributed monitoring and how to implement
    distributed monitoring in the Micronaut framework.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将重点关注分布式监控以及如何在 Micronaut 框架中实现分布式监控。
- en: Distributed monitoring in Micronaut microservices
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Micronaut 微服务的分布式监控
- en: Monitoring is simply recording the key performance metrics to enhance visibility
    into the application state. By recording and surfacing the system performance
    metrics such as CPU usage, thread pools, memory usage, and database connections
    for all the distributed components, it can provide a holistic picture of how a
    microservice system is performing at a given point in time. The distributed nature
    of microservices requires a shift in how the system is monitored. Instead of relying
    on the host environment monitoring tools, we need a unified monitoring solution
    that can combine performance metrics from various services and present a one-stop
    interface. In this section, we will explore how to implement such a distributed
    monitoring solution for the `pet-clinic` application.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 监控简单来说就是记录关键性能指标，以增强对应用程序状态的可见性。通过记录和展示所有分布式组件的系统性能指标，如 CPU 使用率、线程池、内存使用率和数据库连接，它可以提供一个全面的画面，说明在特定时间点微服务系统是如何运行的。微服务的分布式特性要求在监控系统方面进行转变。我们不再依赖于主机环境监控工具，我们需要一个统一的监控解决方案，能够结合来自各种服务的性能指标，并呈现一个一站式界面。在本节中，我们将探讨如何为
    `pet-clinic` 应用程序实现这样的分布式监控解决方案。
- en: 'To implement distributed monitoring, we will use a very popular stack of Prometheus
    and Grafana. Let''s look at our system components for distributed monitoring:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现分布式监控，我们将使用非常流行的 Prometheus 和 Grafana 堆栈。让我们看看我们用于分布式监控的系统组件：
- en: '![](img/Figure_9.7_B16585_Fixed.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Figure_9.7_B16585_Fixed.jpg)'
- en: Figure 9.7 – Distributed monitoring using Prometheus and Grafana
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 使用 Prometheus 和 Grafana 进行分布式监控
- en: As shown in the preceding diagram, the `pet-clinic` microservice will be communicating
    the metrics to the **Prometheus** server and **Grafana** will get the metrics
    to present the user interface. Prometheus configurations will be stored in a YAML
    file.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，`pet-clinic` 微服务将向 **Prometheus** 服务器发送指标，而 **Grafana** 将获取指标以展示用户界面。Prometheus
    配置将存储在 YAML 文件中。
- en: In the next section, we will begin with setting up Prometheus and Grafana in
    Docker.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从在 Docker 中设置 Prometheus 和 Grafana 开始。
- en: Setting up Prometheus and Grafana in Docker
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Docker 中设置 Prometheus 和 Grafana
- en: Before we instantiate Prometheus and Grafana in Docker, we need to define configurations
    for Prometheus so that it can pull required metrics from the `pet-clinic` microservice.
    You can check out `docker-prometheus docker-compose` and `prometheus.yml` from
    [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实例化Docker中的Prometheus和Grafana之前，我们需要为Prometheus定义配置，以便它可以从`pet-clinic`微服务中拉取所需的指标。您可以查看`docker-prometheus
    docker-compose`和`prometheus.yml`，链接为[https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro)。
- en: 'Once checked out locally, you can review the `prometheus.yml` file to be as
    follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在本地检出，您可以查看`prometheus.yml`文件，如下所示：
- en: '[PRE8]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In `prometheus.yml`, we mainly need to configure `scrape_configs`. This will
    be responsible for invoking the microservice endpoints to get the metrics. We
    can specify the `pet-clinic` microservice in the targets. In addition, you can
    note that the scrape interval is `10` seconds. This will configure Prometheus
    to fetch metrics every 10 seconds.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在`prometheus.yml`中，我们主要需要配置`scrape_configs`。这将负责调用微服务端点以获取指标。我们可以在目标中指定`pet-clinic`微服务。此外，请注意抓取间隔为`10`秒。这将配置Prometheus每10秒获取一次指标。
- en: Next, let's set up our distributed monitoring stack in Docker.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在Docker中设置我们的分布式监控堆栈。
- en: 'To set up Prometheus and Grafana in Docker, follow these instructions:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Docker中设置Prometheus和Grafana，请按照以下说明操作：
- en: Check out `docker-prometheus` from [https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro).
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micronaut-petclinic/docker-prometheus](https://github.com/PacktPublishing/Building-Microservices-with-Micronaut/tree/master/Chapter09/micro)检出`docker-prometheus`。
- en: Open any Bash terminal (we used Git Bash).
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开任何Bash终端（我们使用了Git Bash）。
- en: Change directory to where you have checked out `docker-prometheus`.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将目录切换到您检出`docker-prometheus`的位置。
- en: Run `docker compose up –d`.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`docker compose up –d`。
- en: Wait for Docker to download the images and instantiate the Prometheus app container.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待Docker下载镜像并实例化Prometheus应用程序容器。
- en: These instructions will boot up the monitoring app in Docker. You can verify
    the installation by going to the Docker Dashboard and **Containers / Apps**.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些说明将在Docker中启动监控应用程序。您可以通过访问Docker仪表板并进入**容器/应用程序**来验证安装。
- en: In the next section, we will explore how we can integrate the `pet-clinic` microservice
    into Prometheus.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何将`pet-clinic`微服务集成到Prometheus中。
- en: Configuring microservices for distributed monitoring
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置微服务进行分布式监控
- en: To configure the `pet-clinic` microservice for distributed monitoring, we need
    to update the `project` POM with `Micrometer` dependencies.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置`pet-clinic`微服务进行分布式监控，我们需要使用`Micrometer`依赖项更新`project` POM。
- en: 'Add the following dependencies to the `pet-owner` project POM:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下依赖项添加到`pet-owner`项目的POM中：
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By importing the `micronaut-micrometer` dependencies, we can leverage a distributed
    monitoring toolkit in the `pet-owner` microservice.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过导入`micronaut-micrometer`依赖项，我们可以在`pet-owner`微服务中利用分布式监控工具包。
- en: 'To expose service metrics for Prometheus, we need to expose the `metrics` endpoint
    in all the `pet-clinic` microservices. We will add a new controller called `PrometheusController`
    to the `com.packtpub.micronaut.web.rest.commons` package as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向Prometheus公开服务指标，我们需要在所有`pet-clinic`微服务中公开`metrics`端点。我们将在`com.packtpub.micronaut.web.rest.commons`包中添加一个新的控制器，称为`PrometheusController`，如下所示：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`PrometheusController` will expose `prometheusMeterRegistry.scrape()` on the
    `/metrics` endpoint.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`PrometheusController`将在`/metrics`端点上公开`prometheusMeterRegistry.scrape()`。'
- en: '`prometheusMeterRegistry.scrape()` will provide service performance metrics
    as configured in the `application.properties` file.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`prometheusMeterRegistry.scrape()`将提供服务性能指标，这些指标已在`application.properties`文件中配置。'
- en: 'We need to configure the `application.properties` file as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要按照以下方式配置`application.properties`文件：
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In `application.properties`, we are enabling the metrics and exporting the metrics
    in Prometheus format. Furthermore, since we are providing our custom `/metrics`
    endpoint, we are disabling the `metrics` and `prometheus` endpoints in the application
    properties.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在`application.properties`中，我们正在启用指标并将指标以Prometheus格式导出。此外，由于我们提供了自定义的`/metrics`端点，我们在应用程序属性中禁用了`metrics`和`prometheus`端点。
- en: Similarly, we need to modify the project POM, add `PrometheusController`, and
    update the application properties for the `pet-clinic`, `pet-clinic-reviews`,
    and `pet-clinic-concierge` microservices. Afterward, we need to rebuild the Docker
    images for all service projects running the `mvn compile jib:dockerBuild` command
    in the terminal. Once the Docker images are built and uploaded to the local Docker
    repository, we need to decommission the old `pet-clinic` application in Docker
    and rerun `docker compose up –d` to re-instantiate the modified `pet-clinic` application.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要修改项目POM，添加`PrometheusController`，并更新`pet-clinic`、`pet-clinic-reviews`和`pet-clinic-concierge`微服务的应用程序属性。之后，我们需要在终端中运行`mvn
    compile jib:dockerBuild`命令来重新构建所有运行的服务项目的Docker镜像。一旦Docker镜像构建并上传到本地Docker仓库，我们需要退役旧的`pet-clinic`应用程序在Docker中，并重新运行`docker
    compose up –d`来重新实例化修改后的`pet-clinic`应用程序。
- en: In the next section, we will verify the distributed monitoring implementation
    in the `pet-clinic` application.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将验证`pet-clinic`应用程序中的分布式监控实现。
- en: Verifying the distributed monitoring in the pet-clinic application
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证pet-clinic应用程序的分布式监控
- en: 'To verify the distributed monitoring in the `pet-clinic` application, you must
    have the `pet-clinic` and Prometheus applications running in Docker. You need
    to follow these instructions to verify integration between Prometheus and the
    `pet-clinic` application:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证`pet-clinic`应用程序的分布式监控，你必须让`pet-clinic`和Prometheus应用程序在Docker中运行。你需要遵循以下说明来验证Prometheus和`pet-clinic`应用程序之间的集成：
- en: Access the `/metrics` endpoints for all the microservices to verify that services
    are exposing metrics to Prometheus.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问所有微服务的`/metrics`端点以验证服务是否向Prometheus暴露指标。
- en: Verify the `pet-owner` metrics by accessing `http://localhost:32581/metrics`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问`http://localhost:32581/metrics`来验证`pet-owner`指标。
- en: Verify the `pet-clinic` metrics by accessing `http://localhost:32582/metrics`.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问`http://localhost:32582/metrics`来验证`pet-clinic`指标。
- en: Verify the `pet-clinic-reviews` metrics by accessing `http://localhost:32583/metrics`.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问`http://localhost:32583/metrics`来验证`pet-clinic-reviews`指标。
- en: Verify the `pet-clinic-concierge` metrics by accessing `http://localhost:32584/metrics`.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问`http://localhost:32584/metrics`来验证`pet-clinic-concierge`指标。
- en: Navigate to `http://localhost:9090/graph` and check whether you can see the
    `system_cpu_usage` metric.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`http://localhost:9090/graph`并检查是否可以看到`system_cpu_usage`指标。
- en: 'After successful completion of the preceding steps, you will see the following
    screen:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在完成前面的步骤后，你将看到以下屏幕：
- en: '![Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application
    in Prometheus'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.8 – 在Prometheus中访问pet-clinic应用程序的系统CPU使用率图表]'
- en: '](img/Figure_9.8_B16585_Fixed.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_9.8_B16585_Fixed.jpg]'
- en: Figure 9.8 – Accessing the system CPU usage graph for the pet-clinic application
    in Prometheus
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 在Prometheus中访问pet-clinic应用程序的系统CPU使用率图表
- en: In the preceding screenshot, we can verify that the `pet-clinic` microservice
    is able to expose the performance metrics on the endpoint and Prometheus can invoke
    the `/metrics` endpoints. We can see the system CPU usage graph in Prometheus
    graphs but as a system admin or developer, you probably need a system dashboard
    with all the metric graphs in one place.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以验证`pet-clinic`微服务能够在其端点上暴露性能指标，Prometheus可以调用`/metrics`端点。我们可以在Prometheus图表中看到系统CPU使用率图表，但作为系统管理员或开发者，你可能需要一个包含所有指标图表的系统仪表板。
- en: 'In the following instructions, we will integrate Grafana with the Prometheus
    server:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下说明中，我们将集成Grafana与Prometheus服务器：
- en: Navigate to `http://localhost:3000/` and log in with the username as `admin`
    and password as `pass`.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`http://localhost:3000/`并使用用户名`admin`和密码`pass`登录。
- en: After logging in, navigate to `http://localhost:3000/datasources`.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，导航到`http://localhost:3000/datasources`。
- en: Click on the **Add data source** button.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加数据源**按钮。
- en: Under the **Time series databases** list, select **Prometheus**.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**时间序列数据库**列表下，选择**Prometheus**。
- en: In `http://prometheus:9090`.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`http://prometheus:9090`。
- en: Keep the rest of the values as the defaults.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保持其余值保持默认设置。
- en: Click on the **Save and test** button. You should get a successful message.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **保存并测试** 按钮。你应该会收到一条成功消息。
- en: Go to the adjacent **Dashboards** tab and click on **Prometheus 2.0 stats**.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到相邻的 **仪表板** 选项卡，并点击 **Prometheus 2.0 统计**。
- en: 'After successful completion of these steps, you should see the following dashboard:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功完成这些步骤后，你应该会看到以下仪表板：
- en: '![Figure 9.9 – Prometheus dashboard in Grafana'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9 – Grafana 中的 Prometheus 仪表板'
- en: '](img/Figure_9.9_B16585_Fixed.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_9.9_B16585_Fixed.jpg)'
- en: Figure 9.9 – Prometheus dashboard in Grafana
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – Grafana 中的 Prometheus 仪表板
- en: As shown in the preceding screenshot, Grafana provides a very intuitive, unified
    dashboard for accessing the vital system metrics for all the service components
    in the `pet-clinic` application. One-stop access to this telemetry data is very
    handy in addressing any performance issues and system failures in production environments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个屏幕截图所示，Grafana 为访问 `pet-clinic` 应用程序中所有服务组件的关键系统指标提供了一个非常直观、统一的仪表板。在解决生产环境中任何性能问题和系统故障时，一次性访问这些遥测数据非常方便。
- en: In this section, we explored what distributed monitoring is and how we can implement
    distributed monitoring using Prometheus and Grafana in the Micronaut framework.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了分布式监控是什么以及我们如何在 Micronaut 框架中使用 Prometheus 和 Grafana 实现分布式监控。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we began with distributed logging and why it is important in
    any microservice implementation. We implemented an ELK Stack for distributed logging
    in the `pet-clinic` application. Furthermore, we dived into using the Kibana user
    interface for connecting to the Elasticsearch application logs index.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了分布式日志以及为什么它在任何微服务实现中都很重要。我们在 `pet-clinic` 应用程序中实现了 ELK Stack 用于分布式日志记录。此外，我们还深入探讨了如何使用
    Kibana 用户界面连接到 Elasticsearch 应用程序日志索引。
- en: Later, we explored what distributed tracing is and how to implement distributed
    tracing using Zipkin in the Micronaut framework. We also verified the trace of
    an HTTP call in the Zipkin user interface.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们探讨了分布式追踪是什么以及如何在 Micronaut 框架中使用 Zipkin 实现分布式追踪。我们还验证了在 Zipkin 用户界面中的 HTTP
    调用追踪。
- en: In the end, we dived into the world of distributed monitoring and implemented
    a distributed monitoring solution for the `pet-clinic` application using a Prometheus
    and Grafana stack.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入分布式监控的世界，并使用 Prometheus 和 Grafana 栈为 `pet-clinic` 应用程序实现了一个分布式监控解决方案。
- en: This chapter enhanced your Micronaut microservices journey with the observability
    patterns that are distributed logging, distributed tracing, and distributed monitoring
    by enabling you with hands-on knowledge on how to implement these patterns in
    the Micronaut framework.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过使你具备如何在 Micronaut 框架中实现这些模式的手动知识，增强了你的 Micronaut 微服务之旅，这些模式包括分布式日志记录、分布式追踪和分布式监控。
- en: In the next chapter, we will explore how to implement an IoT solution in the
    Micronaut framework.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何在 Micronaut 框架中实现 IoT 解决方案。
- en: Questions
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is distributed logging in microservices?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微服务中的分布式日志是什么？
- en: How do you run an ELK Stack in Docker?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在 Docker 中运行 ELK 栈？
- en: How do you implement distributed logging in the Micronaut framework?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在 Micronaut 框架中实现分布式日志记录？
- en: How do you connect to a Docker Logstash from the Micronaut microservice?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何从 Micronaut 微服务连接到 Docker Logstash？
- en: What is distributed tracing in microservices?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微服务中的分布式追踪是什么？
- en: How do you implement distributed tracing in the Micronaut framework?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在 Micronaut 框架中实现分布式追踪？
- en: What is distributed monitoring in microservices?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微服务中的分布式监控是什么？
- en: How do you run a Prometheus and Grafana stack in Docker?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在 Docker 中运行 Prometheus 和 Grafana 栈？
- en: How do you implement distributed monitoring in the Micronaut framework?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在 Micronaut 框架中实现分布式监控？
