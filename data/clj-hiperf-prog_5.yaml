- en: Chapter 5. Concurrency
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：并发
- en: 'Concurrency was one of the chief design goals of Clojure. Considering the concurrent
    programming model in Java (the comparison with Java is due to it being the predominant
    language on the JVM), it is not only too low level, but rather tricky to get right
    that without strictly following the patterns, one is more likely to shoot oneself
    in the foot. Locks, synchronization, and unguarded mutation are recipes for the
    concurrency pitfalls, unless exercised with extreme caution. Clojure''s design
    choices deeply influence the way in which the concurrency patterns can be achieved
    in a safe and functional manner. In this chapter, we will discuss:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 并发是Clojure的主要设计目标之一。考虑到Java中的并发编程模型（与Java的比较是因为它是JVM上的主要语言），它不仅太底层，而且如果不严格遵循模式，很容易出错，从而自食其果。锁、同步和无保护变异是并发陷阱的配方，除非极端谨慎地使用。Clojure的设计选择深刻影响了并发模式以安全且功能的方式实现的方式。在本章中，我们将讨论：
- en: The low level concurrency support at the hardware and JVM level
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件和JVM级别的底层并发支持
- en: The concurrency primitives of Clojure—atoms, agents, refs and vars
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clojure的并发原语——原子、代理、引用和变量
- en: The built-in concurrency that features in Java safe, and its usefulness with
    Clojure
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java内置的并发特性既安全又实用，与Clojure的结合
- en: Parallelization with the Clojure features and reducers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Clojure特性和reducers进行并行化
- en: Low-level concurrency
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 底层并发
- en: Concurrency cannot be achieved without explicit hardware support. We discussed
    about SMT and the multi-core processors in the previous chapters. Recall that
    every processor core has its own L1 cache, and several cores share the L2 cache.
    The shared L2 cache provides a fast mechanism to the processor cores to coordinate
    their cache access, eliminating the comparatively expensive memory access. Additionally,
    a processor buffers the writes to memory into something known as a **dirty write-buffer**.
    This helps the processor to issue a batch of memory update requests, reorder the
    instructions, and determine the final value to write to memory, known as **write
    absorption**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 没有显式的硬件支持，无法实现并发。我们在前几章讨论了SMT和多核处理器。回想一下，每个处理器核心都有自己的L1缓存，而多个核心共享L2缓存。共享的L2缓存为处理器核心提供了一个快速机制来协调它们的缓存访问，消除了相对昂贵的内存访问。此外，处理器将写入内存的操作缓冲到一个称为**脏写缓冲区**的地方。这有助于处理器发布一系列内存更新请求，重新排序指令，并确定写入内存的最终值，这被称为**写吸收**。
- en: Hardware memory barrier (fence) instructions
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件内存屏障（栅栏）指令
- en: Memory access reordering is great for a sequential (single-threaded) program
    performance, but it is hazardous for the concurrent programs where the order of
    memory access in one thread may disrupt the expectations in another thread. The
    processor needs the means of synchronizing the access in such a way that memory
    reordering is either compartmentalized in code segments that do not care, or is
    prevented where it might have undesirable consequences. The hardware supports
    such a safety measure in terms of a "memory barrier" (also known as "fence").
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 内存访问重排序对于顺序（单线程）程序的性能来说非常好，但对于并发程序来说则是有害的，因为在某个线程中内存访问的顺序可能会破坏另一个线程的预期。处理器需要一种同步访问的手段，以便内存重排序要么被限制在不在乎的代码段中，要么在可能产生不良后果的地方被阻止。硬件通过“内存屏障”（也称为“栅栏”）这一安全措施来支持这种功能。
- en: There are several kinds of memory barrier instructions found in different architectures,
    with potentially different performance characteristics. The compiler (or the JIT
    compiler in the case of the JVM) usually knows about the fence instructions on
    the architectures that it runs on. The common fence instructions are read, write,
    acquire, and release barrier, and more. The barriers do not guarantee the latest
    data, rather they only control the relative ordering of memory access. Barriers
    cause the write-buffer to be flushed after all the writes are issued, before the
    barrier is visible to the processor that issued it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的架构中可以发现多种内存屏障指令，它们可能具有不同的性能特征。编译器（或者在JVM的情况下是JIT编译器）通常了解它在上面运行的架构上的fence指令。常见的fence指令有读、写、获取和释放屏障等。屏障并不保证最新的数据，而是只控制内存访问的相对顺序。屏障会在所有写入发布后、屏障对发出它的处理器可见之前，将写缓冲区刷新。
- en: Read and write barriers control the order of reads and writes respectively.
    Writes happen via a write-buffer; but reads may happen out of order, or from the
    write-buffer. To guarantee the correct ordering, acquire, and release, blocks/barriers
    are used. Acquire and release are considered "half barriers"; both of them together
    (acquire and release) form a "full barrier". A full barrier is more expensive
    than a half barrier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 读写屏障分别控制读和写的顺序。写操作通过写缓冲区进行；但读操作可能发生顺序混乱，或者来自写缓冲区。为了保证正确的顺序，使用获取和释放，块/屏障被使用。获取和释放被认为是
    "半屏障"；两者一起（获取和释放）形成一个 "全屏障"。全屏障比半屏障更昂贵。
- en: Java support and the Clojure equivalent
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java 支持和 Clojure 等价物
- en: 'In Java, the memory barrier instructions are inserted by the higher level coordination
    primitives. Even though fence instructions are expensive (hundreds of cycles)
    to run, they provide a safety net that makes accessing shared variables safe within
    the critical sections. In Java, the `synchronized` keyword marks a "critical section",
    which can be executed by only one thread at a time, thus making is a tool for
    "mutual exclusion". In Clojure, the equivalent of Java''s `synchronized` is the
    `locking` macro:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Java 中，内存屏障指令是由高级协调原语插入的。尽管栅栏指令的运行成本很高（数百个周期），但它们提供了一个安全网，使得在关键部分内访问共享变量是安全的。在
    Java 中，`synchronized` 关键字标记一个 "关键部分"，一次只能由一个线程执行，因此它是一个 "互斥" 工具。在 Clojure 中，Java
    的 `synchronized` 的等价物是 `locking` 宏：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `locking` macro builds upon two special forms, `monitor-enter` and `monitor-exit`.
    Note that the `locking` macro is a low-level and imperative solution just like
    Java's `synchronized` – their use is not considered idiomatic Clojure. The special
    forms `monitor-enter` and `monitor-exit` respectively enter and exit the lock
    object's "monitor" – they are even lower level and not recommended for direct
    use.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`locking` 宏建立在两个特殊形式 `monitor-enter` 和 `monitor-exit` 之上。请注意，`locking` 宏是一个低级和命令式的解决方案，就像
    Java 的 `synchronized` 一样——它们的使用并不被认为是 Clojure 的惯用用法。特殊形式 `monitor-enter` 和 `monitor-exit`
    分别进入和退出锁对象的 "monitor"——它们甚至更低级，不建议直接使用。'
- en: Someone measuring the performance of the code that uses such locking should
    be aware of its single-threaded versus the multi-threaded latencies. Locking in
    a single thread is cheap. However, the performance penalty starts kicking in when
    there are two or more threads contending for a lock on the same object monitor.
    A lock is acquired on the monitor of an object called the "intrinsic" or "monitor"
    lock. Object equivalence (that is, when the `=` function returns as true) is never
    used for the purpose of locking. Make sure that the object references are the
    same (that is, when `identical?` returns as true) when locking from different
    threads.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 测量使用此类锁定代码的性能的人应该意识到其单线程与多线程延迟之间的差异。在单线程中锁定是廉价的。然而，当有两个或更多线程争夺同一对象监视器的锁时，性能惩罚就开始显现。锁是在称为
    "内在" 或 "monitor" 锁的对象监视器上获取的。对象等价性（即，当 `=` 函数返回 true）永远不会用于锁定的目的。确保从不同的线程锁定时对象引用是相同的（即，当
    `identical?` 返回 true）。
- en: Acquiring a monitor lock by a thread entails a read barrier, which invalidates
    the thread-local cached data, the corresponding processor registers, and the cache
    lines. This forces a reread from the memory. On the other hand, releasing the
    monitor lock results in a write barrier, which flushes all the changes to memory.
    These are expensive operations that impact parallelism, but they ensure consistency
    of data for all threads.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 线程通过获取监视器锁涉及一个读屏障，该屏障使线程本地的缓存数据、相应的处理器寄存器和缓存行无效。这迫使从内存中重新读取。另一方面，释放监视器锁会导致写屏障，将所有更改刷新到内存中。这些是昂贵的操作，会影响并行性，但它们确保了所有线程的数据一致性。
- en: 'Java supports a `volatile` keyword for the data members in a class that guarantees
    read and write to an attribute outside of a synchronized block that would not
    be reordered. It is interesting to note that unless an attribute is declared `volatile`,
    it is not guaranteed to be visible in all the threads that are accessing it. The
    Clojure equivalent of Java''s `volatile` is the metadata called `^:volatile-mutable`
    that we discussed in [Chapter 3](ch03.html "Chapter 3. Leaning on Java"), *Leaning
    on Java*. An example of `volatile` in Java and Clojure is as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Java支持一个`volatile`关键字，用于类中的数据成员，它保证在同步块之外对属性的读取和写入不会发生重排序。值得注意的是，除非属性被声明为`volatile`，否则它不能保证在所有访问它的线程中都是可见的。Clojure中Java的`volatile`等价于我们在[第3章](ch03.html
    "第3章。依赖Java"), *依赖Java*中讨论的元数据`^:volatile-mutable`。以下是在Java和Clojure中使用`volatile`的示例：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Reading and writing a `volatile` data requires read-acquire or write-release
    respectively, which means we need only a half-barrier to individually read or
    write the value. Note that due to a half-barrier, the read-followed-by-write operations
    are not guaranteed to be atomic. For example, the `age++` expression first reads
    the value, then increments and sets it. This makes two memory operations, which
    is no more a half-barrier.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和写入`volatile`数据需要分别使用读获取或写释放，这意味着我们只需要一个半屏障就可以单独读取或写入值。请注意，由于半屏障，读取后跟写入的操作不保证是原子的。例如，`age++`表达式首先读取值，然后增加并设置它。这使得有两个内存操作，这不再是半屏障。
- en: 'Clojure 1.7 introduced a first class support for the volatile data using a
    new set of functions: `volatile!`, `vswap!`, `vreset!,` and `volatile?` These
    functions define volatile (mutable) data and work with that. However, make a note
    that these functions do not work with the volatile fields in `deftype`. You can
    see how to use them as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 1.7引入了对`volatile`数据的一级支持，使用一组新的函数：`volatile!`、`vswap!`、`vreset!`和`volatile?`。这些函数定义了可变的（可变的）数据，并与之一起工作。然而，请注意，这些函数不与`deftype`中的可变字段一起工作。以下是如何使用它们的示例：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Operations on volatile data are not atomic, which is why even creating a volatile
    (using `volatile!`) is considered potentially unsafe. In general, volatiles may
    be useful where read consistency is not a high priority but writes must be fast,
    such as real-time trend analysis, or other such analytics reporting. Volatiles
    may also be very useful when writing stateful transducers (refer to [Chapter 2](ch02.html
    "Chapter 2. Clojure Abstractions"), *Clojure Abstractions*), serving as very fast
    state containers. In the next sub-section, we will see the other state abstractions
    that are safer (and mostly slower) than volatiles.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对`volatile`数据的操作不是原子的，这就是为什么即使创建`volatile`（使用`volatile!`）也被认为是潜在不安全的。一般来说，`volatile`可能在读取一致性不是高优先级但写入必须快速的情况下很有用，例如实时趋势分析或其他此类分析报告。`volatile`在编写有状态的转换器（参考[第2章](ch02.html
    "第2章。Clojure 抽象"), *Clojure 抽象*)时也非常有用，作为非常快速的状态容器。在下一小节中，我们将看到其他比`volatile`更安全（但大多数情况下更慢）的状态抽象。
- en: Atomic updates and state
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原子更新和状态
- en: It is a common use case to read a data element, execute some logic, and update
    with a new value. For single-threaded programs, it bears no consequences; but
    for concurrent scenarios, the entire operation must be carried out in a lockstep,
    as an atomic operation. This case is so common that many processors support this
    at the hardware level using a special Compare-and-swap (CAS) instruction, which
    is much cheaper than locking. On x86/x64 architectures, the instruction is called
    CompareExchange (CMPXCHG).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 读取数据元素、执行一些逻辑，并更新为新值的操作是一个常见的用例。对于单线程程序，这不会产生任何后果；但对于并发场景，整个操作必须作为一个原子操作同步执行。这种情况如此普遍，以至于许多处理器在硬件级别使用特殊的比较并交换（CAS）指令来支持这一功能，这比锁定要便宜得多。在x86/x64架构上，这个指令被称为CompareExchange（CMPXCHG）。
- en: Unfortunately, it is possible that another thread updates the variable with
    the same value that the thread, which is working on the atomic update, is going
    to compare the old value against. This is known as the "ABA" problem. The set
    of instructions such as "Load-linked" (LL) and "Store-conditional" (SC), which
    are found in some other architectures, provide an alternative to CAS without the
    ABA problem. After the LL instruction reads the value from an address, the SC
    instruction to update the address with a new value will only go through if the
    address has not been updated since the LL instruction was successful.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，可能存在另一种线程更新了变量，其值与正在执行原子更新的线程将要比较的旧值相同。这被称为“ABA”问题。在某些其他架构中发现的“Load-linked”（LL）和“Store-conditional”（SC）等指令集提供了一种没有ABA问题的CAS替代方案。在LL指令从地址读取值之后，用于更新地址的新值的SC指令只有在LL指令成功后地址未被更新的情况下才会执行。
- en: Atomic updates in Java
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java中的原子更新
- en: 'Java has a bunch of built-in lock free, atomic, thread safe compare-and-swap
    abstractions for the state management. They live in the `java.util.concurrent.atomic`
    package. For primitive types, such as boolean, integer, and long, there are the
    `AtomicBoolean`, `AtomicInteger`, and `AtomicLong` classes respectively. The latter
    two classes support additional atomic add/subtract operations. For atomic reference
    updates, there are the `AtomicReference`, `AtomicMarkableReference`, and `AtomicStampedReference`
    classes for the arbitrary objects. There is also a support available for arrays
    where the array elements can be updated atomically—`AtomicIntegerArray`, `AtomicLongArray`,
    and `AtomicReferenceArray`. They are easy to use; here is the example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Java提供了一组内置的无锁、原子、线程安全的比较和交换抽象，用于状态管理。它们位于`java.util.concurrent.atomic`包中。对于布尔型、整型和长型等原始类型，分别有`AtomicBoolean`、`AtomicInteger`和`AtomicLong`类。后两个类支持额外的原子加减操作。对于原子引用更新，有`AtomicReference`、`AtomicMarkableReference`和`AtomicStampedReference`类用于任意对象。还有对数组的支持，其中数组元素可以原子性地更新——`AtomicIntegerArray`、`AtomicLongArray`和`AtomicReferenceArray`。它们易于使用；以下是一个示例：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: However, where and how to use it is subjected to the update points and the logic
    in the code. The atomic updates are not guaranteed to be non-blocking. Atomic
    updates are not a substitute to locking in Java, but rather a convenience, only
    when the scope is limited to a compare and swap operation for one mutable variable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在哪里以及如何使用它取决于更新点和代码中的逻辑。原子更新不保证是非阻塞的。在Java中，原子更新不是锁的替代品，而是一种便利，仅当范围限制为对一个可变变量的比较和交换操作时。
- en: Clojure's support for atomic updates
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clojure对原子更新的支持
- en: 'Clojure''s atomic update abstraction is called "atom". It uses `AtomicReference`
    under the hood. An operation on `AtomicInteger` or `AtomicLong` may be slightly
    faster than on the Clojure `atom`, because the former uses primitives. But neither
    of them are too cheap, due to the compare-and-swap instruction that they use in
    the CPU. The speed really depends on how frequently the mutation happens, and
    how the JIT compiler optimizes the code. The benefit of speed may not show up
    until the code is run several hundred thousand times, and having an atom mutated
    very frequently will increase the latency due to the retries. Measuring the latency
    under actual (or similar to actual) load can tell better. An example of using
    an atom is as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure的原子更新抽象称为“atom”。它底层使用`AtomicReference`。对`AtomicInteger`或`AtomicLong`的操作可能比Clojure的`atom`稍微快一些，因为前者使用原语。但由于它们在CPU中使用的比较和交换指令，它们并不便宜。速度实际上取决于突变发生的频率以及JIT编译器如何优化代码。速度的好处可能不会在代码运行了几十万次之后显现出来，并且原子频繁突变会增加重试的延迟。在实际（或类似实际）负载下测量延迟可以提供更好的信息。以下是一个使用原子的示例：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `swap!` function provides a notably different style of carrying out atomic
    updates than the `compareAndSwap(oldval, newval)` methods. While `compareAndSwap()`
    compares and sets the value, returning true if it's a success and false if it's
    a failure, `swap!` keeps on trying to update in an endless loop until it succeeds.
    This style is a popular pattern that is followed among Java developers. However,
    there is also a potential pitfall associated with the update-in-loop style. As
    the concurrency of the updaters gets higher, the performance of the update may
    gradually degrade. Then again, high concurrency on the atomic updates raises a
    question of whether or not uncoordinated updates was a good idea at all for the
    use-case. The `compare-and-set!` and `reset!` are pretty straightforward.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`swap!` 函数在执行原子更新时提供了一种与 `compareAndSwap(oldval, newval)` 方法明显不同的风格。当 `compareAndSwap()`
    比较并设置值时，如果成功则返回 true，如果失败则返回 false，而 `swap!` 则会持续在一个无限循环中尝试更新，直到成功。这种风格是Java开发者中流行的模式。然而，与更新循环风格相关的一个潜在陷阱也存在。随着更新者的并发性提高，更新的性能可能会逐渐下降。再次，原子更新的高并发性引发了一个问题：对于使用场景来说，是否真的有必要进行无协调的更新。《compare-and-set!`
    和 `reset!` 是相当直接的。'
- en: The function passed to `swap!` is required to be pure (as in side effect free),
    because it is retried several times in a loop during contention. If the function
    is not pure, the side effect may happen as many times as the retries.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `swap!` 的函数必须是无副作用的（即纯函数），因为在竞争时该函数会在循环中多次重试。如果函数不是纯函数，副作用可能会在重试的次数内发生。
- en: 'It is noteworthy that atoms are not "coordinated", which means that when an
    atom is used concurrently by different threads, we cannot predict the order in
    which the operations work on it, and we cannot guarantee the end result as a consequence.
    The code we write around atoms should be designed with this constraint in mind.
    In many scenarios, atoms may not be a good fit due to the lack of coordination—watch
    out for that in the program design. Atoms support meta data and basic validation
    mechanism via extra arguments. The following examples illustrate these features:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，原子不是“协调”的，这意味着当原子被不同的线程并发使用时，我们无法预测操作在其上工作的顺序，也无法保证最终结果。围绕原子的代码应该考虑到这种约束来设计。在许多场景中，由于缺乏协调，原子可能并不适合——在设计程序时要注意这一点。原子通过额外的参数支持元数据和基本验证机制。以下示例说明了这些功能：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The second important thing is that atoms support is adding and removing watches
    on them. We will discuss watches later in the chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要的事情是，原子支持在它们上添加和移除监视器。我们将在本章后面讨论监视器。
- en: Faster writes with atom striping
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用原子条带化实现更快的写入
- en: 'We know that atoms present contention when multiple threads try to update the
    state at the same time. This implies that atoms have great performance when the
    writes are infrequent. There are some use cases, for example metrics counters,
    where the writes need to be fast and frequent, but the reads are fewer and can
    tolerate some inconsistency. For such use cases, instead of directing all the
    updates to a single atom, we can maintain a bunch of atoms where each thread updates
    a different atom, thus reducing contention. Reads from these atoms cannot be guaranteed
    to be consistent. Let''s develop an example of such a counter:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道当多个线程尝试同时更新状态时，原子会表现出竞争。这意味着当写入不频繁时，原子具有很好的性能。有一些用例，例如度量计数器，需要快速且频繁的写入，但读取较少，可以容忍一些不一致性。对于这样的用例，我们不必将所有更新都指向单个原子，而可以维护一组原子，其中每个线程更新不同的原子，从而减少竞争。从这些原子中读取的值不能保证是一致的。让我们开发这样一个计数器的示例：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the previous example, we created a vector called `counters` of the same
    size as the number of CPU cores in the computer, and initialize each element with
    an atom of initial value 0\. The function called `inc!` updates the counter by
    picking up a random atom from `counters`, and incrementing the value by 1\. We
    also assumed that `rand-int` distributes the picking up of atom uniformly across
    all the processor cores, so that we have almost zero contention. The `value` function
    simply walks over all the atoms and adds up their `deref`''ed values to return
    the counter value. The example uses `clojure.core/rand-int`, which depends on
    `java.lang.Math/random` (due to Java 6 support) to randomly find out the next
    counter atom. Let''s see how we can optimize this when using Java 7 or above:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们创建了一个名为`counters`的向量，其大小与计算机中CPU核心的数量相同，并将每个元素初始化为初始值为0的原子。名为`inc!`的函数通过从`counters`中随机选择一个原子并增加1来更新计数器。我们还假设`rand-int`在所有处理器核心上均匀地分布了原子的选择，因此我们几乎没有任何竞争。`value`函数简单地遍历所有原子，并将它们的`deref`返回值相加以返回计数器的值。该示例使用`clojure.core/rand-int`，它依赖于`java.lang.Math/random`（由于Java
    6支持）来随机找到下一个计数器原子。让我们看看当使用Java 7或更高版本时，我们如何优化它：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we `import` the `java.util.concurrent.ThreadLocalRandom` class, and define
    the `inc!` function to pick up the next random atom using `ThreadLocalRandom`.
    Everything else remains the same.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们`导入`了`java.util.concurrent.ThreadLocalRandom`类，并定义了`inc!`函数，使用`ThreadLocalRandom`来选择下一个随机原子。其他一切保持不变。
- en: Asynchronous agents and state
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步代理和状态
- en: 'While atoms are synchronous, agents are the asynchronous mechanism in Clojure
    to effect any change in the state. Every agent is associated with a mutable state.
    We pass a function (known as "action") to an agent with the optional additional
    arguments. This function gets queued for processing in another thread by the agent.
    All the agents share two common thread pools—one for the low-latency (potentially
    CPU-bound, cache-bound, or memory-bound) jobs, and one for the blocking (potentially
    I/O related or lengthy processing) jobs. Clojure provides the `send` function
    for the low-latency actions, `send-off` for blocking actions, and `send-via` to
    have the action executed on the user-specified thread-pool, instead of either
    of the preconfigured thread pools. All of `send`, `send-off`, and `send-via` return
    immediately. Here is how we can use them:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当原子是同步的，代理（agents）是Clojure中实现状态变化的异步机制。每个代理都与一个可变状态相关联。我们向代理传递一个函数（称为“动作”），并附带可选的额外参数。这个函数由代理排队在另一个线程中处理。所有代理共享两个公共线程池——一个用于低延迟（可能为CPU密集型、缓存密集型或内存密集型）的工作，另一个用于阻塞（可能为I/O相关或长时间处理）的工作。Clojure提供了`send`函数用于低延迟动作，`send-off`用于阻塞动作，以及`send-via`来在用户指定的线程池上执行动作，而不是预配置的线程池之一。`send`、`send-off`和`send-via`都会立即返回。以下是我们可以如何使用它们的示例：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When we inspect the Clojure (as of version 1.7.0) source code, we can find
    that the thread-pool for the low-latency actions is named as `pooledExecutor`
    (a bounded thread-pool, initialized to max ''2 + number of hardware processors''
    threads), and the thread-pool for the high-latency actions is named as `soloExecutor`
    (an unbounded thread pool). The premise of this default configuration is that
    the CPU/cache/memory-bound actions run most optimally on a bounded thread-pool,
    with the default number of threads. The I/O bound tasks do not consume CPU resources.
    Hence, a relatively larger number of such tasks can execute at the same time,
    without significantly affecting the performance of the CPU/cache/memory-bound
    jobs. Here is how you can access and override the thread-pools:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检查Clojure（截至版本1.7.0）的源代码时，我们可以发现低延迟动作的线程池被命名为`pooledExecutor`（一个有界线程池，初始化为最大'2
    + 硬件处理器数量'个线程），而高延迟动作的线程池被命名为`soloExecutor`（一个无界线程池）。这种默认配置的前提是CPU/缓存/内存密集型动作在有限线程池上运行最优化，默认线程数。I/O密集型任务不消耗CPU资源。因此，可以同时执行相对较多的此类任务，而不会显著影响CPU/缓存/内存密集型工作的性能。以下是您可以如何访问和覆盖线程池的示例：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If a program carries out a large number of I/O or blocking operations through
    agents, it probably makes sense to limit the number of threads dedicated for such
    actions. Overriding the `send-off` thread-pool using `set-agent-send-off-executor!`
    is the easiest way to limit the thread-pool size. A more granular way to isolate
    and limit the I/O actions on the agents is to use `send-via` with the thread-pools
    of appropriate sizes for various kinds of I/O and blocking operations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个程序通过代理执行大量I/O或阻塞操作，限制用于此类操作的线程数量可能是有意义的。使用`set-agent-send-off-executor!`覆盖`send-off`线程池是限制线程池大小的最简单方法。通过使用`send-via`与适合各种I/O和阻塞操作的适当大小的线程池，可以更细致地隔离和限制代理上的I/O操作。
- en: Asynchrony, queueing, and error handling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步、队列和错误处理
- en: Sending an action to an agent returns immediately without blocking. If the agent
    is not already busy in executing any action, it "reacts" by enqueuing the action
    that triggers the execution of the action, in a thread, from the respective thread-pool.
    If the agent is busy in executing another action, the new action is simply enqueued.
    Once an action is executed from the action queue, the queue is checked for more
    entries and triggers the next action, if found. This whole "reactive" mechanism
    of triggering actions obviates the need of a message loop, polling the queue.
    This is only possible, because the entry points to an agent's queue are controlled.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 向代理发送操作会立即返回，而不会阻塞。如果代理尚未忙于执行任何操作，它将通过将触发操作执行的队列中的操作入队来“反应”，在相应的线程池中。如果代理正在忙于执行另一个操作，新操作将简单地入队。一旦从操作队列中执行了操作，就会检查队列是否有更多条目，如果找到，则触发下一个操作。这个触发操作的“反应”机制消除了需要消息循环或轮询队列的需要。这之所以可能，是因为控制了指向代理队列的入口点。
- en: 'Actions are executed asynchronously on agents, which raises the question of
    how the errors are handled. Error cases need to be handled with an explicit, predefined
    function. When using a default agent construction, such as `(agent :foo)`, the
    agent is created without any error handler, and gets suspended in the event of
    any exception. It caches the exception, and refuses to accept any more actions.
    It throws the cached exception upon sending any action until the agent is restarted.
    A suspended agent can be reset using the `restart-agent` function. The objective
    of such suspension is safety and supervision. When the asynchronous actions are
    executed on an agent and suddenly an error occurs, it will require attention.
    Check out the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 动作在代理上异步执行，这引发了如何处理错误的疑问。错误情况需要通过一个显式、预定义的函数来处理。当使用默认的代理构造，如`(agent :foo)`时，代理在没有错误处理程序的情况下创建，并在任何异常发生时挂起。它缓存异常，并拒绝接受更多操作。在代理重启之前，它会在发送任何操作时抛出缓存的异常。可以使用`restart-agent`函数重置挂起的代理。这种挂起的目的是安全和监督。当在代理上执行异步操作时，如果突然发生错误，则需要引起注意。查看以下代码：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are two optional parameters `:error-handler` and `:error-mode, which`
    we can configure on an agent to have finer control over the error handling and
    suspension as shown in the following code snippet:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个可选参数`:error-handler`和`:error-mode`，我们可以在代理上配置这些参数以对错误处理和挂起有更精细的控制，如下面的代码片段所示：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Why you should use agents
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么你应该使用代理
- en: Just as the "atom" implementation uses only compare-and-swap instead of locking,
    the underlying "agent" specific implementation uses mostly the compare-and-swap
    operations. The agent implementation uses locks only when dispatching action in
    a transaction (discussed in the next section), or when restarting an agent. All
    the actions are queued and dispatched serially in the agents, regardless of the
    concurrency level. The serial nature makes it possible to execute the actions
    in an independent and contention-free manner. For the same agent, there can never
    be more than one action being executed. Since there is no locking, reads (`deref`
    or `@`) on agents are never blocked due to writes. However, all the actions are
    independent of each other—there is no overlap in their execution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如“原子”实现只使用比较和交换而不是锁定一样，底层的“代理”特定实现主要使用比较和交换操作。代理实现仅在事务（将在下一节讨论）中调度操作或重启代理时使用锁定。所有操作都在代理中排队并串行分发，无论并发级别如何。这种串行性质使得可以独立且无争用地执行操作。对于同一个代理，永远不会同时执行多个操作。由于没有锁定，对代理的读取（`deref`或`@`）永远不会因为写入而被阻塞。然而，所有操作都是相互独立的——它们的执行没有重叠。
- en: The implementation goes so far as to ensure that the execution of an action
    blocks other actions, which follow in the queue. Even though the actions are executed
    in a thread-pool, actions for the same agent are never executed concurrently.
    This is an excellent ordering guarantee that also extends a natural coordination
    mechanism, due to its serial nature. However, note that this ordering coordination
    is limited to only a single agent. If an agent action sends actions to two other
    agents, they are not automatically coordinated. In this situation, you may want
    to use transactions (which will be covered in the next section).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实现甚至确保动作的执行会阻塞队列中后续的动作。尽管动作是在线程池中执行的，但同一代理的动作永远不会并发执行。这是一个出色的排序保证，也由于其串行性质而扩展了自然的协调机制。然而，请注意，这种排序协调仅限于单个代理。如果一个代理的动作发送给两个其他代理，它们不会自动协调。在这种情况下，您可能希望使用事务（将在下一节中介绍）。
- en: Since agents distinguish between the low-latency and blocking jobs, the jobs
    are executed in an appropriate kind of thread-pools. Actions on different agents
    may execute concurrently, thereby making optimum use of the threading resources.
    Unlike atoms, the performance of the agents is not impeded by high contention.
    In fact, for many cases, agents make a lot of sense due to the serial buffering
    of actions. In general, agents are great for high volume I/O tasks, or where the
    ordering of operations provides a win in the high contention scenarios.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代理区分低延迟和阻塞作业，作业将在适当的线程池中执行。不同代理上的动作可以并发执行，从而最大限度地利用线程资源。与原子不同，代理的性能不会因高竞争而受阻。事实上，对于许多情况，由于动作的串行缓冲，代理非常有意义。一般来说，代理非常适合高容量I/O任务，或者在操作顺序在高度竞争场景中提供优势的情况下。
- en: Nesting
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套
- en: When an agent action sends another action to the same agent, that is a case
    of nesting. This would have been nothing special if agents didn't participate
    in STM transactions (which will be covered in the next section). However, agents
    do participate in STM transactions and that places certain constraints on agent
    implementation that warrants a second-layer buffering of actions. For now, it
    should suffice to say that the nested sends are queued in a thread-local queue
    instead of the regular queue in the agent. The thread-local queue is visible only
    to the thread in which the action is executed. Upon executing an action, unless
    there was an error, the agent implicitly calls the equivalent of `release-pending-sends`
    function, which transfers the actions from second level thread-local queue to
    the normal action queue. Note that nesting is simply an implementation detail
    of agents and has no other impact.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个代理的动作发送给同一个代理的另一个动作时，这就是嵌套的情况。如果代理不参与STM事务（将在下一节中介绍），这本来可能没有什么特别之处。然而，代理确实参与了STM事务，这给代理的实现带来了一定的约束，需要对动作进行第二层缓冲。目前，可以说嵌套发送被排队在代理的线程局部队列中，而不是常规队列中。线程局部队列只对执行动作的线程可见。在执行动作时，除非出现错误，否则代理会隐式调用相当于`release-pending-sends`函数的功能，将动作从第二级线程局部队列转移到正常动作队列。请注意，嵌套只是代理的实现细节，没有其他影响。
- en: Coordinated transactional ref and state
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协调事务引用和状态
- en: We saw in an earlier section that an atom provides atomic read-and-update operation.
    What if we need to perform an atomic read-and-update operation across two or even
    more number of atoms? This clearly poses a coordination problem. Some entity has
    to watch over the process of reading and updating, so that the values are not
    corrupted. This is what a ref provides—a **Software Transactional Memory** (**STM**)
    based system that takes care of concurrent atomic read-and-update operations across
    multiple refs, such that either all the updates go through, or in the case of
    failure, none does. Like atoms, on failure, refs retry the whole operation from
    scratch with the new values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们了解到原子提供了原子读-更新操作。如果我们需要在两个或更多原子之间执行原子读-更新操作怎么办？这显然是一个协调问题。某个实体必须监控读取和更新的过程，以确保值不会被破坏。这就是引用的作用——提供一个基于**软件事务内存（STM**）的系统，该系统负责在多个引用之间执行并发原子读-更新操作，以确保所有更新都通过，或者在失败的情况下，没有任何更新。与原子一样，在失败的情况下，引用会从头开始重试整个操作，使用新的值。
- en: Clojure's STM implementation is coarse grained. It works at the application
    level objects and aggregates (that is, references to aggregates), scoped to only
    all the refs in a program, constituting the "Ref world". Any update to a ref can
    only happen synchronously, in a transaction, in a `dosync` block of code, within
    the same thread. It cannot span beyond the current thread. The implementation
    detail reveals that a thread-local transaction context is maintained during a
    lifetime of a transaction. The same context ceases to be available, the moment
    the control reaches another thread.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 的 STM 实现是粗粒度的。它工作在应用级别的对象和聚合（即聚合的引用），范围仅限于程序中的所有 refs，构成了“Ref 世界”。对
    ref 的任何更新都只能以同步方式发生，在事务中，在 `dosync` 代码块内，在同一线程中。它不能跨越当前线程。实现细节揭示，在事务的生命周期内维护了一个线程本地的交易上下文。一旦控制达到另一个线程，相同的上下文就不再可用。
- en: Like the other reference types in Clojure, reads on a ref are never blocked
    by the updates, and vice versa. However, unlike the other reference types, the
    implementation of ref does not depend on a lock-free spinning, but rather, it
    internally uses locks, a low-level wait/notify, a deadlock detection, and the
    age-based barging.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Clojure 中的其他引用类型一样，对 ref 的读取永远不会被更新阻塞，反之亦然。然而，与其他引用类型不同，ref 的实现并不依赖于无锁自旋，而是内部使用锁、低级等待/通知、死锁检测和基于年龄的抢占。
- en: 'The `alter` function is used to read-and-update the value of a ref, and `ref-set`
    is used to reset the value. Roughly, `alter` and `ref-set,` for the refs, are
    analogous to `swap!` and `reset!` for the atoms. Just like `swap!`, `alter` accepts
    a function (and arguments) with no side effects, and may be retried several times
    during the contention. However, unlike with the atoms, not only `alter` but also
    `ref-set` and simple `deref`, may cause a transaction to be retried during the
    contention. Here is a very simple example on how we may use a transaction:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`alter` 函数用于读取和更新一个 ref 的值，而 `ref-set` 用于重置值。大致来说，对于 refs 来说，`alter` 和 `ref-set`
    类似于原子操作中的 `swap!` 和 `reset!`。就像 `swap!` 一样，`alter` 接受一个无副作用的函数（和参数），并且可能在竞争时重试多次。然而，与原子不同，不仅
    `alter`，而且 `ref-set` 和简单的 `deref` 也可能在竞争时导致事务重试。以下是一个如何使用事务的非常简单的例子：'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Ref characteristics
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ref 特性
- en: Clojure maintains the **Atomicity**, **Consistency**, and **Isolation** (**ACI**)
    characteristics in a transaction. This overlaps with A, C, and I of the ACID guarantee
    that many databases provide. Atomicity implies that either all of the updates
    in a transaction will complete successfully or none of them do. Consistency means
    that the transaction must maintain general correctness, and should honor the constraints
    set by the validation—any exception or validation error should roll back the transaction.
    Unless a shared state is guarded, concurrent updates on it may lead a multi-step
    transaction into seeing different values at different steps. Isolation implies
    that all the steps in a transaction will see the same value, no matter how concurrent
    the updates are.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 在事务中维护了 **原子性**、**一致性**和**隔离性**（**ACI**）特性。这与许多数据库提供的 ACID 保证中的 A、C
    和 I 相重叠。原子性意味着事务中的所有更新要么全部成功完成，要么全部不完成。一致性意味着事务必须保持一般正确性，并应遵守验证设置的约束——任何异常或验证错误都应回滚事务。除非共享状态受到保护，否则对它的并发更新可能导致多步事务在不同步骤中看到不同的值。隔离性意味着事务中的所有步骤都将看到相同的值，无论更新多么并发。
- en: The Clojure refs use something known as **Multi Version Concurrency Control**
    (**MVCC**) to provide **Snapshot Isolation** to the transactions. In MVCC, instead
    of locking (which could block the transactions), the queues are maintained, so
    that each transaction can occur using its own snapshot copy, taken at its "read
    point", independent of other transactions. The main benefit of this approach is
    that the read-only out-of-transaction operations can go through without any contention.
    Transactions without the ref contention go through concurrently. In a rough comparison
    with the database systems, the Clojure ref isolation level is "Read Committed"
    for reading a Ref outside of a transaction, and "Repeatable Read" by default when
    inside the transaction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 引用使用一种称为**多版本并发控制（MVCC**）的技术来为事务提供**快照隔离**。在MVCC中，不是通过锁定（这可能会阻塞事务），而是维护队列，以便每个事务都可以使用自己的快照副本进行操作，该副本在其“读点”处获取，独立于其他事务。这种方法的主要好处是，只读事务外的操作可以无冲突地通过。没有引用冲突的事务可以并发进行。在与数据库系统的粗略比较中，Clojure
    引用隔离级别在读取事务外的引用时为“读取已提交”，而在事务内部默认为“可重复读”。
- en: Ref history and in-transaction deref operations
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用历史和在事务中解除引用操作
- en: We discussed earlier that both, read and update operations, on a ref, may cause
    a transaction to be retried. The reads in a transaction can be configured to use
    the ref history in such a manner that the snapshot isolation instances are stored
    in the history queues, and are used by the read operations in the transactions.
    The default, which is not supposed to use the history queues, conserves heap space,
    and provides strong consistency (avoids the staleness of data) in the transactions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过，对引用的读取和更新操作都可能导致事务重试。事务中的读取可以配置为使用引用历史，这样快照隔离实例就存储在历史队列中，并由事务中的读操作使用。默认情况下，不使用历史队列，这可以节省堆空间，并在事务中提供强一致性（避免数据陈旧）。
- en: 'Using the ref history reduces the likelihood of the transaction retries caused
    by read contention, thereby providing a weak consistency. Therefore, it is a tool
    for performance optimization, which comes at the cost of consistency. In many
    scenarios, programs do not need strong consistency—we can choose appropriately
    if we know the trade-off, and what we need. The snapshot isolation mechanism in
    the Clojure ref implementation is backed by the adaptive history queues. The history
    queues grow dynamically to meet the read requests, and do not overshoot the maximum
    limit that is set for the ref. By default, the history is not enabled, so we need
    to specify it during the initialization or set it later. Here is an example of
    how to use the history:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用引用历史可以降低因读冲突导致的事务重试的可能性，从而提供弱一致性。因此，它是一种性能优化的工具，但会牺牲一致性。在许多场景中，程序不需要强一致性——如果我们知道权衡利弊，我们可以适当选择。Clojure
    引用实现中的快照隔离机制由自适应历史队列支持。历史队列会动态增长以满足读请求，并且不会超过为引用设置的极限。默认情况下，历史记录是禁用的，因此我们需要在初始化时指定它，或者稍后设置。以下是如何使用历史的示例：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Minimum/maximum history limits are proportional to the length of the staleness
    window of the data. It also depends on the relative latency difference of the
    update and read operations to see what the range of the min-history and the max-history
    works well on a given host system. It may take some amount of trial and error
    to get the range right. As a ballpark figure, read operations only need as many
    min-history elements to avoid the transaction retries, as many updates can go
    through during one read operation. The max-history elements can be a multiple
    of min-history to cover for any history overrun or underrun. If the relative latency
    difference is unpredictable, then we have to either plan a min-history for the
    worst case scenario, or consider other approaches.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最小/最大历史限制与数据陈旧窗口的长度成比例。它还取决于更新和读操作之间的相对延迟差异，以确定在给定主机系统上最小历史和最大历史的工作范围。可能需要一些尝试和错误才能得到正确的范围。作为一个粗略的估计，读操作只需要足够的min-history元素来避免事务重试，因为在一次读操作期间可以有那么多更新。max-history元素可以是min-history的倍数，以覆盖任何历史超限或欠限。如果相对延迟差异不可预测，那么我们必须为最坏情况规划一个min-history，或者考虑其他方法。
- en: Transaction retries and barging
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 事务重试和抢占
- en: A transaction can internally be in one of the five distinct states—Running,
    Committing, Retry, Killed, and Committed. A transaction can be killed for various
    reasons. Exceptions are the common reasons for killing a transaction. But let's
    consider the corner case where a transaction is retried many times, but it does
    not appear to commit successfully—what is the resolution? Clojure supports age-based
    barging, wherein an older transaction automatically tries to abort a younger transaction,
    so that the younger transaction is retried later. If the barging still doesn't
    work, as a last resort, the transaction is killed after a hard limit of 10,000
    retry attempts, and then the exception is thrown.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个事务可以在五个不同的状态之一内部运行——运行中、提交中、重试、已杀死和已提交。一个事务可能因为各种原因而被杀死。异常是杀死事务的常见原因。但让我们考虑一个特殊情况，即一个事务被多次重试，但似乎没有成功提交——解决方案是什么？Clojure支持基于年龄的抢占，其中较老的事务会自动尝试中止较新的事务，以便较新的事务稍后重试。如果抢占仍然不起作用，作为最后的手段，在10,000次重试尝试的硬性限制之后，事务将被杀死，然后抛出异常。
- en: Upping transaction consistency with ensure
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用ensure提高事务一致性
- en: Clojure's transactional consistency is a good balance between performance and
    safety. However, at times, we may need the **Serializable** consistency in order
    to preserve the correctness of the transaction. Concretely, in the face of the
    transaction retries, when a transaction's correctness depends on the state of
    a ref, in the transaction, wherein the ref is updated simultaneously in another
    transaction, we have a condition called "write skew". The Wikipedia entry on the
    write skew, [https://en.wikipedia.org/wiki/Snapshot_isolation](https://en.wikipedia.org/wiki/Snapshot_isolation),
    describes it well, but let's see a more concrete example. Let's say we want to
    design a flight simulation system with two engines, and one of the system level
    constraints is not to switch off both engines at the same time. If we model each
    engine as a ref, and certain maneuvers do require us to switch off an engine,
    we must ensure that the other engine is on. We can do it with `ensure`. Usually,
    `ensure` is required when maintaining a consistent relationship (invariants) across
    the refs is necessary. This cannot be ensured by the validator functions, because
    they do not come into play until the transaction commits. The validator functions
    will see the same value hence cannot help.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure的事务一致性在性能和安全之间取得了良好的平衡。然而，有时我们可能需要**可序列化**的一致性来保持事务的正确性。具体来说，在面临事务重试的情况下，当一个事务的正确性依赖于一个ref的状态，在该事务中，ref被另一个事务同时更新时，我们有一个称为“写偏斜”的条件。关于写偏斜的维基百科条目[https://en.wikipedia.org/wiki/Snapshot_isolation](https://en.wikipedia.org/wiki/Snapshot_isolation)，描述得很好，但让我们看看一个更具体的例子。假设我们想要设计一个具有两个引擎的飞行模拟系统，并且系统级的一个约束是不允许同时关闭两个引擎。如果我们把每个引擎建模为一个ref，并且某些机动确实需要我们关闭一个引擎，我们必须确保另一个引擎是开启的。我们可以使用`ensure`来实现。通常，当需要在refs之间维护一致的关系（不变性）时，就需要`ensure`。这不能通过验证函数来保证，因为它们只有在事务提交时才会起作用。验证函数将看到相同的值，因此无法提供帮助。
- en: The write-skew can be solved using the namesake `ensure` function that essentially
    prevents a ref from modification by other transactions. It is similar to a locking
    operation, but in practice, it provides better concurrency than the explicit read-and-update
    operations, when the retries are expensive. Using `ensure` is quite simple—`(ensure
    ref-object).` However, it may be performance-wise expensive, due to the locks
    it holds during the transaction. Managing performance with `ensure` involves a
    trade-off between the retry latency, and the lost throughput due to the ensured
    state.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 写偏斜可以通过同名的`ensure`函数来解决，该函数本质上阻止其他事务修改ref。它类似于锁定操作，但在实践中，当重试代价高昂时，它提供了比显式的读取和更新操作更好的并发性。使用`ensure`非常简单——`(ensure
    ref-object)`。然而，由于它在事务期间持有的锁，它可能在性能上代价高昂。使用`ensure`来管理性能涉及到在重试延迟和由于确保状态而丢失的吞吐量之间进行权衡。
- en: Lesser transaction retries with commutative operations
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用交换操作减少事务重试
- en: Commutative operations are independent of the order in which they are applied.
    For example, incrementing a counter ref c1 from transactions t1 and t2 would have
    the same effect irrespective of the order in which t1 and t2 commit their changes.
    Refs have a special optimization for changing functions that are commutative for
    transactions—the `commute` function, which is similar to `alter` (same syntax),
    but with different semantics. Like `alter`, the `commute` functions are applied
    atomically during the transaction commit. However, unlike `alter`, `commute` does
    not cause the transaction retry on contention, and there is no guarantee about
    the order in which the `commute` functions are applied. This effectively makes
    `commute` nearly useless for returning a meaningful value as a result of the operation.
    All the commute functions in a transaction are reapplied with the final in transaction
    ref values during the transaction commit.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 交换操作与它们应用的顺序无关。例如，从事务t1和t2中递增计数器引用c1将产生相同的效果，无论t1和t2提交更改的顺序如何。引用为事务中可交换的更改函数提供了特殊的优化——`commute`函数，它与`alter`（相同的语法）类似，但具有不同的语义。像`alter`一样，`commute`函数在事务提交期间原子性地应用。然而，与`alter`不同，`commute`不会在竞争时导致事务重试，并且没有关于`commute`函数应用顺序的保证。这实际上使得`commute`在作为操作结果返回有意义值时几乎无用。事务中的所有`commute`函数都在事务提交期间使用事务中的最终引用值重新应用。
- en: As we can see, commute reduces the contention, thereby optimizing the performance
    of the overall transaction throughput. Once we know that an operation is commutative
    and we are not going to use its return value in a meaningful way, there is hardly
    any trade-off deciding on whether to use it—we should just go ahead and use it.
    In fact, a program design, with respect to the ref transactions, with commute
    in mind, is not a bad idea.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，交换操作减少了竞争，从而优化了整体事务吞吐量的性能。一旦我们知道一个操作是可交换的，并且我们不会以有意义的方式使用它的返回值，那么在决定是否使用它时几乎没有权衡——我们只需继续使用它。实际上，考虑到引用事务，考虑到交换，程序设计不是一个坏主意。
- en: Agents can participate in transactions
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理可以参与事务
- en: In the previous section on agents, we discussed how agents work with the queued
    change functions. Agents can also participate in the ref transactions, thereby
    making it possible to combine the use of refs and agents in the transactions.
    However, agents are not included in the "Ref world", hence a transaction scope
    is not extended till the execution of the change function in an agent. Rather,
    the transactions only make sure that the changes sent to the agents are queued
    until the transaction commit happens.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于代理的上一节中，我们讨论了代理如何与排队更改函数协同工作。代理也可以参与引用事务，从而使得在事务中结合使用引用和代理成为可能。然而，代理不包括在“引用世界”中，因此事务作用域不会扩展到代理中更改函数的执行。相反，事务仅确保发送给代理的更改在事务提交发生前被排队。
- en: The *Nesting* sub-section, in the earlier section on agents, discusses about
    a second-layer thread-local queue. This thread-local queue is used during a transaction
    to hold the sent changes to an agent until the commit. The thread-local queue
    does not block the other changes that are being sent to an agent. The out-of-transaction
    changes are never buffered in the thread-local queue; rather, they are added to
    the regular queue in the agent.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*嵌套*子节，在关于代理的早期章节中，讨论了一个第二层线程局部队列。这个线程局部队列在事务期间用于持有发送给代理的更改，直到提交。线程局部队列不会阻塞发送给代理的其他更改。事务外部的更改永远不会在线程局部队列中缓冲；相反，它们被添加到代理中的常规队列。'
- en: The participation of agents in the transactions provides an interesting angle
    of design, where the coordinated and independent/sequential operations can be
    pipelined as a workflow for better throughput and performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代理参与事务提供了有趣的设计角度，其中协调的以及独立/顺序操作可以作为工作流程进行流水线处理，以获得更好的吞吐量和性能。
- en: Nested transactions
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套事务
- en: Clojure transactions are nesting aware and they compose well. But, why would
    one need a nested transaction? Often, independent units of code may have their
    own low-granularity transactions that a higher level code can make use of. When
    the higher level caller itself needs to wrap actions in a transaction, nested
    transactions occur. Nested transactions have their own lifecycle and run-state.
    However, an outer transaction can abort an inner transaction on the detection
    of failure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure事务具有嵌套感知性并且可以很好地组合。但是，为什么需要嵌套事务呢？通常，独立的代码单元可能有自己的低粒度事务，高级代码可以利用这些事务。当高级调用者本身需要将操作包装在事务中时，就会发生嵌套事务。嵌套事务有自己的生命周期和运行状态。然而，外部事务可以在检测到失败时取消内部事务。
- en: The "ref world" snapshot `ensure`s and `commute`s are shared among all (that
    is, outer and inner) levels of a nested transaction. Due to this, the inner transaction
    is treated as any other ref change operation (similar to `alter`, `ref-set` and
    so on) within an outer transaction. The watches and internal lock implementation
    are handled at the respective nesting level. The detection of contention in the
    inner transactions causes a restart of not only the inner but also the outer transaction.
    Commits at all the levels are effected as a global state finally when the outermost
    transaction commits. The watches, even though tracked at each individual transaction
    level, are finally effected during the commit. A closer look at the nested transaction
    implementation shows that nesting has little or no impact on the performance of
    transactions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: “ref世界”快照`ensure`和`commute`在嵌套事务的所有（即外部和内部）级别之间共享。因此，内部事务被视为在外部事务内部的其他引用更改操作（类似于`alter`、`ref-set`等）。监视和内部锁的实现由各自的嵌套级别处理。内部事务中的竞争检测会导致内部和外部事务的重新启动。所有级别的提交最终在最外层事务提交时作为全局状态生效。监视器，尽管在每个单独的事务级别上跟踪，但在提交时最终生效。仔细观察嵌套事务实现可以看出，嵌套对事务性能的影响很小或没有。
- en: Performance considerations
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能考虑
- en: Clojure Ref is likely to be the most complex reference type implemented yet.
    Due to its characteristics, especially its transaction retry mechanism, it may
    not be immediately apparent that such a system would have good performance during
    the high-contention scenarios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure Ref可能是迄今为止实现的最复杂的引用类型。由于其特性，特别是其事务重试机制，在高度竞争场景下，这样的系统可能会有良好的性能可能并不立即明显。
- en: 'Understanding its nuances and best ways of use should help:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 理解其细微差别和最佳使用方式将有所帮助：
- en: We do not use changes with the side effects in a transaction, except for possibly
    sending the I/O changes to agents, where the changes are buffered until the commit.
    So by definition, we do not carry out any expensive I/O work in a transaction.
    Hence, a retry of this work would be cheap as well.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在事务中不使用具有副作用的变化，除非可能将I/O变化发送到代理，其中变化被缓冲直到提交。因此，根据定义，我们在事务中不执行任何昂贵的I/O工作。因此，这项工作的重试成本也会很低。
- en: A change function for a transaction should be as small as possible. This lowers
    the latency and hence, the retries will also be cheaper.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事务的更改函数应该尽可能小。这降低了延迟，因此重试的成本也会更低。
- en: Any ref that is not updated along with at least one more ref simultaneously
    needs not be a ref—atoms would do just fine in this case. Now that the refs make
    sense only in a group, their contention is directly proportional to the group
    size. Small groups of refs used in the transactions lead to a low contention,
    lower latency, and a higher throughput.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何没有与至少另一个引用同时更新的引用不需要是引用——在这种情况下原子就足够好了。现在，由于引用只在组中才有意义，它们的竞争与组大小成正比。在事务中使用的小组引用导致低竞争、低延迟和高吞吐量。
- en: Commutative functions provide a good opportunity to enhance the transaction
    throughput without any penalty. Identifying such cases and designing with commute
    in mind can help performance significantly.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交换函数提供了在不产生任何惩罚的情况下提高事务吞吐量的好机会。识别这些情况并考虑交换进行设计可以帮助显著提高性能。
- en: Refs are very coarse grained—they work at the application aggregate level. Often
    a program may need to have more fine-grained control over the transaction resources.
    This can be enabled by Ref striping, such as Megaref ([https://github.com/cgrand/megaref](https://github.com/cgrand/megaref)),
    by providing a scoped view on the associative refs, thereby allowing higher concurrency.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引用非常粗粒度——它们在应用聚合级别工作。通常，程序可能需要更细粒度地控制事务资源。这可以通过引用条带化实现，例如Megaref ([https://github.com/cgrand/megaref](https://github.com/cgrand/megaref))，通过提供关联引用的受限视图，从而允许更高的并发性。
- en: In the high contention scenarios in which the ref group size in a transaction
    cannot be small, consider using agents, as they have no contention due to the
    serial nature. Agents may not be a replacement for the transactions, but rather
    we can employ a pipeline consisting of atoms, refs, and agents to ease out the
    contention versus latency concerns.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高竞争场景中，事务中引用组的大小不能小，考虑使用代理，因为它们由于序列性质而没有竞争。代理可能不是事务的替代品，但我们可以使用由原子、引用和代理组成的管道，以减轻竞争与延迟的担忧。
- en: Refs and transactions have an intricate implementation. Fortunately, we can
    inspect the source code, and browse through available online and offline resources.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 引用和事务的实现相当复杂。幸运的是，我们可以检查源代码，并浏览可用的在线和离线资源。
- en: Dynamic var binding and state
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态变量绑定和状态
- en: 'The fourth kind among the Clojure''s reference types is the dynamic var. Since
    Clojure 1.3, all the vars are static by default. A var must be explicitly declared
    so in order to be dynamic. Once declared, a dynamic var can be bound to new values
    on per-thread basis. Binding on different threads do not block each other. An
    example is shown here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在Clojure的引用类型中，第四种是动态变量。从Clojure 1.3开始，所有变量默认都是静态的。必须显式声明变量以使其成为动态的。一旦声明，动态变量就可以在每线程的基础上绑定到新的值。不同线程上的绑定不会相互阻塞。这里有一个示例：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As the dynamic binding is thread-local, it may be tricky to use in multi-threaded
    scenarios. Dynamic vars have been long abused by libraries and applications as
    a means to pass in a common argument to be used by several functions. However,
    this style is acknowledged to be an anti-pattern, and is discouraged. Typically,
    in the anti-pattern dynamic, vars are wrapped by a macro to contain the dynamic
    thread-local binding in the lexical scope. This causes problems with the multi-threading
    and lazy sequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于动态绑定是线程局部的，所以在多线程场景中使用可能会很棘手。长期以来，动态变量被库和应用滥用，作为传递给多个函数使用的公共参数的手段。然而，这种风格被认为是一种反模式，并受到谴责。通常，在反模式动态变量中，变量会被宏包装，以在词法作用域中包含动态线程局部绑定。这会导致多线程和懒序列出现问题。
- en: So, how can the dynamic vars be used effectively? A dynamic var lookup is more
    expensive than looking up a static var. Even passing a function argument is performance-wise
    much cheaper than looking up a dynamic var. Binding a dynamic var incurs additional
    cost. Clearly, in performance sensitive code, dynamic vars are best not used at
    all. However, dynamic vars may prove to be useful to hold a temporary thread-local
    state in a complex, or recursive call-graph scenario, where the performance does
    not matter significantly, without being advertised or leaked into the public API.
    The dynamic var bindings can nest and unwind like a stack, which makes them both
    attractive and suitable for such tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何有效地使用动态变量呢？动态变量的查找成本比静态变量查找更高。即使是传递一个函数参数，在性能上也要比查找动态变量便宜得多。绑定动态变量会带来额外的开销。显然，在性能敏感的代码中，最好根本不使用动态变量。然而，在复杂或递归调用图场景中，动态变量可能非常有用，用于持有临时的线程局部状态，在这些场景中，性能并不重要，而且不会被宣传或泄露到公共API中。动态变量绑定可以像栈一样嵌套和展开，这使得它们既吸引人又适合这类任务。
- en: Validating and watching the reference types
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证和监视引用类型
- en: 'Vars (both static and dynamic), atoms, refs, and agents provide a way to validate
    the value being set as state—a `validator` function that accepts new value as
    argument, and returns the logical as true if it succeeds, or throws exception/returns
    logical as false (the false and nil values) if there''s an error. They all honor
    what the validator function returns. If it is a success, the update goes through,
    and if an error, an exception is thrown instead. Here is the syntax on how the
    validators can be declared and associated with the reference types:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 变量（静态和动态）、原子、引用和代理提供了一种验证作为状态设置的价值的方法——一个接受新值作为参数的 `validator` 函数，如果成功则返回逻辑
    true，如果出错则抛出异常/返回逻辑 false（错误和 nil 值）。它们都尊重验证函数返回的结果。如果成功，更新将通过，如果出错，则抛出异常。以下是声明验证器并将其与引用类型关联的语法：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Validators cause actual failure within a reference type while updating them.
    For vars and atoms, they simply prevent the update by throwing an exception. In
    an agent, a validation failure causes agent failure, and needs the agent to restart.
    Inside a ref, the validation failure causes the transaction to rollback and rethrow
    the exception.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 验证器在更新引用类型时会导致实际失败。对于变量和原子，它们通过抛出异常简单地阻止更新。在代理中，验证失败会导致代理失败，需要代理重新启动。在引用内部，验证失败会导致事务回滚并重新抛出异常。
- en: 'Another mechanism to observe the changes to the reference types is a "watcher".
    Unlike validators, a watcher is passive—it is notified of the update after the
    fact. Hence, a watcher cannot prevent updates from going through, because it is
    only a notification mechanism. For transactions, a watcher is invoked only after
    the transaction commit. While only one validator can be set on a reference type,
    it is possible to associate multiple watchers to a reference type on the other
    hand. Secondly, when adding a watch, we can specify a key, so that the notifications
    can be identified by the key, and be dealt accordingly by the watcher. Here is
    the syntax on how to use watchers:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 观察引用类型变化的另一种机制是“观察者”。与验证者不同，观察者是被动的——它在更新发生后才会被通知。因此，观察者无法阻止更新通过，因为它只是一个通知机制。对于事务，观察者仅在事务提交后才会被调用。虽然一个引用类型上只能设置一个验证者，但另一方面，可以将多个观察者与一个引用类型关联。其次，在添加观察者时，我们可以指定一个键，这样通知就可以通过键来识别，并由观察者相应地处理。以下是使用观察者的语法：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Like validators, the watchers are executed synchronously in the thread of the
    reference type. For atoms and refs, this may be fine, since the notification to
    the watchers goes on, the other threads may proceed with their updates. However
    in agents, the notification happens in the same thread where the update happens—this
    makes the update latency higher, and the throughput potentially lower.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与验证者一样，观察者是在引用类型的线程中同步执行的。对于原子和引用，这可能没问题，因为通知观察者的同时，其他线程可以继续进行它们的更新。然而在代理中，通知发生在更新发生的同一线程中——这使得更新延迟更高，吞吐量可能更低。
- en: Java concurrent data structures
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java 并发数据结构
- en: Java has a number of mutable data structures that are meant for concurrency
    and thread-safety, which implies that multiple callers can safely access these
    data structures at the same time, without blocking each other. When we need only
    the highly concurrent access without the state management, these data structures
    may be a very good fit. Several of these employ lock free algorithms. We discussed
    about the Java atomic state classes in the *Atomic updates and state section*,
    so we will not repeat them here. Rather, we will only discuss the concurrent queues
    and other collections.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Java 有许多旨在并发和线程安全的可变数据结构，这意味着多个调用者可以同时安全地访问这些数据结构，而不会相互阻塞。当我们只需要高度并发的访问而不需要状态管理时，这些数据结构可能非常适合。其中一些采用了无锁算法。我们已经在*原子更新和状态部分*讨论了
    Java 原子状态类，所以这里不再重复。相反，我们只讨论并发队列和其他集合。
- en: All of these data structures live in the `java.util.concurrent` package. These
    concurrent data structures are tailored to leverage the JSR 133 "Java Memory Model
    and Thread Specification Revision" ([http://gee.cs.oswego.edu/dl/jmm/cookbook.html](http://gee.cs.oswego.edu/dl/jmm/cookbook.html))
    implementation that first appeared in Java 5.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据结构都位于 `java.util.concurrent` 包中。这些并发数据结构是为了利用 JSR 133 “Java 内存模型和线程规范修订”
    ([http://gee.cs.oswego.edu/dl/jmm/cookbook.html](http://gee.cs.oswego.edu/dl/jmm/cookbook.html))
    的实现而量身定制的，该实现首次出现在 Java 5 中。
- en: Concurrent maps
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发映射
- en: Java has a mutable concurrent hash map—`java.util.concurrent.ConcurrentHashMap`
    (CHM in short). The concurrency level can be optionally specified when instantiating
    the class, which is 16 by default. The CHM implementation internally partitions
    the map entries into the hash buckets, and uses multiple locks to reduce the contention
    on each bucket. Reads are never blocked by writes, therefore they may be stale
    or inconsistent—this is countered by built-in detection of such situations, and
    issuing a lock in order to read the data again in the synchronized fashion. This
    is an optimization for the scenarios, where reads significantly outnumber writes.
    In CHM, all the individual operations are near constant-time unless stuck in a
    retry loop due to the lock contention.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Java 有一个可变的并发哈希映射——`java.util.concurrent.ConcurrentHashMap`（简称 CHM）。在实例化类时可以可选地指定并发级别，默认为
    16。CHM 实现在内部将映射条目分区到哈希桶中，并使用多个锁来减少每个桶的竞争。读取永远不会被写入阻塞，因此它们可能是过时的或不一致的——这种情况通过内置的检测来应对，并发出锁以再次以同步方式读取数据。这是针对读取数量远多于写入的场景的优化。在
    CHM 中，所有单个操作几乎都是常数时间，除非由于锁竞争陷入重试循环。
- en: 'In contrast with Clojure''s persistent map, CHM cannot accept `null` (`nil`)
    as the key or value. Clojure''s immutable scalars and collections are automatically
    well-suited for use with CHM. An important thing to note is that only the individual
    operations in CHM are atomic, and exhibit strong consistency. As CHM operations
    are concurrent, the aggregate operations provide a rather weak consistency than
    the true operation-level consistency. Here is how we can use CHM. The individual
    operations in CHM, which provide a better consistency, are safe to use. The aggregate
    operations should be reserved for when we know its consistency characteristics,
    and the related trade-off:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Clojure 的持久映射相比，CHM 不能接受 `null`（`nil`）作为键或值。Clojure 的不可变标量和集合自动适合与 CHM 一起使用。需要注意的是，只有
    CHM 中的单个操作是原子的，并表现出强一致性。由于 CHM 操作是并发的，聚合操作提供的致性比真正的操作级致性要弱。以下是我们可以使用 CHM 的方法。在
    CHM 中提供更好一致性的单个操作是安全的。聚合操作应保留在我们知道其一致性特征和相关权衡的情况下使用：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `java.util.concurrent.ConcurrentSkipListMap` class (CSLM in short) is another
    concurrent mutable map data structure in Java. The difference between CHM and
    CSLM is that CSLM offers a sorted view of the map at all times with the O(log
    N) time complexity. The sorted view has the natural order of keys by default,
    which can be overridden by specifying a Comparator implementation when instantiating
    CSLM. The implementation of CSLM is based on the Skip List, and provides navigation
    operations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`java.util.concurrent.ConcurrentSkipListMap` 类（简称 CSLM）是 Java 中另一种并发可变映射数据结构。CHM
    和 CSLM 之间的区别在于，CSLM 在所有时间都提供具有 O(log N) 时间复杂度的排序视图。默认情况下，排序视图具有键的自然顺序，可以通过在实例化
    CSLM 时指定 Comparator 实现来覆盖。CSLM 的实现基于跳表，并提供导航操作。'
- en: The `java.util.concurrent.ConcurrentSkipListSet` class (CSLS in short) is a
    concurrent mutable set based on the CSLM implementation. While CSLM offers the
    map API, CSLS behaves as a set data structure while borrowing features of CSLM.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`java.util.concurrent.ConcurrentSkipListSet` 类（简称 CSLS）是一个基于 CSLM 实现的并发可变集合。虽然
    CSLM 提供了映射 API，但 CSLS 在行为上类似于集合数据结构，同时借鉴了 CSLM 的特性。'
- en: Concurrent queues
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发队列
- en: Java has a built-in implementation of several kinds of mutable and concurrent
    in-memory queues. The queue data structure is a useful tool for buffering, producer-consumer
    style implementation, and for pipelining such units together to form the high-performance
    workflows. We should not confuse them with durable queues that are used for similar
    purpose in the batch jobs for a high throughput. Java's in-memory queues are not
    transactional, but they provide atomicity and strong consistency guarantee for
    the individual queue operations only. Aggregate operations offer weaker consistency.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Java 内置了多种可变和并发内存队列的实现。队列数据结构是用于缓冲、生产者-消费者风格实现以及将这些单元管道化以形成高性能工作流程的有用工具。我们不应将它们与用于批量作业中类似目的的持久队列混淆。Java
    的内存队列不是事务性的，但它们只为单个队列操作提供原子性和强一致性保证。聚合操作提供较弱的致性。
- en: 'The `java.util.concurrent.ConcurrentLinkedQueue` (CLQ) is a lock-free, wait-free
    unbounded "First In First Out" (FIFO) queue. FIFO implies that the order of the
    queue elements will not change once added to the queue. CLQ''s `size()` method
    is not a constant time operation; it depends on the concurrency level. Few examples
    of using CLQ are here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`java.util.concurrent.ConcurrentLinkedQueue` (CLQ) 是一个无锁、无阻塞的无界“先进先出” (FIFO)
    队列。FIFO 意味着一旦元素被添加到队列中，队列元素的顺序就不会改变。CLQ 的 `size()` 方法不是一个常数时间操作；它取决于并发级别。这里有一些使用
    CLQ 的例子：'
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '| Queue | Blocking? | Bounded? | FIFO? | Fairness? | Notes |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 队列 | 是否阻塞？ | 是否有界？ | FIFO？ | 公平性？ | 备注 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CLQ | No | No | Yes | No | Wait-free, but the size() is not constant time
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| CLQ | 否 | 否 | 是 | 否 | 无阻塞，但 size() 不是常数时间操作 |'
- en: '| ABQ | Yes | Yes | Yes | Optional | The capacity is fixed at instantiation
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| ABQ | 是 | 是 | 是 | 可选 | 容量在实例化时固定 |'
- en: '| DQ | Yes | No | No | No | The elements implement the Delayed interface |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| DQ | 是 | 否 | 否 | 否 | 元素实现了 Delayed 接口 |'
- en: '| LBQ | Yes | Optional | Yes | No | The capacity is flexible, but with no fairness
    option |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LBQ | 是 | 可选 | 是 | 否 | 容量灵活，但没有公平性选项 |'
- en: '| PBQ | Yes | No | No | No | The elements are consumed in a priority order
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| PBQ | 是 | 否 | 否 | 否 | 元素按优先级顺序消费 |'
- en: '| SQ | Yes | – | – | Optional | It has no capacity; it serves as a channel
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SQ | 是 | – | – | 可选 | 没有容量；它充当一个通道 |'
- en: In the `java.util.concurrent` package, `ArrayBlockingQueue` (ABQ), `DelayQueue`
    (DQ), `LinkedBlockingQueue` (LBQ), `PriorityBlockingQueue` (PBQ), and `SynchronousQueue`
    (SQ) implement the `BlockingQueue` (BQ) interface. Its Javadoc describes the characteristics
    of its method calls. ABQ is a fixed-capacity, FIFO queue backed by an array. LBQ
    is also a FIFO queue, backed by the linked nodes, and is optionally bounded (default
    `Integer.MAX_VALUE`). ABQ and LBQ generate "Back pressure" by blocking the enqueue
    operations on full capacity. ABQ supports optional fairness (with performance
    overhead) in the order of the threads that access it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `java.util.concurrent` 包中，`ArrayBlockingQueue` (ABQ)，`DelayQueue` (DQ)，`LinkedBlockingQueue`
    (LBQ)，`PriorityBlockingQueue` (PBQ) 和 `SynchronousQueue` (SQ) 实现了 `BlockingQueue`
    (BQ) 接口。它的 Javadoc 描述了其方法调用的特性。ABQ 是一个基于数组的固定容量、FIFO 队列。LBQ 也是一个 FIFO 队列，由链表节点支持，并且是可选的有界（默认
    `Integer.MAX_VALUE`）。ABQ 和 LBQ 通过阻塞满容量时的入队操作来生成“背压”。ABQ 支持可选的公平性（有性能开销），按访问它的线程顺序。
- en: DQ is an unbounded queue that accepts the elements associated with the delay.
    The queue elements cannot be null, and must implement the `java.util.concurrent.Delayed`
    interface. Elements are available for removal from the queue only after the delay
    has been expired. DQ can be very useful for scheduling the processing of the elements
    at different times.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: DQ 是一个无界队列，接受与延迟相关的元素。队列元素不能为 null，并且必须实现 `java.util.concurrent.Delayed` 接口。元素只有在延迟过期后才能从队列中移除。DQ
    对于在不同时间安排元素的处理非常有用。
- en: PBQ is unbounded and blocking while letting elements be consumed from the queue
    as per priority. Elements have the natural ordering by default that can be overridden
    by specifying a Comparator implementation when instantiating the queue.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: PBQ 是无界且阻塞的，同时允许按优先级从队列中消费元素。元素默认具有自然排序，可以通过在实例化队列时指定 Comparator 实现来覆盖。
- en: SQ is not really a queue at all. Rather, it's just a barrier for a producer
    or consumer thread. The producer blocks until a consumer removes the element and
    vice versa. SQ does not have a capacity. However, SQ supports optional fairness
    (with performance overhead), in the order, in which the threads access it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: SQ 实际上根本不是一个队列。相反，它只是生产者或消费者线程的一个屏障。生产者会阻塞，直到消费者移除元素，反之亦然。SQ 没有容量。然而，SQ 支持可选的公平性（有性能开销），按线程访问的顺序。
- en: There are some new concurrent queue types introduced after Java 5\. Since JDK
    1.6, in the `java.util.concurrent` package Java has **BlockingDeque** (**BD**)
    with **LinkedBlockingDeque** (**LBD**) as the only available implementation. BD
    builds on BQ by adding the **Deque** (**double-ended queue**) operations, that
    is, the ability to add elements and consume the elements from both the ends of
    the queue. LBD can be instantiated with an optional capacity (bounded) to block
    the overflow. JDK 1.7 introduced **TransferQueue** (**TQ**) with **LinkedTransferQueue**
    (**LTQ**) as the only implementation. TQ extends the concept of SQ in such a way
    that the producers and consumers block a queue of elements. This will help utilize
    the producer and consumer threads better by keeping them busy. LTQ is an unbounded
    implementation of TQ where the `size()` method is not a constant time operation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Java 5之后引入了一些新的并发队列类型。自从JDK 1.6以来，在`java.util.concurrent`包中，Java有**BlockingDeque**（**BD**），其中**LinkedBlockingDeque**（**LBD**）是唯一可用的实现。BD通过添加**Deque**（双端队列）操作来构建在BQ之上，即从队列两端添加元素和消费元素的能力。LBD可以实例化一个可选的容量（有限制）以阻塞溢出。JDK
    1.7引入了**TransferQueue**（**TQ**），其中**LinkedTransferQueue**（**LTQ**）是唯一实现。TQ以扩展SQ概念的方式，使得生产者和消费者阻塞元素队列。这将通过保持它们忙碌来更好地利用生产者和消费者线程。LTQ是TQ的无限制实现，其中`size()`方法不是常数时间操作。
- en: Clojure support for concurrent queues
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clojure对并发队列的支持
- en: We covered the persistent queue in [Chapter 2](ch02.html "Chapter 2. Clojure
    Abstractions"), *Clojure Abstractions* earlier. Clojure has a built-in `seque`
    function that builds over a BQ implementation (LBQ by default) to expose a write-ahead
    sequence. The sequence is potentially lazy, and the write-ahead buffer throttles
    how many elements to realize. As opposed to the chunked sequences (of chunk size
    32), the size of the write-ahead buffer is controllable and potentially populated
    at all times until the source sequence is exhausted. Unlike the chunked sequences,
    the realization doesn't happen suddenly for a chunk of 32 elements. It does so
    gradually and smoothly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在《Clojure抽象》的[第2章](ch02.html "第2章。Clojure抽象")中介绍了持久队列。Clojure有一个内置的`seque`函数，它基于BQ实现（默认为LBQ）来暴露预写序列。序列可能是惰性的，预写缓冲区控制要实现多少元素。与块大小为32的块序列不同，预写缓冲区的大小是可控制的，并且在所有时间都可能被填充，直到源序列耗尽。与块序列不同，32个元素的块不会突然实现。它是逐渐和平稳地实现的。
- en: Under the hood, Clojure's `seque` uses an agent to the backfill data in the
    write-ahead buffer. In the arity-2 variant of `seque`, the first argument should
    either be a positive integer, or an instance of BQ (ABQ, LBQ, and more) that is
    preferably bounded.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Clojure的`seque`使用代理在预写缓冲区中填充数据。在`seque`的2参数版本中，第一个参数应该是正整数，或者是一个BQ（ABQ、LBQ等）的实例，最好是有限制的。
- en: Concurrency with threads
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线程的并发
- en: On the JVM, threads are the de-facto fundamental instrument of concurrency.
    Multiple threads live in the same JVM; they share the heap space, and compete
    for the resources.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在JVM上，线程是并发的事实上的基本工具。多个线程生活在同一个JVM中；它们共享堆空间，并竞争资源。
- en: JVM support for threads
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JVM对线程的支持
- en: 'The JVM threads are the Operating System threads. Java wraps an underlying
    OS thread as an instance of the `java.lang.Thread` class, and builds up an API
    around it to work with threads. A thread on the JVM has a number of states: New,
    Runnable, Blocked, Waiting, Timed_Waiting, and Terminated. A thread is instantiated
    by overriding the `run()` method of the `Thread` class, or by passing an instance
    of the `java.lang.Runnable` interface to the constructor of the `Thread` class.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: JVM线程是操作系统线程。Java将底层OS线程包装为`java.lang.Thread`类的实例，并围绕它构建API以与线程一起工作。JVM上的线程有多个状态：新建、可运行、阻塞、等待、定时等待和终止。线程通过覆盖`Thread`类的`run()`方法或通过将`java.lang.Runnable`接口的实例传递给`Thread`类的构造函数来实例化。
- en: Invoking the `start()` method of a `Thread` instance starts its execution in
    a new thread. Even if just a single thread runs in the JVM, the JVM would not
    shut down. Calling the `setDaemon(boolean)` method of a thread with argument `true`
    tags the thread as a daemon that can be automatically shut down if no other non-daemon
    thread is running.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`Thread`实例的`start()`方法将在新线程中启动其执行。即使JVM中只运行一个线程，JVM也不会关闭。调用带有参数`true`的线程的`setDaemon(boolean)`方法将线程标记为守护线程，如果没有其他非守护线程正在运行，则可以自动关闭该线程。
- en: 'All Clojure functions implement the `java.lang.Runnable` interface. Therefore,
    invoking a function in a new thread is very easy:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Clojure 函数都实现了 `java.lang.Runnable` 接口。因此，在新的线程中调用一个函数非常简单：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `run()` method does not accept any argument. We can work around it by creating
    a higher order function that needs no arguments, but internally applies the argument
    `3`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()` 方法不接受任何参数。我们可以通过创建一个不需要参数的高阶函数来解决这个问题，但内部应用参数 `3`。'
- en: Thread pools in the JVM
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JVM 中的线程池
- en: 'Creating threads leads to the Operating System API calls, which is not always
    a cheap operation. The general practice is to create a pool of threads that can
    be recycled for different tasks. Java has a built-in support for threads pools.
    The interface called `java.util.concurrent.ExecutorService` represents the API
    for a thread pool. The most common way to create a thread pool is to use a factory
    method in the `java.util.concurrent.Executors` class:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 创建线程会导致操作系统 API 调用，这并不总是便宜的。一般的做法是创建一个线程池，可以回收用于不同任务。Java 有内置的线程池支持。名为 `java.util.concurrent.ExecutorService`
    的接口代表了线程池的 API。创建线程池最常见的方式是使用 `java.util.concurrent.Executors` 类中的工厂方法：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The previous example is equivalent of the examples with raw threads that we
    saw in the previous sub-section. Thread pools are also capable of helping to track
    the completion, and the return value of a function, executed in a new thread.
    An ExecutorService accepts an instance of the `java.util.concurrent.Callable`
    instance as an argument to several methods that launch a task, and return `java.util.concurrent.Future`
    to track the final result.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子等同于我们在上一小节中看到的原始线程的例子。线程池也有能力帮助跟踪新线程中执行函数的完成情况和返回值。ExecutorService 接受一个
    `java.util.concurrent.Callable` 实例作为参数，用于启动任务的几个方法，并返回 `java.util.concurrent.Future`
    以跟踪最终结果。
- en: 'All the Clojure functions also implement the `Callable` interface, so we can
    use them as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Clojure 函数也实现了 `Callable` 接口，因此我们可以如下使用它们：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The thread pools described here are the same as the ones that we saw briefly
    in the Agents section earlier. Thread pools need to be shut down by calling the
    `shutdown()` method when no longer required.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的线程池与我们在之前的代理部分中简要看到的相同。当不再需要时，线程池需要通过调用 `shutdown()` 方法来关闭。
- en: Clojure concurrency support
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clojure 并发支持
- en: Clojure has some nifty built-in features to deal with concurrency. We already
    discussed about the agents, and how they use the thread pools, in an earlier section.
    There are some more concurrency features in Clojure to deal with the various use
    cases.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 有一些巧妙的内置功能来处理并发。我们已经在之前的小节中讨论了代理，以及它们如何使用线程池。Clojure 中还有一些其他并发功能来处理各种用例。
- en: Future
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Future
- en: 'We saw earlier in this section how to use the Java API to launch a new thread,
    to execute a function. Also, we learned how to get the result back. Clojure has
    a built-in support called "futures" to do these things in a much smoother and
    integrated manner. The basis of the futures is the function `future-call` (it
    takes a `no-arg` function as an argument), and the macro `future` (it takes the
    body of code) that builds on the former. Both of them immediately start a thread
    to execute the supplied code. The following snippet illustrates the functions
    that work with the future, and how to use them:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们之前已经看到了如何使用 Java API 来启动一个新线程，以执行一个函数。我们还学习了如何获取结果。Clojure 有一个内置的支持称为
    "futures"，以更平滑和集成的方式完成这些事情。futures 的基础是 `future-call` 函数（它接受一个无参数函数作为参数），以及基于前者的宏
    `future`（它接受代码体）。两者都会立即启动一个线程来执行提供的代码。以下代码片段说明了与 future 一起工作的函数以及如何使用它们：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'One of the interesting aspects of `future-cancel` is that it can sometimes
    not only cancel tasks that haven''t started yet, but may also abort those that
    are halfway through execution:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`future-cancel` 的一个有趣方面是，它有时不仅能够取消尚未开始的任务，还可能中止那些正在执行一半的任务：'
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The previous scenario happens because Clojure's `future-cancel` cancels a future
    in such a way that if the execution has already started, it may be interrupted
    causing `InterruptedException`, which, if not explicitly caught, would simply
    abort the block of code. Beware of exceptions arising from the code that is executed
    in a future, because, by default, they are not reported verbosely! Clojure futures
    use the "solo" thread pool (used to execute the potentially blocking actions)
    that we discussed earlier with respect to the agents.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的场景发生是因为 Clojure 的 `future-cancel` 以一种方式取消未来（future），如果执行已经开始，可能会被中断，导致 `InterruptedException`，如果未显式捕获，则简单地终止代码块。注意来自未来中执行代码的异常，因为默认情况下，它们不会被详细报告！Clojure
    的未来（futures）使用“solo”线程池（用于执行可能阻塞的操作），这是我们之前在讨论代理时提到的。
- en: Promise
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 承诺
- en: A promise is a placeholder for the result of a computation that may or may not
    have occurred. A promise is not directly associated with any computation. By definition,
    a promise does not imply when the computation might occur, hence realizing the
    promise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一个承诺（promise）是代表一个可能发生也可能不发生的计算结果的占位符。承诺并不直接与任何计算相关联。根据定义，承诺并不暗示计算何时发生，因此实现承诺。
- en: Typically, a promise originates from one place in the code, and is realized
    by some other portion of the code that knows when and how to realize the promise.
    Very often, this happens in a multi-threaded code. If a promise is not realized
    yet, any attempt to read the value blocks all callers. If a promise is realized,
    then all the callers can read the value without being blocked. As with futures,
    a promise can be read with a timeout using `deref`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，承诺起源于代码的一个地方，由知道何时以及如何实现承诺的代码的其他部分实现。这通常发生在多线程代码中。如果承诺尚未实现，任何尝试读取值的尝试都会阻塞所有调用者。如果承诺已经实现，那么所有调用者都可以读取值而不会被阻塞。与未来（futures）一样，可以使用
    `deref` 在超时后读取承诺。
- en: 'Here is a very simple example showing how to use promises:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个非常简单的例子，展示了如何使用承诺：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A promise is a very powerful tool that can be passed around as function arguments.
    It can be stored in a reference type, or simply be used for a high level coordination.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 承诺是一个非常强大的工具，可以作为函数参数传递。它可以存储在引用类型中，或者简单地用于高级协调。
- en: Clojure parallelization and the JVM
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Clojure 并行化与 JVM
- en: We observed in [Chapter 1](ch01.html "Chapter 1. Performance by Design"), *Performance
    by Design* that parallelism is a function of the hardware, whereas concurrency
    is a function of the software, assisted by the hardware support. Except for the
    algorithms that are purely sequential by nature, concurrency is the favored means
    to facilitate parallelism, and achieve better performance. Immutable and stateless
    data is a catalyst to concurrency, as there is no contention between threads,
    due to absence of mutable data.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第 1 章](ch01.html "第 1 章。通过设计实现性能")，*通过设计实现性能* 中观察到，并行化是硬件的函数，而并发是软件的函数，由硬件支持辅助。除了那些本质上纯粹顺序的算法外，并发是实现并行化和提高性能的首选方法。不可变和无状态数据是并发的催化剂，因为没有可变数据，线程之间没有竞争。
- en: Moore's law
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摩尔定律
- en: In 1965, Intel's cofounder, Gordon Moore, made an observation that the number
    of transistors per square inch on Integrated Circuits doubles every 24 months.
    He also predicted that the trend would continue for 10 years, but in practice,
    it has continued till now, marking almost half a century. More transistors have
    resulted in more computing power. With a greater number of transistors in the
    same area, we need higher clock speed to transmit signals to all of the transistors.
    Secondly, transistors need to get smaller in size to fit in. Around 2006-2007,
    the clock speed that the circuitry could work with topped out at about 2.8GHz,
    due to the heating issues and the laws of physics. Then, the multi-core processors
    were born.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1965 年，英特尔联合创始人戈登·摩尔观察到，集成电路每平方英寸的晶体管数量每 24 个月翻一番。他还预测这种趋势将持续 10 年，但实际上，它一直持续到现在，几乎半个世纪。更多的晶体管导致了更强的计算能力。在相同面积内晶体管数量增加，我们需要更高的时钟速度来传输信号到所有的晶体管。其次，晶体管需要变得更小以适应。大约在
    2006-2007 年，电路能够工作的时钟速度达到了大约 2.8GHz，这是由于散热问题和物理定律的限制。然后，多核处理器应运而生。
- en: Amdahl's law
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amdahl 定律
- en: The multi-core processors naturally require splitting up computation in order
    to achieve parallelization. Here begins a conflict—a program that was made to
    be run sequentially cannot make use of the parallelization features of the multi-core
    processors. The program must be altered to find the opportunity to split up computation
    at every step, while keeping the cost of coordination in mind. This results in
    a limitation that a program can be no more faster than its longest sequential
    part (*contention*, or *seriality*), and the coordination overhead. This characteristic
    was described by Amdahl's law.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 多核处理器自然需要分割计算以实现并行化。从这里开始，出现了一个冲突——原本设计为顺序运行的程序无法利用多核处理器的并行化特性。程序必须被修改，以便在每一步找到分割计算的机会，同时考虑到协调成本。这导致了一个限制，即程序的速度不能超过其最长的顺序部分（*竞争*，或*串行性*），以及协调开销。这一特性被
    Amdahl 定律所描述。
- en: Universal Scalability Law
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用可扩展性定律
- en: 'Dr Neil Gunther''s Universal Scalability Law (USL) is a superset of Amdahl''s
    Law that makes both: *contention (α)* and *coherency (β)* the first class concerns
    in quantifying the scalability very closely to the realistic parallel systems.
    Coherency implies the coordination overhead (latency) in making the result of
    one part of a parallelized program to be available to another. While Amdahl''s
    Law states that contention (seriality) causes performance to level off, USL goes
    to show that the performance actually degrades with excessive parallelization.
    USL is described with the following formula:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尼尔·冈瑟博士的通用可扩展性定律（USL）是 Amdahl 定律的超集，它将 *竞争（α）* 和 *一致性（β）* 作为量化可扩展性的首要关注点，使其非常接近现实中的并行系统。一致性意味着协调开销（延迟）在使并行化程序的一部分结果对另一部分可用时的协调。虽然
    Amdahl 定律表明竞争（串行性）会导致性能水平化，但 USL 表明性能实际上随着过度并行化而下降。USL 用以下公式描述：
- en: C(N) = N / (1 + α ((N – 1) + β N (N – 1)))
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: C(N) = N / (1 + α ((N – 1) + β N (N – 1)))
- en: Here, C(N) implies relative capacity or throughput in terms of the source of
    concurrency, such as physical processors, or the users driving the software application.
    α implies the degree of contention because of the shared data or the sequential
    code, and β implies penalty incurred for maintaining the consistency of shared
    data. I would encourage you to pursue USL further ([http://www.perfdynamics.com/Manifesto/USLscalability.html](http://www.perfdynamics.com/Manifesto/USLscalability.html)),
    as this is a very important resource for studying the impact of concurrency on
    scalability and the performance of the systems.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，C(N) 表示相对容量或吞吐量，以并发源为依据，例如物理处理器，或驱动软件应用的用户。α 表示由于共享数据或顺序代码而引起的竞争程度，β 表示维护共享数据一致性所造成的惩罚。我鼓励您进一步研究
    USL（[http://www.perfdynamics.com/Manifesto/USLscalability.html](http://www.perfdynamics.com/Manifesto/USLscalability.html)），因为这是研究并发对可扩展性和系统性能影响的重要资源。
- en: Clojure support for parallelization
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Clojure 对并行化的支持
- en: A program that relies on mutation cannot parallelize its parts without creating
    contention on the mutable state. It requires coordination overhead, which makes
    the situation worse. Clojure's immutable nature is better suited to parallelize
    the parts of a program. Clojure also has some constructs that are suited for parallelism
    by the virtue of Clojure's consideration of available hardware resources. The
    result is, the operations execute optimized for certain use case scenarios.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖于变动的程序在创建对可变状态的竞争之前无法并行化其部分。它需要协调开销，这使情况变得更糟。Clojure 的不可变特性更适合并行化程序的部分。Clojure
    还有一些结构，由于 Clojure 考虑了可用的硬件资源，因此适合并行化。结果是，操作针对某些用例场景进行了优化。
- en: pmap
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pmap
- en: The `pmap` function (similar to `map`) accepts as arguments a function and one,
    or more collections of data elements. The function is applied to each of the data
    elements in such a way that some of the elements are processed by the function
    in parallel. The parallelism factor is chosen at runtime by the `pmap` implementation,
    as two greater than the total number of available processors. It still processes
    the elements lazily, but the realization factor is same as the parallelism factor.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`pmap` 函数（类似于 `map`）接受一个函数和一个或多个数据元素集合作为参数。该函数被应用于数据元素集合中的每个元素，这样一些元素可以并行地由该函数处理。并行度因子由
    `pmap` 实现在运行时选择，通常大于可用的处理器总数。它仍然以惰性方式处理元素，但实现因子与并行度因子相同。'
- en: 'Check out the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码：
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To use `pmap` effectively, it is imperative that we understand what it is meant
    for. As the documentation says, it is meant for computationally intensive functions.
    It is optimized for CPU-bound and cache-bound jobs. High latency and low CPU tasks,
    such as blocking I/O, are a gross misfit for `pmap`. Another pitfall to be aware
    of is whether the function used in `pmap` performs a lot of memory operations
    or not. Since the same function will be applied across all the threads, all the
    processors (or cores) may compete for the memory interconnect and the sub-system
    bandwidth. If the parallel memory access becomes a bottleneck, `pmap` cannot make
    the operation truly parallel, due to the contention on the memory access.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地使用 `pmap`，我们必须理解它的用途。正如文档所述，它是为计算密集型函数设计的。它针对 CPU 密集型和缓存密集型工作进行了优化。对于高延迟和低
    CPU 任务，如阻塞 I/O，`pmap` 是一个不合适的选择。另一个需要注意的陷阱是 `pmap` 中使用的函数是否执行了大量的内存操作。由于相同的函数将在所有线程中应用，所有处理器（或核心）可能会竞争内存互连和子系统带宽。如果并行内存访问成为瓶颈，由于内存访问的竞争，`pmap`
    无法真正实现操作的并行化。
- en: Another concern is what happens when several `pmap` operations run concurrently?
    Clojure does not attempt to detect multiple `pmap`s running concurrently. The
    same number of threads will be launched afresh for every new `pmap` operation.
    The developer is responsible to ensure the performance characteristics, and the
    response time of the program resulting from the concurrent pmap executions. Usually,
    when the latency reasons are paramount, it is advisable to limit the concurrent
    instances of `pmap` running in the program.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关注点是当多个 `pmap` 操作同时运行时会发生什么？Clojure 不会尝试检测同时运行多个 `pmap`。对于每个新的 `pmap` 操作，都会重新启动相同数量的线程。开发者负责确保并发
    `pmap` 执行的性能特性和程序的响应时间。通常，当延迟原因是至关重要的，建议限制程序中运行的 `pmap` 并发实例。
- en: pcalls
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pcalls
- en: The `pcalls` function is built using `pmap`, so it borrows properties from the
    latter. However, the `pcalls` function accepts zero or more functions as arguments
    and executes them in parallel, returning the result values of the calls as a list.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`pcalls` 函数是使用 `pmap` 构建的，因此它借鉴了后者的属性。然而，`pcalls` 函数接受零个或多个函数作为参数，并并行执行它们，将调用结果作为列表返回。'
- en: pvalues
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: pvalues
- en: The `pvalues` macro is built using `pcalls`, so it transitively shares the properties
    of `pmap`. It's behavior is similar to `pcalls`, but instead of functions, it
    accepts zero or more S-expressions that are evaluated in the parallel using `pmap`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`pvalues` 宏是使用 `pcalls` 构建的，因此它间接共享了 `pmap` 的属性。它的行为类似于 `pcalls`，但它接受零个或多个在并行中使用
    `pmap` 评估的 S-表达式。'
- en: Java 7's fork/join framework
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java 7 的 fork/join 框架
- en: Java 7 introduced a new framework for parallelism called "fork/join," based
    on divide-and-conquer and the work-stealing scheduler algorithms. The basic idea
    of how to use the fork/join framework is fairly simple—if the work is small enough,
    then do it directly in the same thread; otherwise, split the work into two pieces,
    invoke them in a fork/join thread pool, and wait for the results to combine.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Java 7 引入了一个名为 "fork/join" 的新并行框架，该框架基于分而治之和工作窃取调度算法。使用 fork/join 框架的基本思路相当简单——如果工作足够小，则直接在同一线程中执行；否则，将工作分成两部分，在
    fork/join 线程池中调用它们，并等待结果合并。
- en: This way, the job gets recursively split into smaller parts such as an inverted
    tree, until the smallest part can be carried out in just a single thread. When
    the leaf/subtree jobs return, the parent combines the result of all children,
    and returns the results.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，工作会递归地分成更小的部分，例如倒置树，直到最小的部分可以仅用单个线程执行。当叶/子树工作返回时，父节点将所有子节点的结果合并，并返回结果。
- en: 'The fork/join framework is implemented in Java 7 in terms of a special kind
    of thread pool; check out `java.util.concurrent.ForkJoinPool`. The specialty of
    this thread pool is that it accepts the jobs of `java.util.concurrent.ForkJoinTask`
    type, and whenever these jobs block, waiting for the child jobs to finish, the
    threads used by the waiting jobs are allocated to the child jobs. When the child
    finishes its work, the thread is allocated back to the blocked parent jobs in
    order to continue. This style of dynamic thread allocation is described as "work-stealing".
    The fork/join framework can be used from within Clojure. The `ForkJoinTask` interface
    has two implementations: `RecursiveAction` and `RecursiveTask` in the `java.util.concurrent`
    package. Concretely, `RecursiveTask` maybe more useful with Clojure, as `RecursiveAction`
    is designed to work with mutable data, and does not return any value from its
    operation.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Fork/Join 框架在 Java 7 中通过一种特殊的线程池实现；请查看 `java.util.concurrent.ForkJoinPool`。这个线程池的特殊之处在于它接受
    `java.util.concurrent.ForkJoinTask` 类型的作业，并且每当这些作业阻塞，等待子作业完成时，等待作业使用的线程会被分配给子作业。当子作业完成其工作后，线程会被分配回阻塞的父作业以继续执行。这种动态线程分配的方式被称为“工作窃取”。Fork/Join
    框架可以从 Clojure 内部使用。`ForkJoinTask` 接口有两个实现：`java.util.concurrent` 包中的 `RecursiveAction`
    和 `RecursiveTask`。具体来说，`RecursiveTask` 在 Clojure 中可能更有用，因为 `RecursiveAction` 是设计用来处理可变数据的，并且其操作不会返回任何值。
- en: Using the fork-join framework entails choosing the batch size to split a job
    into, which is a crucial factor in parallelizing a long job. Too large a batch
    size may not utilize all the CPU cores enough; on the other hand, a small batch
    size may lead to a longer overhead, coordinating across the parent/child batches.
    As we will see in the next section, Clojure integrates with the Fork/join framework
    to parallelize the reducers implementation.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Fork/Join 框架意味着选择将作业拆分成批次的批大小，这是并行化长时间作业的一个关键因素。批大小过大可能无法充分利用所有 CPU 核心；另一方面，批大小过小可能会导致更长的开销，协调父/子批次。正如我们将在下一节中看到的，Clojure
    与 Fork/join 框架集成以并行化减少器实现。
- en: Parallelism with reducers
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用减少器实现并行化
- en: Reducers are a new abstraction introduced in Clojure 1.5, and are likely to
    have a wider impact on the rest of the Clojure implementation in the future versions.
    They depict a different way of thinking about processing collections in Clojure—the
    key concept is to break down the notion that collections can be processed only
    sequentially, lazily, or producing a seq, and more. Moving away from such a behavior
    guarantee raises the potential for eager and parallel operations on one hand,
    whereas incurring constraints on the other. Reducers are compatible with the existing
    collections.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 减少器是 Clojure 1.5 中引入的新抽象，预计在未来版本中将对 Clojure 的其余实现产生更广泛的影响。它们描绘了在 Clojure 中处理集合的另一种思考方式——关键概念是打破集合只能顺序、惰性或产生序列处理的观念，以及更多。摆脱这种行为保证一方面提高了进行贪婪和并行操作的可能性，另一方面则带来了约束。减少器与现有集合兼容。
- en: For an example, a keen observation of the regular `map` function reveals that
    its classic definition is tied to the mechanism (recursion), order (sequential),
    laziness (often), and representation (list/seq/other) aspects of producing the
    result. Most of this actually defines "how" the operation is performed, rather
    than "what" needs to be done. In the case of `map`, the "what" is all about applying
    a function to each element of its collection arguments. But since the collection
    types can be of various types (tree-structured, sequence, iterator, and more),
    the operating function cannot know how to navigate the collection. Reducers decouple
    the "what" and "how" parts of the operation.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对常规的 `map` 函数的敏锐观察揭示，其经典定义与产生结果机制（递归）、顺序（顺序）、惰性（通常）和表示（列表/序列/其他）方面相关联。实际上，这大部分定义了“如何”执行操作，而不是“需要做什么”。在
    `map` 的情况下，“需要做什么”是关于对其集合参数的每个元素应用函数。但由于集合类型可以是各种类型（树状结构、序列、迭代器等），操作函数不知道如何遍历集合。减少器将操作的“需要做什么”和“如何做”部分解耦。
- en: Reducible, reducer function, reduction transformation
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可约性、减少函数、减少转换
- en: Collections are of various kinds, hence only a collection knows how to navigate
    itself. In the reducers model at a fundamental level, an internal "reduce" operation
    in each collection type has access to its properties and behavior, and access
    to what it returns. This makes all the collection types essentially "reducible".
    All the operations that work with collections can be modeled in terms of the internal
    "reduce" operation. The new modeled form of such operations is a "reducing function",
    which is typically a function of two arguments, the first argument being the accumulator,
    and the second being the new input.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 集合种类繁多，因此只有集合本身知道如何导航自己。在归约器模型的基本层面上，每个集合类型内部都有一个“reduce”操作，可以访问其属性和行为，以及它返回的内容。这使得所有集合类型本质上都是“可归约”的。所有与集合一起工作的操作都可以用内部“reduce”操作来建模。这种操作的新建模形式是一个“归约函数”，它通常有两个参数，第一个参数是累加器，第二个是新输入。
- en: How does it work when we need to layer several functions upon another, over
    the elements of a collection? For an example, let's say first we need to "filter",
    "map," and then "reduce". In such cases, a "transformation function" is used to
    model a reducer function (for example, for "filter") as another reducer function
    (for "map") in such a way that it adds the functionality during the transformation.
    This is called "reduction transformation".
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要在集合的元素上叠加多个函数时，它是如何工作的？例如，假设我们首先需要“过滤”、“映射”，然后“归约”。在这种情况下，使用“转换函数”来建模归约函数（例如，对于“过滤”），使其作为另一个归约函数（对于“映射”）出现，这样在转换过程中添加功能。这被称为“归约转换”。
- en: Realizing reducible collections
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现可归约集合
- en: While the reducer functions retain the purity of the abstraction, they are not
    useful all by themselves. The reducer operations in the namespace called as `clojure.core.reducers`
    similar to `map`, `filter`, and more, basically return a reducible collection
    that embed the reducer functions within themselves. A reducible collection is
    not realized, not even lazily realized—rather, it is just a recipe that is ready
    to be realized. In order to realize a reducible collection, we must use one of
    the `reduce` or `fold` operations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然归约函数保留了抽象的纯度，但它们本身并不具有实用性。在名为`clojure.core.reducers`的命名空间中，归约操作与`map`、`filter`等类似，基本上返回一个包含归约函数的归约集合——这些归约函数嵌入在集合内部。一个可归约集合尚未实现，甚至不是懒实现——而只是一个准备实现的配方。为了实现一个可归约集合，我们必须使用`reduce`或`fold`操作之一。
- en: The `reduce` operation that realizes a reducible collection is strictly sequential,
    albeit with the performance gains compared to `clojure.core/reduce`, due to reduced
    object allocations on the heap. The `fold` operation, which realizes a reducible
    collection, is potentially parallel, and uses a "reduce-combine" approach over
    the fork-join framework. Unlike the traditional "map-reduce" style, the use of
    fork/join the reduce-combine approach reduces at the bottom, and subsequently
    combines by the means of reduction again. This makes the `fold` implementation
    less wasteful and better performing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 实现可归约集合的`reduce`操作是严格顺序的，尽管与`clojure.core/reduce`相比，由于堆上对象分配的减少，性能有所提升。实现可归约集合的`fold`操作可能是并行的，并使用“reduce-combine”方法在fork-join框架上操作。与传统的“map-reduce”风格不同，使用fork/join的reduce-combine方法在底层进行归约，然后通过再次归约的方式结合。这使得`fold`实现更加节省资源，性能更优。
- en: Foldable collections and parallelism
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可折叠集合与并行性
- en: Parallel reduction by `fold` puts certain constraints on the collections and
    operations. The tree-based collection types (persistent map, persistent vector,
    and persistent set) are amenable to parallelization. At the same time, the sequences
    may not be parallelized by `fold`. Secondly, `fold` requires that the individual
    reducer functions should be "associative", that is, the order of the input arguments
    applied to the reducer function should not matter. The reason being, `fold` can
    segment the elements of the collection to process in parallel, and the order in
    which they may be combined is not known in advance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`fold`进行的并行化对集合和操作施加了某些限制。基于树的集合类型（持久映射、持久向量和持久集合）适合并行化。同时，序列可能无法通过`fold`进行并行化。其次，`fold`要求单个归约函数应该是“结合律”，即应用于归约函数的输入参数的顺序不应影响结果。原因是，`fold`可以将集合的元素分割成可以并行处理的段，而这些元素可能组合的顺序事先是未知的。
- en: 'The `fold` function accepts few extra arguments, such as the "combine function,"
    and the partition batch size (default being 512) for the parallel processing.
    Choosing the optimum partition size depends on the jobs, host capabilities, and
    the performance benchmarking. There are certain functions that are foldable (that
    is, parallelizable by `fold`), and there are others that are not, as shown here.
    They live in the `clojure.core.reducers` namespace:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`fold`函数接受一些额外的参数，例如“组合函数”，以及用于并行处理的分区批次大小（默认为512）。选择最佳分区大小取决于工作负载、主机能力和性能基准测试。某些函数是可折叠的（即可以通过`fold`并行化），而另一些则不是，如下所示。它们位于`clojure.core.reducers`命名空间中：'
- en: '**Foldable**: `map`, `mapcat`, `filter`, `remove`, and `flatten`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可折叠**：`map`、`mapcat`、`filter`、`remove`和`flatten`'
- en: '**Non-foldable**: `take-while`, `take`, and `drop`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可折叠**：`take-while`、`take`和`drop`'
- en: '**Combine functions**: `cat`, `foldcat`, and `monoid`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合函数**：`cat`、`foldcat`和`monoid`'
- en: A notable aspect of reducers is that it is foldable in parallel only when the
    collection is a tree type. This implies that the entire data set must be loaded
    in the heap memory when folding over them. This has the downside of memory consumption
    during the high load on a system. On the other hand, a lazy sequence is a perfectly
    reasonable solution for such scenarios. When processing large amount of data,
    it may make sense to use a combination of lazy sequences and reducers for performance.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Reducers的一个显著特点是，只有在集合是树类型时，它才能在并行中折叠。这意味着在折叠它们时，整个数据集必须加载到堆内存中。这在系统高负载期间会有内存消耗的缺点。另一方面，对于这种情况，一个懒序列是一个完全合理的解决方案。在处理大量数据时，使用懒序列和reducers的组合来提高性能可能是有意义的。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Concurrency and parallelism are extremely important for performance in this
    multi-core age. Effective use of concurrency requires substantial understanding
    of the underlying principles and details. Fortunately, Clojure provides safe and
    elegant ways to deal with concurrency and state. Clojure's new feature called
    "reducers" provides a way to achieve granular parallelism. In the coming years,
    we are likely to see more and more processor cores, and an increasing demand to
    write code that takes advantage of these. Clojure places us in the right spot
    to meet such challenges.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 并发和并行性在多核时代对性能至关重要。有效地使用并发需要深入了解其底层原理和细节。幸运的是，Clojure提供了安全且优雅的方式来处理并发和状态。Clojure的新特性“reducers”提供了一种实现细粒度并行性的方法。在未来的几年里，我们可能会看到越来越多的处理器核心，以及编写利用这些核心的代码的需求不断增加。Clojure使我们处于应对这些挑战的正确位置。
- en: In the next chapter, we will look at the performance measurement, analysis,
    and monitoring.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨性能测量、分析和监控。
