- en: Kafka Streams
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams
- en: In this chapter, instead of using the Kafka Java API for producers and consumers
    as in previous chapters, we are going to use Kafka Streams, the Kafka module for
    stream processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将不再像前几章那样使用 Kafka Java API 来处理生产者和消费者，而是将使用 Kafka Streams，这是 Kafka 用于流处理的模块。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Kafka Streams in a nutshell
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Streams 简而言之
- en: Kafka Streams project setup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Streams 项目设置
- en: Coding and running the Java `PlainStreamsProcessor`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和运行 Java `PlainStreamsProcessor`
- en: Scaling out with Kafka Streams
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kafka Streams 进行扩展
- en: Coding and running the Java `CustomStreamsProcessor`
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和运行 Java `CustomStreamsProcessor`
- en: Coding and running the Java `AvroStreamsProcessor`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和运行 Java `AvroStreamsProcessor`
- en: Coding and running the Late `EventProducer`
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和运行 Late `EventProducer`
- en: Coding and running the Kafka Streams processor
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码和运行 Kafka Streams 处理器
- en: Kafka Streams in a nutshell
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams 简而言之
- en: 'Kafka Streams is a library and part of Apache Kafka, used to process streams
    into and from Kafka. In functional programming, there are several operations over
    collections, such as the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 是一个库，也是 Apache Kafka 的一部分，用于处理进入和离开 Kafka 的流。在函数式编程中，集合上有几个操作，如下所示：
- en: '`filter`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`'
- en: '`map`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map`'
- en: '`flatMap`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap`'
- en: '`groupBy`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groupBy`'
- en: '`join`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`join`'
- en: The success of streaming platforms such as Apache Spark, Apache Flink, Apache
    Storm, and Akka Streams is to incorporate these stateless functions to process
    data streams. Kafka Streams provides a DSL to incorporate these functions to manipulate
    data streams.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark、Apache Flink、Apache Storm 和 Akka Streams 等流平台的成功在于将这些无状态函数纳入数据处理。Kafka
    Streams 提供了一个 DSL 来将这些函数纳入数据流操作。
- en: 'Kafka Streams also has stateful transformations; these are operations related
    to the aggregation that depend on the state of the messages as a group, for example,
    the windowing functions and support for late arrival data. Kafka Streams is a
    library, and this means that Kafka Streams applications can be deployed by executing
    your application jar. There is no need to deploy the application on a server,
    which means you can use any application to run a Kafka Streams application: Docker,
    Kubernetes, servers on premises, and so on. Something wonderful about Kafka Streams
    is that it allows horizontal scaling. That is, if it runs in the same JVM, it
    executes multiple threads, but if several instances of the application are started,
    it can run several JVMs to scale out.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 还具有有状态的转换；这些是与聚合相关的操作，依赖于消息作为组的状态，例如，窗口函数和对迟到数据的支持。Kafka Streams
    是一个库，这意味着 Kafka Streams 应用程序可以通过执行您的应用程序 jar 来部署。无需在服务器上部署应用程序，这意味着您可以使用任何应用程序来运行
    Kafka Streams 应用程序：Docker、Kubernetes、本地服务器等。Kafka Streams 的美妙之处在于它允许水平扩展。也就是说，如果在同一
    JVM 中运行，它将执行多个线程，但如果启动了多个应用程序实例，它可以在多个 JVM 中运行以进行扩展。
- en: The Apache Kafka core is built in Scala; however, Kafka Streams and KSQL are
    built in Java 8\. Kafka Streams is packaged in the open source distribution of
    Apache Kafka.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka 核心是用 Scala 编写的；然而，Kafka Streams 和 KSQL 是用 Java 8 编写的。Kafka Streams
    包含在 Apache Kafka 的开源发行版中。
- en: Project setup
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目设置
- en: 'The first step is to modify the `kioto` project. We have to add the dependencies
    to `build.gradle`, as shown in *Listing 6.1*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是修改 `kioto` 项目。我们必须在 `build.gradle` 中添加依赖项，如 *Listing 6.1* 所示：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Listing 6.1: Kioto Gradle build file for Kafka Streams'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1：Kioto Gradle 构建文件用于 Kafka Streams
- en: 'For the examples in this chapter, we also need the dependencies for Jackson.
    To use Kafka Streams, we just need one dependency, which is given in the following
    code snippet:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的示例，我们还需要 Jackson 的依赖项。要使用 Kafka Streams，我们只需要一个依赖项，如下面的代码片段所示：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To use Apache Avro with Kafka Streams, we add the serializers and deserializers
    as given in the following code:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kafka Streams 中使用 Apache Avro，我们需要添加以下代码中给出的序列化和反序列化器：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following lines are needed to run a Kafka Streams application as a jar.
    The build generates a fat jar:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行是运行 Kafka Streams 应用程序作为 jar 所必需的。构建生成了一个胖 jar：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The directory tree structure of the project should be as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的目录树结构应该是这样的：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Java PlainStreamsProcessor
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java PlainStreamsProcessor
- en: 'Now, in the `src/main/java/kioto/plain` directory, create a file called `PlainStreamsProcessor.java`
    with the contents of *Listing 6.2,* shown as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 `src/main/java/kioto/plain` 目录下，创建一个名为 `PlainStreamsProcessor.java` 的文件，其内容如
    *Listing 6.2* 所示：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 6.2: PlainStreamsProcessor.java'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2：PlainStreamsProcessor.java
- en: 'All the magic happens inside the `process()` method. The first step in a Kafka
    Streams application is to get a `StreamsBuilder` instance, as shown in the following
    code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的魔法都在 `process()` 方法内部发生。在 Kafka Streams 应用程序中，第一步是获取一个 `StreamsBuilder` 实例，如下面的代码所示：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `StreamsBuilder` is an object that allows building a topology. A topology
    in Kafka Streams is a structural description of a data pipeline. The topology
    is a succession of steps that involve transformations between streams. A topology
    is a very important concept in streams; it is also used in other technologies
    such as Apache Storm.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamsBuilder` 是一个允许构建拓扑的对象。在 Kafka Streams 中，拓扑是数据管道的结构描述。拓扑是涉及流之间转换的一系列步骤。拓扑在流中是一个非常重要的概念；它也被用于其他技术，如
    Apache Storm。'
- en: 'The `StreamsBuilder` is used to consume data from a topic. There are other
    two important concepts in the context of Kafka Streams: a `KStream`, a representation
    of a stream of records, and a `KTable`, a log of the changes in a stream (we will
    see KTables in detail in [Chapter 7](cafc7e52-647d-4f97-8a42-965ff94e678c.xhtml),
    *KSQL*). To obtain a `KStream` from a topic, we use the `stream()` method of the
    `StreamsBuilder`, shown as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamsBuilder` 用于从主题中消费数据。在 Kafka Streams 的上下文中，还有两个重要的概念：`KStream`，它是记录流的表示，以及
    `KTable`，它是流中变化的日志（我们将在第 7 章 [KSQL](cafc7e52-647d-4f97-8a42-965ff94e678c.xhtml)
    中详细看到 KTables）。要从主题获取 `KStream`，我们使用 `StreamsBuilder` 的 `stream()` 方法，如下所示：'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: There is an implementation of the `stream()` method that just receives the topic
    name as a parameter. But, it is good practice to use the implementation where
    we can also specify the serializers, as in this example we have to specify the
    Serializer for the key and the Serializer for the value for the `Consumed` class;
    in this case, both are strings.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个 `stream()` 方法的实现，它只接收主题名称作为参数。但是，使用可以指定序列化器的实现是一个好的实践，就像在这个例子中，我们必须指定 `Consumed`
    类的键和值的序列化器；在这种情况下，两者都是字符串。
- en: Don't let the serializers be specified through application-wide properties,
    because the same Kafka Streams application might read from several data sources
    with different data formats.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不要让序列化器通过应用程序范围的属性来指定，因为同一个 Kafka Streams 应用程序可能需要从具有不同数据格式的多个数据源中读取。
- en: 'We have obtained a JSON stream. The next step in the topology is to obtain
    the `HealthCheck` object stream, and we do so by building the following Stream:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经获取了一个 JSON 流。拓扑中的下一步是获取 `HealthCheck` 对象流，我们通过构建以下流来实现：
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: First, note that we are using the `mapValues()` method, so as in Java 8, the
    method receives a lambda expression. There are other implementations for the `mapValues()`
    method, but here we are using the lambda with just one argument (`v->`).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意我们正在使用 `mapValues()` 方法，所以就像在 Java 8 中，该方法接收一个 lambda 表达式。`mapValues()`
    方法还有其他实现，但在这里我们使用只有一个参数的 lambda (`v->`)。
- en: 'The `mapValues()` here could be read as follows: for each element in the input
    Stream, we are applying a transformation from the JSON object to the `HealthCheck`
    object, and this transformation could raise an `IOException`, so we are catching
    it.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `mapValues()` 可以这样理解：对于输入流中的每个元素，我们都在将 JSON 对象转换到 `HealthCheck` 对象，这种转换可能会抛出
    `IOException`，因此我们捕获了它。
- en: Recapitulating until the moment, in the first transformation, we read from the
    topic a stream with (`String, String`) pairs. In the second transformation, we
    go from the value in JSON to the value in `HealthCheck` objects.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾到目前，在第一次转换中，我们从主题读取了一个带有 (`String, String`) 对的流。在第二次转换中，我们从 JSON 中的值转换到 `HealthCheck`
    对象中的值。
- en: 'In the third step, we are going to calculate the `uptime` and send it to the
    `uptimeStream`, as shown in the following block:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步，我们将计算 `uptime` 并将其发送到 `uptimeStream`，如下面的代码块所示：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we are using the `map()` method, also as in Java 8, the method receives
    a lambda expression. There are other implementations for the `map()` method; here,
    we are using a lambda with two arguments (`(k, v)->`)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们正在使用 `map()` 方法，同样地，就像在 Java 8 中，该方法接收一个 lambda 表达式。`map()` 方法还有其他实现；在这里，我们使用有两个参数的
    lambda (`(k, v)->`)。
- en: 'The `map()` here could be read as follows: for each element in the input stream,
    we extract the tuples (key, value). We are using just the value (anyway, the key
    is `null`), cast it to `HealthCheck`, extract two attributes (the start time and
    the `SerialNumber`), calculate the `uptime`, and return a new `KeyValue` pair
    with (`SerialNumber`, `uptime`).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`map()`可以读作如下：对于输入流中的每个元素，我们提取键值对（键，值）。我们只使用值（无论如何，键是`null`），将其转换为`HealthCheck`，提取两个属性（开始时间和`SerialNumber`），计算`uptime`，并返回一个新的键值对（`SerialNumber`，`uptime`）。
- en: 'The last step is to write these values into the `uptimes` topic, shown as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将这些值写入`uptimes`主题，如下所示：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Again, I will emphasize it until I get tired: it is widely recommended to declare
    the data types of our Streams. Always stating, in this case for example, that
    key value pairs are of type (`String, String`).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，直到我累为止：强烈建议声明我们Stream的数据类型。例如，在这种情况下，始终声明键值对是类型（`String, String`）。
- en: 'Here is a summary of the steps:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是步骤的总结：
- en: Read from the input topic key value pairs of type (`String, String`)
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入主题读取类型为（`String, String`）的键值对
- en: Deserialize each JSON object to `HealthCheck`
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个JSON对象反序列化为`HealthCheck`
- en: Calculate the `uptimes`
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`uptimes`
- en: Write the `uptimes` to the output topic in key value pairs of type (`String,
    String`)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`uptimes`以键值对形式（`String, String`）写入输出主题
- en: Finally, it is time to start the Kafka Streams engine.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候启动Kafka Streams引擎了。
- en: 'Before starting it, we need to specify the topology and two properties, the
    broker and the application ID, shown as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动之前，我们需要指定拓扑和两个属性，即代理和应用程序ID，如下所示：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that the serializers and deserializers are just explicitly defined when
    reading from and writing to topics. So, we are not tied application-wide to a
    single data type, and we can read from and write to topics with different data
    types, as happens continuously in practice.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，序列化和反序列化器只是在从和向主题读写时明确定义的。因此，我们与应用程序范围内的单一数据类型无关，我们可以使用不同的数据类型从和向主题读写，这在实践中是持续发生的。
- en: Also with this good practice, between different topics, there is no ambiguity
    about which Serde to use.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，遵循这个良好的实践，在不同主题之间，关于使用哪个Serde没有歧义。
- en: Running the PlainStreamsProcessor
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行PlainStreamsProcessor
- en: 'To build the project, run this command from the `kioto` directory:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建项目，从`kioto`目录运行以下命令：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If everything is correct, the output is something like the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，输出将类似于以下内容：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    in the following code snippet:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是运行`uptimes`主题的控制台消费者，如下面的代码片段所示：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the IDE, run the main method of the `PlainStreamsProcessor`
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从IDE中运行`PlainStreamsProcessor`的主方法
- en: From the IDE, run the main method of the `PlainProducer` (built in previous
    chapters)
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从IDE中运行`PlainProducer`（在之前的章节中构建）的主方法
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uptimes`主题的控制台消费者的输出应该类似于以下内容：'
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Scaling out with Kafka Streams
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka Streams进行扩展
- en: 'To scale out the architecture as promised, we must follow these steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了按照承诺扩展架构，我们必须遵循以下步骤：
- en: 'Run a console consumer for the `uptimes` topic, shown as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`uptimes`主题的控制台消费者，如下所示：
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run the application jar from the command line, shown in the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行运行应用程序jar，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This is when we verify that our application really scales out.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这时我们验证我们的应用程序是否真的可以扩展。
- en: 'From a new command-line window, we execute the same command, shown as follows:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个新的命令行窗口，我们执行相同的命令，如下所示：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output should be something like the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we remember the theory of [Chapter 1](0bde3875-cd30-435c-9b32-b96fccb2e6d0.xhtml),
    *Configuring Kafka*, when we created our topic, we specified that it had four
    partitions. This nice message from Kafka Streams is telling us that the application
    was assigned to partitions two and three of our topic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还记得[第1章](0bde3875-cd30-435c-9b32-b96fccb2e6d0.xhtml)中关于配置Kafka的理论，当我们创建主题时，我们指定它有四个分区。这个来自Kafka
    Streams的好消息告诉我们，应用程序被分配到我们的主题的两个和三个分区。
- en: 'Take a look at the following log:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的日志：
- en: '[PRE20]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can read that the first instance was using the four partitions, then when
    we ran the second instance, it entered a state where the partitions were reassigned
    to consumers; to the first instance was assigned two partitions: `healthchecks-0`
    and `healthchecks-1`.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以读到第一个实例使用了四个分区，然后当我们运行第二个实例时，它进入了一个状态，其中分区被重新分配给消费者；第一个实例被分配了两个分区：`healthchecks-0`和`healthchecks-1`。
- en: And this is how Kafka Streams smoothly scale out. Remember that all this works
    because the consumers are part of the same consumer group and are controlled from
    Kafka Streams through the `application.id` property.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Kafka Streams如何平滑扩展的方式。记住，这一切之所以能够工作，是因为消费者是同一个消费者组的成员，并且通过`application.id`属性由Kafka
    Streams进行控制。
- en: We must also remember that the number of threads assigned to each instance of
    our application can also be modified by setting the `num.stream.threads` property.
    Thus, each thread would be independent, with its own producer and consumer. This
    ensures that the resources of our servers are used in a more efficient way.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须记住，可以通过设置`num.stream.threads`属性来修改分配给应用程序每个实例的线程数。这样，每个线程都将独立，拥有自己的生产者和消费者。这确保了我们的服务器资源被更有效地使用。
- en: Java CustomStreamsProcessor
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java CustomStreamsProcessor
- en: Summing up what has happened so far, in previous chapters we saw how to make
    a producer, a consumer, and a simple processor in Kafka. We also saw how to do
    the same with a custom SerDe, how to use Avro, and the Schema Registry. So far
    in this chapter, we have seen how to make a simple processor with Kafka Streams.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结到目前为止发生的事情，在之前的章节中，我们看到了如何在Kafka中创建生产者、消费者和简单的处理器。我们还看到了如何使用自定义SerDe、使用Avro和Schema
    Registry来完成相同的事情。到目前为止，在本章中，我们看到了如何使用Kafka Streams创建一个简单的处理器。
- en: In this section, we will use all our knowledge so far to build a `CustomStreamsProcessor`
    with Kafka Streams to use our own SerDe.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用到目前为止的所有知识，使用Kafka Streams构建一个`CustomStreamsProcessor`来使用我们自己的SerDe。
- en: 'Now, in the `src/main/java/kioto/custom` directory, create a file called `CustomStreamsProcessor.java`
    with the contents of *Listing 6.3*, shown as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`src/main/java/kioto/custom`目录中，创建一个名为`CustomStreamsProcessor.java`的文件，其内容如*列表6.3*所示，如下所示：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Listing 6.3: CustomStreamsProcessor.java'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '列表6.3: CustomStreamsProcessor.java'
- en: All the magic happens inside the `process()` method.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的魔法都在`process()`方法内部发生。
- en: 'The first step in a Kafka Streams application is to get a `StreamsBuilder`
    instance, shown as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams应用程序的第一步是获取一个`StreamsBuilder`实例，如下所示：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can reuse the `Serdes` built in the previous chapters. The following code
    creates a `KStream` that deserializes the values of the messages as `HealthCheck`
    objects.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重用之前章节中构建的`Serdes`。以下代码创建了一个将消息值反序列化为`HealthCheck`对象的`KStream`。
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `serdeFrom()` method of the `Serde` class dynamically wraps our `HealthCheckSerializer`
    and `HealthCheckDeserializer` into a single `HealthCheck Serde`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`Serde`类的`serdeFrom()`方法动态地将我们的`HealthCheckSerializer`和`HealthCheckDeserializer`包装成一个单一的`HealthCheck
    Serde`。'
- en: We can reuse the `Serdes` built on the previous chapters. The following code
    creates a `KStream` that deserializes the values of the messages as `HealthCheck`
    objects.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重用之前章节中构建的`Serdes`。以下代码创建了一个将消息值反序列化为`HealthCheck`对象的`KStream`。
- en: 'The `StreamsBuilder` is used to consume data from a topic. The same as in previous
    sections, to obtain a `KStream` from a topic, we use the `stream()` method of
    the `StreamsBuilder`, shown as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`StreamsBuilder`用于从主题中消费数据。与之前的章节相同，要从主题中获取`KStream`，我们使用`StreamsBuilder`的`stream()`方法，如下所示：'
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We use the implementation where we can also specify the serializers, as in this
    example, we have to specify the serializer for the key, and the serializer for
    the value for the `Consumed` class, in this case the key is a String (always `null`),
    and the serializer for the value is our new `customSerde`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个实现，其中我们也可以指定序列化器，就像在这个例子中，我们必须指定`Consumed`类的序列化器，在这种情况下，键是一个字符串（总是`null`），而值的序列化器是我们的新`customSerde`。
- en: 'The magic here is that the rest of the code of the `process()` method remains
    the same as in the previous section; it is also shown as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的魔法在于`process()`方法的其余代码与上一节相同；它也如下所示：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Running the CustomStreamsProcessor
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行CustomStreamsProcessor
- en: 'To build the project, run this command from the `kioto` directory:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建项目，请在`kioto`目录中运行以下命令：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If everything is correct, the output is something like the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正确，输出将类似于以下内容：
- en: '[PRE27]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    as follows:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是运行 `uptimes` 主题的控制台消费者，如下所示：
- en: '[PRE28]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From our IDE, run the main method of the `CustomStreamsProcessor`
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的 IDE 中运行 `CustomStreamsProcessor` 的主方法
- en: From our IDE, run the main method of the `CustomProducer` (built in previous
    chapters)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的 IDE 中运行 `CustomProducer` 的主方法（在前面章节中构建）
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uptimes` 主题的控制台消费者的输出应类似于以下内容：'
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Java AvroStreamsProcessor
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java AvroStreamsProcessor
- en: 'In this section we will see how to use all this power gathered together: Apache
    Avro, Schema Registry, and Kafka Streams.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何使用汇集的所有这些功能：Apache Avro、Schema Registry 和 Kafka Streams。
- en: 'Now, we are going to use Avro format in our messages, as we did in previous
    chapters. We consumed this data by configuring the Schema Registry URL and using
    the Kafka Avro deserializer. For Kafka Streams, we need to use a Serde, so we
    added the dependency in the Gradle build file, given as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用与前面章节相同的 Avro 格式在我们的消息中。我们通过配置 Schema Registry URL 并使用 Kafka Avro 反序列化器来消费这些数据。对于
    Kafka Streams，我们需要使用一个 Serde，因此我们在 Gradle 构建文件中添加了依赖项，如下所示：
- en: '[PRE30]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This dependency has the `GenericAvroSerde` and specific `avroSerde` explained
    in previous chapters. Both Serde implementations allow us to work with Avro records.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个依赖关系包含了前面章节中解释的 `GenericAvroSerde` 和特定的 `avroSerde`。这两个 Serde 实现允许我们使用 Avro
    记录。
- en: 'Now, in the `src/main/java/kioto/avro` directory, create a file called `AvroStreamsProcessor.java`
    with the contents of *Listing 6.4,* shown as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在 `src/main/java/kioto/avro` 目录下，创建一个名为 `AvroStreamsProcessor.java` 的文件，其中包含
    *列表 6.4* 的内容，如下所示：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Listing 6.4: AvroStreamsProcessor.java'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4：AvroStreamsProcessor.java
- en: One main difference with the previous code listings is the specification of
    the Schema Registry URL. The same as before, the magic happens inside the `process()`
    method.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的代码列表相比，一个主要的不同点是 Schema Registry URL 的指定。与之前一样，魔法发生在 `process()` 方法内部。
- en: 'The first step in a Kafka Streams Application is to get a `StreamsBuilder`
    instance, given as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 应用程序的第一步是获取一个 `StreamsBuilder` 实例，如下所示：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The seconds step is to get an instance of the `GenericAvroSerde` object, shown
    as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是获取 `GenericAvroSerde` 对象的实例，如下所示：
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As we are using the `GenericAvroSerde`, we need to configure it with the Schema
    Registry URL (as in previous chapters); it is shown in the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是 `GenericAvroSerde`，我们需要使用 Schema Registry URL（如前所述）对其进行配置；如下代码所示：
- en: '[PRE34]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `configure()` method of `GenericAvroSerde` receives a map as a parameter;
    as we just need a map with a single entry, we used the singleton map method.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`GenericAvroSerde` 的 `configure()` 方法接收一个映射作为参数；因为我们只需要一个包含单个条目的映射，所以我们使用了单例映射方法。'
- en: 'Now, we can create a `KStream` using this Serde. The following code generates
    an Avro Stream that contains `GenericRecord` objects:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个 Serde 创建一个 `KStream`。以下代码生成一个包含 `GenericRecord` 对象的 Avro 流：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note how we request the name of the `AvroTopic`, and that we have to specify
    the serializer for the key and the serializer for the value for the `Consumed`
    class; in this case, the key is a `String` (always `null`), and the serializer
    for the value is our new `avroSerde`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们请求的 `AvroTopic` 的名称，以及我们必须指定 `Consumed` 类的键和值的序列化器；在这种情况下，键是一个 `String`（始终为
    `null`），值的序列化器是我们的新 `avroSerde`。
- en: 'To deserealize the values for the `HealthCheck` Stream, we apply the same methods
    used in previous chapters inside the lambda of the `mapValues()` method with one
    argument (`v->`), shown as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要反序列化 `HealthCheck` 流的值，我们在 `mapValues()` 方法的 lambda 表达式中应用了前面章节中使用的相同方法，其中一个参数（`v->`），如下所示：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And again, the rest of the code of the `process()` method remains the same
    as in previous sections, shown as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，`process()` 方法的其余代码与前面章节相同，如下所示：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note that the code could be cleaner: we could create our own Serde that includes
    the deserialization code, so we can directly deserialize Avro Objects into `HealthCheck`
    Objects. To achieve this, this class has to extend the generic Avro deserializer.
    We leave this as an exercise for you.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意代码可以更简洁：我们可以创建自己的 Serde，其中包含反序列化代码，这样我们就可以直接将 Avro 对象反序列化为 `HealthCheck` 对象。为了实现这一点，这个类必须扩展通用的
    Avro 反序列化器。我们将这个作为练习留给您。
- en: Running the AvroStreamsProcessor
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 AvroStreamsProcessor
- en: 'To build the project, run this command from the `kioto` directory:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建项目，请在 `kioto` 目录下运行以下命令：
- en: '[PRE38]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If everything is correct, the output is something like the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，输出应类似于以下内容：
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first step is to run a console consumer for the `uptimes` topic, shown
    as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是运行 `uptimes` 主题的控制台消费者，如下所示：
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: From our IDE, run the main method of the `AvroStreamsProcessor`
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的 IDE 中运行 `AvroStreamsProcessor` 的主方法
- en: From our IDE, run the main method of the `AvroProducer` (built in previous chapters)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的 IDE 中运行 `AvroProducer` 的主方法（在之前的章节中构建）
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uptimes` 主题的控制台消费者的输出应类似于以下内容：'
- en: '[PRE41]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Late event processing
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迟到事件处理
- en: Previously, we talked about message processing, but now we will talk about events.
    An event in this context is something that happens at a particular time. An event
    is a message that happens at a point in time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们讨论了消息处理，但现在我们将讨论事件。在这个上下文中，事件是在特定时间发生的事情。事件是在某个时间点发生的信息。
- en: 'In order to understand events, we have to know the timestamp semantics. An
    event always has two timestamps, shown as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解事件，我们必须了解时间戳语义。一个事件总是有两个时间戳，如下所示：
- en: '**Event time**: The point in time when the event happened at the data source'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件时间**：事件在数据源发生的时间点'
- en: '**Processing time**: The point in time when the event is processed in the data
    processor'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理时间**：事件在数据处理器中被处理的时间点'
- en: 'Due to limitations imposed by the laws of physics, the processing time will
    always be subsequent to and necessarily different from the event time, for the
    following reasons:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于物理定律的限制，处理时间将始终在事件时间之后，并且必然与事件时间不同，原因如下：
- en: '**There is always network latency**: The time to travel from the data source
    to the Kafka broker cannot be zero.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络延迟总是存在**：从数据源到 Kafka 代理的传输时间不能为零。'
- en: '**The client could have a cache**: If the client cached some events before,
    send them to the data processor. As an example, think about a mobile device that
    is not always connected to the network because there are zones without network
    access, and the device holds some data before sending it.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端可能有一个缓存**：如果客户端之前缓存了一些事件，将它们发送到数据处理器。例如，考虑一个不是总是连接到网络的移动设备，因为有些区域没有网络访问，设备在发送数据之前会保存一些数据。'
- en: '**The existence of back pressure**: Sometimes, the broker will not process
    the events as they arrive, because it is busy and there are too many.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背压的存在**：有时，代理不会按到达顺序处理事件，因为它很忙，事件太多。'
- en: Having said the previous points, it is always important that our messages have
    a timestamp. Since version 0.10 of Kafka, the messages stored in Kafka always
    have an associated timestamp. The timestamp is normally assigned by the producer;
    if the producer sends a message without a timestamp, the broker assigns it one.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 说到前面提到的几点，我们的消息带有时间戳始终很重要。自从 Kafka 的 0.10 版本以来，存储在 Kafka 中的消息总是有一个相关的时间戳。时间戳通常由生产者分配；如果生产者发送的消息没有时间戳，则代理会为其分配一个。
- en: As a professional tip, when generating messages, always assign a timestamp from
    the producer.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为专业提示，在生成消息时，始终由生产者分配时间戳。
- en: Basic scenario
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本场景
- en: 'To explain late events, we need a system where the events arrive periodically
    and we want to know how many events are produced by unit of time. In *Figure 6.1*,
    we show this scenario:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释迟到的事件，我们需要一个事件定期到达的系统，并且我们想知道每单位时间内产生了多少事件。在 *图 6.1* 中，我们展示了这个场景：
- en: '![](img/e64ca6a4-d30b-4ff2-8794-c0c1222dcb37.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e64ca6a4-d30b-4ff2-8794-c0c1222dcb37.png)'
- en: 'Figure 6.1: The events as they were produced'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：事件的生产情况
- en: In the preceding figure, each marble represents an event. They are not supposed
    to have dimensions as they are at a specific point in time. Events are punctual,
    but for demonstration purposes, we represent them as balls. As we can see in **t1**
    and **t2,** two different events can happen at the same time.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，每个弹珠代表一个事件。它们不应该有维度，因为它们是在特定的时间点。事件是瞬时的，但为了演示目的，我们用球体来表示它们。正如我们在 **t1**
    和 **t2** 中所看到的，两个不同的事件可以同时发生。
- en: In our figure, tn represents the n^(th) time unit. Each marble represents a
    single event. To differentiate between them, the events on **t1** have one stripe,
    the events on **t2** have two stripes, and the events on **t3** have three stripes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的图中，tn 代表第 n 个时间单位。每个弹珠代表一个单独的事件。为了区分它们，**t1** 上的事件有一条条纹，**t2** 上的事件有两条条纹，**t3**
    上的事件有三条条纹。
- en: 'We want to count the events per unit of time, so we have the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想按单位时间计算事件，所以我们有以下：
- en: '**t1** has six events'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t1** 有六个事件'
- en: '**t2** has four events'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t2** 有四个事件'
- en: '**t3** has three events'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t3** 有三个事件'
- en: 'As systems have failures (such as network latency, shutdown of servers, network
    partitioning, power failures, voltage variations, and so on), suppose that an
    event that happened during **t2** has a delay and reached our system at **t3,**
    shown as follows in *Figure 6.2*:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于系统可能发生故障（例如网络延迟、服务器关闭、网络分区、电源故障、电压变化等），假设在 **t2** 发生的事件有一个延迟，并在 **t3** 时到达我们的系统，如下
    *图 6.2* 所示：
- en: '![](img/4402dcaa-bf85-4335-9fda-37b380982e3c.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4402dcaa-bf85-4335-9fda-37b380982e3c.png)'
- en: 'Figure 6.2: The events as they were processed'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：事件的处理过程
- en: 'If we count our events using the **processing time**, we have the following
    results:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用**处理时间**来计数事件，我们得到以下结果：
- en: '**t1** has six events'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t1** 有六个事件'
- en: '**t2** has three events'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t2** 有三个事件'
- en: '**t3** has four events'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**t3** 有四个事件'
- en: If we have to calculate how many events were produced per time unit, our results
    would be incorrect.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们必须计算每个时间单位产生的事件数量，我们的结果将是错误的。
- en: 'The event that arrived on **t3** instead of **t2** is called a late event.
    We just have two alternatives, they are given as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **t3** 而不是 **t2** 到达的事件被称为延迟事件。我们只有两个选择，如下所示：
- en: 'When **t2** ends, produce a preliminary result that the count for **t2** is
    three events. And then, during processing, when we find in another time an event
    belonging to **t2**, we update the result for **t2**: **t2** has four events.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 **t2** 结束时，生成一个初步结果，即 **t2** 的计数为三个事件。然后，在处理过程中，当我们发现另一个时间的事件属于 **t2** 时，我们更新
    **t2** 的结果：**t2** 有四个事件。
- en: When each window ends, we wait a little after the end before we produce a result.
    For example, we could wait another time unit. In this case, the results for tn
    are obtained when t(n+1) ends. Remember, the time to wait to produce results might
    not be related to the time unit size.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当每个窗口结束时，我们在生成结果之前会稍作等待。例如，我们可能再等待一个时间单位。在这种情况下，当 t(n+1) 结束时，我们获得 tn 的结果。记住，等待生成结果的时间可能并不与时间单位的大小相关。
- en: As you can guess, these scenarios are quite common in practice, and there are
    currently many interesting proposals. One of the most complete and advanced suites
    for handling late events is the Apache Beam proposal. However, Apache Spark, Apache
    Flink, and Akka Streams are also very powerful and attractive.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，这些场景在实践中相当常见，目前有许多有趣的提议。处理延迟事件最完整和先进的套件之一是 Apache Beam 提案。然而，Apache Spark、Apache
    Flink 和 Akka Streams 也非常强大和吸引人。
- en: As we want to see how it is solved with Kafka Streams here, let's see that.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想看看如何使用 Kafka Streams 解决这个问题，让我们看看。
- en: Late event generation
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟事件生成
- en: To test the Kafka Streams solution for late events, the first thing we need
    is a late event generator.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试 Kafka Streams 的延迟事件解决方案，我们首先需要一个延迟事件生成器。
- en: 'To simplify things, our generator will constantly send events at a fixed rate.
    And from time to time, it will generate a late event. The generator generates
    events with the following process:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们的生成器将以固定的速率不断发送事件。偶尔，它还会生成一个延迟事件。生成器按照以下过程生成事件：
- en: Each window is 10 seconds long
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个窗口长度为 10 秒
- en: It produces one event every second
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它每秒产生一个事件
- en: The event should be generated in 54^(th) second of each minute, and will be
    delayed by 12 seconds; that is, it will arrive in the sixth second of the next
    minute (in the next window)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件应该在每分钟的 54 秒生成，并将延迟 12 秒；也就是说，它将在下一分钟的第六秒到达（在下一个窗口）
- en: When we say that the window is of 10 seconds, we mean that we will make aggregations
    every 10 seconds. Remember that the objective of the test is that the late events
    are counted in the correct window.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说窗口是 10 秒时，我们的意思是我们将每 10 秒进行一次聚合。记住，测试的目标是确保延迟事件被计入正确的窗口。
- en: 'Create the `src/main/java/kioto/events` directory and, inside it, create a
    file called `EventProducer.java` with the contents of *Listing 6.5*, shown as
    follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `src/main/java/kioto/events` 目录，并在其中创建一个名为 `EventProducer.java` 的文件，其内容如
    *列表 6.5* 所示，如下所示：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Listing 6.5: EventProducer.java'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.5：EventProducer.java
- en: The event generator is a Java `KafkaProducer`, so declare the same properties
    as all the Kafka Producers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 事件生成器是一个 Java `KafkaProducer`，因此需要声明与所有 Kafka Producers 相同的属性。
- en: 'The generator code is very simple, and the first thing that is required is
    a timer that generates an event every second. The timer triggers 0.3 seconds after
    every second to avoid messages sent at 0.998 seconds, for example. The `produce()`
    method is shown as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器代码非常简单，首先需要的是一个每秒生成一个事件的计时器。计时器在每秒后的 0.3 秒触发，以避免在 0.998 秒发送消息，例如。`produce()`
    方法如下所示：
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: When the timer is triggered, the run method is executed. We send one event each
    second except on second 54, where we delay this event by 12 seconds. Then, we
    send this late event in the sixth second of the next minute, modifying the timestamp.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当计时器触发时，执行run方法。我们每秒发送一个事件，除了第54秒，我们延迟这个事件12秒。然后，我们在下一分钟的第六秒发送这个延迟的事件，修改时间戳。
- en: 'In the `sendMessage()` method, we just assign the timestamp of the event, shown
    as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sendMessage()`方法中，我们只是分配事件的戳记，如下所示：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Running the EventProducer
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行EventProducer
- en: 'To run the `EventProducer`, we follow these steps:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行`EventProducer`，我们遵循以下步骤：
- en: 'Create the events topic, as shown in the following block:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建事件主题，如下所示：
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Run a console consumer for the events topic using the following command:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令运行事件主题的控制台消费者：
- en: '[PRE46]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: From the IDE, run the main method of the `EventProducer`.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从IDE中运行`EventProducer`的main方法。
- en: 'The output on the console consumer for the events topic should be similar to
    the following:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事件主题的控制台消费者的输出应类似于以下内容：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note that each event window changes every 10 seconds. Also, note how the 54^(th)
    event is not sent between the 53^(rd) and 55^(th) events. The 54^(th) event, belonging
    to a previous window, arrives in the next minute between the sixth and seventh
    seconds.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个事件窗口每10秒变化一次。还要注意第54个事件在53个和55个事件之间没有发送。第54个事件属于前一个窗口，在下一分钟的第六秒和第七秒之间到达。
- en: Kafka Streams processor
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams处理器
- en: Now, let's solve the problem of counting how many events are in each window.
    For this, we will use Kafka Streams. When we do this type of analysis, it is said
    that we are doing **streaming aggregation**.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解决如何计算每个窗口中有多少事件的问题。为此，我们将使用Kafka Streams。当我们进行此类分析时，我们称之为**流聚合**。
- en: 'In the `src/main/java/kioto/events` directory, create a file called `EventProcessor.java`
    with the contents of *Listing 6.6*, shown as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在`src/main/java/kioto/events`目录下，创建一个名为`EventProcessor.java`的文件，包含*列表6.6*的内容，如下所示：
- en: '[PRE48]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Listing 6.6: EventProcessor.java'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6：EventProcessor.java
- en: 'All the processing logic is contained in the `process()` method. The first
    step is to create a `StreamsBuilder` to create the `KStream`, shown as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的处理逻辑都包含在`process()`方法中。第一步是创建一个`StreamsBuilder`来创建`KStream`，如下所示：
- en: '[PRE49]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: As we know, we specify from topic we are reading the events in this case is
    called **events**, and then we always specify the `Serdes`, both keys and values
    of type `String`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，我们指定从主题读取事件，在这种情况下称为**events**，然后我们总是指定`Serdes`，键和值都是`String`类型。
- en: If you remember, we have each step as a transformation from one stream to another.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得，我们每个步骤都是一个从一条流到另一条流的转换。
- en: 'The next step is to build a `KTable`. To do so, we first use the `groupBy()`
    function, which receives a key-value pair, and we assign a key called `"foo"`,
    because it is not relevant but we need to specify one. Then, we apply the `windowedBy()`
    function, specifying that the window will be 10 seconds long. Finally, we use
    the `count()` function, so we are producing key-value pairs with `String` as keys
    and `long` as values. This number is the count of the events for each window (the
    key is the window start time):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建一个`KTable`。要做到这一点，我们首先使用`groupBy()`函数，它接收一个键值对，我们分配一个名为`"foo"`的键，因为它并不重要，但我们需要指定一个。然后，我们应用`windowedBy()`函数，指定窗口长度为10秒。最后，我们使用`count()`函数，因此我们产生键值对，其中键是`String`类型，值是`long`类型。这个数字是每个窗口的事件计数（键是窗口开始时间）：
- en: '[PRE50]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'If you have problems with the conceptual visualization of the `KTable`, which
    keys are of type `KTable<Windowed<String>>` and values are of type `long`, and
    printing it (in the KSQL chapter we will see how to do it), would be something
    like the one, as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到关于`KTable`的概念可视化问题，比如哪些键是`KTable<Windowed<String>>`类型，值是`long`类型，并且打印它（在KSQL章节中我们将看到如何做），可能会像下面这样：
- en: '[PRE51]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The key has the window ID and the utility aggregation key with value `"foo"`.
    The value is the number of elements counted in the window at a specific point
    of time.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 键包含窗口ID和具有值`"foo"`的实用聚合键。值是在特定时间点窗口中计数的元素数量。
- en: 'Next, as we need to output the `KTable` to a topic, we need to convert it to
    a `KStream` as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于我们需要将`KTable`输出到主题，我们需要将其转换为`KStream`，如下所示：
- en: '[PRE52]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The `toStream()` method of the `KTable` returns a `KStream`. We use a `map()`
    function that receives two values, the window and the count, then we extract the
    window start time as the key and the count as the value. The `to()` method specifies
    to which topic we want to output (always specifying the serdes as a good practice).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`KTable`的`toStream()`方法返回一个`KStream`。我们使用一个`map()`函数，该函数接收两个值，窗口和计数，然后我们提取窗口开始时间作为键，计数作为值。`to()`方法指定我们想要输出到哪个主题（始终指定serdes作为良好实践）。'
- en: 'Finally, as in previous sections, we need to start the topology and the application,
    shown as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如前几节所述，我们需要启动拓扑和应用，如下所示：
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Remember that the `commit.interval.ms` property indicates how many milliseconds
    we will wait to write the results to the `aggregates` topic.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`commit.interval.ms`属性表示我们将等待多少毫秒将结果写入`aggregates`主题。
- en: Running the Streams processor
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Streams处理器
- en: 'To run the `EventProcessor`, follow these steps:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`EventProcessor`，请按照以下步骤操作：
- en: 'Create the `aggregates` topic as follows:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式创建`aggregates`主题：
- en: '[PRE54]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Run a console consumer for the aggregates topic, as follows:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式运行`aggregates`主题的控制台消费者：
- en: '[PRE55]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: From the IDE, run the main method of the `EventProducer`.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从IDE中运行`EventProducer`的main方法。
- en: From the IDE, run the main method of the `EventProcessor`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从IDE中运行`EventProcessor`的main方法。
- en: 'Remember that it writes to the topic every 30 seconds. The output on the console
    consumer for the aggregates topic should be similar to the following:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，它每30秒向主题写入一次。`aggregates`主题的控制台消费者的输出应类似于以下内容：
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'After the second window, we can see that the values in the `KTable` are updated
    with fresh (and correct) data, shown as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个窗口之后，我们可以看到`KTable`中的值使用新鲜（且正确）的数据进行了更新，如下所示：
- en: '[PRE57]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note how in the first print, the value for the last window is 3, and the window
    started in `1532529070000` has a value of `9`. Then in the second print, the values
    are correct. This behavior is because in the first print, the delayed event had
    not arrived yet. When this finally arrived, the count values were corrected for
    all the windows.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在第一次打印中，最后一个窗口的值为3，窗口在`1532529070000`开始，其值为`9`。然后在第二次打印中，值是正确的。这种行为是因为在第一次打印中，延迟的事件尚未到达。当这个事件最终到达时，所有窗口的计数值都被更正。
- en: Stream processor analysis
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Streams处理器分析
- en: If you have a lot of questions here, it is normal.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这里有很多问题，这是正常的。
- en: The first thought to consider is that in streaming aggregation, and in streaming
    in general, the Streams are unbounded. It is never clear when we will take the
    final results, that is, we as programmers have to decide when to consider a partial
    value of an aggregation as a final result.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要考虑的是，在流聚合和流处理中，Streams是无界的。我们永远不清楚何时会得到最终结果，也就是说，作为程序员，我们必须决定何时将聚合的部分值视为最终结果。
- en: 'Recall that the print of the Stream is an instant photo of the `KTable` at
    a certain time. Therefore, the results of a `KTable` are only valid at the time
    of the output. It is important to remember that in the future, the values of the
    `KTable` may be different. Now, to see results more frequently, change the value
    of the commit interval to zero, shown as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，流的打印是某个时间点的`KTable`的瞬间快照。因此，`KTable`的结果仅在输出时有效。重要的是要记住，在未来，`KTable`的值可能会有所不同。现在，为了更频繁地看到结果，将提交间隔的值更改为零，如下所示：
- en: '[PRE58]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This line says that the results of the `KTable` will be printed when they are
    modified, that is, it will print new values every second. If you run the program,
    the value of the `KTable` will be printed with each update (every second), shown
    as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这行说明当`KTable`被修改时，将打印其结果，也就是说，它将每秒打印新值。如果你运行程序，`KTable`的值将在每次更新（每秒）时打印，如下所示：
- en: '[PRE59]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Keep a note of two effects:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下两个效果：
- en: The aggregate result (the count) for the window stops at 9 when the window ends
    and the next window events begin to arrive
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当窗口结束时，窗口的聚合结果（计数）停止在9，并且下一个窗口的事件开始到达
- en: When the late event finally arrives, it produces an update in the window's count
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当延迟事件最终到达时，它会在窗口的计数中产生更新
- en: Yes, Kafka Streams apply event time semantics in order to do the aggregation.
    It is important to remember that in order to visualize the data, we had to modify
    the commit interval. Leaving this value at zero would have negative repercussions
    on a production environment.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，Kafka Streams应用事件时间语义来进行聚合。重要的是要记住，为了可视化数据，我们必须修改提交间隔。将此值保留为零会对生产环境产生负面影响。
- en: As you may guess, processing an event stream is much more complex than processing
    a fixed dataset. The events usually arrive late, in disorder, and it is practically
    impossible to know when the totality of the data has arrived. How do you know
    when there are late events? If there is, how much should we expect for them? When
    should we discard a late event?
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，处理事件流比处理固定数据集要复杂得多。事件通常迟到，无序，实际上很难知道何时所有数据都已经到达。你如何知道有迟到的事件？如果有，我们应该期望它们有多少？何时应该丢弃一个迟到的事件？
- en: The quality of a programmer is determined by the quality of their tools. The
    capabilities of the processing tool make a big difference when processing data.
    In this context, we have to reflect when the results are produced and when they
    arrived late.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员的质量取决于他们工具的质量。处理工具的能力在处理数据时会产生很大影响。在这种情况下，我们必须反思结果何时产生以及何时到达较晚。
- en: 'The process of discarding events has a special name: watermarking. In Kafka
    Streams, this is achieved through setting the aggregation windows'' retention
    period.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃事件的过程有一个特殊名称：水印。在 Kafka Streams 中，这是通过设置聚合窗口的保留期来实现的。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Kafka Streams is a powerful library, and is the only option when building data
    pipelines with Apache Kafka. Kafka Streams removes much of the boilerplate work
    needed when implementing plain Java clients. Compared to Apache Spark or Apache
    Flink, the Kafka Streams applications are much simpler to build and manage.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 是一个强大的库，当使用 Apache Kafka 构建数据管道时，它是唯一的选择。Kafka Streams 移除了实现纯
    Java 客户端时所需的大量样板工作。与 Apache Spark 或 Apache Flink 相比，Kafka Streams 应用程序构建和管理起来要简单得多。
- en: We also have seen how to improve a Kafka Streams application to deserialize
    data in JSON and Avro formats. The serialization part (writing to a topic) is
    very similar since we are using SerDes that are capable of both data serialization
    and deserialization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也看到了如何改进 Kafka Streams 应用以反序列化 JSON 和 Avro 格式的数据。由于我们使用的是能够进行数据序列化和反序列化的 SerDes，因此序列化部分（写入主题）非常相似。
- en: For those who work with Scala, there is a library for Kafka Streams called circe
    that offers SerDes to manipulate JSON data. The circe library is the equivalent
    in Scala of the Jackson library.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 Scala 的开发者来说，有一个名为 circe 的 Kafka Streams 库，它提供了 SerDes 来操作 JSON 数据。circe
    库在 Scala 中相当于 Jackson 库。
- en: As mentioned earlier, Apache Beam has a more complex suite of tools, but is
    totally focused on Stream management. Its model is based on triggers and semantics
    between events. It also has a powerful model for watermark handling.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Apache Beam 拥有更复杂的工具集，但完全专注于流管理。其模型基于触发器和事件之间的语义。它还有一个强大的水印处理模型。
- en: One notable advantage of Kafka Streams over Apache Beam is that its deployment
    model is simpler. This leads many developers to adopt it. However, for more complex
    problems, Apache Beam may be a better tool.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 相比于 Apache Beam 的一个显著优势是其部署模型更为简单，这使得许多开发者倾向于采用它。然而，对于更复杂的问题，Apache
    Beam 可能是更好的工具。
- en: 'In the following chapters, we will talk about how to get the best of two worlds:
    Apache Spark and Kafka Streams.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论如何充分利用两个世界：Apache Spark 和 Kafka Streams。
