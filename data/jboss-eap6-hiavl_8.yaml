- en: Chapter 8. Developing Distributed Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 开发分布式应用程序
- en: In the previous chapters, we have all concentrated on building a stateless cluster.
    That means we don't need to maintain a session for each user request, and a load
    balancer can freely choose a worker node to serve the user requests.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们一直专注于构建无状态集群。这意味着我们不需要为每个用户请求维护会话，负载均衡器可以自由选择一个工作节点来处理用户请求。
- en: A stateless cluster is more flexible and can scale well, so it's always the
    first choice when we are building a cluster. In essence, HTTP is a stateless protocol,
    so it lacks the ability to maintain a session for user requests. To solve this
    problem, web servers usually pass a session ID to the users' web browsers to maintain
    a long conversation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态集群更加灵活，并且可以很好地扩展，因此在我们构建集群时，它总是首选。本质上，HTTP是一个无状态协议，因此它缺乏维护用户请求会话的能力。为了解决这个问题，Web服务器通常会向用户的Web浏览器传递一个会话ID以维持长时间的对话。
- en: For example, if we are building an online shopping system, we have to maintain
    a shopping cart for each user. When a user is checking out his/her cart, the total
    price of the goods in the shopping cart will be calculated. All this data needs
    to be stored either on the server side or in the cookies of users' web browsers
    and the data needs to be held across multiple pages, so the session ID is the
    key to refer to this data of a user. For JBoss EAP, the session ID is called JSESSIONID.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们正在构建一个在线购物系统，我们必须为每个用户维护一个购物车。当用户检查他们的购物车时，购物车中商品的总价将被计算。所有这些数据都需要存储在服务器端或用户Web浏览器的cookies中，并且数据需要在多个页面之间保持，所以会话ID是引用这些用户数据的键。对于JBoss
    EAP，会话ID被称为JSESSIONID。
- en: In a clustering environment, the situation becomes more complex, because there
    are multiple servers instead of just one, so their statuses need to be replicated.
    For example, if a worker node `A` is serving one user's request, then the data
    of the shopping cart may be saved on the worker node `A`. If the load balancer
    now redirects the user request to the worker node `B`, then the user will find
    that his/her shopping cart becomes empty. Even if the JSESSIONID is passed to
    worker `B`, the data related with the JSESSIONID is stored in worker `A`. So the
    data of the user still gets lost.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群环境中，情况变得更加复杂，因为这里有多台服务器而不是一台，所以需要复制它们的状态。例如，如果一个工作节点 `A` 正在处理一个用户的请求，那么购物车的数据可能就保存在工作节点
    `A` 上。如果负载均衡器现在将用户请求重定向到工作节点 `B`，那么用户会发现他的/她的购物车变空了。即使JSESSIONID被传递给工作节点 `B`，与JSESSIONID相关的数据仍然存储在工作节点
    `A` 上。所以用户的数据仍然会丢失。
- en: There are two ways that are commonly used to solve this problem. The first option
    is named **sticky sessions** . This is a straightforward way to solve the problem.
    It means the load balancer will stick a user session to a specific worker node.
    For example, if one user visits our website and the load balancer chooses worker
    node `A` to serve the request, then this worker node will be used forever to serve
    the following requests from this user until he or she quits the web browser or
    the session ends.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常用的方法来解决这个问题。第一种选择被称为**粘性会话**。这是一种直接解决问题的方法。这意味着负载均衡器将用户会话粘附到特定的一个工作节点上。例如，如果一个用户访问我们的网站，负载均衡器选择工作节点
    `A` 来处理请求，那么这个工作节点将永远用于处理该用户后续的请求，直到他/她退出Web浏览器或会话结束。
- en: This solution is easy to apply, and it fits many situations in practice. However,
    it only partly solves the problem, because a worker node may fail, and the load
    balancer would like to failover the user requests to another worker node. In this
    situation, all the sessions on the crashed worker node will still get lost.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案易于应用，并且适合实践中的许多情况。然而，它只部分解决了问题，因为工作节点可能会失败，负载均衡器会希望将用户请求故障转移到另一个工作节点。在这种情况下，崩溃的工作节点上的所有会话仍然会丢失。
- en: So here is the second solution to avoid the preceding problem, it is better
    for the worker nodes to replicate the session data among one another. Thus when
    one worker crashes, its session can be restored from other workers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是避免上述问题的第二种解决方案，即工作节点之间相互复制会话数据会更好。这样，当一个工作节点崩溃时，它的会话可以从其他工作节点恢复。
- en: In this chapter, we will learn how to configure session replications between
    EAP6 servers, and then we'll see how to configure sticky sessions in httpd.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何配置EAP6服务器之间的会话复制，然后我们将看到如何配置httpd中的粘性会话。
- en: Web session replication
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web会话复制
- en: 'EAP6 provides web session replication out of the box when it''s running in
    the domain mode (or in the standalone mode with the `*-ha` profile enabled). The
    session replication is supported by the Infinispan subsystem, and the session
    container is defined in `domain.xml` (and `standalone-*-ha.xml`):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当EAP6以域模式（或启用`*-ha`配置文件的单机模式）运行时，它将默认提供Web会话复制。会话复制由Infinispan子系统支持，会话容器在`domain.xml`（和`standalone-*-ha.xml`）中定义：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this section, we'll use a sample project to demonstrate the usage of web
    session replication. The project is named as `clusterbench`. It has been developed
    by my colleagues *Radoslav Husar* and *Michal Babacek* at Red Hat.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个示例项目来演示Web会话复制的使用方法。该项目命名为`clusterbench`。它是由我的同事*Radoslav Husar*和*Michal
    Babacek*在Red Hat开发的。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The project is located at [https://github.com/clusterbench/clusterbench](https://github.com/clusterbench/clusterbench).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目位于[https://github.com/clusterbench/clusterbench](https://github.com/clusterbench/clusterbench)。
- en: This project has some excellent demonstration codes for us to use. So we'll
    directly deploy it into our EAP6 servers for testing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此项目为我们提供了一些优秀的演示代码。因此，我们将直接将其部署到我们的EAP6服务器上进行测试。
- en: 'In the demo project, there is a submodule called `clusterbench-ee6-web`. In
    this module, we can see how the session is enabled in `web.xml`. It uses a single
    line of configuration to enable web session replication as shown in the following
    screenshot:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在演示项目中，有一个名为`clusterbench-ee6-web`的子模块。在这个模块中，我们可以看到如何在`web.xml`中启用会话。它使用一行配置来启用Web会话复制，如下面的截图所示：
- en: '![Web session replication](img/2432_08_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![Web会话复制](img/2432_08_01.jpg)'
- en: 'With `distributable` enabled in `web.xml`, the web sessions will be replicated
    across the EAP6 servers. This is a JavaEE standard requirement. As JBoss EAP6
    conforms to the JavaEE standard, it supports this feature. This web project has
    also provided us a servlet for testing:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在`web.xml`中启用`distributable`后，Web会话将在EAP6服务器之间进行复制。这是一个JavaEE标准要求。由于JBoss EAP6符合JavaEE标准，因此它支持此功能。此Web项目还为我们提供了一个用于测试的servlet：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding class `HttpSessionServlet` extends the `CommonHttpSessionServlet`.
    The `CommonHttpSessionServlet` is defined in `clusterbench-common`. Here is an
    abstract of the `CommonHttpSessionServlet`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的类`HttpSessionServlet`扩展了`CommonHttpSessionServlet`。`CommonHttpSessionServlet`在`clusterbench-common`中定义。以下是`CommonHttpSessionServlet`的摘要：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The main purpose of this servlet is to put a counter into the web session,
    and each time a user sends a request, the counter will increase by 1\. Please
    note I''ve added a line of code in the preceeding class:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此servlet的主要目的是将计数器放入Web会话中，每次用户发送请求时，计数器将增加1。请注意，我在前面的类中添加了一行代码：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So we can see the output from the server console later. Now we can deploy this
    project into our cluster, and then we can access the servlet to use the counter.
    We can see the server output to determine which node is actually serving this
    request. Then we shutdown the working node and access the cluster again. We should
    expect another EAP6 server to serve the request. If the sessions are replicated
    successfully, we should see the counter is not reset, and it goes on increasing.
    In conclusion, this is an example that demonstrates the session replication among
    EAP6 servers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以在稍后查看服务器控制台输出。现在我们可以将此项目部署到我们的集群中，然后我们可以访问servlet来使用计数器。我们可以查看服务器输出以确定哪个节点正在实际处理此请求。然后关闭工作节点并再次访问集群。我们应该期望另一个EAP6服务器来处理请求。如果会话复制成功，我们应该看到计数器没有重置，而是继续增加。总之，这是一个演示EAP6服务器之间会话复制的示例。
- en: To do the testing, we could use the cluster we've set in the previous chapters,
    using either `JK` or `mod_cluster` as the load balancer, and then deploy the project
    `clusterbench-ee6.ear` into the EAP6 domain.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行测试，我们可以使用上一章中设置的集群，使用`JK`或`mod_cluster`作为负载均衡器，然后将项目`clusterbench-ee6.ear`部署到EAP6域中。
- en: 'After the preceding preparations are done and both the load balancer and EAP6
    servers are running, let''s access the cluster by **cURL** for the first time:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成前面的准备工作并且负载均衡器和EAP6服务器都运行后，让我们第一次通过**cURL**访问集群：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We see that the counter value is set to `0`. The `-cmysession.txt` option tells
    cURL to store the session cookie in a file named `mysession.txt`. We will check
    this file later. Now we can check the server side. From the EAP6 server console
    output, you can see the master is serving the user request:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到计数器值被设置为`0`。`-cmysession.txt`选项告诉cURL将会话cookie存储在名为`mysession.txt`的文件中。我们稍后会检查这个文件。现在我们可以检查服务器端。从EAP6服务器控制台输出中，我们可以看到主服务器正在处理用户请求：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding console output of the master server, we can see a new session
    was created for the counter, and the session ID is `5LQpRPxdSCupM5eHYd93S2wR`.
    In addition, we see the counter is initialized to `0`, which matches the result
    from the client side.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在主服务器的先前控制台输出中，我们可以看到为计数器创建了一个新的会话，会话ID为`5LQpRPxdSCupM5eHYd93S2wR`。此外，我们看到计数器被初始化为`0`，这与客户端的结果相匹配。
- en: 'Now let''s go back to client side and check `mysession.txt`. Here are the contents
    of the file:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到客户端并检查`mysession.txt`文件。以下是文件内容：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can see that the JSESSIONID is stored in cookies. Now let''s use this cookie
    file to access the cluster again:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到JSESSIONID存储在cookie中。现在让我们使用这个cookie文件再次访问集群：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `-b` option will let cURL read an existing cookie file and send the cookies
    to the server, which means the previous session is continued. Because the counter
    increments by 1, it means our session is held by JSESSIONID. We can check the
    output of the EAP6 server again:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`-b`选项将让cURL读取一个现有的cookie文件并将cookie发送到服务器，这意味着之前的会话将继续。因为计数器增加1，这意味着我们的会话由JSESSIONID保持。我们可以再次检查EAP6服务器的输出：'
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'So the counter on the master is not reset, and it keeps increasing in one session.
    Now let''s shutdown the master server:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，主服务器上的计数器没有重置，它在一个会话中持续增加。现在让我们关闭主服务器：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we access the cluster again:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们再次访问集群：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Because the master server is down, this time it''s the slave server serving
    the request:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因为主服务器已关闭，这次是从服务器在处理请求：
- en: '[PRE11]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Though the request was redirected to the slave server, the session is held,
    and the counter increased from 1 to 2\. This verified that the session replication
    works properly between two servers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管请求被重定向到从服务器，但会话保持不变，计数器从1增加到2。这验证了两个服务器之间会话复制工作正常。
- en: CDI-session-scoped bean replication
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CDI会话作用域bean复制
- en: 'The usage of a CDI-session-scoped bean is similar to a web session bean. In
    the demo project, it provides a `CdiServlet` for testing:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CDI会话作用域bean的使用与web会话bean类似。在演示项目中，它提供了一个`CdiServlet`用于测试：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This servlet is also a counter, and it uses a session scoped CDI bean named
    `SessionScopedCdiSerialBean`. Here is the definition of this bean:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个servlet也是一个计数器，它使用一个名为`SessionScopedCdiSerialBean`的会话作用域CDI bean。以下是这个bean的定义：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The bean is declared as `SessionScoped`, so it will be replicated across the
    cluster. The `SerialBean` is a POJO that holds the counter. Now we can test it
    in our cluster. First we need to access the servlet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该bean被声明为`SessionScoped`，因此它将在集群中复制。`SerialBean`是一个包含计数器的POJO。现在我们可以在我们的集群中测试它。首先我们需要访问servlet：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And then we need to check which EAP server is serving the user request. In
    my environment, the master server is serving the request:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要检查哪个EAP服务器正在处理用户请求。在我的环境中，主服务器正在处理请求：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In `mysession.txt`, we can see the JESSIONID is stored:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mysession.txt`中，我们可以看到JESSIONID被存储：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now I disconnect the master server by shutting it down, and access the cluster
    with the session cookie again:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我通过关闭主服务器来断开连接，并再次使用会话cookie访问集群：
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can see that the slave server is serving the request:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到从服务器正在处理请求：
- en: '[PRE18]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As shown in the preceding code snippet, we can see that the session was replicated
    from master to slave.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，我们可以看到会话已从主服务器复制到从服务器。
- en: Configuring sticky sessions with JK
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用JK配置粘性会话
- en: 'In the previous sections, we have looked at how to configure and use session
    replication in EAP6\. In this section, let''s move to the load balancer side and
    see how we can configure a sticky session. With sticky session enabled, the load
    balancer will use one worker node to serve all the requests from one user. Let''s
    start from the JK configuration. The sticky session is automatically enabled with
    JK. We can check this in its management console as shown in the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们探讨了如何在EAP6中配置和使用会话复制。在本节中，让我们转向负载均衡器一侧，看看我们如何配置一个粘性会话。启用粘性会话后，负载均衡器将使用一个工作节点来处理来自一个用户的全部请求。让我们从JK配置开始。JK会自动启用粘性会话。我们可以在其管理控制台中查看这一点，如下面的截图所示：
- en: '![Configuring sticky sessions with JK](img/2432_08_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![使用 JK 配置粘性会话](img/2432_08_06.jpg)'
- en: 'From the preceding diagram, we can see the **Sticky Sessions** option is enabled
    by default. Now we need to consider how a load balancer implements session stickiness:
    if there are thousands of user requests coming to a cluster, and if sticky session
    is enabled, then the requests of each user is stuck to a specific worker node.
    So the load balancer needs some way to record this relationship.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到 **粘性会话** 选项默认是启用的。现在我们需要考虑负载均衡器如何实现会话粘性：如果有成千上万的用户请求进入一个集群，并且启用了粘性会话，那么每个用户的请求都会粘附到一个特定的工作节点。因此，负载均衡器需要某种方式来记录这种关系。
- en: Storing the relationship in the load balancer is not a good idea. The relationships
    data will increase linearly by the number of users. The situation becomes worse
    if there are multiple load balancers, and then the stickiness relationship has
    to be replicated across the load balancers. The load balancer cannot afford to
    maintain this huge data and its performance will be throttled by querying the
    stickiness relationship.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在负载均衡器中存储这种关系不是一个好主意。关系数据将随着用户数量的线性增长而增加。如果有多个负载均衡器，情况会变得更糟，粘性关系必须在负载均衡器之间复制。负载均衡器无法承担维护如此大量数据的工作，并且其性能将因查询粘性关系而受限。
- en: 'To solve this problem, both `JK` and `mod_clusteruse` offer a simpler solution:
    it will put a server ID called `jvmRoute` in the JSESSIONID. The `jvmRoute` value
    is UUID, so it can be used to identify each worker node. As the `jvmRoute` becomes
    part of the session ID, the load balancer will directly extract it from the JSESSIONID
    and knows which server this session is bound to.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，`JK` 和 `mod_clusteruse` 都提供了一个更简单的解决方案：它将在 JSESSIONID 中放置一个名为 `jvmRoute`
    的服务器 ID。`jvmRoute` 的值是 UUID，因此它可以用来识别每个工作节点。随着 `jvmRoute` 成为会话 ID 的一部分，负载均衡器将直接从
    JSESSIONID 中提取它，并知道这个会话绑定到哪个服务器。
- en: 'To enable sticky session, we need to edit the configuration of EAP6 to set
    this server ID. What we should do is open `domain.xml` and add an `instance-id`
    element in the web subsystem:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用粘性会话，我们需要编辑 EAP6 的配置来设置此服务器 ID。我们应该做的是打开 `domain.xml` 并在 web 子系统中添加一个 `instance-id`
    元素：
- en: '[PRE19]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The element `instance-id` is the value of `jvmRoute`. We've used `${jboss.server.name}`
    to be its value. This is a variable provided by EAP6, its value is the server
    name set in `host.xml`. So we know the value of `instance-id` for our two EAP6
    servers are `master-server` and `slave-server`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`instance-id` 元素是 `jvmRoute` 的值。我们使用了 `${jboss.server.name}` 作为其值。这是一个由 EAP6
    提供的变量，其值是 `host.xml` 中设置的服务器名称。因此，我们知道我们两个 EAP6 服务器的 `instance-id` 值分别是 `master-server`
    和 `slave-server`。'
- en: 'To reflect the configuration in EAP6, we need to put these two names into `worker.properties`
    in the httpd side, so that JK will know the name of its workers. Here are the
    complete contents of `worker.properties`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 EAP6 中反映配置，我们需要将这些名称放入 httpd 侧的 `worker.properties` 中，这样 JK 就会知道其工作者的名称。以下是
    `worker.properties` 的完整内容：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We must ensure that the worker name corresponds to the `instance-id` settings
    in the domain controller, so JK can find the correct servers that the session
    sticks to.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保工作器名称与域控制器中的 `instance-id` 设置相匹配，这样 JK 才能找到会话粘附的正确服务器。
- en: 'Now we can test our cluster with the `clusterbench` project deployed in the
    previous sections. We can still use the cURL command to access the cluster:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以测试之前章节中部署的 `clusterbench` 项目。我们仍然可以使用 cURL 命令访问集群：
- en: '[PRE21]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From `mysession.txt`, the JSESSIONID is:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `mysession.txt` 中，JSESSIONID 是：
- en: '[PRE22]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see the session is divided into two parts separated by a dot now. The
    first part is still the session ID and the second part is the `jvmRoute` carried
    in session, and its value is `master-server`. In the server output, you can also
    notice that the session has been created and the session ID displayed on `stdout`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到会话被分成两部分，由点分隔。第一部分仍然是会话 ID，第二部分是会话中携带的 `jvmRoute`，其值为 `master-server`。在服务器输出中，你还可以注意到会话已经创建，会话
    ID 显示在 `stdout` 上：
- en: '[PRE23]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With the information in `jvmRoute`, load balancer will stick the following requests
    from the user to `mast` `er-server`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 `jvmRoute` 中的信息，负载均衡器将粘附以下用户请求到 `mast` `er-server`。
- en: Configuring sticky sessions with mod_cluster
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 mod_cluster 配置粘性会话
- en: To enable sticky sessions in `mod_cluster`, we need to add some configuration
    in the `mod_cluster` subsystem of EAP6\. For the standalone mode, we can configure
    the `*-ha.xml` profiles that contain the `mod_cluster` subsystem; for the domain
    mode, we can edit `domain.xml` of the domain controller.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要在`mod_cluster`中启用粘性会话，我们需要在EAP6的`mod_cluster`子系统中添加一些配置。对于独立模式，我们可以配置包含`mod_cluster`子系统的`*-ha.xml`配置文件；对于域模式，我们可以编辑域控制器的`domain.xml`。
- en: 'The sticky session is enabled by default by the `mod_cluster` subsystem. Meanwhile,
    `mod_cluster` uses the same scheme like JK to handle session stickiness, so we
    should also add the `instance-id` configuration in the web subsystem:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`mod_cluster`子系统启用了粘性会话。同时，`mod_cluster`使用与JK相同的方案来处理会话粘性，因此我们还需要在Web子系统中添加`instance-id`配置：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'That''s all we need to configure. We don''t need to do any configuration on
    the httpd side, because `mod_cluster` will discover the worker node dynamically.
    Now we can start our cluster and check the management console of `mod_cluster`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的所有配置。我们不需要在httpd端进行任何配置，因为`mod_cluster`将动态发现工作节点。现在我们可以启动我们的集群并检查`mod_cluster`的管理控制台：
- en: '![Configuring sticky sessions with mod_cluster](img/2432_08_11.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![使用mod_cluster配置粘性会话](img/2432_08_11.jpg)'
- en: 'From the previous screenshot, we can see two EAP6 server names become `master-server`
    and `slave-server`, which means the setting of `instance-id` is enabled. Now we
    access our cluster:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图，我们可以看到两个EAP6服务器名称变为`master-server`和`slave-server`，这意味着`instance-id`的设置已被启用。现在我们访问我们的集群：
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And then we check the contents of `mysession.txt`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们检查`mysession.txt`的内容：
- en: '[PRE26]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We can see the JSESSIONID carries the `jvmRoute` information now. So httpd will
    send the following requests from the user to the master server.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到JSESSIONID携带了`jvmRoute`信息。因此，httpd将从用户发送以下请求到主服务器。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed two solutions that handle the stateful applications
    in clusters. One is sticky sessions and the other is session replication. These
    two solutions are usually used together to provide high availability in a Stateful
    cluster.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了两种处理集群中状态化应用的解决方案。一个是粘性会话，另一个是会话复制。这两个解决方案通常一起使用，以在状态化集群中提供高可用性。
- en: When we are building a cluster, we should always consider building a stateless
    one at first because a stateless cluster is very easy to scale, and it doesn't
    have performance bottleneck on session replication.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建集群时，我们首先应该考虑构建无状态集群，因为无状态集群非常容易扩展，并且在会话复制上没有性能瓶颈。
