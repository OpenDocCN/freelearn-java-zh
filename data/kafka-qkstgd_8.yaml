- en: Kafka Connect
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect
- en: In this chapter, instead of using the Kafka Java API for producers and consumers,
    Kafka Streams, or KSQL as in previous chapters, we are going to connect Kafka
    with Spark Structured Streaming, the Apache Spark solution to process streams
    with its Datasets API.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不是使用Kafka Java API进行生产者和消费者，也不是使用Kafka Streams或KSQL（如前几章所述），我们将使用Spark
    Structured Streaming连接Kafka，这是Apache Spark用于处理流数据的解决方案，它使用其Datasets API。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Spark Streaming processor
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming处理器
- en: Reading Kafka from Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Spark读取Kafka
- en: Data conversion
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换
- en: Data processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理
- en: Writing to Kafka from Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Spark写入Kafka
- en: Running the `SparkProcessor`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`SparkProcessor`
- en: Kafka Connect in a nutshell
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect概述
- en: Kafka Connect is an open source framework, part of Apache Kafka; it is used
    to connect Kafka with other systems, such as structured databases, column stores,
    key-value stores, filesystems, and search engines.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect是一个开源框架，是Apache Kafka的一部分；它用于将Kafka与其他系统连接起来，例如结构化数据库、列存储、键值存储、文件系统和搜索引擎。
- en: Kafka Connect has a wide range of built-in connectors. If we are reading from
    the external system, it is called a **data source**; if we are writing to the
    external system, it is called a **data sink**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect拥有广泛的内置连接器。如果我们从外部系统读取，它被称为**数据源**；如果我们向外部系统写入，它被称为**数据接收器**。
- en: 'In previous chapters, we created a Java Kafka Producer that sends JSON data
    to a topic called `healthchecks` in messages like these three:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们创建了一个Java Kafka生产者，它以如下三条消息的形式将JSON数据发送到名为`healthchecks`的主题。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we are going to process this data to calculate the machine''s uptime and
    to obtain a topic with messages like these three:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将处理这些数据以计算机器的运行时间和获取包含如下三条消息的主题：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Project setup
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目设置
- en: 'The first step is to modify our Kioto project. We have to add the dependencies
    to `build.gradle`, as shown in *Listing 8.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是修改我们的Kioto项目。我们必须在`build.gradle`中添加依赖项，如下所示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Listing 8.1: Kioto gradle build file for Spark'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1：Kioto gradle构建文件用于Spark
- en: 'To use Apache Spark, we need the dependency, shown as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Apache Spark，我们需要以下依赖项：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To connect Apache Spark with Kafka, we need the dependency, shown as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接Apache Spark与Kafka，我们需要以下依赖项：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are using an old Spark version, 2.2.2, for the following two reasons:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用旧版本的Spark，2.2.2，以下两个原因：
- en: At the moment you are reading this, surely the Spark version will be superior.
    The reason why I chose this version (and not the last one at the time of writing)
    is because the connector with Kafka works perfectly with this version (in performance
    and with regard to bugs).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你阅读这段内容时，Spark版本肯定已经更新了。我选择这个版本（而不是写作时的最后一个版本）的原因是，与Kafka的连接器在这个版本上工作得非常好（在性能和错误方面）。
- en: The Kafka connector that works with this version is several versions behind
    the most modern version of the Kafka connector. You always have to consider this
    when upgrading production environments**.**
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此版本一起工作的Kafka连接器比最现代的Kafka连接器版本落后几个版本。在升级生产环境时，你始终必须考虑这一点**。**
- en: Spark Streaming processor
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming处理器
- en: 'Now, in the `src/main/java/kioto/spark` directory, create a file called `SparkProcessor.java`
    with the contents of *Listing 8.2*, shown as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在`src/main/java/kioto/spark`目录中，创建一个名为`SparkProcessor.java`的文件，其内容如列表8.2所示，如下所示：
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Listing 8.2: SparkProcessor.java'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2：SparkProcessor.java
- en: Note that, as in previous examples, the main method invoked the `process()`
    method with the IP address and the port of the Kafka brokers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与之前的示例一样，主方法使用Kafka代理的IP地址和端口号调用了`process()`方法。
- en: 'Now, let''s fill the `process()` method. The first step is to initialize Spark,
    as demonstrated in the following block:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们填充`process()`方法。第一步是初始化Spark，如下面的代码块所示：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In Spark, the application name must be the same for each member in the cluster,
    so here we call it Kioto (original, isn't it?).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，集群中每个成员的应用名称必须相同，因此这里我们称之为Kioto（原始的，不是吗？）。
- en: As we are going to run the application locally, we are setting the Spark master
    to `local[*]`, which means that we are creating a number of threads equivalent
    to the machine CPU cores.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们打算在本地运行应用程序，我们将Spark master设置为`local[*]`，这意味着我们正在创建与机器CPU核心数量相等的线程。
- en: Reading Kafka from Spark
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Spark读取Kafka
- en: There are several connectors for Apache Spark. In this case, we are using the
    Databricks Inc. (the company responsible for Apache Spark) connector for Kafka.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark有几个连接器。在这种情况下，我们使用Databricks Inc.（Apache Spark的负责公司）的Kafka连接器。
- en: 'Using this Spark Kafka connector, we can read data with Spark Structured Streaming
    from a Kafka topic:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个Spark Kafka连接器，我们可以从Kafka主题中读取数据，使用Spark Structured Streaming：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Simply by saying Kafka format, we can read a stream from the topic specified
    in the `subscribe` option, running on the brokers specified.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 只需说Kafka格式，我们就可以从`subscribe`选项指定的主题中读取流，在指定的代理上运行。
- en: 'At this point in the code, if you invoke the `printSchema()` method on the
    `inputDataSet`, the result will be something similar to *Figure 8.1*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的这个位置，如果你在`inputDataSet`上调用`printSchema()`方法，结果将类似于*图8.1*：
- en: '![](img/66bd4c3c-1dd9-45e3-b5e8-28a379cb8a5f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66bd4c3c-1dd9-45e3-b5e8-28a379cb8a5f.png)'
- en: 'Figure 8.1: Print schema output'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：打印模式输出
- en: 'We can interpret this as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样理解：
- en: The key and the value are binary data. Here in Spark, unfortunately, andunlike
    Kafka, it is not possible to specify deserializers for our data. So, it is necessary
    to use Dataframe operations to do the deserialization.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键和值都是二进制数据。在这里，不幸的是，与Kafka不同，Spark中无法指定我们的数据反序列化器。因此，有必要使用Dataframe操作来进行反序列化。
- en: For each message, we can know the topic, the partition, the offset, and the
    timestamp.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每条消息，我们可以知道主题、分区、偏移量和时间戳。
- en: The timestamp type is always zero.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间戳类型始终为零。
- en: As with Kafka Streams, with Spark Streaming, in each step we have to generate
    a new data stream in order to apply transformations and get new ones.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与Kafka Streams一样，在Spark Streaming中，在每一步我们都必须生成一个新的数据流，以便应用转换并获得新的数据流。
- en: 'In each step, if we need to print our data stream (to debug the application),
    we can use the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，如果我们需要打印我们的数据流（用于调试应用程序），我们可以使用以下代码：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first line is optional, because we really don't need to assign the result
    to an object, just the code execution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是可选的，因为我们实际上不需要将结果分配给一个对象，只需要代码执行。
- en: 'The output of this snippet is something like *Figure 8.2*. The message value
    is certainly binary data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段的输出类似于*图8.2*。消息值肯定是二进制数据：
- en: '![](img/0e9880ca-2571-4043-86cc-1fa6ef189d25.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e9880ca-2571-4043-86cc-1fa6ef189d25.png)'
- en: 'Figure 8.2: Data stream console output'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：数据流控制台输出
- en: Data conversion
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: 'We know that when we produced the data, it was in JSON format, although Spark
    reads it in binary format. To convert the binary message to string, we use the
    following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，当我们产生数据时，它是以JSON格式，尽管Spark以二进制格式读取它。为了将二进制消息转换为字符串，我们使用以下代码：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `Dataset` console output is now human-readable, and is shown as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`控制台输出现在是可读的，如下所示：'
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next step is to provide the fields list to specify the data structure of
    the JSON message, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是提供字段列表以指定JSON消息的数据结构，如下所示：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we deserialize the String in JSON format. The simplest way is to use
    the prebuilt `from_json()` function in the `org.apache.spark.sql.functions` package,
    which is demonstrated in the following block:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们反序列化JSON格式的String。最简单的方法是使用`org.apache.spark.sql.functions`包中预构建的`from_json()`函数，如下所示：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we print the `Dataset` at this point, we can see the columns nested as we
    indicated in the schema:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这个时候打印`Dataset`，我们可以看到列嵌套正如我们在模式中指示的那样：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The next step is to flatten this `Dataset`, as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这个`Dataset`展开，如下所示：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To visualize the flattening, if we print the `Dataset`, we get the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化展开，如果我们打印`Dataset`，我们得到以下内容：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that we read the startup time as a string. This is because internally the
    `from_json()` function uses the Jackson library. Unfortunately, there is no way
    to specify the format of the date to be read.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们以字符串的形式读取启动时间。这是因为内部`from_json()`函数使用了Jackson库。不幸的是，没有方法可以指定要读取的日期格式。
- en: 'For these purposes, fortunately there is the `to_timestamp()` function in the
    same functions package. There is also the `to_date()` function if it is necessary
    to read only a date, ignoring the time specification. Here, we are rewriting the
    `lastStartedAt` column, similar to this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，为了这些目的，同一个函数包中有一个`to_timestamp()`函数。如果只需要读取日期，忽略时间指定，还有一个`to_date()`函数。在这里，我们正在重写`lastStartedAt`列，类似于以下内容：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Data processing
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理
- en: Now, what we are going to do is to calculate the `uptimes`. As is to be expected,
    Spark does not have a built-in function to calculate the number of days between
    two dates, so we are going to create a user-defined function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要做的是计算`uptimes`。正如预期的那样，Spark没有内置函数来计算两个日期之间的天数，因此我们将创建一个用户定义的函数。
- en: If we remember the KSQL chapter, it is also possible to build and use new UDFs
    in KSQL.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们记得KSQL章节，我们也可以在KSQL中构建和使用新的UDFs。
- en: 'To achieve this, the first thing we do is build a function that receives as
    input a `java.sql.Timestamp`, as shown in the following code (this is how timestamps
    are represented in the Spark DataSets) and returns an integer with the number
    of days from that date:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这一点，我们首先构建一个函数，该函数接收一个`java.sql.Timestamp`作为输入，如下所示（这是Spark DataSets中表示时间戳的方式）并返回一个表示从该日期起的天数的整数：
- en: '[PRE17]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The next step is to generate a Spark UDF as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是生成一个Spark UDF，如下所示：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And finally, apply that UDF to the `lastStartedAt` column to create a new column
    in the `Dataset` called `uptime`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将该UDF应用到`lastStartedAt`列以在`Dataset`中创建一个名为`uptime`的新列。
- en: Writing to Kafka from Spark
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Spark写入Kafka
- en: As we already processed the data and calculated the `uptime`, now all we need
    to do is to write these values in the Kafka topic called `uptimes`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经处理了数据并计算了`uptime`，现在我们只需要将这些值写入名为`uptimes`的Kafka主题。
- en: Kafka's connector allows us to write values to Kafka. The requirement is that
    the `Dataset` to write must have a column called `key` and another column called
    `value`; each one can be of the type String or binary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka的连接器允许我们将值写入Kafka。要求是写入的`Dataset`必须有一个名为`key`的列和一个名为`value`的列；每个都可以是String或二进制类型。
- en: Since we want the machine serial number to be the key, there is no problem if
    it is already of String type. Now, we just have to convert the `uptime` column
    from binary into String.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望机器序列号作为键，如果它已经是String类型就没有问题。现在，我们只需要将`uptime`列从二进制转换为String。
- en: 'We use the `select()` method of the `Dataset` class to calculate these two
    columns and assign them new names using the `as()` method, shown as follows (to
    do this, we could also use the `alias()` method of that class):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Dataset`类的`select()`方法来计算这两个列，并使用`as()`方法给它们赋予新的名称，如下所示（为此，我们也可以使用该类中的`alias()`方法）：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Our `Dataset` is ready and it has the format expected by the Kafka connector.
    The following code is to tell Spark to write these values to Kafka:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`Dataset`已经准备好了，并且它符合Kafka连接器预期的格式。以下代码是告诉Spark将这些值写入Kafka：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note that we added the checkpoint location in the options. This is to ensure
    the high availability of Kafka. However, this does not guarantee that messages
    are delivered in exactly once mode. Nowadays, Kafka can guarantee exactly once
    delivery; Spark for the moment, can only guarantee the at least once delivery
    mode.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在选项中添加了checkpoint位置。这是为了确保Kafka的高可用性。然而，这并不保证消息以精确一次模式传递。如今，Kafka可以保证精确一次传递；而Spark目前只能保证至少一次传递模式。
- en: 'Finally, we call the `awaitAnyTermination()` method, shown as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用`awaitAnyTermination()`方法，如下所示：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'An important note is to mention that if Spark leaves a console output inside
    the code, it implies that all queries must call its `start()` method before calling
    any `awaitTermination()` method, shown as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的一点是提到，如果Spark在代码中留下控制台输出，这意味着所有查询必须在调用任何`awaitTermination()`方法之前调用其`start()`方法，如下所示：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Also note that we can replace all the `awaitTermination()` calls at the end
    with a single call to `awaitAnyTermination()`, as we did in the original code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我们可以将末尾的所有`awaitTermination()`调用替换为对`awaitAnyTermination()`的单次调用，就像我们在原始代码中所做的那样。
- en: Running the SparkProcessor
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行SparkProcessor
- en: 'To build the project, run this command from the `kioto` directory as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建项目，请从`kioto`目录运行以下命令：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If everything is OK, the output is something similar to the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，输出类似于以下内容：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'From a command-line terminal, move to the `Confluent` directory and start it
    as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行终端，移动到`Confluent`目录并按如下方式启动：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Run a console consumer for the `uptimes` topic, shown as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`uptimes`主题的控制台消费者，如下所示：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: From our IDE, run the main method of the `PlainProducer` built in previous chapters
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们之前的章节中构建的`PlainProducer`的IDE中运行主方法
- en: 'The output on the console consumer of the producer should be similar to the
    following:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生产者的控制台消费者输出应类似于以下内容：
- en: '[PRE27]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: From our IDE, run the main method of the `SparkProcessor`
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的IDE中运行`SparkProcessor`的主方法
- en: 'The output on the console consumer for the `uptimes` topic should be similar
    to the following:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`uptimes`主题的控制台消费者的输出应类似于以下内容：'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: If you are someone who uses Spark for batch processing, Spark Structured Streaming
    is a tool you should try, as its API is similar to its batch processing counterpart.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个使用 Spark 进行批量处理的人，Spark Structured Streaming 是你应该尝试的工具，因为它的 API 与其批量处理对应工具类似。
- en: Now, if we compare Spark to Kafka for stream processing, we must remember that
    Spark streaming is designed to handle throughput, not latency, and it becomes
    very complicated to handle streams with low latency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们比较 Spark 和 Kafka 在流处理方面的表现，我们必须记住 Spark Streaming 是为了处理吞吐量而设计的，而不是延迟，处理低延迟的流变得非常复杂。
- en: The Spark Kafka connector has always been a complicated issue. For example,
    we have to use previous versions of both, because with each new version, there
    are too many changes on both sides.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Kafka 连接器一直是一个复杂的问题。例如，我们必须使用两者的旧版本，因为随着每个新版本的发布，两边都有太多的变化。
- en: In Spark, the deployment model is always much more complicated than with Kafka
    Streams. Although Spark, Flink, and Beam can perform tasks much more complex tasks,
    than Kafka Streams, the easiest to learn and implement has always been Kafka.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，部署模型总是比 Kafka Streams 复杂得多。尽管 Spark、Flink 和 Beam 可以执行比 Kafka Streams
    更复杂的任务，但学习和实现起来最简单的始终是 Kafka。
