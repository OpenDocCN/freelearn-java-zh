- en: Chapter 8. Application Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 应用性能
- en: The earliest computing devices were built to perform automatic computations
    and, as computers grew in power, they became increasingly popular because of how
    much and how fast they could compute. Even today, this essence lives on in our
    anticipation that computers can execute our business calculations faster than
    before by means of the applications we run on them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 最早的计算设备是为了执行自动计算而建造的，随着计算机能力的增强，它们因为能够进行多少以及多快地计算而变得越来越受欢迎。即使今天，这种本质仍然存在于我们通过在计算机上运行的应用程序来期待计算机能够比以前更快地执行我们的业务计算的预期中。
- en: 'Compared to performance analysis and optimization at a smaller component level,
    as we saw in previous chapters, it takes a holistic approach to improve performance
    at the application level. The higher-level concerns, such as serving a certain
    number of users in a day, or handling an identified quantum of load through a
    multi-layered system, requires us to think about how the components fit together
    and how the load is designed to flow through it. In this chapter, we will discuss
    such high-level concerns. Like the previous chapter, by and large this chapter
    applies to applications written in any JVM language, but with a focus on Clojure.
    In this chapter, we will discuss general performance techniques that apply to
    all layers of the code:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前几章中看到的较小组件级别的性能分析和优化相比，提高应用层性能需要整体方法。更高级别的关注点，例如每天服务一定数量的用户，或者通过多层系统处理已识别的负载量，需要我们思考组件如何组合以及负载是如何设计通过它的。在本章中，我们将讨论这些高级关注点。与上一章类似，总体而言，本章适用于用任何JVM语言编写的应用程序，但重点在于Clojure。在本章中，我们将讨论适用于代码所有层的通用性能技术：
- en: Choosing libraries
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择库
- en: Logging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录
- en: Data sizing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据大小
- en: Resource pooling
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源池化
- en: Fetch and compute in advance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前获取和计算
- en: Staging and batching
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶段化和批量处理
- en: Little's law
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李特尔法则
- en: Choosing libraries
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择库
- en: Most non-trivial applications depend a great deal on third-party libraries for
    various functionality, such as logging, serving web requests, connecting to databases,
    writing to message queues, and so on. Many of these libraries not only carry out
    parts of critical business functionality but also appear in the performance-sensitive
    areas of our code, impacting the overall performance. It is imperative that we
    choose libraries wisely (with respect to features versus performance trade off)
    after due performance analysis.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非平凡应用都高度依赖于第三方库来实现各种功能，例如日志记录、处理网络请求、连接数据库、写入消息队列等。许多这些库不仅执行关键业务功能的一部分，而且出现在性能敏感的代码区域，影响整体性能。在充分进行性能分析之后，我们明智地选择库（在功能与性能权衡方面）是至关重要的。
- en: The crucial factor in choosing libraries is not identifying which library to
    use, rather it is having a performance model of our applications and having the
    use cases benchmarked under representative load. Only benchmarks can tell us whether
    the performance is problematic or acceptable. If the performance is below expectation,
    a drill-down profiling can show us whether a third-party library is causing the
    performance issue. In [Chapter 6](ch06.html "Chapter 6. Measuring Performance"),
    *Measuring Performance* and [Chapter 7](ch07.html "Chapter 7. Performance Optimization"),
    *Performance Optimization* we discussed how to measure performance and identify
    bottlenecks. You can evaluate multiple libraries for performance-sensitive use
    cases and choose what suits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 选择库的关键因素不是确定使用哪个库，而是拥有我们应用程序的性能模型，并且对代表性负载下的用例进行了基准测试。只有基准测试才能告诉我们性能是否存在问题或可接受。如果性能低于预期，深入分析可以显示第三方库是否导致了性能问题。在第6章[测量性能](ch06.html
    "第6章 测量性能")和第7章[性能优化](ch07.html "第7章 性能优化")中，我们讨论了如何测量性能和识别瓶颈。您可以针对性能敏感的用例评估多个库，并选择适合的库。
- en: Libraries often improve (or occasionally lose) performance with new releases,
    so measurement and profiling (comparative, across versions) should be an ongoing
    practice for the development and maintenance lifecycle of our applications. Another
    factor to note is that libraries may show different performance characteristics
    based on the use case, load, and the benchmark. The devil is in the benchmark
    details. Be sure that your benchmarks are as close as possible to the representative
    scenario for your application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 库通常会随着新版本的发布而提高（或偶尔降低）性能，因此测量和配置（比较，跨版本）应该成为我们应用程序开发和维护生命周期中的持续实践。另一个需要注意的因素是，库可能会根据用例、负载和基准表现出不同的性能特征。魔鬼在于基准的细节。确保你的基准尽可能接近你应用程序的代表性场景。
- en: Making a choice via benchmarks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过基准测试进行选择
- en: Let's take a brief look at a few general use cases where performance of third-party
    libraries are exposed via benchmarks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看看一些通用用例，在这些用例中，第三方库的性能是通过基准测试暴露的。
- en: Web servers
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web服务器
- en: 'Web servers are typically subject to quite a bit of performance benchmarking
    due to their generic nature and scope. One such benchmark for Clojure web servers
    exists here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其通用性和范围，Web服务器通常会受到大量的性能基准测试。这里有一个针对Clojure Web服务器的基准测试示例：
- en: '[https://github.com/ptaoussanis/clojure-web-server-benchmarks](https://github.com/ptaoussanis/clojure-web-server-benchmarks)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/ptaoussanis/clojure-web-server-benchmarks](https://github.com/ptaoussanis/clojure-web-server-benchmarks)'
- en: Web servers are complex pieces of software and they may exhibit different characteristics
    under various conditions. As you will notice, the performance numbers vary based
    on keep-alive versus non-keep-alive modes and request volume—at the time of writing,
    Immutant-2 came out better in keep-alive mode but fared poorly in the non-keep-alive
    benchmark. In production, people often front their application servers with reverse
    proxy servers, for example Nginx or HAProxy, which make keep-alive connections
    to application servers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Web服务器是复杂的软件组件，它们可能在各种条件下表现出不同的特性。正如你将注意到的，性能数字根据keep-alive模式与非keep-alive模式以及请求量而变化——在撰写本文时，Immutant-2在keep-alive模式下表现更好，但在非keep-alive基准测试中表现不佳。在生产中，人们通常在应用程序服务器前使用反向代理服务器，例如Nginx或HAProxy，这些服务器与应用程序服务器建立keep-alive连接。
- en: Web routing libraries
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网络路由库
- en: 'There are several web routing libraries for Clojure, as listed here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如此列出的Clojure有几个Web路由库：
- en: '[https://github.com/juxt/bidi#comparison-with-other-routing-libraries](https://github.com/juxt/bidi#comparison-with-other-routing-libraries)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/juxt/bidi#comparison-with-other-routing-libraries](https://github.com/juxt/bidi#comparison-with-other-routing-libraries)'
- en: 'The same document also shows a performance benchmark with **Compojure** as
    the baseline, in which (at the time of writing) Compojure turns out to be performing
    better than **Bidi**. However, another benchmark compares Compojure, **Clout**
    (the library that Compojure internally uses), and **CalfPath** routing here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的文档还显示了一个以**Compojure**为基准的性能基准测试，其中（在撰写本文时）Compojure的表现优于**Bidi**。然而，另一个基准测试比较了Compojure、**Clout**（Compojure内部使用的库）和**CalfPath**路由。
- en: '[https://github.com/kumarshantanu/calfpath#development](https://github.com/kumarshantanu/calfpath#development)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kumarshantanu/calfpath#development](https://github.com/kumarshantanu/calfpath#development)'
- en: In this benchmark, as of this writing, Clout performs better than Compojure,
    and CalfPath outperforms Clout. However, you should be aware of any caveats in
    the faster libraries.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个基准测试中，截至本文撰写时，Clout的表现优于Compojure，而CalfPath的表现优于Clout。然而，你应该注意任何更快库的注意事项。
- en: Data serialization
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据序列化
- en: 'There are several ways to serialize data in Clojure, for example EDN and Fressian.
    Nippy is another serialization library with benchmarks to demonstrate how well
    it performs over EDN and Fressian:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在Clojure中，有几种方法可以序列化数据，例如EDN和Fressian。Nippy是另一个序列化库，它有基准测试来展示它在EDN和Fressian上的性能表现：
- en: '[https://github.com/ptaoussanis/nippy#performance](https://github.com/ptaoussanis/nippy#performance)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/ptaoussanis/nippy#performance](https://github.com/ptaoussanis/nippy#performance)'
- en: We covered Nippy in [Chapter 2](ch02.html "Chapter 2. Clojure Abstractions"),
    *Clojure Abstractions* to show how it uses transients to speed up its internal
    computations. Even within Nippy, there are several flavors of serialization that
    have different features/performance trade-offs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.html "第2章。Clojure抽象")*Clojure抽象*中介绍了Nippy，展示了它是如何使用transients来加速其内部计算的。即使在Nippy内部，也有几种不同的序列化方式，它们具有不同的特性/性能权衡。
- en: JSON serialization
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JSON序列化
- en: 'Parsing and generating JSON is a very common use case in RESTful services and
    web applications. The Clojure contrib library clojure/data.json ([https://github.com/clojure/data.json](https://github.com/clojure/data.json))
    provides this functionality. However, many people have found out that the Cheshire
    library [https://github.com/dakrone/cheshire](https://github.com/dakrone/cheshire)
    performs much better than the former. The included benchmarks in Cheshire can
    be run using the following command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解析和生成 JSON 是 RESTful 服务和 Web 应用程序中非常常见的用例。Clojure contrib 库 clojure/data.json
    ([https://github.com/clojure/data.json](https://github.com/clojure/data.json))
    提供了这项功能。然而，许多人发现 Cheshire 库 [https://github.com/dakrone/cheshire](https://github.com/dakrone/cheshire)
    的性能远优于前者。Cheshire 包含的基准测试可以通过以下命令运行：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Cheshire internally uses the Jackson Java library [https://github.com/FasterXML/jackson](https://github.com/FasterXML/jackson),
    which is known for its good performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Cheshire 内部使用 Jackson Java 库 [https://github.com/FasterXML/jackson](https://github.com/FasterXML/jackson)，它以其良好的性能而闻名。
- en: JDBC
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JDBC
- en: 'JDBC access is another very common use case among applications using relational
    databases. The Clojure contrib library `clojure/java.jdbc` [https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    provides a Clojure JDBC API. Asphalt [https://github.com/kumarshantanu/asphalt](https://github.com/kumarshantanu/asphalt)
    is an alternative JDBC library where the comparative benchmarks can be run as
    follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用关系型数据库的应用程序中，JDBC 访问是另一个非常常见的用例。Clojure contrib 库 `clojure/java.jdbc` [https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    提供了 Clojure JDBC API。Asphalt [https://github.com/kumarshantanu/asphalt](https://github.com/kumarshantanu/asphalt)
    是一个替代的 JDBC 库，比较基准测试可以按照以下方式运行：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As of this writing, Asphalt outperforms `clojure/java.jdbc` by several micro
    seconds, which may be useful in low-latency applications. However, note that JDBC
    performance is usually dominated by SQL queries/joins, database latency, connection
    pool parameters, and so on. We will discuss more about JDBC in later sections.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文写作时为止，Asphalt 的性能优于 `clojure/java.jdbc` 几微秒，这在低延迟应用中可能很有用。然而，请注意，JDBC 性能通常受
    SQL 查询/连接、数据库延迟、连接池参数等因素的影响。我们将在后面的章节中更详细地讨论 JDBC。
- en: Logging
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录
- en: 'Logging is a prevalent activity that almost all non-trivial applications do.
    Logging calls are quite frequent, hence it is important to make sure our logging
    configuration is tuned well for performance. If you are not familiar with logging
    systems (especially on the JVM), you may want to take some time to get familiar
    with those first. We will cover the use of `clojure/tools.logging`, **SLF4J**
    and **LogBack** libraries (as a combination) for logging, and look into how to
    make them perform well:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录是一种普遍的活动，几乎所有非平凡的应用程序都会进行。日志调用非常频繁，因此确保我们的日志配置针对性能进行了优化非常重要。如果您对日志系统（尤其是在
    JVM 上）不熟悉，您可能需要花些时间先熟悉这些内容。我们将介绍 `clojure/tools.logging`、**SLF4J** 和 **LogBack**
    库（作为一个组合）的使用，并探讨如何使它们性能良好：
- en: Clojure/tools.logging [https://github.com/clojure/tools.logging](https://github.com/clojure/tools.logging)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clojure/tools.logging [https://github.com/clojure/tools.logging](https://github.com/clojure/tools.logging)
- en: 'SLF4J: [http://www.slf4j.org/](http://www.slf4j.org/)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SLF4J: [http://www.slf4j.org/](http://www.slf4j.org/)'
- en: 'LogBack: [http://logback.qos.ch/](http://logback.qos.ch/)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LogBack: [http://logback.qos.ch/](http://logback.qos.ch/)'
- en: Why SLF4J/LogBack?
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择 SLF4J/LogBack？
- en: Besides SLF4J/LogBack, there are several logging libraries to choose from in
    the Clojure application, for example Timbre, Log4j and java.util.logging. While
    there is nothing wrong with these libraries, we are often constrained into choosing
    something that covers most other third-party libraries (also including Java libraries)
    in our applications for logging purposes. SLF4J is a Java logger facade that detects
    any available implementation (LogBack, Log4j, and so on) —we choose LogBack simply
    because it performs well and is highly configurable. The library clojure/tools.logging
    provides a Clojure logging API that detects SLF4J, Log4j or java.util.logging
    (in that order) in the classpath and uses whichever implementation is found first.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 SLF4J/LogBack，Clojure 应用程序中还有几个日志库可供选择，例如 Timbre、Log4j 和 java.util.logging。虽然这些库本身没有问题，但我们通常被迫选择一个可以覆盖我们应用程序中大多数其他第三方库（包括
    Java 库）的库进行日志记录。SLF4J 是一个 Java 日志门面，它可以检测任何可用的实现（LogBack、Log4j 等）——我们选择 LogBack
    只是因为它性能良好且高度可配置。clojure/tools.logging 库提供了一个 Clojure 日志 API，它会在类路径中检测 SLF4J、Log4j
    或 java.util.logging（按此顺序），并使用找到的第一个实现。
- en: The setup
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: Let's walk through how to set up a logging system for your application using
    LogBack, SLF4J and `clojure/tools.logging` for a project built using Leiningen.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用LogBack、SLF4J和`clojure/tools.logging`为使用Leiningen构建的项目设置日志系统。
- en: Dependencies
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 依赖项
- en: 'Your `project.clj` file should have the LogBack, SLF4J and `clojure/tools.logging`
    dependencies under the `:dependencies` key:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您的`project.clj`文件应该在`:dependencies`键下包含LogBack、SLF4J和`clojure/tools.logging`依赖项：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The previously mentioned versions are current and work as of the time of writing.
    You may want to use updated versions, if available.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的版本是当前的，并且是在写作时有效的。如果您有的话，您可能想使用更新的版本。
- en: The logback configuration file
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LogBack配置文件
- en: 'You need to create a `logback.xml` file in the `resources` directory:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在`resources`目录下创建一个`logback.xml`文件：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The previous `logback.xml` file is simple on purpose (for illustration) and
    has just enough configuration to get you started with logging using LogBack.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`logback.xml`文件故意设计得简单（为了说明），只包含足够的配置，以便您开始使用LogBack进行日志记录。
- en: Optimization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化
- en: The optimization points are highlighted in the `logback.xml` file we saw earlier
    in this section. We set the `immediateFlush` attribute to `false` such that the
    messages are buffered before flushing to the appender. We also wrapped the regular
    file appender with an asynchronous appender and edited the `queueSize` and `discardingThreshold`
    attributes, which gets us much better results than the default.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 优化点在我们在本节中之前看到的`logback.xml`文件中被突出显示。我们将`immediateFlush`属性设置为`false`，这样消息在刷新到追加器之前会被缓冲。我们还用异步追加器包装了常规文件追加器，并编辑了`queueSize`和`discardingThreshold`属性，这比默认设置得到了更好的结果。
- en: Unless optimized, logging configurations are usually a common source of suboptimal
    performance in many applications. Usually, the performance problems show up only
    at high load when the log volume is very high. The optimizations discussed previously
    are only a few of the many possible optimizations that one can experiment with.
    The chapters in LogBack documentation, such as **encoders** ([http://logback.qos.ch/manual/encoders.html](http://logback.qos.ch/manual/encoders.html)),
    **appenders** ([http://logback.qos.ch/manual/appenders.html](http://logback.qos.ch/manual/appenders.html))
    and **configuration** ([http://logback.qos.ch/manual/configuration.html](http://logback.qos.ch/manual/configuration.html))
    have useful **information**. There are also tips [http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/](http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/)
    on the Internet that may provide useful pointers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除非进行优化，否则日志配置通常是许多应用程序性能不佳的常见原因。通常，性能问题只有在高负载且日志量非常大时才会显现。之前讨论的优化只是众多可能的优化中的一部分。在LogBack文档中，例如**编码器**([http://logback.qos.ch/manual/encoders.html](http://logback.qos.ch/manual/encoders.html))、**追加器**([http://logback.qos.ch/manual/appenders.html](http://logback.qos.ch/manual/appenders.html))和**配置**([http://logback.qos.ch/manual/configuration.html](http://logback.qos.ch/manual/configuration.html))部分提供了有用的**信息**。互联网上也有关于如何使用7个LogBack调整来即时提高Java日志的技巧[http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/](http://blog.takipi.com/how-to-instantly-improve-your-java-logging-with-7-logback-tweaks/)，可能提供有用的指导。
- en: Data sizing
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据大小
- en: The cost of abstractions in terms of the data size plays an important role.
    For example, whether or not a data element can fit into a processor cache line
    depends directly upon its size. On a Linux system, we can find out the cache line
    size and other parameters by inspecting the values in the files under the `/sys/devices/system/cpu/cpu0/cache/`
    directory. Refer to [Chapter 4](ch04.html "Chapter 4. Host Performance"), *Host
    Performance*, where we discussed how to compute the size of primitives, objects,
    and data elements.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据大小方面，抽象的成本起着重要作用。例如，一个数据元素是否可以适应处理器缓存行直接取决于其大小。在Linux系统中，我们可以通过检查`/sys/devices/system/cpu/cpu0/cache/`目录下的文件中的值来找出缓存行大小和其他参数。请参阅[第4章](ch04.html
    "第4章。主机性能")，*主机性能*，其中我们讨论了如何计算原语、对象和数据元素的大小。
- en: 'Another concern we generally find with data sizing is how much data we hold
    at any time in the heap. As we noted in earlier chapters, GC has direct consequences
    on the application performance. While processing data, often we do not really
    need all the data we hold on to. Consider the example of generating a summary
    report of sold items for a certain period (months) of time. After the subperiod
    (month-wise) summary data is computed, we do not need the item details anymore,
    hence it''s better to remove the unwanted data while we add the summaries. See
    the following example:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据大小方面通常遇到的一个问题是我们在任何时间点在堆中保留多少数据。正如我们在前面的章节中提到的，垃圾回收（GC）对应用程序性能有直接影响。在处理数据时，我们通常并不真的需要我们持有的所有数据。考虑一下生成某个时间段（月份）内已售商品的总结报告的例子。在计算子时间段（按月）的总结数据后，我们不再需要项目详情，因此在我们添加总结的同时删除不需要的数据会更好。请看以下示例：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Had we not used `select-keys` in the previous `summarize` function, it would
    have returned a map with extra :`summary` data along with all other existing keys
    in the map. Now, such a thing is often combined with lazy sequences, so for this
    scheme to work it is important not to hold onto the head of the lazy sequence.
    Recall that in [Chapter 2](ch02.html "Chapter 2. Clojure Abstractions"), *Clojure
    Abstractions* we discussed the perils of holding onto the head of a lazy sequence.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在之前的`summarize`函数中没有使用`select-keys`，它将返回一个包含额外`:summary`数据以及地图中所有其他现有键的映射。现在，这种事情通常与懒序列结合使用，因此为了使此方案有效，重要的是不要保留懒序列的头部。回想一下，在[第2章](ch02.html
    "第2章。Clojure抽象")中，我们讨论了保留懒序列头部所带来的风险。
- en: Reduced serialization
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 减少序列化
- en: We discussed in earlier chapters that serialization over an I/O channel is a
    common source of latency. The perils of over-serialization cannot be overstated.
    Whether we read or write data from a data source over an I/O channel, all of that
    data needs to be prepared, encoded, serialized, de-serialized, and parsed before
    being worked upon. The less data that is involved, the better it is for every
    step in order to lower the overhead. Where there is no I/O involved (such as in-process
    communication), it generally makes no sense to serialize.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中讨论过，通过I/O通道进行序列化是延迟的常见来源。过度序列化的风险不容忽视。无论我们是从数据源通过I/O通道读取还是写入数据，所有这些数据都需要在处理之前准备、编码、序列化、反序列化和解析。涉及的数据越少，对每一步来说越好，以便降低开销。在没有涉及I/O（如进程间通信）的情况下，通常没有必要进行序列化。
- en: A common example of over-serialization is when working with SQL databases. Often,
    there are common SQL query functions that fetch all columns of a table or a relation—they
    are called by various functions that implement business logic. Fetching data that
    we do not need is wasteful and detrimental to performance for the same reason
    that we discussed in the previous paragraph. While it may seem more work to write
    one SQL statement and one database-query function for each use case, it pays off
    with better performance. Code that uses NoSQL databases is also subject to this
    anti-pattern—we have to take care to fetch only what we need even though it may
    lead to additional code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在与SQL数据库一起工作时，过度序列化的一个常见例子是。通常，有一些常见的SQL查询函数可以检索表或关系的所有列——它们被各种实现业务逻辑的函数调用。检索我们不需要的数据是浪费的，并且与我们在上一段中讨论的原因一样，对性能有害。虽然为每个用例编写一个SQL语句和一个数据库查询函数可能看起来工作量更大，但这样做会带来更好的性能。使用NoSQL数据库的代码也容易受到这种反模式的影响——我们必须注意只检索我们需要的，即使这可能导致额外的代码。
- en: There's a pitfall to be aware of when reducing serialization. Often, some information
    needs to be inferred in the absence of serialized data. In such cases, where some
    of the serialization is dropped so that we can infer other information, we must
    compare the cost of inference versus the serialization overhead. The comparison
    may not necessarily be only per operation, but rather on the whole, such that
    we can consider the resources we can allocate in order to achieve capacities for
    various parts of our systems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在减少序列化时，有一个需要注意的陷阱。通常，在没有序列化数据的情况下，需要推断一些信息。在这种情况下，如果我们删除了一些序列化以推断其他信息，我们必须比较推断成本与序列化开销。这种比较可能不仅限于每个操作，而且可能是整体上的，这样我们就可以考虑我们可以分配的资源，以便为我们的系统各个部分实现能力。
- en: Chunking to reduce memory pressure
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据块化以减轻内存压力
- en: What happens when we slurp a text file regardless of its size? The contents
    of the entire file will sit in the JVM heap. If the file is larger than the JVM
    heap capacity, the JVM will terminate, throwing `OutOfMemoryError`. If the file
    is large, but not enough to force the JVM into OOM error, it leaves relatively
    less JVM heap space for other operations to continue in the application. Similar
    situations take place when we carry out any operation disregarding the JVM heap
    capacity. Fortunately, this can be fixed by reading data in chunks and processing
    them before reading more. In [Chapter 3](ch03.html "Chapter 3. Leaning on Java"),
    *Leaning on Java*, we briefly discussed memory mapped buffers, which is another
    complementary solution that you may like to explore.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不考虑文件大小而读取文本文件时会发生什么？整个文件的内容将驻留在JVM堆中。如果文件大于JVM堆容量，JVM将终止，抛出`OutOfMemoryError`。如果文件很大，但不足以迫使JVM进入OOM错误，它将为其他操作在应用程序中留下相对较少的JVM堆空间。当我们执行任何不考虑JVM堆容量的操作时，也会发生类似的情况。幸运的是，可以通过分块读取数据并在读取更多之前处理它们来解决这个问题。在[第3章](ch03.html
    "第3章。依赖Java")《依赖Java》中，我们简要讨论了内存映射缓冲区，这是另一种补充解决方案，你可能愿意探索。
- en: Sizing for file/network operations
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件/网络操作的大小
- en: Let's take the example of a data ingestion process where a semi-automated job
    uploads large **Comma Separated File (CSV)** files via **File Transfer Protocol
    (FTP)** to a file server, and another automated job (written in Clojure) runs
    periodically to detect the arrival of files via a Network File System (NFS). After
    detecting a new file, the Clojure program processes the file, updates the result
    in a database, and archives the file. The program detects and processes several
    files concurrently. The size of the CSV files is not known in advance, but the
    format is predefined.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个数据摄取过程为例，其中半自动作业通过**文件传输协议（FTP）**将大型**逗号分隔文件（CSV）**上传到文件服务器，另一个自动作业（用Clojure编写）定期运行以通过网络文件系统（NFS）检测文件的到达。检测到新文件后，Clojure程序处理文件，更新数据库中的结果，并归档文件。程序检测并处理多个文件并发。CSV文件的大小事先未知，但格式是预定义的。
- en: 'As per the previous description, one potential problem is, since there could
    be multiple files being processed concurrently, how do we distribute the JVM heap
    among the concurrent file-processing jobs? Another issue at hand could be that
    the operating system imposes a limit on how many files could be open at a time;
    on Unix-like systems you can use the `ulimit` command to extend the limit. We
    cannot arbitrarily slurp the CSV file contents—we must limit each job to a certain
    amount of memory, and also limit the number of jobs that can run concurrently.
    At the same time, we cannot read a very small number of rows from a file at a
    time because this may impact performance:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的描述，一个潜在的问题是，由于可能同时处理多个文件，我们如何分配JVM堆空间给并发文件处理作业？另一个问题是操作系统对一次可以打开的文件数量有限制；在类Unix系统中，你可以使用`ulimit`命令来扩展限制。我们不能任意地读取CSV文件内容——我们必须限制每个作业的内存量，并限制可以并发运行的作业数量。同时，我们也不能一次读取文件中的非常少的行，因为这可能会影响性能：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Fortunately, we can specify the buffer size when reading from a file (or even
    from a network stream) so as to tune the memory usage and performance as appropriate.
    In the previous code example, we explicitly set the buffer size of the reader
    to facilitate the same.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以在从文件（甚至从网络流）读取时指定缓冲区大小，以便根据需要调整内存使用和性能。在前面的代码示例中，我们明确设置了读取器的缓冲区大小，以促进这一点。
- en: Sizing for JDBC query results
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JDBC查询结果的大小
- en: 'Java''s interface standard for SQL databases, JDBC (which is technically not
    an acronym), supports *fetch size* for fetching query results via JDBC drivers.
    The default fetch size depends on the JDBC driver. Most of the JDBC drivers keep
    a low default value to avoid high memory usage and for internal performance optimization
    reasons. A notable exception to this norm is the MySQL JDBC driver that completely
    fetches and stores all rows in memory by default:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Java的SQL数据库接口标准JDBC（技术上不是一个缩写），支持通过JDBC驱动程序获取查询结果时的*获取大小*。默认的获取大小取决于JDBC驱动程序。大多数JDBC驱动程序保持一个较低的默认值，以避免高内存使用和内部性能优化原因。这个规范的一个显著例外是MySQL
    JDBC驱动程序，它默认完全获取并存储所有行到内存中：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When using the Clojure contrib library `java.jdbc` ([https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)
    as of version 0.3.7), the fetch size can be set while preparing a statement as
    shown in the previous example. Note that the fetch size does not guarantee proportional
    latency; however, it can be used safely for memory sizing. We must test any performance-impacting
    latency changes due to fetch size at different loads and use cases for the particular
    database and JDBC driver. Another important factor to note is that the benefit
    of `:fetch-size` can be useful only if the query result set is consumed incrementally
    and lazily—if a function extracts all rows from a result set to create a vector,
    then the benefit of `:fetch-size` is nullified from a memory conservation point
    of view. Besides fetch size, we can also pass the `:max-rows` argument to limit
    the maximum rows to be returned by a query—however, this implies that the extra
    rows will be truncated from the result, and not whether the database will internally
    limit the number of rows to realize.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Clojure contrib库的`java.jdbc`（自版本0.3.7起[https://github.com/clojure/java.jdbc](https://github.com/clojure/java.jdbc)），可以在准备语句时设置获取大小，如前一个示例所示。请注意，获取大小并不能保证成比例的延迟；然而，它可以安全地用于内存大小调整。我们必须测试由于获取大小引起的性能影响延迟变化，这在不同负载和特定数据库及JDBC驱动程序的使用场景中是必须的。另一个需要注意的重要因素是，`:fetch-size`的好处只有在查询结果集是增量且惰性消费的情况下才有用——如果函数从结果集中提取所有行以创建一个向量，那么从内存节省的角度来看，`:fetch-size`的好处就消失了。除了获取大小之外，我们还可以传递`:max-rows`参数来限制查询返回的最大行数——然而，这表示额外的行将从结果中截断，而不是数据库是否会在内部限制行数以实现这一点。
- en: Resource pooling
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源池
- en: There are several types of resources on the JVM that are rather expensive to
    initialize. Examples are HTTP connections, execution threads, JDBC connections,
    and so on. The Java API recognizes such resources and has built-in support for
    creating a pool of some of those resources, such that the consumer code borrows
    a resource from a pool when required and at the end of the job simply returns
    it to the pool. Java's thread pools (discussed in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*) and JDBC data sources are prominent examples. The idea is to preserve
    the initialized objects for reuse. Even though Java does not support pooling of
    a resource type directly, one can always create a pool abstraction around custom
    expensive resources. Note that the pooling technique is common in I/O activities,
    but can be equally applicable to non-I/O purposes where initialization cost is
    high.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在JVM上，有一些资源初始化成本相当高。例如，HTTP连接、执行线程、JDBC连接等。Java API识别这些资源，并内置了对创建某些资源池的支持，这样消费者代码在需要时可以从池中借用资源，并在工作结束时简单地将其返回到池中。Java的线程池（在第5章[Chapter
    5](ch05.html "Chapter 5. Concurrency")中讨论，*并发*）和JDBC数据源是突出的例子。这个想法是保留初始化对象以供重用。尽管Java不支持直接对资源类型进行池化，但总可以在自定义昂贵资源周围创建一个池抽象。请注意，池化技术在I/O活动中很常见，但也可以同样适用于初始化成本高的非I/O目的。
- en: JDBC resource pooling
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JDBC资源池
- en: Java supports the obtaining of JDBC connections via the `javax.sql.DataSource`
    interface, which can be pooled. A JDBC connection pool implements this interface.
    Typically, a JDBC connection pool is implemented by third-party libraries or a
    JDBC driver itself. Generally, very few JDBC drivers implement a connection pool,
    so Open Source third-party JDBC resource pooling libraries such as Apache DBCP,
    c3p0, BoneCP, HikariCP, and so on are popular. They also support validation queries
    for eviction of stale connections that might result from network timeouts and
    firewalls, and guard against connection leaks. Apache DBCP and HikariCP are accessible
    from Clojure via their respective Clojure wrapper libraries Clj-DBCP ([https://github.com/kumarshantanu/clj-dbcp](https://github.com/kumarshantanu/clj-dbcp))
    and HikariCP ([https://github.com/tomekw/hikari-cp](https://github.com/tomekw/hikari-cp)),
    and there are Clojure examples describing how to construct C3P0 and BoneCP pools
    ([http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html](http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Java 支持通过 `javax.sql.DataSource` 接口获取 JDBC 连接，该接口可以被池化。一个 JDBC 连接池实现了这个接口。通常，JDBC
    连接池是由第三方库或 JDBC 驱动本身实现的。一般来说，很少 JDBC 驱动实现连接池，因此像 Apache DBCP、c3p0、BoneCP、HikariCP
    等开源第三方 JDBC 资源池库非常流行。它们还支持验证查询以清除可能由网络超时和防火墙引起的陈旧连接，并防止连接泄漏。Apache DBCP 和 HikariCP
    可以通过它们各自的 Clojure 包装库 Clj-DBCP ([https://github.com/kumarshantanu/clj-dbcp](https://github.com/kumarshantanu/clj-dbcp))
    和 HikariCP ([https://github.com/tomekw/hikari-cp](https://github.com/tomekw/hikari-cp))
    从 Clojure 访问，并且有一些 Clojure 示例描述了如何构建 C3P0 和 BoneCP 池 ([http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html](http://clojure-doc.org/articles/ecosystem/java_jdbc/connection_pooling.html))。
- en: Connections are not the only JDBC resources that need to be pooled. Every time
    we create a new JDBC prepared statement, depending on the JDBC driver implementation,
    often the entire statement template is sent to the database server in order to
    obtain a reference to the prepared statement. As the database servers are generally
    deployed on separate hardware, there may be network latency involved. Hence, the
    pooling of prepared statements is a very desirable property of JDBC resource pooling
    libraries. Apache DBCP, C3P0, and BoneCP all support statement pooling, and the
    Clj-DBCP wrapper enables the pooling of prepared statements out-of-the-box for
    better performance. HikariCP has the opinion that statement pooling, nowadays,
    is already done internally by JDBC drivers, hence explicit pooling is not required.
    I would strongly advise running your benchmarks with the connection pooling libraries
    to determine whether or not it really works for your JDBC driver and application.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 连接不是唯一需要池化的 JDBC 资源。每次我们创建一个新的 JDBC 预编译语句时，根据 JDBC 驱动程序的实现，通常整个语句模板都会发送到数据库服务器以获取预编译语句的引用。由于数据库服务器通常部署在不同的硬件上，可能存在网络延迟。因此，预编译语句的池化是
    JDBC 资源池库的一个非常理想特性。Apache DBCP、C3P0 和 BoneCP 都支持语句池化，而 Clj-DBCP 包装库可以开箱即用地实现预编译语句的池化，以获得更好的性能。HikariCP
    认为，如今，语句池化已经由 JDBC 驱动程序内部完成，因此不需要显式池化。我强烈建议使用连接池库进行基准测试，以确定它是否真的适用于您的 JDBC 驱动程序和应用程序。
- en: I/O batching and throttling
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I/O 批处理和节流
- en: It is well known that chatty I/O calls generally lead to poor performance. In
    general, the solution is to batch together several messages and send them in one
    payload. In databases and network calls, batching is a common and useful technique
    to improve throughput. On the other hand, large batch sizes may actually harm
    throughput as they tend to incur memory overhead, and components may not be ready
    to handle a large batch at once. Hence, sizing the batches and throttling are
    just as important as batching. I would strongly advise conducting your own tests
    to determine the optimum batch size under representative load.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，嘈杂的 I/O 调用通常会导致性能不佳。一般来说，解决方案是将几条消息批在一起，然后在一个负载中发送。在数据库和网络调用中，批处理是一种常见且有用的技术，可以提高吞吐量。另一方面，大的批处理大小实际上可能会损害吞吐量，因为它们倾向于产生内存开销，并且组件可能无法一次性处理大的批处理。因此，确定批处理大小和节流与批处理本身一样重要。我强烈建议在代表性负载下进行自己的测试，以确定最佳批处理大小。
- en: JDBC batch operations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JDBC 批处理操作
- en: 'JDBC has long had batch-update support in its API, which includes the `INSERT`,
    `UPDATE`, `DELETE` statements. The Clojure contrib library `java.jdbc` supports
    JDBC batch operations via its own API, as we can see as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: JDBC 在其 API 中已经很长时间支持批量更新，包括 `INSERT`、`UPDATE`、`DELETE` 语句。Clojure contrib 库
    `java.jdbc` 通过其自己的 API 支持 JDBC 批量操作，如下所示：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Besides batch-update support, we can also batch JDBC queries. One of the common
    techniques is to use the SQL `WHERE` clause to avoid the `N+1` selects issue.
    The `N+1` issue indicates the situation when we execute one query in another child
    table for every row in a rowset from a master table. A similar technique can be
    used to combine several similar queries on the same table into just one, and segregate
    the data in the program afterwards.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了批量更新支持外，我们还可以进行批量 JDBC 查询。其中一种常见的技术是使用 SQL `WHERE` 子句来避免 `N+1` 查询问题。`N+1`
    问题指的是当我们对主表中的每一行执行一个查询到另一个子表时的情况。类似的技巧可以用来将同一表上的几个相似查询合并为一个，并在程序之后分离数据。
- en: 'Consider the following example that uses clojure.java.jdbc 0.3.7 and the MySQL
    database:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下使用 clojure.java.jdbc 0.3.7 和 MySQL 数据库的示例：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the previous example there are two tables: `orders` and `items`. The first
    snippet reads all order IDs from the `orders` table, and then iterates through
    them to query corresponding entries in the `items` table in a loop. This is the
    `N+1` selects performance anti-pattern you should keep an eye on. The second snippet
    avoids `N+1` selects by issuing a single SQL query, but may not perform very well
    unless the column `fk_order_id` is indexed.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，有两个表：`orders` 和 `items`。第一个片段从 `orders` 表中读取所有订单 ID，然后通过循环查询 `items`
    表中的相应条目。这是你应该注意的 `N+1` 查询性能反模式。第二个片段通过发出单个 SQL 查询来避免 `N+1` 查询，但除非列 `fk_order_id`
    已索引，否则可能表现不佳。
- en: Batch support at API level
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API 级别的批量支持
- en: When designing any service, it is very useful to provide an API for batch operations.
    This builds flexibility in the API such that batch sizing and throttling can be
    controlled in a fine-grained manner. Not surprisingly, it is also an effective
    recipe for building high-performance services. A common overhead we encounter
    when implementing batch operations is the identification of each item in the batch
    and their correlation across requests and responses. The problem becomes more
    prominent when requests are asynchronous.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计任何服务时，提供一个用于批量操作的 API 非常有用。这为 API 增加了灵活性，使得批量大小和节流可以以细粒度的方式控制。不出所料，这也是构建高性能服务的一个有效方法。在实现批量操作时，我们经常遇到的一个常见开销是识别每个批次中的每个项目及其在请求和响应之间的关联。当请求是异步的时，这个问题变得更加突出。
- en: The solution to the item identification issue is resolved either by assigning
    a canonical or global ID to each item in the request (batch), or by assigning
    every request (batch) a unique ID and each item in the request an ID that is local
    to the batch.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 解决项目识别问题的解决方案是通过为请求（批次）中的每个项目分配一个规范或全局 ID，或者为每个请求（批次）分配一个唯一的 ID，并为请求中的每个项目分配一个属于批次的本地
    ID。
- en: 'The choice of the exact solution usually depends on the implementation details.
    When requests are synchronous, you can do away with identification of each request
    item (see the Facebook API for reference: [http://developers.facebook.com/docs/reference/api/batch/](http://developers.facebook.com/docs/reference/api/batch/))
    where the items in response follow the same order as in the request. However,
    in asynchronous requests, items may have to be tracked via status-check call or
    callbacks. The desired tracking granularity typically guides the appropriate item
    identification strategy.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 确切解决方案的选择通常取决于实现细节。当请求是同步的时，你可以省略对每个请求项的识别（参考 Facebook API：[http://developers.facebook.com/docs/reference/api/batch/](http://developers.facebook.com/docs/reference/api/batch/)，其中响应中的项遵循请求中的相同顺序）。然而，在异步请求中，可能需要通过状态检查调用或回调来跟踪项。所需的跟踪粒度通常指导适当的项识别策略。
- en: For example, if we have a batch API for order processing, every order would
    have a unique Order-ID that can be used in subsequent status-check calls. In another
    example, let's say there is a batch API for creating API keys for **Internet of
    Things** (**IoT**) devices—here, the API keys are not known beforehand, but they
    can be generated and returned in a synchronous response. However, if this has
    to be an asynchronous batch API, the service should respond with a batch request
    ID that can be used later to find the status of the request. In a batch response
    for the request ID, the server can include request item IDs (for example device
    IDs, which may be unique for the client but not unique across all clients) with
    their respective status.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个用于订单处理的批量API，每个订单都会有一个唯一的Order-ID，可以在后续的状态检查调用中使用。在另一个例子中，假设有一个用于创建物联网（**IoT**）设备API密钥的批量API——在这里，API密钥事先并不知道，但它们可以在同步响应中生成和返回。然而，如果这必须是一个异步批量API，服务应该响应一个批量请求ID，稍后可以使用该ID查找请求的状态。在请求ID的批量响应中，服务器可以包括请求项目ID（例如设备ID，对于客户端可能是唯一的，但不是所有客户端都是唯一的）及其相应的状态。
- en: Throttling requests to services
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 节流对服务的请求
- en: As every service can handle only a certain capacity, the rate at which we send
    requests to a service is important. The expectations about the service behavior
    are generally in terms of both throughput and latency. This requires us to send
    requests at a specified rate, as a rate lower than that may lead to under-utilization
    of the service, and a higher rate may overload the service or result in failure,
    thus leading to client-side under-utilization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个服务只能处理一定的容量，因此我们向服务发送请求的速率很重要。对服务行为的期望通常涉及吞吐量和延迟两个方面。这要求我们以指定的速率发送请求，因为低于该速率可能会导致服务利用率不足，而高于该速率可能会使服务过载或导致失败，从而引起客户端利用率不足。
- en: Let's say a third-party service can accept 100 requests per second. However,
    we may not know how robustly the service is implemented. Though sometimes it is
    not exactly specified, sending 100 requests at once (within 20ms, let's say) during
    each second may lead to lower throughput than expected. Evenly distributing the
    requests across the one-second duration, for example sending one request every
    10ms (1000ms / 100 = 10ms), may increase the chance of attaining the optimum throughput.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第三方服务每秒可以接受100个请求。然而，我们可能不知道该服务的实现有多稳健。尽管有时并没有明确指定，但在每秒内一次性发送100个请求（例如在20毫秒内），可能会低于预期的吞吐量。例如，将请求均匀地分布在1秒的时间内，比如每10毫秒发送一个请求（1000毫秒
    / 100 = 10毫秒），可能会增加达到最佳吞吐量的机会。
- en: For throttling, **Token bucket** ([https://en.wikipedia.org/wiki/Token_bucket](https://en.wikipedia.org/wiki/Token_bucket))
    and **Leaky bucket** ([https://en.wikipedia.org/wiki/Leaky_bucket](https://en.wikipedia.org/wiki/Leaky_bucket))
    algorithms can be useful. Throttling at a very fine-grained level requires that
    we buffer the items so that we can maintain a uniform rate. Buffering consumes
    memory and often requires ordering; queues (covered in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*), pipeline and persistent storage usually serve that purpose well.
    Again, buffering and queuing may be subject to back pressure due to system constraints.
    We will discuss pipelines, back pressure and buffering in a later section in this
    chapter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于节流，**令牌桶** ([https://zh.wikipedia.org/wiki/Token_bucket](https://zh.wikipedia.org/wiki/Token_bucket))
    和 **漏桶** ([https://zh.wikipedia.org/wiki/Leaky_bucket](https://zh.wikipedia.org/wiki/Leaky_bucket))
    算法可能很有用。在非常细粒度的层面上进行节流需要我们缓冲项目，以便我们可以保持均匀的速率。缓冲会消耗内存，并且通常需要排序；队列（在第5章中介绍，*并发*），管道和持久存储通常很好地服务于这个目的。再次强调，由于系统限制，缓冲和排队可能会受到背压的影响。我们将在本章后面的部分讨论管道、背压和缓冲。
- en: Precomputing and caching
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预计算和缓存
- en: While processing data, we usually come across instances where few common computation
    steps precede several kinds of subsequent steps. That is to say, some amount of
    computation is common and the remaining is different. For high-latency common
    computations (I/O to access the data and memory/CPU to process it), it makes a
    lot of sense to compute them once and store in digest form, such that the subsequent
    steps can simply use the digest data and proceed from that point onward, thus
    resulting in reduced overall latency. This is also known as staging of semi-computed
    data and is a common technique to optimize processing of non-trivial data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，我们通常会遇到一些情况，其中一些常见的计算步骤先于几种后续步骤。也就是说，一部分计算是共同的，而剩余的是不同的。对于高延迟的常见计算（如访问数据时的I/O和内存/CPU处理），将它们一次性计算并存储为摘要形式是非常有意义的，这样后续步骤就可以简单地使用摘要数据并从该点继续进行，从而降低整体延迟。这也被称为半计算数据的分阶段处理，是优化非平凡数据处理的一种常见技术。
- en: Clojure has decent support for caching. The built-in `clojure.core/memoize`
    function performs basic caching of computed results with no flexibility in using
    specific caching strategies and pluggable backends. The Clojure contrib library
    `core.memoize` offsets the lack of flexibility in `memoize` by providing several
    configuration options. Interestingly, the features in `core.memoize` are also
    useful as a separate caching library, so the common portion is factored out as
    a Clojure contrib library called `core.cache` on top of which `core.memoize` is
    implemented.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Clojure 对缓存有良好的支持。内置的 `clojure.core/memoize` 函数执行基本的计算结果缓存，但在使用特定的缓存策略和可插拔后端方面没有灵活性。Clojure
    的 contrib 库 `core.memoize` 通过提供几个配置选项来弥补 `memoize` 的缺乏灵活性。有趣的是，`core.memoize`
    中的功能也作为单独的缓存库很有用，因此公共部分被提取出来，作为名为 `core.cache` 的 Clojure contrib 库，`core.memoize`
    是在这个库之上实现的。
- en: As many applications are deployed on multiple servers for availability, scaling
    and maintenance reasons, they need distributed caching that is fast and space
    efficient. The open source memcached project is a popular in-memory, distributed
    key-value/object store that can act as a caching server for web applications.
    It hashes the keys to identify the server to store the value on, and has no out-of-the-box
    replication or persistence. It is used to cache database query results, computation
    results, and so on. For Clojure, there is a memcached client library called SpyGlass
    ([https://github.com/clojurewerkz/spyglass](https://github.com/clojurewerkz/spyglass)).
    Of course, memcached is not limited to just web applications; it can be used for
    other purposes too.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多应用程序出于可用性、扩展性和维护原因部署在多个服务器上，它们需要快速且空间高效的分布式缓存。开源的 memcached 项目是一个流行的内存分布式键值/对象存储，可以作为Web应用的缓存服务器。它通过散列键来识别存储值的服务器，并且没有开箱即用的复制或持久性。它用于缓存数据库查询结果、计算结果等。对于Clojure，有一个名为SpyGlass的memcached客户端库（[https://github.com/clojurewerkz/spyglass](https://github.com/clojurewerkz/spyglass)）。当然，memcached不仅限于Web应用；它也可以用于其他目的。
- en: Concurrent pipelines
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行管道
- en: Imagine a situation where we have to carry out jobs at a certain throughput,
    such that each job includes the same sequence of differently sized I/O task (task
    A), a memory-bound task (task B) and, again, an I/O task (task C). A naïve approach
    would be to create a thread pool and run each job off it, but soon we realize
    that this is not optimum because we cannot ascertain the utilization of each I/O
    resource due to unpredictability of the threads being scheduled by the OS. We
    also observe that even though several concurrent jobs have similar I/O tasks,
    we are unable to batch them in our first approach.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样的情况，我们必须以一定的吞吐量执行工作，每个工作包括相同序列的不同大小的I/O任务（任务A），一个内存受限的任务（任务B），以及再次，一个I/O任务（任务C）。一个简单的方法是创建一个线程池并在其上运行每个工作，但很快我们会意识到这不是最佳方案，因为我们无法确定每个I/O资源的利用率，因为操作系统调度线程的不确定性。我们还观察到，尽管几个并发工作有类似的I/O任务，但我们无法在我们的第一种方法中批量处理它们。
- en: As the next iteration, we split each job in stages (A, B, C), such that each
    stage corresponds to one task. Since the tasks are well known, we create one thread
    pool (of appropriate size) per stage and execute tasks in them. The result of
    task A is required by task B, and B's result is required by task C—we enable this
    communication via queues. Now, we can tune the thread pool size for each stage,
    batch the I/O tasks, and throttle them for an optimum throughput. This kind of
    an arrangement is a concurrent pipeline. Some readers may find this feebly resembling
    the actor model or **Staged Event Driven Architecture** (**SEDA**) model, which
    are more refined models for this kind of approach. Recall that we discussed several
    kinds of in-process queues in [Chapter 5](ch05.html "Chapter 5. Concurrency"),
    *Concurrency*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个迭代中，我们将每个作业分成阶段（A、B、C），使得每个阶段对应一个任务。由于任务已知，我们为每个阶段创建一个适当大小的线程池并执行其中的任务。任务A的结果需要由任务B使用，B的结果需要由任务C使用——我们通过队列启用这种通信。现在，我们可以调整每个阶段的线程池大小，批量处理I/O任务，并调整它们以实现最佳吞吐量。这种安排是一种并发管道。一些读者可能会觉得这种安排与actor模型或**阶段事件驱动架构**（**SEDA**）模型有微妙的相似之处，这些是针对此类方法更精细的模型。回想一下，我们在[第5章](ch05.html
    "第5章。并发")中讨论了几种进程内队列，*并发*。
- en: Distributed pipelines
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式管道
- en: With this approach, it is possible to scale out the job execution to multiple
    hosts in a cluster using network queues, thereby offloading memory consumption,
    durability, and delivery to the queue infrastructure. For example, in a given
    scenario there could be several nodes in a cluster, all of them running the same
    code and exchanging messages (requests and intermediate result data) via network
    queues.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，可以通过网络队列将作业执行扩展到集群中的多个主机，从而卸载内存消耗、持久性和交付到队列基础设施。例如，在某个场景中，集群中可能有几个节点，它们都在运行相同的代码，并通过网络队列交换消息（请求和中间结果数据）。
- en: 'The following diagram depicts how a simple invoice-generation system might
    be connected to network queues:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了简单的发票生成系统如何连接到网络队列：
- en: '![Distributed pipelines](img/3642_08_03.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![分布式管道](img/3642_08_03.jpg)'
- en: RabbitMQ, HornetQ, ActiveMQ, Kestrel and Kafka are some well-known Open Source
    queue systems. Once in a while, the jobs may require distributed state and coordination.
    The Avout ([http://avout.io/](http://avout.io/)) project implements the distributed
    version of Clojure's atom and ref, which can be used for this purpose. Tesser
    ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser)) is another
    library for local and distributed parallelism using Clojure. The Storm ([http://storm-project.net/](http://storm-project.net/))
    and Onyx ([http://www.onyxplatform.org/](http://www.onyxplatform.org/)) projects
    are distributed, real-time stream processing systems implemented using Clojure.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: RabbitMQ、HornetQ、ActiveMQ、Kestrel和Kafka是一些知名的开源队列系统。偶尔，作业可能需要分布式状态和协调。Avout
    ([http://avout.io/](http://avout.io/)) 项目实现了Clojure的原子和ref的分布式版本，可用于此目的。Tesser
    ([https://github.com/aphyr/tesser](https://github.com/aphyr/tesser)) 是另一个用于本地和分布式并行性的Clojure库。Storm
    ([http://storm-project.net/](http://storm-project.net/)) 和Onyx ([http://www.onyxplatform.org/](http://www.onyxplatform.org/))
    项目是使用Clojure实现的分布式实时流处理系统。
- en: Applying back pressure
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用反向压力
- en: We discussed back pressure briefly in the last chapter. Without back pressure
    we cannot build a reasonable load-tolerant system with predictable stability and
    performance. In this section, we will see how to apply back pressure in different
    scenarios in an application. At a fundamental level, we should have a threshold
    of a maximum number of concurrent jobs in the system and, based on that threshold,
    we should reject new requests above a certain arrival rate. The rejected messages
    may either be retried by the client or ignored if there is no control over the
    client. When applying back pressure to user-facing services, it may be useful
    to detect system load and deny auxiliary services first in order to conserve capacity
    and degrade gracefully in the face of high load.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上章简要讨论了反向压力。没有反向压力，我们无法构建一个具有可预测稳定性和性能的合理负载容忍系统。在本节中，我们将看到如何在应用程序的不同场景中应用反向压力。在基本层面上，我们应该有一个系统最大并发作业数的阈值，并根据该阈值，在一定的到达率之上拒绝新的请求。被拒绝的消息可能由客户端重试，如果没有对客户端的控制，则忽略。在应用反向压力到面向用户的服务时，检测系统负载并首先拒绝辅助服务可能是有用的，以保存容量并在高负载下优雅地降级。
- en: Thread pool queues
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线程池队列
- en: 'JVM thread pools are backed by queues, which means that when we submit a job
    into a thread pool that already has the maximum jobs running, the new job lands
    in the queue. The queue is by default an unbounded queue, which is not suitable
    for applying back pressure. So, we have to create the thread pool backed by a
    bounded queue:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: JVM 线程池由队列支持，这意味着当我们向已经达到最大运行作业数量的线程池提交作业时，新作业将进入队列。默认情况下，队列是无界的，这不适合应用背压。因此，我们必须创建一个由有界队列支持的线程池：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, on this pool, whenever there is an attempt to add more jobs than the capacity
    of the queue, it will throw an exception. The caller should treat the exception
    as a buffer-full condition and wait until the buffer has idle capacity again by
    periodically pooling the `java.util.concurrent.BlockingQueue.remainingCapacity()`
    method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个池中，每当尝试添加的作业数量超过队列容量时，它将抛出一个异常。调用者应将异常视为缓冲区满的条件，并通过定期调用 `java.util.concurrent.BlockingQueue.remainingCapacity()`
    方法等待直到缓冲区再次有空闲容量。
- en: Servlet containers such as Tomcat and Jetty
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Servlet 容器，如 Tomcat 和 Jetty
- en: In the synchronous **Tomcat** and **Jetty** versions, each HTTP request is given
    a dedicated thread from a common thread pool that a user can configure. The number
    of simultaneous requests being served is limited by the thread pool size. A common
    way to control the arrival rate is to set the thread pool size of the server.
    The **Ring** library uses an embedded jetty server by default in development mode.
    The embedded Jetty adapter (in Ring) can be programmatically configured with a
    thread pool size.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在同步的 **Tomcat** 和 **Jetty** 版本中，每个 HTTP 请求都会从用户可以配置的公共线程池中分配一个专用线程。正在服务的并发请求数量受线程池大小的限制。一种常见的控制到达率的方法是设置服务器的线程池大小。在开发模式下，**Ring**
    库默认使用嵌入的 Jetty 服务器。Ring 中的嵌入 Jetty 适配器（在 Ring 中）可以通过程序配置线程池大小。
- en: In the asynchronous (Async Servlet 3.0) versions of Tomcat and Jetty beside
    the thread pool size, it is also possible to specify the timeout for processing
    each request. However, note that the thread pool size does not limit the number
    of requests in asynchronous versions in the way it does on synchronous versions.
    The request processing is transferred to an ExecutorService (thread pool), which
    may buffer requests until a thread is available. This buffering behavior is tricky
    because this may cause system overload—you can override the default behavior by
    defining your own thread pool instead of using the servlet container's thread
    pool to return a HTTP error at a certain threshold of waiting requests.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Tomcat 和 Jetty 的异步（Async Servlet 3.0）版本中，除了线程池大小外，还可以指定处理每个请求的超时时间。然而，请注意，线程池大小在异步版本中不会像在同步版本中那样限制请求数量。请求处理被转移到
    ExecutorService（线程池），它可能会缓冲请求，直到有可用的线程。这种缓冲行为很棘手，因为这可能会导致系统过载——你可以通过定义自己的线程池来覆盖默认行为，而不是使用
    servlet 容器的线程池，在等待请求达到一定阈值时返回 HTTP 错误。
- en: HTTP Kit
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTP Kit
- en: '**HTTP Kit** ([http://http-kit.org/](http://http-kit.org/)) is a high-performance
    asynchronous (based on Java NIO implementation) web server for Clojure. It has
    built-in support for applying back pressure to new requests via a specified queue
    length. As of HTTP Kit 2.1.19, see the following snippet:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**HTTP Kit** ([http://http-kit.org/](http://http-kit.org/)) 是一个高性能的异步（基于 Java
    NIO 实现）的 Clojure 网络服务器。它内置了对通过指定队列长度应用背压以处理新请求的支持。截至 HTTP Kit 2.1.19，请参阅以下代码片段：'
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the previous snippet, the worker thread pool size is 32 and the max queue
    length is specified as 600\. When not specified, 20480 is the default maximum
    queue length for applying back pressure.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，工作线程池大小为 32，最大队列长度指定为 600。如果没有指定，则应用背压的默认最大队列长度为 20480。
- en: Aleph
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Aleph
- en: 'Aleph ([http://aleph.io/](http://aleph.io/)) is another high-performance asynchronous
    web server based on the Java Netty ([http://netty.io/](http://netty.io/)) library,
    which in turn is based on Java NIO. Aleph extends Netty with its own primitives
    compatible with Netty. The worker thread pool in Aleph is specified via an option,
    as we can see in the following snippet as of Aleph 0.4.0:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Aleph ([http://aleph.io/](http://aleph.io/)) 是另一个基于 Java Netty ([http://netty.io/](http://netty.io/))
    库的高性能异步网络服务器，而 Java Netty 又是基于 Java NIO。Aleph 通过其与 Netty 兼容的自定义原语扩展了 Netty。Aleph
    的工作线程池通过一个选项指定，如下面的代码片段所示，截至 Aleph 0.4.0：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `tpool` refers to a bounded thread pool as discussed in the subsection
    *Thread pool queues*. By default, Aleph uses a dynamic thread pool capped at maximum
    512 threads aimed at 90 percent system utilization via the **Dirigiste** ([https://github.com/ztellman/dirigiste](https://github.com/ztellman/dirigiste))
    library.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`tpool`指的是在子节*线程池队列*中讨论的有界线程池。默认情况下，Aleph使用一个动态线程池，最大限制为512个线程，通过**Dirigiste**
    ([https://github.com/ztellman/dirigiste](https://github.com/ztellman/dirigiste))库达到90%的系统利用率。
- en: Back pressure not only involves enqueuing a limited number of jobs, but slows
    down the processing rate of a job when the peer is slow. Aleph deals with per-request
    back pressure (for example, when streaming response data) by "not accepting data
    until it runs out of memory" — it falls back to blocking instead of dropping data,
    or raising exceptions and closing connections
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 反压不仅涉及将有限数量的工作项入队，当对等方速度慢时，还会减慢工作项的处理速度。Aleph通过“在内存耗尽之前不接受数据”——它回退到阻塞而不是丢弃数据，或者引发异常并关闭连接来处理每个请求的反压（例如，在流式响应数据时）。
- en: Performance and queueing theory
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能和排队理论
- en: If we observe the performance benchmark numbers across a number of runs, even
    though the hardware, loads and OS remain the same, the numbers are rarely exactly
    the same. The difference between each run may be as much as -8 percent to 8 percent
    for no apparent reason. This may seem surprising, but the deep-rooted reason is
    that the performances of computer systems are *stochastic* by nature. There are
    many small factors in a computer system that make performance unpredictable at
    any given point of time. At best, the performance variations can be explained
    by a series of probabilities over random variables.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察多次运行中的性能基准数字，即使硬件、负载和操作系统保持不变，这些数字也很少完全相同。每次运行之间的差异可能高达-8%到8%，没有明显的原因。这可能会让人感到惊讶，但深层次的原因是计算机系统的性能本质上具有**随机性**。计算机系统中存在许多小因素，使得在任何给定时间点的性能不可预测。至多，性能变化可以通过一系列随机变量的概率来解释。
- en: The basic premise is that each subsystem is more or less like a queue where
    requests await their turn to be served. The CPU has an instruction queue with
    unpredictable fetch/decode/branch-predict timings, the memory access again depends
    on cache hit ratio and whether it needs to be dispatched via the interconnect,
    and the I/O subsystem works using interrupts that may again depend on mechanical
    factors of the I/O device. The OS schedules threads that wait while not executing.
    The software built on the top of all this basically waits in various queues to
    get the job done.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 基本前提是每个子系统或多或少像是一个队列，其中请求等待它们的轮次被服务。CPU有一个指令队列，其fetch/decode/branch-predict的时序不可预测，内存访问再次取决于缓存命中率以及是否需要通过互连进行调度，而I/O子系统使用中断来工作，这些中断可能又依赖于I/O设备的机械因素。操作系统调度等待而不执行线程。构建在所有这些之上的软件基本上在各种队列中等待以完成任务。
- en: Little's law
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Little定律
- en: 'Little''s law states that, over steady state, the following holds true:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Little定律指出，在稳态下，以下情况成立：
- en: '![Little''s law](img/3642_08_01.jpg)![Little''s law](img/3642_08_02.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![Little定律](img/3642_08_01.jpg)![Little定律](img/3642_08_02.jpg)'
- en: This is a rather important law that gives us insight into the system capacity
    as it is independent of other factors. For an example, if the average time to
    satisfy a request is 200 ms and the service rate is about 70 per second, then
    the mean number of requests being served is *70 req/second x 0.2 second = 14 requests*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当重要的定律，它使我们能够了解系统容量，因为它独立于其他因素。例如，如果满足请求的平均时间是200毫秒，而服务率约为每秒70次，那么正在被服务请求的平均数量是*70次/秒
    x 0.2秒 = 14个请求*。
- en: Note that Little's law does not talk about spikes in request arrival rate or
    spikes in latency (due to GC and/or other bottlenecks) or system behavior in response
    to these factors. When the arrival rate spikes at one point, your system must
    have enough resources to handle the number of concurrent tasks required to serve
    the requests. We can infer here that Little's law is helpful to measure and tune
    average system behavior over a duration, but we cannot plan capacity based solely
    on this.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Little定律没有讨论请求到达率或延迟（由于GC和/或其他瓶颈）的峰值，或者系统对这些因素的响应行为。当到达率在某个点出现峰值时，您的系统必须拥有足够的资源来处理服务请求所需的并发任务数量。我们可以推断出，Little定律有助于测量和调整一段时间内的平均系统行为，但我们不能仅基于这一点来规划容量。
- en: Performance tuning with respect to Little's law
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据 Little 定律进行性能调整
- en: In order to maintain good throughput, we should strive to maintain an upper
    limit on the total number of tasks in the system. Since there can be many kinds
    of tasks in a system and lot of tasks can happily co-exist in the absence of bottlenecks,
    a better way to say it is to ensure that the system utilization and bottlenecks
    remain in limit.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持良好的吞吐量，我们应该努力维持系统总任务数的上限。由于系统中可能存在许多种类的任务，并且在没有瓶颈的情况下，许多任务可以愉快地共存，因此更好的说法是确保系统利用率和瓶颈保持在限制范围内。
- en: Often, the arrival rate may not be within the control of a system. For such
    scenarios, the only option is to minimize the latency as much as possible and
    deny new requests after a certain threshold of total jobs in the system. You may
    be able to know the right threshold only through performance and load tests. If
    you can control the arrival rate, you can throttle the arrival (based on performance
    and load tests) so as to maintain a steady flow.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，到达率可能不在系统的控制范围内。对于此类场景，唯一的选择是尽可能减少延迟，并在系统中的总作业达到一定阈值后拒绝新的请求。你可能只能通过性能和负载测试来了解正确的阈值。如果你可以控制到达率，你可以根据性能和负载测试来调节到达（流量），以保持稳定的流动。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Designing an application for performance should be based on the use cases and
    patterns of anticipated system load and behavior. Measuring performance is extremely
    important to guide optimization in the process. Fortunately, there are several
    well-known optimization patterns to tap into, such as resource pooling, data sizing,
    pre-fetch and pre-compute, staging, batching, and so on. As it turns out, application
    performance is not only a function of the use cases and patterns—the system as
    a whole is a continuous stochastic turn of events that can be assessed statistically
    and is guided by probability. Clojure is a fun language to do high-performance
    programming. This book prescribes many pointers and practices for performance,
    but there is no mantra that can solve everything. The devil is in the details.
    Know the idioms and patterns, experiment to see what works for your applications,
    and know which rules you can bend for performance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 设计用于性能的应用程序应基于预期的系统负载和使用案例的模式。测量性能对于指导过程中的优化至关重要。幸运的是，有几个著名的优化模式可以利用，例如资源池、数据大小、预取和预计算、分阶段、批量处理等等。实际上，应用程序的性能不仅取决于使用案例和模式——整个系统是一个连续的随机事件序列，可以通过统计方法进行评估，并由概率指导。Clojure是一种用于高性能编程的有趣语言。这本书规定了性能的许多指针和实践，但没有一个咒语可以解决所有问题。魔鬼在于细节。了解惯用和模式，通过实验看看哪些适用于你的应用程序，并知道哪些规则你可以为了性能而弯曲。
