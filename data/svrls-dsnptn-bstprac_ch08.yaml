- en: The MapReduce Pattern
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce模式
- en: MapReduce is a common data processing pattern made famous by Google and now
    implemented in various systems and frameworks, most notably Apache Hadoop. Nowadays,
    this pattern is familiar and easy to understand at its core, but running large-scale
    systems such as Hadoop comes with its own set of challenges and cost of ownership.
    In this chapter, we'll show how this pattern can be implemented on your own using
    serverless technologies.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce是一种由Google使流行起来的常见数据处理模式，现在在各种系统和框架中实现，最著名的是Apache Hadoop。如今，这种模式在核心上是熟悉且易于理解的，但运行像Hadoop这样的大规模系统会带来其自身的挑战和拥有成本。在本章中，我们将展示如何使用无服务器技术实现这种模式。
- en: Implementing big data applications in a serverless environment may seem counter-intuitive
    due to the computing limitations of FaaS. Certain types of problems fit very well
    into a serverless ecosystem, especially considering we practically have unlimited
    file storage with distributed filesystems such as AWS S3\. Additionally, MapReduce's
    magic is not so much in the application of an algorithm, but in the distribution
    of computing power such that computation is performed in parallel.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器环境中实现大数据应用可能看起来与FaaS的计算限制相矛盾。某些类型的问题非常适合无服务器生态系统，特别是考虑到我们实际上有无限的文件存储，例如AWS
    S3这样的分布式文件系统。此外，MapReduce的魔力不在于算法的应用，而在于计算能力的分布，使得计算可以并行执行。
- en: In this chapter, we will discuss the application and development of a MapReduce
    pattern in a serverless environment. I'll cover the use cases for such a design
    and when it may or may not be a good fit. I'll also show how simple this pattern
    is within a serverless platform and what you should take into consideration before
    embarking on building your system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在无服务器环境中应用和开发MapReduce模式。我会介绍这种设计的用例以及何时可能或可能不适合。我还会展示这种模式在无服务器平台上的简单性以及在你开始构建系统之前应该考虑的事项。
- en: 'At the end of this chapter, you can expect to understand the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你可以期待了解以下主题：
- en: What problem MapReduce solves and when it may be appropriate to implement in
    a serverless environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MapReduce解决的问题以及何时在无服务器环境中实现可能更合适
- en: Design and scaling considerations when applying this pattern on your own
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用此模式时设计和扩展的考虑因素
- en: How to implement your own MapReduce serverless system to count occurrences of
    from-to combinations from a corpus of email messages
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现自己的无服务器MapReduce系统来统计来自电子邮件语料库中from-to组合的出现次数
- en: How to use the Fanout pattern as a sub-component of the MapReduce pattern
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将Fanout模式作为MapReduce模式的子组件使用
- en: Introduction to MapReduce
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce简介
- en: 'MapReduce as a pattern and programming model has been around for many years,
    arising from parallel computing research and industry implementations. Most famously,
    MapReduce hit the mainstream with Google''s 2004 paper entitled *MapReduce—Simplified
    Data Processing on Large Clusters* ([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html)).
    Much of the benefit of Google''s initial MapReduce implementation was:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种模式和编程模型，MapReduce已经存在很多年了，它起源于并行计算研究和行业实施。最著名的是，MapReduce随着Google在2004年发表的论文《MapReduce—Simplified
    Data Processing on Large Clusters》([https://research.google.com/archive/mapreduce.html](https://research.google.com/archive/mapreduce.html))进入了主流。Google最初MapReduce实现的许多好处包括：
- en: Automatic parallelization and distribution
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动并行化和分布
- en: Fault-tolerance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容错性
- en: I/O scheduling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: I/O调度
- en: Status and monitoring
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态和监控
- en: If you take a step back and look at that list, it should look familiar. FaaS
    systems such as AWS Lambda give us most of these benefits. While status and monitoring
    aren't inherently baked into FaaS platforms, there are ways to ensure our functions
    are executing successfully. On that same topic, MapReduce systems were initially,
    and still are, very often, managed at the OS level, meaning operators are in charge
    of taking care of crashed or otherwise unhealthy nodes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你退一步看看这个列表，它应该看起来很熟悉。FaaS系统如AWS Lambda为我们提供了这些好处中的大多数。虽然状态和监控不是FaaS平台固有的，但有一些方法可以确保我们的函数正在成功执行。关于这个话题，MapReduce系统最初，现在仍然经常在操作系统级别上进行管理，这意味着操作员负责处理崩溃或其他不健康的节点。
- en: 'The preceding list of benefits is listed in the following slide from a presentation-like
    form of the research paper: [https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上述好处列表列在以下类似演示文稿的研究论文幻灯片中：[https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0002.html)
- en: Not too long after Google's 2004 MapReduce paper, the Apache Hadoop project
    was born. Hadoop's goal was an open source implementation of the MapReduce pattern
    for big data processing. Since then, Hadoop has arguably become the most popular
    MapReduce framework today. Additionally, the term *Hadoop* has evolved to include
    many other frameworks for big data processing and refers more to the ecosystem
    of tools rather than the single framework.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google 2004年的MapReduce论文不久之后，Apache Hadoop项目诞生了。Hadoop的目标是为大数据处理实现MapReduce模式的开源实现。从那时起，Hadoop可能已经成为今天最受欢迎的MapReduce框架。此外，术语*Hadoop*已经演变为包括许多其他用于大数据处理的框架，更多地指的是工具生态系统而不是单一框架。
- en: As powerful and popular as Hadoop is, it's a complicated beast in practice.
    In order to run a Hadoop cluster of any significance, one needs to run and master
    Zookeeper and **HDFS** (**Hadoop Distributed File System**), in addition to the
    Hadoop master and worker nodes themselves. For those unfamiliar with these tools
    and all of the DevOps ownership that comes with them, running a Hadoop cluster
    is not only daunting but impractical.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Hadoop功能强大且受欢迎，但在实践中它是一个复杂的怪物。为了运行一个具有任何重要性的Hadoop集群，除了运行和掌握Zookeeper和**HDFS**（**Hadoop分布式文件系统**）以及Hadoop的主节点和工作节点本身之外，还需要做很多事情。对于那些不熟悉这些工具及其所有DevOps所有权的人来说，运行一个Hadoop集群不仅令人畏惧，而且不切实际。
- en: MapReduce example
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce示例
- en: If you've never worked with a MapReduce framework or system, the overall concepts
    are not incredibly complex. In fact, we can implement a single-process and single-threaded
    MapReduce system in a few lines of code.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从未使用过MapReduce框架或系统，整体概念并不特别复杂。实际上，我们可以在几行代码中实现一个单进程和单线程的MapReduce系统。
- en: Overall, MapReduce is designed to extract a specific bit of information from
    a body of data and distill it down into some final result. That may sound very
    vague and arbitrary, and it is. The beauty of MapReduce is that one can apply
    it to so many different problems. A few examples should better demonstrate what
    MapReduce is and how you can use it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，MapReduce旨在从数据集中提取特定的信息片段，并将其提炼成某种最终结果。这听起来可能非常模糊和任意，确实如此。MapReduce的美丽之处在于它可以应用于许多不同的问题。一些例子可以更好地说明MapReduce是什么以及如何使用它。
- en: 'A *Hello World* MapReduce program counts the number of occurrences of a particular
    word in a body of text. The following code block does this with a few lines of
    Python code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*Hello World* MapReduce程序通过几行Python代码统计文本中特定单词的出现次数：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First, this code performs a mapping phase that emits a two element tuple for
    every occurrence of a work. For example, a given word would emit ` ('amet', 1)`
    for the word `amet`. The result from this mapping phase is a list of `(word, 1)`
    pairs, where the `1` simply means we've encountered the word.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，此代码执行一个映射阶段，为每个单词的出现生成一个包含两个元素的元组。例如，对于单词`amet`，会输出`('amet', 1)`。这个映射阶段的输出是一个`(word,
    1)`对的列表，这里的`1`仅仅意味着我们遇到了这个单词。
- en: 'The job of the reducer is to aggregate the mapper''s output into some final
    format. In our case, we''d like a final tally of the number of occurrences for
    each word. Reading through the preceding `reducer` function, it should be obvious
    how I''m doing that. A snippet from the final output is shown in the following
    code block. You can see that `amet` only shows up once in the `Lorem`, `ipsum`
    text blog, but `sit` shows up nine times:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Reducer的职责是将mapper的输出聚合到某种最终格式中。在我们的例子中，我们希望得到每个单词出现次数的最终总计。阅读前面的`reducer`函数，应该很明显我是在如何做这件事。以下代码块展示了最终输出的一个片段。你可以看到，`amet`在`Lorem`,
    `ipsum`文本博客中只出现了一次，但`sit`出现了九次：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Role of the mapper
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mapper的角色
- en: The primary purpose of the `mapper` is to emit data that the reducer will later
    aggregate into a final. In this trivial example, each occurrence of a word results
    in a `(word, 1)` pair since we're giving a single appearance of a word a score
    of `1`. We very well could have emitted the word by itself (that is, `'amet'`)
    and put the score of `1` in the reducer; however, this would have made the code
    less general. If we wanted to give a heavier weighting to certain words, we'd
    merely change our mapper to output a different number based on the word and would
    leave our `reducer` code as-is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapper` 的主要目的是发出 reducer 将后来聚合到最终结果中的数据。在这个简单的例子中，每个单词的出现都会产生一个 `(word, 1)`
    对，因为我们给单词的单次出现打分是 `1`。我们完全可以只发出单词本身（即 `''amet''`）并将分数 `1` 放入 reducer；然而，这会使代码变得不那么通用。如果我们想给某些单词赋予更高的权重，我们只需更改我们的
    mapper 以根据单词输出不同的数字，而我们的 `reducer` 代码保持不变。'
- en: 'The following code block shows how we would give the word `amet` a score of
     `10` while all other words count as `1`. Of course, this is no longer counting
    word occurrences but instead scoring words:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了我们如何给单词 `amet` 赋予 `10` 分，而所有其他单词计为 `1`。当然，这不再是计算单词出现次数，而是对单词进行评分：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we were computing something completely different, you should see now that
    we''d need to update the mapper function. Some examples of additional calculations
    we could make based on this Lorem ipsum text could be:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在计算完全不同的事情，你现在应该看到我们需要更新 mapper 函数。基于这个 Lorem ipsum 文本，我们可以进行一些额外的计算，例如：
- en: Number of uppercase letters in a word
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词中的大写字母数量
- en: Number of vowels in a word
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词中的元音字母数量
- en: The average length of a word
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词的平均长度
- en: Some of these would require changes to the reducer step, which we'll cover in
    the following section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些可能需要更改 reducer 步骤，我们将在下一节中介绍。
- en: Role of the reducer
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reducer 的作用
- en: While the mapper's job is to extract and emit some form of data to be aggregated,
    the reducer's job is to perform that aggregation. In this example, the reducer
    receives the full list of `(word, 1)` pairs and just adds up the counts (`1`,
    in this case) for each word. If we were to perform a different aggregation, the
    `reducer` function would need to change.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 mapper 的任务是提取和发出一些将被聚合的数据，但 reducer 的任务是执行聚合。在这个例子中，reducer 接收完整的 `(word,
    1)` 对列表，并仅对每个单词的计数（在这种情况下是 `1`）进行求和。如果我们执行不同的聚合，`reducer` 函数就需要更改。
- en: 'Rather than counting the number of occurrences, let''s calculate the average
    length of a word. In this case, both our `mapper` and our `reducer` will need
    updating with the more significant changes happening within the `reducer`. The
    following code block changes our example to calculate the average word length
    for a body of text broken up into words:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是计算出现的次数，让我们计算单词的平均长度。在这种情况下，我们的 `mapper` 和 `reducer` 都需要更新，其中 `reducer` 中的更改更为显著。以下代码块将我们的示例更改为计算文本块的平均单词长度：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As in the prior example, the mapper is quite dumb and only returns the word
    length. Since this example doesn't rely on anything specific to the words, there
    is no need to return the word itself. The reducer code becomes even more straightforward.
    The input to the `reducer` is now a list of numbers. To calculate the average,
    it's a simple task of returning the total divided by the number of elements.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，mapper 非常简单，只返回单词长度。由于这个例子不依赖于单词的任何特定内容，因此没有必要返回单词本身。`reducer` 代码变得更加简单。`reducer`
    的输入现在是一个数字列表。为了计算平均值，只需将总和除以元素数量即可。
- en: MapReduce architecture
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce 架构
- en: The real magic behind MapReduce implementations such as Hadoop is the distribution
    and parallelization of computation. Our trivial example would work well running
    on your laptop even when the input data was several megabytes. However, imagine
    a case where you would like to perform some analysis like this on data that is
    hundreds of gigabytes, terabytes, or even in the petabyte range.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce 实现如 Hadoop 背后的真正魔法是计算的分布和并行化。我们的简单例子在笔记本电脑上运行良好，即使输入数据是几个兆字节。然而，想象一下，你想要在数百吉字节、太字节甚至甚至达到拍字节范围的数据上执行这种分析的情况。
- en: Real MapReduce systems use two essential tricks to do this work efficiently.
    One is working in parallel as I've already mentioned. This means, for example,
    that multiple instances that do the computation comprise a Hadoop system or cluster.
    The other trick is co-locating data with the worker node that does the work. Data
    co-location reduces network traffic and speeds up overall processing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的MapReduce系统使用两个基本技巧来高效地完成这项工作。一个是并行工作，正如我之前提到的。这意味着，例如，执行计算的多个实例构成了一个Hadoop系统或集群。另一个技巧是将数据与执行工作的worker节点协同放置。数据协同放置减少了网络流量并加快了整体处理速度。
- en: 'Mappers begin their work on a subset of the input data. You can imagine that
    when working on petabytes of data, there could be hundreds or thousands of nodes
    involved. Once the mappers have completed their job, they send their data to the
    reducers for final processing. The following diagram shows the details of this
    architecture from a conceptual standpoint:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Mappers从输入数据的一个子集开始工作。你可以想象，当处理PB级的数据时，可能涉及数百或数千个节点。一旦mappers完成他们的工作，他们将数据发送到reducers进行最终处理。以下图从概念上展示了这个架构的细节：
- en: '![](img/47a8dfe6-67d6-4fd4-822c-695345c7deef.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47a8dfe6-67d6-4fd4-822c-695345c7deef.png)'
- en: Image adapted from *MapReduce:*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图片改编自*MapReduce*:'
- en: '*Simplified Data Processing on Large Clusters*, Jeff Dean, Sanjay Ghemawat'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*大规模集群上的简化数据处理*，Jeff Dean，Sanjay Ghemawat'
- en: Google, Inc. [https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Google, Inc. [https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html](https://research.google.com/archive/mapreduce-osdi04-slides/index-auto-0008.html)
- en: A key phase in Hadoop is the *shuffle* phase, labeled partitioning function in
    the previous diagram. The arrows coming out of the Map Tasks show that a subset
    of mapper data will be sent to various reducers. In Hadoop, all output for specific
    keys is sent to the same reducer node. For example, in our case of the word count,
    the key `('amet', 1)` would be sent to the same reducer machine/node regardless
    of which mapper emitted that key. The reason behind this is to reduce network
    latency and reduce complexity for the reducers. By guaranteeing that a reducer
    has all of the data needed to perform its final reduce task, reducers are both
    faster and simpler to implement. Without this guarantee, the framework would need
    to designate a master reducer and crawl horizontally to final all the necessary
    data. Not only is that complex, but it's also slow because of all of the network
    latency.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop中的一个关键阶段是*shuffle*阶段，在之前的图中被标记为分区函数。从Map任务中出来的箭头表明，映射数据的一部分将被发送到不同的reducer。在Hadoop中，所有特定键的输出都被发送到同一个reducer节点。例如，在我们的单词计数案例中，键`('amet',
    1)`将发送到同一个reducer机器/节点，无论哪个mapper产生了这个键。这样做的原因是为了减少网络延迟和简化reducer的复杂性。通过保证reducer拥有执行最终reduce任务所需的所有数据，reducer既快又易于实现。如果没有这个保证，框架将需要指定一个主reducer并水平爬行以收集所有必要的数据。这不仅复杂，而且由于网络延迟，速度也慢。
- en: There are many details that we cannot cover in a system as complex as Hadoop.
    If you have unanswered questions at this point, I'd encourage you to do some more
    investigation on your own. Hopefully, this discussion has been enough to set the
    stage for our serverless implementation of MapReduce in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Hadoop这样复杂的系统中，有许多细节我们无法涵盖。如果你现在还有未解答的问题，我鼓励你自行进行更多调查。希望这次讨论已经足够为下一节中我们无服务器实现MapReduce奠定基础。
- en: MapReduce serverless architecture
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MapReduce无服务器架构
- en: MapReduce on a serverless platform is very different than in a system such as
    Hadoop. Most of the differences occur on the operational and system architecture
    side of things. Another huge difference is the limited processing power and memory
    we have with our FaaS. Because FaaS providers put in hard limits for both temporary
    storage space and memory, there are some problems that you cannot realistically
    solve with a serverless MapReduce implementation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器平台上运行MapReduce与在Hadoop这样的系统中非常不同。大部分差异出现在操作和系统架构方面。另一个巨大的差异是我们拥有的FaaS有限的处理能力和内存。因为FaaS提供商为临时存储空间和内存都设置了硬限制，所以有些问题你无法用无服务器MapReduce实现来实际解决。
- en: The good news is that the foundational ideas in the MapReduce design still hold
    true. If you look back up at the start of the initial list of benefits provided
    by MapReduce, we naturally get many of these for free, albeit with a few caveats.
    MapReduce truly shines, due in large part to the parallelization of computation.
    We have that with serverless functions. Similarly, much work goes into ensuring
    Hadoop nodes are healthy and able to perform work. Again, we get that for free
    with serverless functions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，MapReduce 设计中的基础思想仍然有效。如果你回顾一下 MapReduce 提供的初始好处列表，我们自然可以得到其中许多，尽管有一些限制。MapReduce
    真正闪耀，很大程度上归功于计算的并行化。我们有了无服务器函数。同样，很多工作都投入到确保 Hadoop 节点健康并能够执行工作。同样，我们也可以免费获得这一点。
- en: A significant feature we do *not* get is the co-location of our data and the
    processing of that data. Our distributed filesystem in this example will be AWS
    S3\. The only way to get data to our serverless functions is to either send that
    data via API or have our functions fetch the data across the network. Hadoop storage
    and computing co-location mean that each mapper node processes the data that it
    has stored locally, using the HDFS. This drastically cuts down on the amount of
    data being transferred over the network and is an implementation detail that makes
    the entire system possible.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有得到的一个显著特性是我们数据和数据处理的位置相同。在这个例子中，我们的分布式文件系统将是 AWS S3。将数据发送到我们的无服务器函数的唯一方法是通过
    API 发送数据，或者让我们的函数通过网络获取数据。Hadoop 存储和计算位置相同意味着每个 mapper 节点使用 HDFS 处理它存储在本地的数据。这大大减少了通过网络传输的数据量，这是使整个系统成为可能的一个实现细节。
- en: Before you start your implementation of this pattern, ask yourself whether you
    can segment your data to a point where processing it with a Lambda function is
    possible. If your input data is 100 GB, it's feasible that you may have 100 functions
    handling 1 GB each, even with paying the penalty of network bandwidth. However,
    it won't be practical to expect a reducer to produce a 100 GB output file since
    it would need to hold that in memory to calculate the final result.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始实现这个模式之前，问问自己你是否可以将你的数据分割到可以用 Lambda 函数处理的地步。如果你的输入数据是 100 GB，那么你可能会有 100
    个函数处理每个 1 GB 的数据，即使要付出网络带宽的代价。然而，期望一个 reducer 生成 100 GB 的输出文件并不实际，因为它需要将所有这些数据加载到内存中以计算最终结果。
- en: Either way, we need to consider the size of the data we're processing, both
    concerning reads and writes. Fortunately, it's easy to scale out Lambda functions,
    so executing 10s or 100s of Lambda functions are of little difference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，我们都需要考虑我们正在处理的数据的大小，无论是读取还是写入。幸运的是，扩展 Lambda 函数很容易，所以执行 10 个或 100 个 Lambda
    函数几乎没有区别。
- en: 'I''ve drawn the overall architecture of our system in the following block diagram.
    We''ll walk through each of the five steps in detail. For this diagram, the actual
    problem at hand is less important than the real architecture:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在下面的框图中绘制了我们系统的整体架构。我们将详细讲解五个步骤中的每一个。对于这个图，实际的问题不如实际的架构重要：
- en: '![](img/cc0d6620-3097-4c26-8755-6c6946dfc3d1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cc0d6620-3097-4c26-8755-6c6946dfc3d1.png)'
- en: 'Our implementation can be broken down into five significant steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现可以分为五个重要的步骤：
- en: We trigger a `driver` function that lists the content of a particular bucket
    on S3\. For each file in S3, the driver triggers an SNS event that ultimately
    triggers a `mapper` function. Each `mapper` function receives a different payload
    that corresponds to a file in S3.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们触发一个 `driver` 函数，该函数列出 S3 上特定存储桶的内容。对于 S3 中的每个文件，driver 触发一个 SNS 事件，最终触发一个
    `mapper` 函数。每个 `mapper` 函数接收一个不同的有效载荷，对应于 S3 中的一个文件。
- en: Mappers read data from S3 and perform a first level aggregation.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 映射器从 S3 读取数据并执行第一级聚合。
- en: The mappers write the intermediate *keys* result to S3.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 映射器将中间的 *键* 结果写入 S3。
- en: The data writes to S3 trigger reducers. Every time a reducer is triggered, it
    checks whether all of the intermediate *keys* data is ready. If not, the reducer
    does nothing.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写入 S3 的数据触发 reducer。每次触发 reducer 时，它都会检查所有中间 *键* 数据是否已准备好。如果没有，reducer 就不执行任何操作。
- en: Once all of the *keys* data is ready, a reducer will run a final aggregation
    across all intermediate *keys* files and write the final results to S3.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦所有的 *键* 数据都准备好了，一个 reducer 将在所有中间 *键* 文件上运行最终的聚合，并将最终结果写入 S3。
- en: Many things are going on here, each of which we will discuss in detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中，我们将在详细讨论。
- en: Processing Enron emails with serverless MapReduce
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无服务器 MapReduce 处理 Enron 邮件
- en: I've based our example application on the Enron email corpus, which is publicly
    available on Kaggle. This data is made up of some 500,000 emails from the Enron
    corporation. In total, this dataset is approximately 1.5 GB. What we will be doing
    is counting the number of From-To emails. That is, for each person who *sent*
    an email, we will generate a count of the number of times they sent *to* a particular
    person.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例应用程序基于公开可用的Kaggle上的Enron电子邮件语料库。这些数据由大约50万封来自Enron公司的电子邮件组成。总而言之，这个数据集大约有1.5
    GB。我们将要做的就是计算From-To电子邮件的数量。也就是说，对于每个发送电子邮件的人，我们将生成他们向特定人发送的次数。
- en: Anyone may download and work with this dataset: [https://www.kaggle.com/wcukierski/enron-email-dataset.](https://www.kaggle.com/wcukierski/enron-email-dataset) The
    original data from Kaggle comes as a single file in CSV format. To make this data
    work with this example MapReduce program, I broke the single ~1.4 GB file into
    roughly 100 MB chunks. During this example, it's important to remember that we
    are starting from 14 separate files on S3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人都可以下载并使用这个数据集：[https://www.kaggle.com/wcukierski/enron-email-dataset.](https://www.kaggle.com/wcukierski/enron-email-dataset)
    Kaggle上的原始数据以单个CSV格式的文件形式提供。为了使这些数据与这个示例MapReduce程序兼容，我将这个大约1.4 GB的单个文件分割成了大约100
    MB的块。在这个示例中，重要的是要记住，我们从S3上的14个单独的文件开始。
- en: The data format in our dataset is a CSV with two columns, the first being the
    email message location (on the mail server, presumably) and the second being the
    full email message. Since we're only concerned with the `From` and `To` fields,
    we'll just concern ourselves with the email message.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集中的数据格式是CSV，有两列，第一列是电子邮件消息的位置（在邮件服务器上，可能是这样），第二列是完整的电子邮件消息。由于我们只关心`From`和`To`字段，所以我们只关注电子邮件消息。
- en: The code for this chapter may be found at: [https://github.com/brianz/serverless-design-patterns/tree/master/ch8](https://github.com/brianz/serverless-design-patterns/tree/master/ch8)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下位置找到：[https://github.com/brianz/serverless-design-patterns/tree/master/ch8](https://github.com/brianz/serverless-design-patterns/tree/master/ch8)
- en: Driver function
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱动函数
- en: To initiate the entire process, some event needs to be triggered. Here, we'll
    do this manually. The `driver` function is responsible for setting up the whole
    job and invoking the mappers in parallel. We'll accomplish this using some straightforward
    techniques.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动整个过程，需要触发某个事件。在这里，我们将手动进行。`driver`函数负责设置整个作业并并行调用映射器。我们将使用一些简单的方法来完成这项工作。
- en: By their nature, MapReduce jobs are batch-oriented, meaning they start up, do
    their work, write the results somewhere, and finally shut down. As such, doing
    this on some schedule (whether it be hourly, nightly, or weekly) makes sense.
    If we were doing this for real where the input data was changing, it would be
    trivial to set up this `driver` function to run on a schedule.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce作业本质上是批处理导向的，这意味着它们启动、执行工作、将结果写入某处，最后关闭。因此，按某种时间表（无论是每小时、每晚还是每周）执行此操作是有意义的。如果我们真的在处理输入数据不断变化的情况，设置这个`driver`函数按时间表运行将变得非常简单。
- en: 'As usual, the entry point for all our functions is the `handler.py` file, which
    I have not shown. The `driver` function will invoke the `crawl` function located
    in `mapreduce/driver.py`. The crawl function contains all of the logic, so we''ll
    focus on this. I''ve shown the full listing of `mapreduce/driver.py` in the following
    code block:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，所有函数的入口点是`handler.py`文件，我没有展示。`driver`函数将调用位于`mapreduce/driver.py`中的`crawl`函数。`crawl`函数包含所有逻辑，所以我们将关注这一点。我在以下代码块中展示了`mapreduce/driver.py`的完整列表：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: One implementation detail we will use is uniquely identifying each MapReduce
    run using a UUID. In this way, it will be easy for a given run to find the necessary
    files to work within S3\. Without this, it would be much harder or impossible
    to know what files a given Lambda function should be looking at or processing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的一个实现细节是使用UUID唯一标识每个MapReduce运行。这样，对于给定的运行，将很容易找到在S3中工作的必要文件。没有这个，确定给定的Lambda函数应该查看或处理哪些文件将变得非常困难或不可能。
- en: As this crawler process starts, it lists the content of our input bucket on
    S3\. Each file or S3 `key` the crawler finds is wrapped up into a payload that
    it later uses to trigger the mappers. In the preceding code block, you can see
    the format of the payload objects. Downstream, the reducers will need to know
    how many total mappers were executed so that they know when to begin their work.
    The final `for` loop will amend each payload with the total number of mapper jobs
    being executed along with a unique `job_id`, which is merely an integer from `0`
    to `number_of_mappers - 1`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个爬虫进程开始时，它会列出S3上我们的输入桶的内容。爬虫找到的每个文件或S3的`key`都被包装成一个负载，它稍后会使用这个负载来触发映射器。在前面的代码块中，你可以看到负载对象的格式。在下游，reducer需要知道执行了多少个总映射器，以便他们知道何时开始工作。最后的`for`循环将每个负载修改为正在执行的映射器作业总数以及一个唯一的`job_id`，这只是一个从`0`到`number_of_mappers
    - 1`的整数。
- en: To trigger the mappers in parallel, the crawler sends an SNS event. We could
    have accomplished this with mostly the same result by invoking the mappers directly.
    Personally, I prefer using SNS in these cases since the behavior is asynchronous
    by default. If you remember back to the chapter on the Fanout pattern, invoking
    an asynchronously Lambda function requires you to pass the correct argument to
    the Lambda `invoke` API. In this case, there isn't anything special to remember,
    and our code can trigger the event in the most basic fashion. In this particular
    case, there is otherwise little difference between the two methods and either
    would work.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了并行触发映射器，爬虫发送一个SNS事件。我们可以通过直接调用映射器来实现，结果大致相同。我个人更喜欢在这些情况下使用SNS，因为默认情况下其行为是异步的。如果你还记得关于扇出模式的章节，调用异步Lambda函数需要你向Lambda的`invoke`
    API传递正确的参数。在这种情况下，没有什么特别需要记住的，我们的代码可以以最基本的方式触发事件。在这种情况下，两种方法之间没有太大的区别，任何一种都可以工作。
- en: What is important to recognize here is that an SNS event is triggered for each
    file the crawl function finds in S3\. In our example, there are 14 different files
    of approximately 100 MB each. Fourteen records mean that we will have 14 mapper
    functions running in parallel, each processing a specific S3 file. Mappers know
    which file to process because we've told them via the `bucket` and `key` arguments
    in the payload.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里重要的是要认识到，爬虫函数在S3中找到的每个文件都会触发一个SNS事件。在我们的例子中，有14个大约100MB的不同文件。14条记录意味着我们将有14个映射器函数并行运行，每个处理一个特定的S3文件。映射器知道要处理哪个文件，因为我们通过负载中的`bucket`和`key`参数告诉了它们。
- en: Astute readers may recognize this sub-pattern in the `crawl` function. A single
    function spawning multiple processes asynchronously is exactly what we discussed
    and implemented in the earlier chapters concerning the Fanout pattern. As noted
    in that chapter, you may use Fanout inside other more complex patterns such as
    MapReduce. As you move along with your serverless systems, look for opportunities
    to reuse patterns as they make sense when composing larger and more complex systems.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会在`crawl`函数中认出这个子模式。一个函数异步地生成多个进程正是我们在早期章节中讨论和实现的扇出模式。正如该章节中所述，你可以在其他更复杂的模式（如MapReduce）中使用扇出。随着你继续使用无服务器系统，寻找可以重用模式的机会，当它们在构建更大、更复杂的系统时是有意义的。
- en: Mapper implementation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 映射器实现
- en: Now that we have a way to invoke mappers in parallel, let's look at the logic
    that they implement. Remember again that our task is to count the number of `(From,
    To)` email addresses from a large number of email messages.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了并行调用映射器的方法，让我们来看看它们实现的逻辑。再次记住，我们的任务是统计大量电子邮件中`(From, To)`电子邮件地址的数量。
- en: 'The work involved here is relatively straightforward. With each mapper receiving
    a unique 100 MB file, each invocation will perform the same set of tasks:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里涉及的工作相对简单。每个映射器接收一个独特的100MB文件，每次调用都将执行相同的一组任务：
- en: Download the file from S3
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3下载文件
- en: Parse each message and extract the `From` and `To` fields, making sure to account
    for group sends (where the `From` user sends to multiple `To` addresses)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析每条消息并提取`From`和`To`字段，确保考虑到群发（在这种情况下，`From`用户向多个`To`地址发送邮件）
- en: Count the number of `(From, To)` occurrences
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 统计`(From, To)`出现的次数
- en: Write the results to S3
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果写入S3
- en: 'I''ve shown the full listing of `mapreduce/mapper.py`  in the following code
    block:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在下面的代码块中展示了`mapreduce/mapper.py`的完整列表：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As with the crawler, there isn't much complexity to this mapper code. To count
    the number of `(From, To)` combinations I'm using a basic Python dictionary with
    the keys being a two-element tuple of `(From, To)` and the value being a number.
    The other bits of code around this deal with downloading the file from S3, parsing
    the email message, and calculating all of the `(From, To)` combinations, when
    an email contains multiple To recipients.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 就像爬虫一样，这个映射器代码的复杂性并不大。为了计算`(From, To)`组合的数量，我使用了一个基本的Python字典，键是一个包含`(From,
    To)`两个元素的元组，值是一个数字。围绕这个的其他代码块处理从S3下载文件、解析电子邮件消息以及计算所有`(From, To)`组合，当电子邮件包含多个收件人时。
- en: 'Once the final result is ready, the mapper writes a new CSV file to S3\. Using
    the `Metadata` argument, we can communicate any extra information to our reducers
    without having to write to the file content. Here, we need to tell the reducers
    a few extra things such as:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最终结果准备就绪，映射器将一个新的CSV文件写入S3。使用`Metadata`参数，我们可以向我们的减少器传达任何额外信息，而无需写入文件内容。在这里，我们需要告诉减少器一些额外的事情，例如：
- en: The `run_id`, which is used to limit the files scanned and processed since we're
    sharing an S3 bucket across MapReduce runs
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run_id`，它用于限制扫描和处理的文件，因为我们是在MapReduce运行之间共享S3存储桶'
- en: The `job_id`, so we know which individual mapper job has finished
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`job_id`，这样我们就可以知道哪个单独的映射器作业已经完成'
- en: The total number of jobs, so the reducer will only start once all mappers have
    completed
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业总数，这样减少器只有在所有映射器都完成后才会开始
- en: Reducer implementation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少器实现
- en: 'At this point in our MapReduce run, the mappers have run and eventually write
    their intermediate output data to S3\. Mappers were triggered by an invocation
    of SNS events on a given SNS topic. We will set up the reducers to be triggered
    based on an `s3:ObjectCreated` event. Taking a look at the `serverless.yml` file,
    we can see how I''ve done this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们MapReduce运行的这个阶段，映射器已经运行完毕，并最终将它们的中间输出数据写入S3。映射器是由在给定SNS主题上调用SNS事件触发的。我们将设置减少器，使其基于`s3:ObjectCreated`事件触发。查看`serverless.yml`文件，我们可以看到我是如何做到这一点的：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `s3` section in the `events` block says: *Whenever a new object is uploaded
    to s3 with the *`-done.csv` *suffix, invoke the* `hander.reducer` *function*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在`events`块中的`s3`部分说：*每当上传到s3的新对象带有`-done.csv`后缀时，调用`hander.reducer`函数*。
- en: 'Just as the mapper was reasonably straightforward, so too is the reducer. Much
    of the logic in the reducer is a matter of coordination, determining whether it''s
    time to do its work. Let''s enumerate the steps in the reducer to show precisely
    what it''s doing:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 就像映射器一样，减少器也是相当直接的。减少器中的大部分逻辑都是协调问题，确定是否是执行其工作的时候。让我们列举减少器中的步骤，以精确地展示它在做什么：
- en: Extra metadata from the S3 file that triggered the invocation. Key pieces of
    data in that `Metadata` attribute are necessary for coordination of the entire
    process.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由触发调用的S3文件提供的额外元数据。`Metadata`属性中的关键数据对于整个过程的协调是必要的。
- en: List the contents of our S3 bucket and `run_id` prefix to determine whether
    all mappers have finished.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出我们的S3存储桶和`run_id`前缀的内容，以确定所有映射器是否已经完成。
- en: If there are still reducers running, there is nothing more to do. If all of
    the reducers *have* finished, start the final reduce step.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果还有减少器正在运行，就没有更多的事情要做。如果所有的减少器*都*已经完成，开始最终的减少步骤。
- en: Write an empty file to S3, as a way to claim a lock on the final reduce step.
    Without this, it would be possible for two or more reducers to run concurrently
    if they were invoked at nearly the same time.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向S3写入一个空文件，作为对最终减少步骤的锁定方式。如果没有这个，如果两个或多个减少器几乎同时被调用，它们可能会并发运行。
- en: In the final reduce step, download all of the intermediate files from the mappers
    and perform the final aggregation.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最终的减少步骤中，从映射器下载所有中间文件并执行最终的聚合。
- en: Write the final output to S3.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最终输出写入S3。
- en: 'The full listing of `mapreduce/reducer.py` is shown as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapreduce/reducer.py`的完整列表如下所示：'
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Stepping through this code is hopefully a simple exercise. As you can see, most
    of the work is that of coordination, reading data from S3, and determining whether
    it's time to perform the final reduce step. You can see that when the last mapper
    is finished, the total number of intermediate files will equal the number of mapper
    jobs initially invoked.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步执行这段代码应该是一个简单的练习。正如你所看到的，大部分工作都是协调工作，从S3读取数据，并确定是否是执行最终减少步骤的时候。你可以看到，当最后一个映射器完成时，中间文件的总数将等于最初调用的映射器作业的数量。
- en: 'Looking at S3, we can see the final results after a successful run:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 查看S3，我们可以看到成功运行后的最终结果：
- en: '![](img/7d49365c-7db4-4d3f-8054-0c4d4e4ece2b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7d49365c-7db4-4d3f-8054-0c4d4e4ece2b.png)'
- en: Here, each mapper job created a unique `mapper-job_id-done.csv` file. Once all
    14 files arrived in S3, the final reducer step began, which ultimately read all
    14 files and produced the `FinalResults.csv` file. You can also see how individual
    MapReduce runs are segregated in S3 with the UUID embedded in each S3 key path.
    This is necessary so that each run can operate independently and know which files
    it should be scanning through in S3\. Again, a critical check in the final reducer
    step is to determine whether all of the mappers have finished their work and uploaded
    their results to S3\. The reducer will determine the mapper's state of completeness
    by counting the number of files in S3 using the `run_id` as a prefix during the
    S3 scan. If the number of these `-done.csv` files is less than the total number
    of mappers, they have not all completed.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个映射器作业创建了一个唯一的`mapper-job_id-done.csv`文件。一旦所有14个文件都到达S3，最终归约步骤就开始了，它最终读取所有14个文件并生成`FinalResults.csv`文件。你还可以看到如何使用嵌入在S3键路径中的UUID将单个MapReduce运行在S3中隔离开来。这是必要的，以便每个运行可以独立操作并知道它应该在S3中扫描哪些文件。再次强调，在最终归约步骤中的一个关键检查是确定所有映射器是否已经完成工作并将结果上传到S3。归约器将通过在S3扫描期间使用`run_id`作为前缀来计算S3中的文件数量，以确定映射器的完成状态。如果这些`-done.csv`文件的数目少于映射器的总数，则它们并未全部完成。
- en: 'If we take a look at `FinalResults.csv`, we can see the count of the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`FinalResults.csv`，我们可以看到以下计数：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: What is neat about this is that the processing of all 1.5 GB of data happens
    quite quickly. In my testing, the system produced the final results after approximately
    50 seconds.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 令人印象深刻的是，处理所有1.5 GB的数据发生得相当快。在我的测试中，系统在大约50秒后产生了最终结果。
- en: Understanding the limitations of serverless MapReduce
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解无服务器MapReduce的限制
- en: MapReduce on a serverless platform can work very well. However, there are limitations
    that you need to keep in mind. First and foremost, memory, storage, and time limits
    will ultimately determine whether this pattern is possible for your dataset. Additionally,
    systems such as Hadoop are frameworks that one may use for any analysis. When
    implementing MapReduce in a serverless context, you will likely be implementing
    a system that will solve a particular problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在无服务器平台上进行MapReduce可以非常有效。然而，你需要记住有一些限制。首先，内存、存储和时间限制最终将决定这种模式是否适用于你的数据集。此外，像Hadoop这样的系统是可能用于任何分析的框架。当在无服务器环境中实现MapReduce时，你可能会实现一个解决特定问题的系统。
- en: I find that a serverless MapReduce implementation is viable when your final
    dataset is relatively small (a few hundred megabytes) such that your reducer can
    process all of the data without going over the memory limits for your FaaS provider.
    I will talk through some of the details behind that sentiment in the following.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现，当你的最终数据集相对较小（几百兆字节）时，无服务器MapReduce实现是可行的，这样你的归约器就可以处理所有数据而不会超出你的FaaS提供商的内存限制。我将在以下内容中详细说明这一观点背后的细节。
- en: Memory limits
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存限制
- en: In the reducer phase, all of the data produced from the mappers must, at some
    point, be read and stored in memory. In our example application, the reducer reads
    14 separate files sequentially and builds up a mapping of `(From, To)` addresses
    with corresponding numbers. The number of unique combinations for this dataset
    is 311,209\. That is, our final results file is a CSV with just over 311,000 lines
    for a total of 18.2 MB. As you can imagine, this is well within the boundaries
    of a single Lambda function; reading 14 files keeping approximately 18 MB of data
    in memory isn't beyond the abilities of an individual Lambda function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在归约阶段，来自映射器的所有数据必须在某一点被读取并存储在内存中。在我们的示例应用中，归约器按顺序读取14个单独的文件，并构建一个`(From, To)`地址与相应数字的映射。该数据集的独特组合数是311,209。也就是说，我们的最终结果文件是一个包含超过311,000行的CSV文件，总大小为18.2
    MB。正如你可以想象的那样，这完全在单个Lambda函数的能力范围内；读取14个文件并保持大约18 MB的数据在内存中并不超出单个Lambda函数的能力。
- en: Imagine a case where we are counting IP addresses from a large number of large
    log files along with some other metric. IP addresses have the form `192.168.1.200`
    and can vary in their lengths when represented as a string. For this example,
    presume the format of the lines produced by the reducer will look like `176.100.206.13,0.6088772`,
    which is a single line of CSV with the IP address in the first column and a made-up
    metric in the second column. This string is 24 bytes long. Currently, the maximum
    memory for a single Lambda function is 3 GB, which is 3,221,225,472 bytes. With
    an average length of 24 bytes per IP address, we can hold less than 135 million
    unique IP addresses in memory—`3,221,225,472 / 24 = 134,217,728`. There are approximately
    3,706,452,992 unique IP4 addresses. It's clear that a serverless MapReduce implementation
    for working with IP addresses would break down if the number of unique IP addresses
    in the dataset was in the order of 100 million or more.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，我们需要从大量的大型日志文件中计数IP地址，同时还有一些其他指标。IP地址的形式为`192.168.1.200`，当以字符串形式表示时，其长度可能有所不同。在这个例子中，假设由reducer生成的行格式看起来像`176.100.206.13,0.6088772`，这是一行CSV，其中第一列是IP地址，第二列是虚构的指标。这个字符串长度为24字节。目前，单个Lambda函数的最大内存为3
    GB，即3,221,225,472字节。以每个IP地址平均24字节长度计算，我们只能在内存中存储不到1.35亿个唯一的IP地址——`3,221,225,472
    / 24 = 134,217,728`。大约有3,706,452,992个唯一的IP4地址。很明显，如果数据集中唯一的IP地址数量达到1亿或更多，用于处理IP地址的无服务器MapReduce实现将会崩溃。
- en: Storage limits
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储限制
- en: FaaS systems have storage limits just like they have memory limits. If you have
    looked at the code I've implemented in the example application, you may have noticed
    that I download files and store them in `/tmp` before processing them. This strategy
    isn't necessary, as it's possible to read data from S3 and store it in memory.
    In my testing, I found performance gains when downloading the files to disk and
    then reading them with the standard filesystem `open` calls. Some of the CSV APIs
    I was using were also easier to use with a real file handler rather than a String
    in memory.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FaaS系统有存储限制，就像它们有内存限制一样。如果你查看我在示例应用程序中实现的代码，你可能已经注意到我在处理之前将文件下载并存储在`/tmp`中。这种策略并不是必要的，因为可以从S3读取数据并将其存储在内存中。在我的测试中，我发现将文件下载到磁盘并使用标准的文件系统`open`调用读取它们时，性能有所提升。我使用的某些CSV
    API也更容易与真实的文件处理器而不是内存中的字符串一起使用。
- en: When downloading data and storing the files locally, you must keep in mind the
    storage limits enforced by your FaaS provider. For example, AWS Lambda currently
    gives you 512 MB of ephemeral storage in `/tmp`. If you have a need to download
    files larger than 512 MB, you would need to find another solution, such as reading
    data directly into memory and skipping disks entirely. Reading large data into
    memory will cut into your memory for the final result set, so the balance of getting
    this right when dealing with huge datasets can be tricky.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当下载数据并将文件存储在本地时，你必须记住你的FaaS提供商施加的存储限制。例如，AWS Lambda目前在`/tmp`中提供512 MB的临时存储。如果你需要下载大于512
    MB的文件，你需要找到另一种解决方案，例如直接将数据读入内存并完全跳过磁盘。将大量数据读入内存将占用最终结果集的内存，因此在处理大型数据集时，正确平衡这一点可能很棘手。
- en: Time limits
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间限制
- en: The final limit to keep in mind is the execution limit. Even if your MapReduce
    implementation can stay within the storage and memory limits of your FaaS provider,
    you will still have to ensure your functions complete their work within a given
    time limit. As of this writing, AWS Lambda functions have an upper limit of 300
    seconds. If any part of your MapReduce system takes longer than 300 seconds, you're
    out of luck and will have to find a workaround. With mappers, it's relatively
    simple to break the work into smaller pieces and execute more concurrent mappers.
    However, when the reducer runs, it must load all of the mapper data to compile
    it down to the final result set. If this takes longer than 300 seconds, it will
    be impossible to produce the final results.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的最后一个限制是执行限制。即使你的MapReduce实现可以保持在你的FaaS提供商的存储和内存限制内，你仍然必须确保你的函数在给定的时间限制内完成工作。截至本文撰写时，AWS
    Lambda函数的上限为300秒。如果你的MapReduce系统的任何部分需要超过300秒，你就没有运气了，并且必须找到解决方案。对于mapper来说，将工作分解成更小的部分并执行更多并发mapper相对简单。然而，当reducer运行时，它必须加载所有mapper数据以将其编译成最终结果集。如果这需要超过300秒，将无法产生最终结果。
- en: Exploring alternate implementations
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索替代实现
- en: While you may find great success implementing your serverless MapReduce system,
    there are alternatives that still fall under the serverless umbrella or leverage
    managed services, which should give you a high degree of confidence. I'll talk
    through some of the other systems or techniques you should consider when working
    on your own data analysis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可能在实现无服务器MapReduce系统时取得巨大成功，但仍有其他替代方案仍然属于无服务器范畴或利用托管服务，这应该会给您带来很高的信心。我将讨论一些您在处理自己的数据分析时应考虑的其他系统或技术。
- en: AWS Athena
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Athena
- en: AWS Athena is a relatively new service from AWS. Of course, this is specific
    to AWS, but other cloud providers may offer comparable services. Athena gives
    you the ability to write SQL queries to analyze data stored on S3\. Before you
    can analyze your data with SQL, you must create a virtual *database* with associated
    *tables* across your structured or semi-structured S3 files. You may create these
    tables manually or with another AWS service called **Glue**.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Athena是AWS相对较新的服务。当然，这仅限于AWS，但其他云服务提供商可能提供类似的服务。Athena让您能够编写SQL查询来分析存储在S3上的数据。在您可以使用SQL分析数据之前，您必须创建一个虚拟的*数据库*，并关联到您结构化或半结构化的S3文件中的相关*表*。您可以通过手动方式或使用另一个名为**Glue**的AWS服务来创建这些表。
- en: I won't go into all of the details of setting up a new Athena database or tables
    but will show you the results and ease of use after you've set those up. In this
    example, I've created a database and table for web server logs from the **Big
    Data Benchmark** dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细介绍设置新的Athena数据库或表的所有细节，但会在设置好这些之后向您展示结果和易用性。在这个例子中，我已经为来自**大数据基准**数据集的Web服务器日志创建了一个数据库和表。
- en: This data is publicly accessible on S3, and the details may be found at the
    Big Data Benchmark website: [https://amplab.cs.berkeley.edu/benchmark/](https://amplab.cs.berkeley.edu/benchmark/)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据在S3上是公开可访问的，详细信息可以在大数据基准网站上找到：[https://amplab.cs.berkeley.edu/benchmark/](https://amplab.cs.berkeley.edu/benchmark/)
- en: 'Following is a screenshot from the AWS Athena console. As you can see on the
    left, I''ve loaded up my `uservisits` table, which merely points to the public
    S3 bucket for the `uservisits` log data. I''ve already created a table so that
    Athena knows the structure and datatypes for the CSV data stored on S3\. Once
    this is done, I can use ANSI-SQL queries to analyze the data. In the following
    screenshot, you can see how I''ve selected the first 10 rows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是AWS Athena控制台的屏幕截图。如图所示，我已加载我的`uservisits`表，该表仅指向`uservisits`日志数据的公共S3存储桶。我已经创建了一个表，以便Athena知道存储在S3上的CSV数据的结构和数据类型。一旦完成，我就可以使用ANSI-SQL查询来分析数据。在下面的屏幕截图中，您可以看到我如何选择了前10行：
- en: '![](img/79604fe2-00cc-4317-8d2d-72cbb745cd15.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79604fe2-00cc-4317-8d2d-72cbb745cd15.png)'
- en: It's also possible to rename columns to something meaningful and change datatypes
    for each column. In my previous example table, several columns are named `colX`,
    where I've renamed other columns `ip`, `score`, and `agent`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将列重命名为有意义的名称，并为每个列更改数据类型。在我之前的示例表中，有几个列被命名为`colX`，我已经将其他列重命名为`ip`、`score`和`agent`。
- en: 'From there, I''ll run a query that calculates the total score grouped by IP
    address. This query and the results can be seen in the following screenshot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我将运行一个查询，按IP地址分组计算总分。这个查询和结果可以在下面的屏幕截图中看到：
- en: '![](img/6adb762a-eb47-483c-8545-78a2be0914ff.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6adb762a-eb47-483c-8545-78a2be0914ff.png)'
- en: 'The final results of this query were quite impressive in my opinion. The query
    scanned just over 24 GB of data on S3 and took just under five minutes to execute.
    I can view this metadata in the History area of Athena:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这个查询的最终结果相当令人印象深刻。查询在S3上扫描了超过24 GB的数据，并且只用了不到五分钟的时间来执行。我可以在Athena的历史区域查看这个元数据：
- en: '![](img/24150ad7-b44c-4528-9db6-53bfbe198aab.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24150ad7-b44c-4528-9db6-53bfbe198aab.png)'
- en: Given the simplicity of the Athena system, it's one that I would strongly suggest
    you investigate. What is nice about this is that new data may arrive on S3 at
    regular intervals, and your queries would reflect the results whenever they're
    run. Additionally, the overhead and price for running these queries are quite
    low. Anything you can do with ANSI-SQL is possible with Athena. However, Athena
    has limitations in that your data needs to be well structured and the data prepared
    ahead of time. In our example MapReduce application, we had application logic
    that was extracting the `To` and `From` fields from email text. To do this with
    Athena would require a data preparation step to extract that data from the source
    data, and then store the extracted and structured information on S3.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Athena系统的简单性，我强烈建议您调查它。令人高兴的是，新数据可能定期到达S3，并且您的查询会在运行时反映结果。此外，运行这些查询的 overhead
    和价格相当低。您可以使用ANSI-SQL做的任何事情都可以用Athena做。然而，Athena有一些限制，即您的数据需要很好地结构化，并且数据需要提前准备。在我们的示例MapReduce应用程序中，我们有应用逻辑从电子邮件文本中提取`To`和`From`字段。要使用Athena做这件事，需要数据准备步骤从源数据中提取该数据，然后将提取和结构化的信息存储在S3上。
- en: Using a data store for results
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据存储结果
- en: In our example MapReduce system, we stored the state in S3\. That is, every
    mapper would work on a subset of the dataset, do some initial reduce step, and
    then save the intermediate results as a file on S3\. This technique is helpful
    since it's relatively simple and storing static data on S3 is painless. Since
    each mapper is writing a unique file to S3, we also didn't have to worry much
    about race conditions or other mappers overwriting our data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例MapReduce系统中，我们将状态存储在S3上。也就是说，每个映射器都会处理数据集的一个子集，执行一些初始的减少步骤，然后将中间结果作为文件存储在S3上。这种技术很有帮助，因为它相对简单，在S3上存储静态数据并不痛苦。由于每个映射器都在向S3写入一个唯一的文件，所以我们也不必过多担心竞态条件或其他映射器覆盖我们的数据。
- en: The downside of this technique is that our reducer needs to read in all of the
    intermediate results to do the final reduce step. As I explained earlier, this
    *could* be a limiting factor for your system depending on the size of the final
    result. One alternative implementation would be using a data store such as Redis
    to store the mapper keys and values.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术的缺点是，我们的减少器需要读取所有中间结果来完成最终的减少步骤。正如我之前解释的，这*可能*是您系统的一个限制因素，具体取决于最终结果的大小。一个替代的实现方案是使用如Redis这样的数据存储来存储映射器的键和值。
- en: The way this would work is that mappers, working in parallel, would process
    subsets of the initial dataset. Once the mappers have finished their initial aggregation,
    they would write the results to Redis, which would act as a central location for
    all of the reduced data. Mappers would either insert new records for a particular
    key if that key did not exist or update the data for a key. In some cases, such
    as counting items, we wouldn't even need a reducer as the mappers would merely
    increment the value stored in Redis if the key was already present.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方式的工作原理是，映射器并行工作，处理初始数据集的子集。一旦映射器完成了他们的初始聚合，他们就会将结果写入Redis，Redis将作为所有减少数据的中心位置。映射器会为特定键插入新记录，如果该键不存在，或者更新键的数据。在某些情况下，例如计数项目，我们甚至不需要减少器，因为如果键已经存在，映射器只需增加Redis中存储的值。
- en: In cases where we would like to calculate an average or something else that
    depends on keeping track of all values for a particular key, the reduce step would
    consist of scanning through all keys and performing the final reduce step based
    on the values stored for each key.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们想要计算平均值或其他依赖于跟踪特定键所有值的场景中，减少步骤将包括遍历所有键并根据每个键存储的值执行最终的减少步骤。
- en: 'Imagine a case where we were calculating the average value per key. Mappers
    would perform work that looked something along these lines:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样一个场景，我们正在计算每个键的平均值。映射器将执行类似以下的工作：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Making sure to use the `pipeline` technique to ensure we don''t hit a race
    condition, our mappers push results into Redis as lists for each key. Reducers
    would then iterate around all of the keys and perform a count of the number of
    items in each list, as well as summing up the entire list. For example, the average
    value for a particular key named `height` would look like the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 确保使用`pipeline`技术来确保我们不会遇到竞态条件，我们的映射器将结果推送到Redis中，为每个键创建列表。然后，减少器将遍历所有键，对每个列表中的项目数量进行计数，以及计算整个列表的总和。例如，特定键名为`height`的平均值可能如下所示：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: While Redis is incredibly performant, it would still be easy to overwhelm a
    single Redis server with enough concurrency from serverless functions. The Redis
    (or another data store) technique could also be a good workaround for cases when
    you reach memory limitation in your serverless MapReduce systems. There are other
    things to consider too, such as, how do you finally report the entire result set
    in aggregate, if needed. Also, how or when do you clear out the Redis DB?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Redis 的性能非常出色，但仍然很容易因为无服务器函数带来的足够并发而使单个 Redis 服务器过载。当你在无服务器 MapReduce 系统中达到内存限制时，Redis（或另一个数据存储）技术也可以作为一个很好的解决方案。还有其他一些事情需要考虑，例如，如果需要，你如何最终报告整个结果集的汇总，以及如何或何时清理
    Redis 数据库？
- en: Using Elastic MapReduce
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Elastic MapReduce
- en: Elastic MapReduce (EMR) from AWS is another alternative if you need the full
    power of Hadoop. EMR is just what it sounds like, a managed Hadoop system that
    is easy to scale up or down as required. The advantage of EMR is that Hadoop developers
    should feel comfortable since it is a managed Hadoop infrastructure on demand.
    EMR can also run other frameworks, such as Spark and Hive.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 的 Elastic MapReduce (EMR) 是如果你需要 Hadoop 全部功能时的另一个选择。EMR 正如其名，是一个易于根据需要扩展或缩减的托管
    Hadoop 系统。EMR 的优势在于，Hadoop 开发者应该会感到舒适，因为它是一个按需提供的托管 Hadoop 基础设施。EMR 还可以运行其他框架，如
    Spark 和 Hive。
- en: EMR doesn't fit with the *serverless* theme really, since you pay for every
    minute that a cluster is up, regardless of whether it's running any of your jobs.
    Still, the fact that you can have a fully-managed Hadoop cluster is quite attractive
    if your use cases warrant it. Another beautiful thing about EMR, as with all things
    cloud, is that it's possible to create a cluster on-demand, run your jobs, and
    then shut it down. Creating and destroying an EMR cluster requires some form of
    automation with API calls, CloudFormation, or Terraform, but it's still possible
    and the more automation you can put in place, the better.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: EMR 并不完全符合 *无服务器* 主题，因为你需要为集群每运行一分钟付费，无论是否运行你的作业。尽管如此，如果你有需要完全托管 Hadoop 集群的使用案例，这一点仍然非常有吸引力。EMR
    的另一个美妙之处，就像所有云服务一样，是你可以按需创建集群，运行作业，然后关闭它。创建和销毁 EMR 集群需要某种形式的自动化，例如 API 调用、CloudFormation
    或 Terraform，但这仍然是可能的，并且你可以放置的自动化越多，效果越好。
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I gave an overview of what the MapReduce pattern looked like
    in a general sense and demonstrated how MapReduce works with some example code.
    From there, we reviewed the MapReduce pattern as applied to serverless architectures.
    We stepped through the details of implementing this pattern by parsing 1.5 GB
    of email data and counting the unique occurrences of `From` and `To` email addresses.
    I showed that a serverless system could be built using this pattern to perform
    our task in less than a minute, on average.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我概述了 MapReduce 模式在一般意义上的样子，并通过一些示例代码演示了 MapReduce 的工作原理。从那里，我们回顾了 MapReduce
    模式在无服务器架构中的应用。我们通过解析 1.5 GB 的电子邮件数据并计算 `From` 和 `To` 电子邮件地址的唯一出现次数来详细介绍了实现此模式的过程。我展示了可以使用此模式构建一个无服务器系统，平均在不到一分钟的时间内完成任务。
- en: We covered some of the limitations of this pattern when implemented on a serverless
    platform. Finally, we discussed alternative solutions for general data analysis
    problems using serverless platforms such as AWS Athena and managed systems such
    as EMR, as well as ways to use a centralized data store such as Redis in a serverless
    MapReduce system.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了在无服务器平台上实现此模式时的一些局限性。最后，我们讨论了使用无服务器平台（如 AWS Athena）和托管系统（如 EMR）以及使用集中式数据存储（如
    Redis）在无服务器 MapReduce 系统中的方法来解决一般数据分析问题。
