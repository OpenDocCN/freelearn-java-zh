- en: Chapter 7. Advanced Performance Strategies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 高级性能策略
- en: In this chapter, we will look at some advanced strategies for improving the
    performance and scalability of production applications, through code as well as
    server architecture. We will explore options for running applications in multi-node
    server clusters, to spread out and handle user requests in a distributed fashion.
    We will also learn how to use sharding to help make our Lucene indexes faster
    and more manageable.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些高级策略，通过代码以及服务器架构来提高生产应用程序的性能和可伸缩性。我们将探讨运行应用程序的多节点服务器集群选项，以分布式方式分散和处理用户请求。我们还将学习如何使用分片来使我们的Lucene索引更快且更易于管理。
- en: General tips
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通用建议
- en: 'Before diving into some advanced strategies for improving performance and scalability,
    let''s briefly recap some of the general performance tips already spread across
    the book:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨一些提高性能和可伸缩性的高级策略之前，让我们简要回顾一下书中已经提到的某些通用性能优化建议。
- en: 'When mapping your entity classes for Hibernate Search, use the optional elements
    of the `@Field` annotation to strip the unnecessary bloat from your Lucene indexes
    (see [Chapter 2](ch02.html "Chapter 2. Mapping Entity Classes"), *Mapping Entity
    Classes*):'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当为Hibernate Search映射实体类时，使用`@Field`注解的可选元素去除Lucene索引中的不必要膨胀（参见[第2章](ch02.html
    "第2章 映射实体类")，*映射实体类*）：
- en: If you are definitely not using index-time boosting (see [Chapter 4](ch04.html
    "Chapter 4. Advanced Mapping"), *Advanced Mapping*), then there is no reason to
    store the information needed to make this possible. Set the `norms` element to
    `Norms.NO`.
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你确实不使用索引时提升（参见[第4章](ch04.html "第4章 高级映射")，*高级映射*），那么就没有理由存储实现此功能所需的信息。将`norms`元素设置为`Norms.NO`。
- en: By default, the information needed for a projection-based query is not stored
    unless you set the `store` element to `Store.YES` or `Store.COMPRESS` (see [Chapter
    5](ch05.html "Chapter 5. Advanced Querying"), *Advanced Querying*). If you had
    projection-based queries that are no longer being used, then remove this element
    as part of the cleanup.
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，除非将`store`元素设置为`Store.YES`或`Store.COMPRESS`（参见[第5章](ch05.html "第5章 高级查询")，*高级查询*），否则基于投影的查询所需的信息不会被存储。如果你有不再使用的基于投影的查询，那么在进行清理时删除这个元素。
- en: Use conditional indexing (see [Chapter 4](ch04.html "Chapter 4. Advanced Mapping"),
    *Advanced Mapping*) and partial indexing ([Chapter 2](ch02.html "Chapter 2. Mapping
    Entity Classes"), *Mapping Entity Classes*) to reduce the size of Lucene indexes.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用条件索引（参见[第4章](ch04.html "第4章 高级映射")，*高级映射*）和部分索引（参见[第2章](ch02.html "第2章 映射实体类")，*映射实体类*）来减小Lucene索引的大小。
- en: Rely on filters to narrow your results at the Lucene level, rather than using
    a `WHERE` clause at the database query level (see [Chapter 5](ch05.html "Chapter 5. Advanced
    Querying"), *Advanced Querying*).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于过滤器在Lucene层面缩小结果，而不是在数据库查询层面使用`WHERE`子句（参见[第5章](ch05.html "第5章 高级查询")，*高级查询*）。
- en: Experiment with projection-based queries wherever possible (see [Chapter 5](ch05.html
    "Chapter 5. Advanced Querying"), *Advanced Querying*), to reduce or eliminate
    the need for database calls. Be aware that with advanced database caching, the
    benefits might not always justify the added complexity.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能尝试使用基于投影的查询（参见[第5章](ch05.html "第5章 高级查询")，*高级查询*），以减少或消除对数据库调用的需求。请注意，随着数据库缓存的提高，这些好处可能并不总是值得增加的复杂性。
- en: Test various index manager options (see [Chapter 6](ch06.html "Chapter 6. System
    Configuration and Index Management"), *System Configuration and Index Management*),
    such as trying the near-real-time index manager or the `async` worker execution
    mode.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试各种索引管理器选项（参见[第6章](ch06.html "第6章 系统配置和索引管理")，*系统配置和索引管理*），例如尝试近实时索引管理器或`async`工作执行模式。
- en: Running applications in a cluster
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在集群中运行应用程序
- en: Making modern Java applications scale in a production environment usually involves
    running them in a cluster of server instances. Hibernate Search is perfectly at
    home in a clustered environment, and offers multiple approaches for configuring
    a solution.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中使现代Java应用程序扩展通常涉及在服务器实例的集群中运行它们。Hibernate Search非常适合集群环境，并提供了多种配置解决方案的方法。
- en: Simple clusters
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单集群
- en: 'The most straightforward approach requires very little Hibernate Search configuration.
    Just set up a file server for hosting your Lucene indexes and make it available
    to every server instance in your cluster (for example, NFS, Samba, and so on):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法需要非常少的Hibernate Search配置。只需为托管您的Lucene索引设置一个文件服务器，并使其可供您集群中的每个服务器实例使用（例如，NFS、Samba等）：
- en: '![Simple clusters](img/9207OS_07_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![简单集群](img/9207OS_07_01.jpg)'
- en: A simple cluster with multiple server nodes using a common Lucene index on a
    shared drive
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 具有多个服务器节点的简单集群，使用共享驱动上的公共Lucene索引
- en: Each application instance in the cluster uses the default index manager, and
    the usual `filesystem` directory provider (see [Chapter 6](ch06.html "Chapter 6. System
    Configuration and Index Management"), *System Configuration and Index Management*).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的每个应用程序实例都使用默认的索引管理器，以及常用的`filesystem`目录提供程序（参见[第6章](ch06.html "Chapter 6. System
    Configuration and Index Management")，*系统配置和索引管理*）。
- en: In this arrangement, all of the server nodes are true peers. They each read
    from the same Lucene index, and no matter which node performs an update, that
    node is responsible for the write. To prevent corruption, Hibernate Search depends
    on simultaneous writes being blocked, by the locking strategy (that is, either
    "simple" or "native", see [Chapter 6](ch06.html "Chapter 6. System Configuration
    and Index Management"), *System Configuration and Index Management*).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种安排中，所有的服务器节点都是真正的对等节点。它们各自从同一个Lucene索引中读取，无论哪个节点执行更新，那个节点就负责写入。为了防止损坏，Hibernate
    Search 依赖于锁定策略（即“简单”或“本地”，参见[第6章](ch06.html "Chapter 6. System Configuration and
    Index Management")，*系统配置和索引管理*)同时写入被阻止。
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Recall that the "near-real-time" index manager is explicitly incompatible with
    a clustered environment.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，“近实时”索引管理器与集群环境是不兼容的。
- en: The advantage of this approach is two-fold. First and foremost is simplicity.
    The only steps involved are setting up a filesystem share, and pointing each application
    instance's directory provider to the same location. Secondly, this approach ensures
    that Lucene updates are instantly visible to all the nodes in the cluster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是两方面的。首先是简单性。涉及的步骤仅包括设置一个文件系统共享，并将每个应用程序实例的目录提供程序指向同一位置。其次，这种方法确保Lucene更新对集群中的所有节点立即可见。
- en: However, a serious downside is that this approach can only scale so far. Very
    small clusters may work fine, but larger numbers of nodes trying to simultaneously
    access the same shared files will eventually lead to lock contention.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的严重缺点是它只能扩展到一定程度。非常小的集群可能运行得很好，但是尝试同时访问同一共享文件的更多节点最终会导致锁定争用。
- en: Also, the file server on which the Lucene indexes are hosted is a single point
    of failure. If the file share goes down, then your search functionality breaks
    catastrophically and instantly across the entire cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，托管Lucene索引的文件服务器是一个单点故障。如果文件共享挂了，那么在整个集群中的搜索功能会立即灾难性地崩溃。
- en: Master-slave clusters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主从集群
- en: When your scalability needs outgrow the limitations of a simple cluster, Hibernate
    Search offers more advanced models to consider. The common element among them
    is the idea of a master node being responsible for all Lucene write operations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的可扩展性需求超出简单集群的限制时，Hibernate Search 提供了更高级别的模型供您考虑。它们之间的共同点是主节点负责所有Lucene写操作的理念。
- en: Clusters may also include any number of slave nodes. Slave nodes may still initiate
    Lucene updates, and the application code can't really tell the difference. However,
    under the covers, slave nodes delegate that work to be actually performed by the
    master node.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 集群还可能包括任何数量的从节点。从节点仍然可以初始化Lucene更新，应用程序代码实际上无法区分。然而，在底层，从节点将这项工作委托给主节点实际执行。
- en: Directory providers
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目录提供程序
- en: 'In a master-slave cluster, there is still an "overall master" Lucene index,
    which logically stands apart from all of the nodes. This may be filesystem-based,
    just as it is with a simple cluster. However, it may instead be based on JBoss
    Infinispan ([http://www.jboss.org/infinispan](http://www.jboss.org/infinispan)),
    an open source in-memory NoSQL datastore sponsored by the same company that principally
    sponsors Hibernate development:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在主从集群中，仍然有一个“总体主”Lucene索引，它在逻辑上与所有节点区分开来。这个索引可能是基于文件系统的，正如它在一个简单集群中一样。然而，它可能是基于JBoss
    Infinispan（[http://www.jboss.org/infinispan](http://www.jboss.org/infinispan)），一个由同一公司主要赞助Hibernate开发的开源内存中NoSQL数据存储：
- en: In a **filesystem-based** approach, all nodes keep their own local copies of
    the Lucene indexes. The master node actually performs updates on the overall master
    indexes, and all of the nodes periodically read from that overall master to refresh
    their local copies.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**基于文件系统的**方法中，所有节点都保留它们自己的Lucene索引的本地副本。主节点实际上在整体主索引上执行更新，所有节点定期从那个整体主索引中读取以刷新它们的本地副本。
- en: In an **Infinispan-based** approach, the nodes all read from the Infinispan
    index (although it is still recommended to delegate writes to a master node).
    Therefore, the nodes do not need to maintain their own local index copies. In
    reality, because Infinispan is a distributed datastore, portions of the index
    will reside on each node anyway. However, it is still best to visualize the overall
    index as a separate entity.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**Infinispan基于**的方法中，所有节点都从Infinispan索引中读取（尽管仍然建议将写操作委派给主节点）。因此，节点不需要维护它们自己的本地索引副本。实际上，由于Infinispan是一个分布式数据存储，索引的某些部分将驻留在每个节点上。然而，最好还是将整个索引视为一个单独的实体。
- en: Worker backends
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作端后端
- en: 'There are two available mechanisms by which slave nodes delegate write operations
    to the master node:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 奴隶节点将写操作委派给主节点的两种可用机制：
- en: A **JMS** message queue provider creates a queue, and slave nodes send messages
    to this queue with details about Lucene update requests. The master node monitors
    this queue, retrieves the messages, and actually performs the update operations.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JMS**消息队列提供程序创建一个队列，奴隶节点将有关Lucene更新请求的详细信息发送到这个队列。主节点监控这个队列，检索消息，并实际执行更新操作。'
- en: You may instead replace JMS with **JGroups** ([http://www.jgroups.org](http://www.jgroups.org)),
    an open source multicast communication system for Java applications. This has
    the advantage of being faster and more immediate. Messages are received in real-time,
    synchronously rather than asynchronously.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以选择用**JGroups**（[http://www.jgroups.org](http://www.jgroups.org)）替换JMS，这是一个用于Java应用程序的开源多播通信系统。它的优点是速度更快，更立即。消息实时接收，同步而不是异步。
- en: However, JMS messages are generally persisted to a disk while awaiting retrieval,
    and therefore can be recovered and processed later, in the event of an application
    crash. If you are using JGroups and the master node goes offline, then all the
    update requests sent by slave nodes during that outage period will be lost. To
    fully recover, you would likely need to reindex your Lucene indexes manually.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，JMS消息通常在等待检索时持久化到磁盘上，因此可以在应用程序崩溃的情况下恢复并稍后处理。如果您使用JGroups并且主节点离线，那么在停机期间奴隶节点发送的所有更新请求都将丢失。为了完全恢复，您可能需要手动重新索引您的Lucene索引。
- en: '![Worker backends](img/9207OS_07_02.jpg)'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Worker backends](img/9207OS_07_02.jpg)'
- en: A master-slave cluster using a directory provider based on filesystem or Infinispan,
    and worker based on JMS or JGroups. Note that when using Infinispan, nodes do
    not need their own separate index copies.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个基于文件系统或Infinispan的目录提供程序和基于JMS或JGroups的工作程序的主从集群。请注意，当使用Infinispan时，节点不需要它们自己的单独索引副本。
- en: A working example
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个工作示例
- en: Experimenting with all of the possible clustering strategies requires consulting
    the Hibernate Search Reference Guide, as well as the documentation for Infinispan
    and JGroups. However, we will get started by implementing a cluster with the filesystem
    and JMS approach, since everything else is just a variation on this standard theme.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试所有可能的集群策略，需要查阅Hibernate Search参考指南，以及Infinispan和JGroups的文档。然而，我们将从实现使用文件系统和JMS方法的集群开始，因为其他所有内容都只是这个标准主题的变体。
- en: This chapter's version of the VAPORware Marketplace application discards the
    Maven Jetty plugin that we've been using all along. This plugin is great for testing
    and demo purposes, but it is meant for running a single server instance, and we
    now need to run at least two Jetty instances simultaneously.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本章版本的VAPORware Marketplace应用摒弃了我们一直使用的Maven Jetty插件。这个插件非常适合测试和演示目的，但它只适用于运行单个服务器实例，而我们现在需要同时运行至少两个Jetty实例。
- en: 'To accomplish this, we will configure and launch Jetty instances programmatically.
    If you look under `src/test/java/` in the `chapter7` project, there is now a `ClusterTest`
    class. It is structured for JUnit 4, so that Maven can automatically invoke its
    `testCluster()` method after a build. Let''s take a look at the relevant portions
    of that test case method:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将以编程方式配置和启动Jetty实例。如果你在`chapter7`项目的`src/test/java/`目录下查看，现在有一个`ClusterTest`类。它为JUnit
    4设计，以便Maven可以在构建后自动调用其`testCluster()`方法。让我们看看那个测试用例方法的相关部分：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although this is all running on one physical machine, we are simulating a cluster
    for test and demo purposes. One Jetty server instance launches on port 8080 as
    the master node, and another Jetty server launches on port 8181 as a slave node.
    The difference between the two nodes is that they use separate `web.xml` files,
    which in turn load different listeners upon startup.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些都在一台物理机器上运行，但我们为了测试和演示目的模拟了一个集群。一个Jetty服务器实例在端口8080上作为主节点启动，另一个Jetty服务器在端口8181上作为从节点启动。这两个节点之间的区别在于，它们使用不同的`web.xml`文件，在启动时相应地加载不同的监听器。
- en: In the previous versions of this application, a `StartupDataLoader` class handled
    all of the database and Lucene initialization. Now, the two nodes use `MasterNodeInitializer`
    and `SlaveNodeInitializer`, respectively. These in turn load Hibernate ORM and
    Hibernate Search settings from separate files, named `hibernate.cfg.xml` and `hibernate-slave.cfg.xml`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用程序的先前版本中，一个`StartupDataLoader`类处理了所有数据库和Lucene的初始化。现在，两个节点分别使用`MasterNodeInitializer`和`SlaveNodeInitializer`。这些依次从名为`hibernate.cfg.xml`和`hibernate-slave.cfg.xml`的不同文件加载Hibernate
    ORM和Hibernate Search设置。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There are many ways in which you might configure an application for running
    as the master node or as a slave node instance. Rather than building separate
    WARs, with separate versions of `web.xml` or `hibernate.cfg.xml`, you might use
    a dependency injection framework to load the correct settings based on something
    in the environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以配置一个应用程序以作为主节点或从节点实例运行。而不是构建不同的WAR，具有不同的`web.xml`或`hibernate.cfg.xml`版本，你可能会使用依赖注入框架根据环境中的某个内容加载正确的设置。
- en: 'Both versions of the Hibernate the `config` file set the following Hibernate
    Search properties:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Hibernate的两种版本都设置了`config`文件中的以下Hibernate Search属性：
- en: '`hibernate.search.default.directory_provider`: In previous chapters we have
    seen this populated with either `filesystem` or `ram`. The other option discussed
    earlier is `infinispan`.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.directory_provider`：在之前的章节中，我们看到这个属性被设置为`filesystem`或`ram`。之前讨论过的另一个选项是`infinispan`。'
- en: Here, we use `filesystem-master` and `filesystem-slave` on the master and slave
    node, respectively. Both of these directory providers are similar to regular `filesystem`,
    and work with all of the related properties that we've seen so far (e.g. location,
    locking strategy, etc).
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们在主节点和从节点上分别使用`filesystem-master`和`filesystem-slave`。这两个目录提供者都与常规的`filesystem`类似，并且与迄今为止我们看到的所有相关属性（如位置、锁定策略等）一起工作。
- en: However, the "master" variant includes functionality for periodically refreshing
    the overall master Lucene indexes. The "slave" variant does the reverse, periodically
    refreshing its local copy with the overall master contents.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，“主”变体包含了定期刷新整体主Lucene索引的功能。而“从”变体则相反，定期用整体主内容刷新其本地副本。
- en: '`hibernate.search.default.indexBase`: Just as we''ve seen with single-node
    versions in the earlier chapters, this property contains the base directory for
    the *local* Lucene indexes. Since our example cluster here is running on the same
    physical machine, the master and slave nodes use different values for this property.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.indexBase`：正如我们之前在单节点版本中看到的，这个属性包含了*本地* Lucene 索引的基础目录。由于我们这里的示例集群在同一台物理机器上运行，主节点和从节点对这个属性使用不同的值。'
- en: '`hibernate.search.default.sourceBase`: This property contains the base directory
    for the *overall master* Lucene indexes. In a production setting, this would be
    on some sort of shared filesystem, mounted and accessible to all nodes. Here,
    the nodes are running on the same physical machine, so the master and slave nodes
    use the same value for this property.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.sourceBase`：这个属性包含了*整体主* Lucene 索引的基础目录。在生产环境中，这将是某种共享文件系统，挂在并可供所有节点访问。在这里，节点在同一台物理机器上运行，所以主节点和从节点对这个属性使用相同的值。'
- en: '`hibernate.search.default.refresh`: This is the interval (in seconds) between
    index refreshes. The master node will refresh the overall master indexes after
    each interval, and slave nodes will use the overall master to refresh their own
    local copies. This chapter''s version of the VAPORware Marketplace application
    uses a 10-second setting for demo purposes, but that would be far too short for
    production. The default setting is 3600 seconds (one hour).'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.refresh`：这是索引刷新之间的间隔（以秒为单位）。主节点在每个间隔后刷新整体主索引，奴隶节点使用整体主索引刷新它们自己的本地副本。本章的VAPORware
    Marketplace应用程序使用10秒的设置作为演示目的，但在生产环境中这太短了。默认设置是3600秒（一小时）。'
- en: 'To establish a JMS worker backend, there are three additional settings required
    for the slave node *only*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立一个JMS工作后端，奴隶节点*仅*需要三个额外的设置：
- en: '`hibernate.search.default.worker.backend`: Set this value to `jms`. The default
    value, `lucene`, has been applied in earlier chapters because no setting was specified.
    If you use JGroups, then it would be set to `jgroupsMaster` or `jgroupsSlave`
    depending upon the node type.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.worker.backend`：将此值设置为`jms`。默认值`lucene`在之前的章节中已经应用，因为没有指定设置。如果你使用JGroups，那么它将被设置为`jgroupsMaster`或`jgroupsSlave`，这取决于节点类型。'
- en: '`hibernate.search.default.worker.jms.connection_factory`: This is the name
    by which Hibernate Search looks up your JMS connection factory in JNDI. This is
    similar to how Hibernate ORM uses the `connection.datasource` property to retrieve
    a JDBC connection from the database.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.worker.jms.connection_factory`：这是Hibernate Search在JNDI中查找你的JMS连接工厂的名称。这与Hibernate
    ORM使用`connection.datasource`属性从数据库检索JDBC连接的方式类似。'
- en: In both the cases, the JNDI configuration is specific to the app server in which
    your application runs. To see how the JMS connection factory is set up, see the
    `src/main/webapp/WEB-INF/jetty-env.xml` Jetty configuration file. We are using
    Apache ActiveMQ in this demo, but any JMS-compatible provider would work just
    as well.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这两种情况下，JNDI配置都是特定于你的应用程序运行的应用服务器。要了解JMS连接工厂是如何设置的，请查看`src/main/webapp/WEB-INF/jetty-env.xml`这个Jetty配置文件。在这个示例中我们使用Apache
    ActiveMQ，但任何兼容JMS的提供商都会同样适用。
- en: '`hibernate.search.default.worker.jms.queue`: The JNDI name of the JMS queue
    to which slave nodes send write requests to Lucene. This too is configured at
    the app server level, right alongside the connection factory.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hibernate.search.default.worker.jms.queue`：从奴隶节点向Lucene发送写请求的JMS队列的JNDI名称。这也是在应用服务器级别配置的，紧挨着连接工厂。'
- en: With these worker backend settings, a slave node will automatically send a message
    to the JMS queue that a Lucene update is needed. To see that this is happening,
    the new `MasterNodeInitializer` and `SlaveNodeInitializer` classes each load half
    of the usual test data set. We will know that our cluster works if all of the
    test entities are eventually indexed together, and are being returned by search
    queries that are run from either nodes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些工作后端设置，奴隶节点将自动向JMS队列发送一条消息，表明需要Lucene更新。为了看到这种情况的发生，新的`MasterNodeInitializer`和`SlaveNodeInitializer`类各自加载了一半的通常测试数据集。如果我们所有的测试实体最终都被一起索引，并且可以从任一节点运行的搜索查询中检索到它们，那么我们就会知道我们的集群运行正常。
- en: Although Hibernate Search sends messages from the slave nodes to the JMS queue
    automatically, it is your responsibility to have the master node retrieve those
    messages and process them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Hibernate Search会自动从奴隶节点向JMS队列发送消息，但让主节点检索这些消息并处理它们是你的责任。
- en: In a JEE environment, you might use a message-driven bean, as is suggested by
    the Hibernate Search documentation. Spring also has a task execution framework
    that can be leveraged. However, in any framework, the basic idea is that the master
    node should spawn a background thread to monitor the JMS queue and process its
    messages.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在JEE环境中，你可能会使用消息驱动bean，正如Hibernate Search文档所建议的那样。Spring也有一个可以利用的任务执行框架。然而，在任何框架中，基本思想是主节点应该产生一个后台线程来监控JMS队列并处理其消息。
- en: This chapter's version of the VAPORware Marketplace application contains a `QueueMonitor`
    class for this purpose, which is wrapped into a `Thread` object and spawned by
    the `MasterNodeInitializer` class.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的VAPORware Marketplace应用程序包含一个用于此目的的`QueueMonitor`类，该类被包装在一个`Thread`对象中，由`MasterNodeInitializer`类产生。
- en: To perform the actual Lucene updates, the easiest approach is to create your
    own custom subclass of `AbstractJMSHibernateSearchController`. Our implementation
    is called `QueueController`, and does little more than wrapping this abstract
    base class.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行实际的Lucene更新，最简单的方法是创建您自己的自定义子类`AbstractJMSHibernateSearchController`。我们的实现称为`QueueController`，所做的只是包装这个抽象基类。
- en: When the queue monitor receives a `javax.jms.Message` object from the JMS queue,
    it is simply passed as-is to the controller's base class method `onMessage`. That
    built-in method handles the Lucene update for us.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当队列监视器从JMS队列中接收到`javax.jms.Message`对象时，它只是原样传递给控制器的基类方法`onMessage`。那个内置方法为我们处理Lucene更新。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: As you can see, there is a lot more involved to a master-slave clustering approach
    than there is to a simple cluster. However, the master-slave approach offers a
    dramatically greater upside in scalability.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，主从集群方法涉及的内容比简单集群要多得多。然而，主从方法在可扩展性方面提供了巨大的优势。
- en: It also reduces the single-point-of-failure risk. It is true that this architecture
    involves a single "master" node, through which all Lucene write operations must
    flow. However, if the master node goes down, the slave nodes continue to function,
    because their search queries run against their own local index copies. Also, update
    requests should be persisted by the JMS provider, so that those updates can still
    be performed once the master node is brought back online.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 它还减少了单点故障的风险。确实，这种架构涉及一个单一的“主”节点，所有Lucene写操作都必须通过这个节点。然而，如果主节点宕机，从节点仍然可以继续工作，因为它们的搜索查询针对的是自己的本地索引副本。此外，更新请求应该由JMS提供商持久化，以便在主节点重新上线后，这些更新仍然可以执行。
- en: 'Because we are spinning up Jetty instances programmatically, rather than through
    the Maven plugin, we pass a different set of goals to each Maven build. For the
    `chapter7` project, you should run Maven as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们程序化地启动Jetty实例，而不是通过Maven插件，因此我们将不同的目标传递给每个Maven构建。对于`chapter7`项目，您应该像以下这样运行Maven：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will be able to access the "master" node at `http://localhost:8080`, and
    the "slave" node at `http://localhost:8181`. If you are very quick about firing
    off a search query on the master node the moment it starts, then you will see
    it returning only half of the expected results! However, within a few seconds,
    the slave node updates arrive through JMS. Both the halves of the data set will
    merge and be available across the cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您将能够通过`http://localhost:8080`访问“主”节点，通过`http://localhost:8181`访问“从”节点。如果您在主节点启动后立即发送一个搜索查询，那么您将看到它只返回预期结果的一半！然而，在几秒钟内，从节点通过JMS更新。数据集的两个部分将合并并在整个集群中可用。
- en: Sharding Lucene indexes
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分片Lucene索引
- en: Just as you can balance your application load across multiple nodes in a cluster,
    you may also split up your Lucene indexes through a process called **sharding**.
    You might consider sharding for performance reasons if your indexes grow to a
    very large size, as larger index files take longer to index and optimize than
    smaller shards.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以在集群中的多个节点之间平衡应用程序负载一样，您还可以通过一个称为**分片（sharding）**的过程将Lucene索引拆分。如果您的索引变得非常大，出于性能原因，您可能会考虑进行分片，因为较大的索引文件比小型分片索引和优化需要更长的时间。
- en: Sharding may offer additional benefits if your entities lend themselves to partitioning
    (for example, by language, geography, and so on). Performance may be improved
    if you can predictably steer queries toward the specific appropriate shard. Also,
    it sometimes makes lawyers happy when you can store "sensitive" data at a physically
    different location.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的实体适合于分区（例如，按语言、地理区域等），分片可能会提供额外的优势。如果您能够可预测地将查询引导到特定的适当分片，性能可能会得到改善。此外，当您能够在物理位置不同的地方存储“敏感”数据时，有时会让律师感到高兴。
- en: 'Even though its dataset is very small, this chapter''s version of the VAPORware
    Marketplace application now splits its `App` index into two shards. The relevant
    line in `hibernate.cfg.xml` looks similar to the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它的数据集非常小，但本章的VAPORware Marketplace应用程序现在将其`App`索引分成两个分片。`hibernate.cfg.xml`中的相关行类似于以下内容：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As with all of the other Hibernate Search properties that include the substring
    `default`, this is a global setting. It can be made index-specific by replacing
    `default` with an index name (for example, `App`).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有包含子字符串`default`的其他Hibernate Search属性一样，这是一个全局设置。可以通过用索引名称（例如`App`）替换`default`来使其特定于索引。
- en: Note
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This exact line appears in both `hibernate.cfg.xml` (used by our "master" node),
    and `hibernate-slave.cfg.xml` (used by our "slave" node). When running in a clustered
    environment, your sharding configuration should match all the nodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个确切的行出现在`hibernate.cfg.xml`（由我们的“主”节点使用）和`hibernate-slave.cfg.xml`（由我们的“从”节点使用）中。在集群环境中运行时，你的分片配置应与所有节点匹配。
- en: 'When an index is split into multiple shards, each shard includes the normal
    index name followed by a number (starting with zero). For example, `com.packtpub.hibernatesearch.domain.App.0`
    instead of just `com.packtpub.hibernatesearch.domain.App`. This screenshot shows
    the Lucene directory structure of our two-node cluster, while it is up and running
    with both nodes configured for two shards:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个索引被分成多个分片时，每个分片都包括正常的索引名称后面跟着一个数字（从零开始）。例如，是`com.packtpub.hibernatesearch.domain.App.0`，而不仅仅是`com.packtpub.hibernatesearch.domain.App`。这张截图展示了我们双节点集群的Lucene目录结构，在两个节点都配置为两个分片的情况下运行中：
- en: '![Sharding Lucene indexes](img/9205_07_03.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![分片Lucene索引](img/9205_07_03.jpg)'
- en: An example of sharded Lucene indexes running in a cluster (note the numbering
    of each `App` entity directory)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中运行的分片Lucene索引的一个示例（注意每个`App`实体目录的编号）
- en: 'Just as the shards are numbered on the filesystem, they can be separately configured
    by number in `hibernate.cfg.xml`. For example, if you want to store the shards
    at different locations, you might set properties as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如分片在文件系统上编号一样，它们可以在`hibernate.cfg.xml`中按编号单独配置。例如，如果你想将分片存储在不同的位置，你可能如下设置属性：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When a Lucene write operation is performed for an entity, or when a search query
    needs to read from an entity's index, a **sharding strategy** determines which
    shard to use.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当对实体执行Lucene写操作时，或者当搜索查询需要从实体的索引中读取时，**分片策略**确定使用哪个分片。
- en: If you are sharding simply to reduce the file size, then the default strategy
    (implemented by `org.hibernate.search.store.impl.IdHashShardingStrategy`) is perfectly
    fine. It uses each entity's ID to calculate a unique hash code, and distributes
    the entities among the shards in a roughly even manner. Because the hashing calculation
    is reproducible, the strategy is able to direct future updates for an entity towards
    the appropriate shard.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是分片以减少文件大小，那么默认策略（由`org.hibernate.search.store.impl.IdHashShardingStrategy`实现）完全没问题。它使用每个实体的ID来计算一个唯一的哈希码，并将实体在分片之间大致均匀地分布。因为哈希计算是可复制的，策略能够将实体的未来更新引导到适当的分片。
- en: To create your own custom sharding strategy with more exotic logic, you can
    create a new subclass inheriting from `IdHashShardingStrategy`, and tweak it as
    needed. Alternatively, you can completely start from scratch with a new class
    implementing the `org.hibernate.search.store.IndexShardingStrategy` interface,
    perhaps referring to the source code of `IdHashShardingStrategy` for guidance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建具有更复杂逻辑的自定义分片策略，你可以创建一个新子类，继承自`IdHashShardingStrategy`，并按需调整。或者，你可以完全从零开始，创建一个实现`org.hibernate.search.store.IndexShardingStrategy`接口的新类，或许可以参考`IdHashShardingStrategy`的源代码作为指导。
- en: Summary
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to work with applications in a modern distributed
    server architecture, to allow for scalability and better performance. We have
    seen a working cluster implemented with a filesystem-based directory provider
    and JMS-based backend, and now have enough knowledge to explore other approaches
    involving Inifinispan and JGroups. We used sharding to split a Lucene index into
    smaller chunks, and know how to go about implementing our own custom sharding
    strategy, if necessary.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在现代分布式服务器架构中与应用程序一起工作，以实现可扩展性和更好的性能。我们看到了一个使用基于文件系统的目录提供程序和基于JMS的后端实现的集群，现在有了足够的知识去探索涉及Inifinispan和JGroups的其他方法。我们使用了分片将Lucene索引分成更小的块，并知道如何实施自己的自定义分片策略。
- en: This brings us to the end of our little adventure with Hibernate Search! We
    have covered a lot of critical concepts about Hibernate, Lucene and Solr, and
    searches in general. We have learned how to map our data to search indexes, query
    and update those indexes at runtime, and arrange it all in the best architecture
    for a given project. We did all of this through an example application, that grew
    with our knowledge from simple to advanced along the way.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这带我们结束了与Hibernate Search的这次小冒险！我们已经涵盖了关于Hibernate、Lucene和Solr以及搜索的一般性关键概念。我们学会了如何将我们的数据映射到搜索索引中，在运行时查询和更新这些索引，并将其安排在给定项目的最佳架构中。这一切都是通过一个示例应用程序完成的，这个应用程序随着我们的知识从简单到复杂一路成长。
- en: There's always more to learn. Hibernate Search can work with dozens of Solr
    components for more advanced functionality, and integrating with a new generation
    of "NoSQL" data stores is possible as well. However, you are now equipped with
    enough core knowledge to explore these horizons independently, if you wish. Until
    next time, thank you for reading! You can find me online at `steveperkins.net`,
    and I would love to hear from you.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 学无止境。Hibernate Search 可以与 Solr 的数十个组件协同工作，以实现更高级的功能，同时也能与新一代的“NoSQL”数据存储集成。然而，现在你已经拥有了足够的核心知识，可以独立探索这些领域，如果你愿意的话。下次再见，感谢您的阅读！您可以在
    `steveperkins.net` 上找到我，我很乐意收到您的来信。
