- en: Batch and Streaming Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量分析和流式分析
- en: In the previous chapter, we introduced Spark and obtained BTC/USD transaction
    data from [www.bitstamp.net](http://www.bitstamp.net). Using that data, we can
    now perform some analysis on it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了Spark并从[www.bitstamp.net](http://www.bitstamp.net)获取了BTC/USD交易数据。使用这些数据，我们现在可以对其进行一些分析。
- en: First, we are going to query this data using a notebook tool named Apache Zeppelin.
    After that, we will write a program that receives the live transactions from [https://www.bitstamp.net/](https://www.bitstamp.net/) and
    sends them to a Kafka topic as they arrive.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用名为Apache Zeppelin的笔记本工具查询这些数据。之后，我们将编写一个程序，从[https://www.bitstamp.net/](https://www.bitstamp.net/)接收实时交易并将它们发送到Kafka主题。
- en: Finally, we will use Zeppelin again to run some streaming analytics queries
    on the data coming to the Kafka topic.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将再次使用Zeppelin在到达Kafka主题的数据上运行一些流式分析查询。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to Zeppelin
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeppelin简介
- en: Analyzing transactions with Zeppelin
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zeppelin分析交易
- en: Introducing Apache Kafka
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Apache Kafka
- en: Streaming transactions to Kafka
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式交易到Kafka
- en: Introducing Spark Streaming
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark流式简介
- en: Analyzing Streaming transactions with Zeppelin
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Zeppelin分析流式交易
- en: Introduction to Zeppelin
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Zeppelin简介
- en: Apache Zeppelin is an open source software offering a web interface to create
    notebooks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Zeppelin是一个开源软件，提供了一个网页界面来创建笔记本。
- en: In a notebook, you can inject some data, execute snippets of code to perform
    analysis on the data, and then visualize it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，您可以注入一些数据，执行代码片段以对数据进行分析，然后可视化。
- en: 'Zeppelin is a collaborative tool; several users can use it simultaneously.
    You can share notebooks and define the role of each user. You would typically
    define two different roles:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin是一个协作工具；多个用户可以同时使用它。您可以共享笔记本并定义每个用户的角色。您通常会定义两个不同的角色：
- en: The writer, usually a developer, can edit all the paragraphs and create forms.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者，通常是开发者，可以编辑所有段落并创建表单。
- en: The end user does not know much about the technical implementation details.
    He will just want to change some values in a form and then look at the effect
    on the results. The results can be a table or a graph, and can be exported to
    CSV.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终用户对技术实现细节了解不多。他只想在表单中更改一些值，然后查看结果的影响。结果可以是表格或图表，并且可以导出为CSV。
- en: Installing Zeppelin
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Zeppelin
- en: Before installing Zeppelin, you need to have Java and Spark installed on your
    machine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装Zeppelin之前，您需要在您的机器上安装Java和Spark。
- en: 'Follow these steps to install Zeppelin:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤安装Zeppelin：
- en: Download the binary from: [http://zeppelin.apache.org/download.html](http://zeppelin.apache.org/download.html).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接下载二进制文件：[http://zeppelin.apache.org/download.html](http://zeppelin.apache.org/download.html)。
- en: Explode the `.tgz` file using your favorite program
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您喜欢的程序解压缩`.tgz`文件
- en: That's it. Zeppelin is installed and configured with default settings. The next
    step is to start it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。Zeppelin已安装并使用默认设置配置。下一步是启动它。
- en: Starting Zeppelin
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动Zeppelin
- en: 'To start the Zeppelin daemon process, run the following command in the Terminal:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Zeppelin守护进程，请在终端中运行以下命令：
- en: 'Linux and macOS:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux和macOS：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Windows:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Windows：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Zeppelin is now running and ready to accept requests on [http://localhost:8080](http://localhost:8080).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin现在正在运行，并准备好在[http://localhost:8080](http://localhost:8080)上接受请求。
- en: 'Open the URL in your favorite browser. You should see this page:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在您最喜欢的浏览器中打开URL。您应该看到这个页面：
- en: '![](img/765efbd0-b62f-4e64-9acf-7d7b0ac45600.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/765efbd0-b62f-4e64-9acf-7d7b0ac45600.png)'
- en: Testing Zeppelin
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Zeppelin
- en: Let's create a new notebook to test if our installation is working correctly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的笔记本来测试我们的安装是否正确。
- en: 'From the [http://localhost:8080](http://localhost:8080) home page, click on Create
    New Note and set `Demo` as a name in the popup:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://localhost:8080](http://localhost:8080)主页，点击创建新笔记，并在弹出窗口中将`Demo`设置为名称：
- en: '![](img/0d5686f6-7f3e-4bab-a385-af87777a33c2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0d5686f6-7f3e-4bab-a385-af87777a33c2.png)'
- en: As mentioned in the window, the default interpreter will be Spark, and this
    is what we want. Click on Create now.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如窗口中所述，默认解释器将是Spark，这正是我们想要的。现在点击创建。
- en: 'You just created your first notebook. You should see the following in your
    browser:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚创建了您的第一个笔记本。您应该在浏览器中看到以下内容：
- en: '![](img/cd21ed57-f511-4700-8c70-aabea16b0e7b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cd21ed57-f511-4700-8c70-aabea16b0e7b.png)'
- en: Structure of a notebook
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 笔记本结构
- en: A notebook is a document in which you can add **paragraphs**. Each paragraph
    can use a different **interpreter**, which interacts with a specific framework
    or language.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本是一个文档，在其中你可以添加**段落**。每个段落可以使用不同的**解释器**，它与特定的框架或语言交互。
- en: 'In each paragraph, there are two sections:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个段落中，有两个部分：
- en: The top one is an editor, into which you can type some source code and run it
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上面一个是编辑器，你可以在这里输入一些源代码并运行它
- en: The bottom one displays the results
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下面一个是显示结果
- en: If, for example, you choose to use a Spark interpreter, all the lines of code
    you write in the paragraph will be interpreted (like the REPL seen in [Chapter
    1](db4d7854-92ff-43c8-a87b-2d605bf88d1b.xhtml), *Writing Your First Program*).
    All variables defined will be kept in memory and shared with all the other paragraphs
    of the notebook. Similarly to the Scala REPL, when you execute it, the output
    of the execution along with the types of the variables defined will be printed
    in the result section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，例如，你选择使用Spark解释器，段落中你写的所有代码都将被解释（就像在[第1章](db4d7854-92ff-43c8-a87b-2d605bf88d1b.xhtml)，*编写你的第一个程序*中看到的REPL一样）。所有定义的变量都将保留在内存中，并与笔记本的所有其他段落共享。类似于Scala
    REPL，当你执行它时，执行输出的输出以及定义的变量的类型将在结果部分打印出来。
- en: Zeppelin is installed by default with interpreters like Spark, Python, Cassandra,
    Angular, HDFS, Groovy, and JDBC.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Zeppelin默认安装了Spark、Python、Cassandra、Angular、HDFS、Groovy和JDBC等解释器。
- en: Writing a paragraph
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写段落
- en: 'Well, our first notebook is completely empty! We can reuse the example used
    in [Chapter 10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml), *Fetching and Persisting
    Bitcoin Market Data*, in the section *Exploring Spark''s API with the Scala console*.
    If you remember, we created `Dataset` from a sequence of strings. Enter the following
    in the notebook:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们的第一个笔记本是完全空的！我们可以重用第10章中使用的示例，*使用Scala控制台探索Spark的API*部分中的*获取和持久化比特币市场数据*。如果你记得，我们是从字符串序列创建`Dataset`的。在笔记本中输入以下内容：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then hit *Shift* + *Enter* (or click on the play triangle of the UI).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后按`Shift`+`Enter`（或点击UI上的播放三角形）。
- en: 'The interpreter runs and you should see the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 解释器运行后，你应该看到以下内容：
- en: '![](img/67ae70c1-f3e9-42f9-9385-087a1b9c22d8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67ae70c1-f3e9-42f9-9385-087a1b9c22d8.png)'
- en: The notebook created the dataset from the sequence and printed in the result
    section of the paragraph.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本从序列创建数据集并在段落的输出部分打印出来。
- en: Notice that in the previous chapter, when we executed this code from the Scala
    console, we had to create `SparkSession` and had to add some imports. When we
    use the Spark interpreter in Zeppelin, all the implicits and imports are done
    automatically and `SparkSession` is created for us.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前一章中，当我们从Scala控制台执行此代码时，我们必须创建`SparkSession`并添加一些导入。当我们使用Zeppelin中的Spark解释器时，所有隐式和导入都是自动完成的，并且`SparkSession`为我们创建。
- en: '`SparkSession` is exposed with the variable name `spark`. You can, for instance,
    get the Spark version with the following code in a paragraph:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`以变量名`spark`暴露。例如，你可以在一个段落中使用以下代码获取Spark版本：'
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After having run the paragraph, you should see the version printed in the result
    section:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 执行段落后，你应该在结果部分看到打印的版本：
- en: '![](img/573f1cad-9491-45e0-8cee-bd1ae4a8b863.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/573f1cad-9491-45e0-8cee-bd1ae4a8b863.png)'
- en: At that point, we tested the installation and it is working properly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个时刻，我们测试了安装，并且它运行正常。
- en: Drawing charts
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制图表
- en: In this section, we are going to create a basic `Dataset` and draw a chart of
    its data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个基本的`Dataset`并绘制其数据的图表。
- en: 'In a new paragraph, add the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在新段落中，添加以下内容：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We define a `Demo` class with `id` and a `data` property, then we create a list
    of different `Demo` objects and convert it into `Dataset`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个`Demo`类，具有`id`和`data`属性，然后我们创建一个不同的`Demo`对象列表，并将其转换为`Dataset`。
- en: 'From the `dataDS` dataset ,we call the `.createOrReplaceTempView("demoView")` method.
    This function is going to register the dataset as a temporary view. With this
    view defined, we can use SQL to query this dataset. We can try it by adding in
    a new paragraph, as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从`dataDS`数据集中，我们调用`.createOrReplaceTempView("demoView")`方法。这个函数将数据集注册为临时视图。有了这个视图定义，我们可以使用SQL查询这个数据集。我们可以通过添加一个新段落来尝试它，如下所示：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The newly created paragraph starts with `%sql`. This defines the interpreter
    that we use. In our case, this is the Spark SQL interpreter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的段落以`%sql`开头。这定义了我们使用的解释器。在我们的例子中，这是Spark SQL解释器。
- en: 'The query selects all the columns from `demoView`. After you hit *Shift* + *Enter*,
    the following table will be shown:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 查询选择了`demoView`中的所有列。在按下*Shift* + *Enter*之后，以下表格将会显示：
- en: '![](img/45161a31-1db6-4730-bd04-f17c97cc92f3.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/45161a31-1db6-4730-bd04-f17c97cc92f3.png)'
- en: As you can see, the SQL interpreter shows the result of the query by displaying
    a table in the result section of the paragraph.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，SQL解释器通过在段落的输出部分显示表格来显示查询的结果。
- en: Notice the menu at the top of the table. There are multiple ways to represent
    the data—bar chart, pie chart, area chart, line chart, and scatter chart.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意表格顶部的菜单。有多种方式来表示数据——柱状图、饼图、面积图、折线图和散点图。
- en: 'Click on the bar chart. The notebook now looks like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 点击柱状图。笔记本现在看起来是这样的：
- en: '![](img/3cbf9992-902a-46de-841d-d804651de9e8.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3cbf9992-902a-46de-841d-d804651de9e8.png)'
- en: It might seem odd that we don't have data available. Actually, we need to configure
    the chart to get it working. Click on settings, then define which column is the
    value and on which one you want to aggregate the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 可能看起来有些奇怪，我们没有可用的数据。实际上，我们需要配置图表才能使其工作。点击设置，然后定义哪个列是值列，以及您想要对哪个列进行数据聚合。
- en: 'Drag the `id` tag to the groups box and `data` to the values box. As we choose
    to group the IDs, `SUM` of `data` is performed. The configuration and the updated
    chart should look like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 将`id`标签拖到`groups`框中，将`data`拖到`values`框中。因为我们选择按ID分组，所以执行`data`的`SUM`。配置和更新的图表应该看起来像这样：
- en: '![](img/5bad433c-ef81-4b33-8833-a42750cf47e8.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5bad433c-ef81-4b33-8833-a42750cf47e8.png)'
- en: The chart is now correct. The sum of all the `id` `a` is `3`, `8` for the `b` `id` and `4` for
    the `c` `id`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图表现在正确。所有`id` `a`的总和是`3`，`b` `id`的总和是`8`，`c` `id`的总和是`4`。
- en: We are now familiar enough with Zeppelin to perform analytics on the Bitcoin
    transactions data that we produced in the previous chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对Zeppelin足够熟悉，可以对我们上一章产生的比特币交易数据进行分析。
- en: Analyzing transactions with Zeppelin
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zeppelin分析交易
- en: In the previous chapter, we wrote a program that saves BTC/USD transactions
    to Parquet files. In this section, we are going to use Zeppelin and Spark to read
    those files and draw some charts.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们编写了一个程序，将BTC/USD交易保存到Parquet文件中。在本节中，我们将使用Zeppelin和Spark读取这些文件并绘制一些图表。
- en: If you came directly to this chapter, you first need to set up the `bitcoin-analyser` project,
    as explained in [Chapter 10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml), *Fetching
    and Persisting Bitcoin Market Data*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您直接来到这一章，您首先需要设置`bitcoin-analyser`项目，如第10章中所述，*获取和持久化比特币市场数据*。
- en: 'Then you can either:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以：
- en: Run `BatchProducerAppIntelliJ`. This will save the last 24 hours of transactions
    in the `data` folder of the project directory, then save new transactions every
    hour.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`BatchProducerAppIntelliJ`。这将保存项目目录下`data`文件夹中最后24小时的交易，然后每小时保存新的交易。
- en: Use the sample transaction data that is committed in GitHub. You will have to
    check out this project: [https://github.com/PacktPublishing/Scala-Programming-Projects](https://github.com/PacktPublishing/Scala-Programming-Projects).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GitHub上提交的示例交易数据。您将需要检出此项目：[https://github.com/PacktPublishing/Scala-Programming-Projects](https://github.com/PacktPublishing/Scala-Programming-Projects)。
- en: Drawing our first chart
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制我们的第一个图表
- en: 'With these Parquet files ready, create a new notebook in Zeppelin and name
    it `Batch analytics`. Then, in the first cell, type the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好这些Parquet文件后，在Zeppelin中创建一个新的笔记本，并将其命名为`Batch analytics`。然后在第一个单元格中输入以下内容：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first line creates `DataFrame` from the transactions files. You need to
    replace the absolute path in the `parquet` function with the path to your Parquet
    files. The second line uses the special `z` variable to show the content of `DataFrame` in
    a table. This `z` variable is automatically provided in all notebooks. Its type
    is `ZeppelinContext`, and it allows you to interact with the Zeppelin renderer
    and interpreter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行从交易文件创建`DataFrame`。您需要将`parquet`函数中的绝对路径替换为您的Parquet文件路径。第二行使用特殊的`z`变量以表格形式显示`DataFrame`的内容。这个`z`变量在所有笔记本中自动提供。它的类型是`ZeppelinContext`，允许您与Zeppelin渲染器和解释器交互。
- en: 'Execute the cell with *Shift* + *Enter*. You should see the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*Shift* + *Enter*执行单元格。您应该看到以下内容：
- en: '![](img/65ccb66a-9c04-49f3-88a0-7a9815fedc81.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/65ccb66a-9c04-49f3-88a0-7a9815fedc81.png)'
- en: There is a warning message saying that the output is truncated. This is because
    we retrieved too much data. If we then attempt to draw a chart, some data will
    be missing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个警告信息说输出被截断。这是因为我们检索了太多的数据。如果我们尝试绘制一个图表，一些数据将会缺失。
- en: The solution to this issue could be to change Zeppelin's settings and increase
    the limit. But if we were to do that, the browser would have to keep a lot of
    data in memory, and the notebook's file when saved to disk would be large as well.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法可以是更改Zeppelin的设置并增加限制。但如果我们这样做，浏览器将不得不在内存中保留大量数据，而且保存到磁盘上的笔记本文件也会很大。
- en: 'A better solution is to aggregate the data. We cannot show all the transactions
    that happened in a day on a chart, but if we can aggregate them with a window
    of `20 minutes`, that will reduce the number of data points to display. Create
    a new cell with the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的解决方案是对数据进行聚合。我们无法在图表上显示一天内发生的所有交易，但如果我们可以用`20分钟`的时间窗口来聚合它们，这将减少要显示的数据点的数量。创建一个新的单元格，并输入以下代码：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run this new cell. You should see the following output:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个新单元格。你应该看到以下输出：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is a further description for the preceding code:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对前面代码的进一步描述：
- en: First, we group our transactions with a window of `20 minutes`. In the output,
    you can see that the type of `group` is `RelationalGroupedDataset`. This is an
    intermediate type on which we must call an aggregation method to produce another `DataFrame`.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们以`20分钟`的时间窗口对交易进行分组。在输出中，你可以看到`group`的类型是`RelationalGroupedDataset`。这是一个中间类型，我们必须调用一个聚合方法来生成另一个`DataFrame`。
- en: Then we call the `agg` method on `group` to compute several aggregations in
    one go. The `agg` method takes several `Column` arguments as `vararg`. The `Column` objects
    we pass are obtained by calling various aggregation functions from the `org.apache.spark.sql.functions` object.
    Each column is renamed so that we can refer to it easily later on.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们在“group”上调用“agg”方法一次性计算几个聚合。`agg`方法接受几个`Column`参数作为`vararg`。我们传递的`Column`对象是通过调用`org.apache.spark.sql.functions`对象的各种聚合函数获得的。每个列都被重命名，这样我们就可以在以后轻松地引用它。
- en: The resulting variable `tmpAgg` is `DataFrame` which has a column `window` of
    type `struct`. `struct` nests several columns. In our case, it has a column `start` and
    a column `end` of type `timestamp`. `tmpAgg` also has all the columns containing
    the aggregations—`count`, `avgPrice`, and `sumAmount`.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果变量`tmpAgg`是一个`DataFrame`，它有一个类型为`struct`的`window`列。`struct`嵌套了几个列。在我们的例子中，它有一个类型为`timestamp`的`start`列和`end`列。`tmpAgg`还包含所有包含聚合的列——`count`、`avgPrice`和`sumAmount`。
- en: After that, we select only the columns that we are interested in and then assign
    the resulting `DataFrame` in a variable `aggregate`. Notice that we can refer
    to the nested columns of the window column with the `“.”` notation. Here we select
    all rows apart from `window.end`. We then `sort` the `DataFrame` by ascending `start` time
    so that we can use `start` as the *x* axis of our future chart. Finally, we `cache` the `DataFrame` so
    that Spark will not have to reprocess it when we create other cells with different
    charts.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们只选择我们感兴趣的列，然后将结果`DataFrame`赋值给变量`aggregate`。注意，我们可以使用`“.”`符号来引用窗口列的嵌套列。在这里，我们选择除了`window.end`之外的所有行。然后我们按升序`start`时间对`DataFrame`进行`sort`，这样我们就可以将`start`作为我们未来图表的*x*轴。最后，我们`cache`这个`DataFrame`，这样Spark在创建其他具有不同图表的单元格时就不必重新处理它。
- en: 'Underneath the output, Zeppelin displays a table, but this time it does not
    give us any warning. It is able to load all the data without truncation. We can,
    therefore, draw a chart:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出下方，Zeppelin显示了一个表格，但这次它没有给出任何警告。它能够加载所有数据而不会截断。因此，我们可以绘制一个图表：
- en: Click on the line chart button
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击折线图按钮
- en: Drag and drop the `start` column in the section
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将“开始”列拖放到该部分
- en: Drag and drop `avgPrice` and `lastPrice` in the values section
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`avgPrice`和`lastPrice`拖放到值部分
- en: 'You should see something like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似这样的：
- en: '![](img/d7f27a93-4652-4491-9064-297ecfcf2506.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7f27a93-4652-4491-9064-297ecfcf2506.png)'
- en: We can see the evolution of the average price and of the last price in 20-minute
    increments.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到平均价格和最后价格在20分钟增量中的演变。
- en: 'If you hover the mouse on the chart, Zeppelin displays the information of the
    corresponding data point:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在图表上悬停鼠标，Zeppelin会显示对应数据点的信息：
- en: '![](img/3eb66130-e99b-49a2-8ccd-93e824f6a5b9.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eb66130-e99b-49a2-8ccd-93e824f6a5b9.png)'
- en: Feel free to try different types of charts, with different *values* for the *y *axis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试不同类型的图表，为*y*轴设置不同的*值*。
- en: Drawing more charts
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制更多图表
- en: 'Since our `aggregate DataFrame` is available in scope, we can create new cells
    to plot different charts. Create two new cells with the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的`aggregate DataFrame`在作用域内可用，我们可以创建新的单元格来绘制不同的图表。使用以下代码创建两个新的单元格：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, on each new cell, click on the line chart button and drag the `start` column
    to the keys section. After that:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在每个新的单元格上，点击折线图按钮并将`start`列拖到“键”部分。之后：
- en: In the first chart, drag and drop `sumAmount` in the values section. The chart
    shows the evolution of the volume exchanged.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个图表中，将`sumAmount`拖放到“值”部分。该图表显示了交易量的演变。
- en: In the second chart, drag and drop `stddevPrice` in the values section. The
    chart shows the evolution of the standard deviation.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二个图表中，将`stddevPrice`拖放到“值”部分。该图表显示了标准差的演变。
- en: Click on settings on both cells to hide the settings.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个单元格上点击“设置”以隐藏设置。
- en: Click on Hide editor on both cells.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个单元格上点击“隐藏编辑器”。
- en: 'You should obtain something like this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似这样的结果：
- en: '![](img/c9121c11-a18f-456a-b412-cde98bbeba92.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9121c11-a18f-456a-b412-cde98bbeba92.png)'
- en: We can observe spikes at around the same time—when there is a spike in volume
    exchanged, the standard deviation spikes as well. This is because many large transactions
    move the price significantly, which increases the standard deviation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到在相同时间点出现峰值——当交易量激增时，标准差也会激增。这是因为许多大交易显著地移动了价格，从而增加了标准差。
- en: You now have the basic building blocks to run your own analysis. With the power
    of the `Dataset` API, you could drill down to a specific period using `filter` and
    then plot the evolution of a moving average over different time spans.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在有了运行自己的分析的基本构建块。借助`Dataset` API的力量，你可以使用`filter`钻取到特定时间段，然后在不同时间段内绘制移动平均线的演变。
- en: The only problem with our notebook is that we would only get new transactions
    every hour. The `BatchProducerApp` that we wrote in the previous chapter does
    not produce transactions more frequently, and if you try to call the REST API
    every few seconds, you will get blacklisted by the Bitstamp server. The preferred
    way of getting live transactions is to use a WebSocket API.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们笔记本的唯一问题是每小时只会得到新的交易。我们在上一章中编写的`BatchProducerApp`不会频繁产生交易，而且如果你每隔几秒就尝试调用REST
    API，你将被Bitstamp服务器列入黑名单。获取实时交易的最佳方式是使用WebSocket API。
- en: For solving this, in the next section, we are going to build an application
    called `StreaminProducerApp` that will push live transactions to a Kafka topic.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，在下一节中，我们将构建一个名为`StreaminProducerApp`的应用程序，该应用程序将实时交易推送到Kafka主题。
- en: Introducing Apache Kafka
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Apache Kafka
- en: In the previous section, *Introducing Lambda Architecture*, we mentioned that
    Kafka is used for stream processing. Apache Kafka is a high throughput distributed
    messaging system. It allows decoupling the data coming in with the data going
    out.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节“介绍Lambda架构”中，我们提到Kafka用于流处理。Apache Kafka是一个高吞吐量的分布式消息系统。它允许解耦进入的数据和出去的数据。
- en: It means that multiple systems (**producers**) can send messages to Kafka. Kafka
    will then deliver these messages out to the **consumers** registered.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着多个系统（**生产者**）可以向Kafka发送消息。然后Kafka将这些消息发送给已注册的**消费者**。
- en: Kafka is distributed, resilient, and fault-tolerant, and has a very low latency.
    Kafka can scale horizontally by adding more machines to the system. It is written
    in Scala and Java.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是分布式、弹性、容错的，并且具有非常低的延迟。Kafka可以通过向系统中添加更多机器来水平扩展。它是用Scala和Java编写的。
- en: Kafka is broadly used; Airbnb, Netflix, Uber, and LinkedIn use this technology.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka被广泛使用；Airbnb、Netflix、Uber和LinkedIn都使用这项技术。
- en: The purpose of this chapter is not to become an expert in Kafka, but rather
    to familiarize you with the fundamentals of this technology. By the end of the
    chapter, you will be able to understand the use case developed in this chapter—streaming
    bitcoin transactions in a Lambda architecture.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的不是让你成为Kafka的专家，而是让你熟悉这项技术的 fundamentals。到本章结束时，你将能够理解本章中开发的用例——在Lambda架构中流式传输比特币交易。
- en: Topics, partitions, and offsets
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题、分区和偏移量
- en: In order to process the messages exchanged between the producers and the consumer,
    Kafka defines three main components—topics, partitions, and offsets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理生产者和消费者之间交换的消息，Kafka定义了三个主要组件——主题、分区和偏移量。
- en: A **topic** groups messages of the same type for streaming. It has a name and
    a number of partitions. You can have as many topics as you want. As Kafka can
    be distributed on multiple nodes, it needs a way to materialize the stream of
    messages on these different nodes. This is why the message stream (topic) is split into
    multiple **partitions**. Each partition contains a portion of the messages sent
    to a topic.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题**将相同类型的消息分组用于流式传输。它有一个名称和分区数。你可以有任意多的主题。由于Kafka可以在多个节点上分布式部署，它需要一种方法将这些不同节点上的消息流（主题）分割成多个**分区**。每个分区包含发送到主题的消息的一部分。'
- en: Each node of the Kafka cluster manages several partitions. A given partition
    is assigned to several nodes. This avoids losing data if a node is lost, and allows
    a higher throughput. By default, Kafka uses the hash code of the message to assign
    it to a partition. You can define a key for a message to control this behavior.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群中的每个节点管理多个分区。一个特定的分区被分配给多个节点。这样，如果一个节点丢失，可以避免数据丢失，并允许更高的吞吐量。默认情况下，Kafka使用消息的哈希码将其分配到分区。你可以为消息定义一个键来控制这种行为。
- en: In a partition, the order of the message is guaranteed, and once a message is
    written on it, it cannot be changed. The messages are **immutable**.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在分区中，消息的顺序是有保证的，一旦消息被写入，就不能更改。消息是**不可变的**。
- en: 'A topic can be consumed by zero to many **consumer** processes. A key feature
    of Kafka is that each consumer can consume the stream at his own pace: a producer
    can be sending message 120, while one consumer is processing message 40, and another
    one is processing message 100.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一个主题可以被零个到多个**消费者**进程消费。Kafka的一个关键特性是每个消费者可以以自己的速度消费流：一个生产者可以发送消息120，而一个消费者正在处理消息40，另一个消费者正在处理消息100。
- en: This asymmetry is made possible by storing the messages on disk. Kafka keeps
    the messages for a limited amount of time; the default setting is one week.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不对称性是通过将消息存储在磁盘上实现的。Kafka将消息保留一定时间；默认设置是一周。
- en: Internally, Kafka uses IDs to keep track of the messages and a sequence number
    to generate these IDs. It maintains one unique sequence per partition. This sequence
    number is called an **offset**. An offset only has a meaning for a specific partition.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 内部，Kafka使用ID来跟踪消息，并使用序列号来生成这些ID。它为每个分区维护一个唯一的序列号。这个序列号被称为**偏移量**。偏移量只对特定的分区有意义。
- en: Each node of the Kafka cluster runs a process called a **broker. **Each broker
    manages several topics with one or more partitions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka集群中的每个节点运行一个称为**代理**的过程。每个代理管理一个或多个主题的分区。
- en: 'Let''s summarize everything with an example. We can define a topic named `shapes` with
    a number of partitions equal to two. This topic receives messages, as shown in
    the following diagram:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来总结一下。我们可以定义一个名为`shapes`的主题，其分区数等于两个。该主题接收消息，如下面的图所示：
- en: '![](img/c0ac8b7b-69e6-47fd-9d1c-62480007107d.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c0ac8b7b-69e6-47fd-9d1c-62480007107d.png)'
- en: 'Let''s say we have three nodes in the cluster. The representation of the brokers,
    partitions, offsets, and messages would be the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们集群中有三个节点。代理、分区、偏移量和消息的表示如下：
- en: '![](img/58ec2d48-b82b-4421-9885-563af723c6e5.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/58ec2d48-b82b-4421-9885-563af723c6e5.png)'
- en: Notice that as we defined only two partitions and we have three machines, one
    of the machines is not going to be used.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因为我们只定义了两个分区，而我们有三台机器，所以其中一台机器将不会被使用。
- en: Another option available when you define a partition is the number of replicas.
    To be resilient, Kafka replicates the data in multiple brokers, so if one broker
    is failing, the data can be retrieved from another one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当你定义分区时，另一个可用的选项是副本数。为了提高容错性，Kafka在多个代理中复制数据，这样如果某个代理失败，可以从另一个代理检索数据。
- en: 'You should now be more familiar with the fundamentals of the Kafka architecture.
    We will now spend a little bit of time on two other components: the producer and
    the consumer.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该对Kafka架构的基本原理更加熟悉了。我们现在将花一点时间讨论两个其他组件：生产者和消费者。
- en: Producing data into Kafka
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据生产到Kafka
- en: The component to send messages to a topic is called a **producer** in Kafka.
    The responsibility of a producer is to automatically select a partition through
    a broker to write messages. In case of failure, the producer should automatically
    recover.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kafka中，将消息发送到主题的组件被称为**生产者**。生产者的责任是通过代理自动选择一个分区来写入消息。在发生故障的情况下，生产者应该自动恢复。
- en: The partition selection is based on a key. The producer will take care of sending
    all messages with the same key to the same partition. If there is no key provided
    with the message, the producer load balances the messages using a round-robin
    algorithm.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分区选择基于键。生产者将负责将所有具有相同键的消息发送到同一个分区。如果没有提供消息的键，生产者将使用轮询算法进行消息负载均衡。
- en: 'You can configure the producer with the level of acknowledgment you want to
    receive. There are three levels:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以配置生产者以接收你想要的确认级别。有三个级别：
- en: '`acks=0`: The producer sends the data and forgets it; no acknowledgment is
    done. There is no guarantee, and messages could be lost.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acks=0`: 生产者发送数据后就会忘记它；不进行任何确认。没有保证，消息可能会丢失。'
- en: '`acks=1`: The producer waits for an acknowledgment of the first replicas. You
    can be sure that no data will be lost as long as the broker that acknowledged
    does not crash.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acks=1`: 生产者等待第一个副本的确认。只要确认的代理没有崩溃，你可以确信不会丢失数据。'
- en: '`acks=all`: The producer waits for an acknowledgment of all the replicas. You
    can be sure that no data will be lost even if one broker crashes.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acks=all`: 生产者等待所有副本的确认。你可以确信，即使一个代理崩溃，也不会丢失数据。'
- en: Of course, if you want an acknowledgment of all the replicas, you might expect
    longer latencies. Acknowledgment of the first replica (`ack=1`) is a good compromise
    between safety and latency.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你想要所有副本的确认，你可能会期望更长的延迟。第一个副本（`ack=1`）的确认是在安全性和延迟之间的一种良好折衷。
- en: Consuming data from Kafka
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Kafka 消费数据
- en: To read messages from a topic, you need to run a **consumer**. As with the producer,
    the consumer will automatically select the broker to read from and will recover
    in case of failure.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 要从主题中读取消息，你需要运行一个**消费者**。与生产者一样，消费者将自动选择读取的代理，并在失败时恢复。
- en: The consumer reads the messages from all partitions. Within a partition, it
    is guaranteed to receive messages in the same order as they were produced.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者从所有分区读取消息。在分区内部，它保证按它们产生的顺序接收消息。
- en: Consumer group
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消费者组
- en: You can have multiple consumers for the same topic in a **consumer group**.
    If you have the same number of consumers and partitions in a group, each consumer
    will read only one partition. This allows parallelizing the consumption. If you
    have more consumers than partitions, the first consumer takes the partition and
    the rest of the consumers are in waiting mode. They will consume only if a consumer
    reading on a partition is failing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在一个**消费者组**中为同一个主题拥有多个消费者。如果你在一个组中有相同数量的消费者和分区，每个消费者只会读取一个分区。这允许并行化消费。如果你有比分区更多的消费者，第一个消费者将获取分区，其余的消费者将处于等待模式。只有在读取分区的消费者失败时，它们才会进行消费。
- en: Until now, we only have one group of consumers reading from partitions. In a
    typical system, you would have many consumer groups. For example, in the case
    of a Bitcoin transaction, we could have a consumer group reading the messages
    to perform analytics on it, and another group for a user interface that shows
    a feed of all the transactions. The latency between the two cases are not the
    same, and we don't want to have a dependency between each use case. For that purpose,
    Kafka uses the notion of groups.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只有一个消费者组正在读取分区。在一个典型的系统中，你会有许多消费者组。例如，在比特币交易的情况下，我们可能有一个消费者组读取消息以执行分析，另一个组用于显示所有交易的用户界面。这两种情况之间的延迟不同，我们不希望每个用例之间有依赖关系。为此目的，Kafka
    使用了组的概念。
- en: Offset management
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏移量管理
- en: 'Another important concept is that when a consumer reads a message, it will
    automatically inform Kafka of the offset that was read. This way, if a consumer
    dies, Kafka knows the offset of the last message read. When the consumer restarts,
    it can send the next message to it. As for the producer, we can decide when to
    commit offsets. There are three options :'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的概念是，当消费者读取一条消息时，它会自动通知 Kafka 已读取的偏移量。这样，如果消费者崩溃，Kafka 就知道最后读取的消息的偏移量。当消费者重新启动时，它可以发送下一条消息。至于生产者，我们可以决定何时提交偏移量。有三个选项：
- en: At most once; as soon as the message is received, the offset is committed.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至多一次；一旦收到消息，就提交偏移量。
- en: At least once; the offset is committed after the message has been processed.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少一次；消息处理完成后提交偏移量。
- en: Exactly once; the offset is committed after the message has been processed,
    and there are additional constraints on the producer—it must not resend messages
    in case of network failures. The producer must have idempotent and transactional
    capabilities, which were introduced in Kafka 0.11
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确一次；消息处理完成后提交偏移量，并且对生产者有额外的约束——在网络故障的情况下，生产者不得重发消息。生产者必须具有幂等性和事务性能力，这些能力在 Kafka
    0.11 中引入。
- en: The most commonly used is the *at least once* option. If the processing of the
    message is failing, you can reprocess it, but you might occasionally receive the
    same message multiple times. In the case of *at most once*, if anything goes wrong
    during the process, the message will be lost.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的选项是 *至少一次*。如果消息的处理失败，你可以重新处理它，但你可能会偶尔接收到相同的消息多次。在 *最多一次* 的情况下，如果在处理过程中出现任何问题，消息将会丢失。
- en: OK, enough theor. We have learned about topics, partitions, offsets, consumers,
    and producers. The last piece of missing knowledge is a simple question—how do
    I connect my producer or consumer to Kafka?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，理论就到这里。我们已经了解了主题、分区、偏移量、消费者和生产者。最后缺失的知识点是一个简单的问题——我如何将我的生产者或消费者连接到 Kafka？
- en: Connecting to Kafka
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连接到 Kafka
- en: We introduced in the section *Topics, partitions, and offsets* the notion of
    brokers. Brokers are deployed on multiple machines, and all the brokers form what
    we call a Kafka cluster.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *主题、分区和偏移量* 这一部分，我们介绍了代理的概念。代理部署在多台机器上，所有代理共同构成了我们所说的 Kafka 集群。
- en: If you want to connect to a Kafka cluster, all you need to know is the address
    of one of the brokers. All the brokers know about all the metadata of the cluster—brokers,
    partitions, and topics. Internally, Kafka uses a product named **Zookeeper**.
    This allows the sharing of all this metadata between brokers.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要连接到 Kafka 集群，你所需要知道的就是其中一个代理的地址。所有代理都知道集群的所有元数据——代理、分区和主题。内部，Kafka 使用一个名为
    **Zookeeper** 的产品。这允许代理之间共享所有这些元数据。
- en: Streaming transactions to Kafka
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将事务流式传输到 Kafka
- en: 'In this section, we are going to write a program that produces a stream of
    the BTC/USD transactions that happen in real time. Our program will:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写一个程序来生成实时 BTC/USD 交易的流。我们的程序将：
- en: Subscribe to the WebSocket API of Bitstamp to get a stream of transactions in
    JSON format.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 订阅 Bitstamp 的 WebSocket API 以获取 JSON 格式的交易流。
- en: 'For each transaction coming in the stream, it will:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于流中进入的每个事务，它将：
- en: Deserialize it
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化它
- en: Convert it to the same `Transaction` case class that we used in `BatchProducer` in
    the previous chapter
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为我们在上一章的 `BatchProducer` 中使用的相同的 `Transaction` case class
- en: Serialize it
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列化它
- en: Send it to a Kafka topic
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其发送到 Kafka 主题
- en: In the next section, we will use Zeppelin again with Spark Streaming to query
    the data streamed to the Kafka topic.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将再次使用 Zeppelin 和 Spark Streaming 来查询流式传输到 Kafka 主题的数据。
- en: Subscribing with Pusher
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pusher 订阅
- en: Go to Bitstamp's WebSocket API for live transactions: [https://www.bitstamp.net/websocket/](https://www.bitstamp.net/websocket/).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 Bitstamp 的 WebSocket API 以获取实时交易：[https://www.bitstamp.net/websocket/](https://www.bitstamp.net/websocket/)。
- en: You will see that this API uses a tool called Pusher channels for real-time WebSocket
    streaming. The API documentation provides a Pusher Key that we need to use to
    receive live transactions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这个 API 使用一个名为 Pusher channels 的工具进行实时 WebSocket 流式传输。API 文档提供了我们需要使用的 Pusher
    Key 来接收实时交易。
- en: '**Pusher** channels is a hosted solution for delivering a stream of messages
    using a publish/subscribe pattern. You can find out more on their website: [https://pusher.com/features](https://pusher.com/features).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pusher** channels 是一个托管解决方案，用于使用发布/订阅模式发送消息流。你可以在他们的网站上了解更多信息：[https://pusher.com/features](https://pusher.com/features)。'
- en: 'Let''s try to use Pusher to receive some live BTC/USD transactions. Open the
    project `bitcoin-analyser`, start a new Scala console, and type the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用 Pusher 接收一些实时的 BTC/USD 交易。打开项目 `bitcoin-analyser`，启动一个新的 Scala 控制台，并输入以下内容：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s have a look in detail:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看：
- en: In the first line, we create the Pusher client with the key that was specified
    in Bitstamp's documentation.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一行，我们使用在 Bitstamp 文档中指定的密钥创建 Pusher 客户端。
- en: Then we connect to the remote Pusher server and subscribe to the `"live_trades"` channel.
    We obtain an object of type `channel`.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们连接到远程 Pusher 服务器并订阅 `"live_trades"` 通道。我们获得一个 `channel` 类型的对象。
- en: Finally, we use the `channel` to register (`bind`) a callback function that
    will be called every time the `channel` receives a new event with the name `trade`.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用 `channel` 注册（绑定）一个回调函数，每当 `channel` 收到名为 `trade` 的新事件时，该函数将被调用。
- en: 'After a few seconds, you should see some trades being printed:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，您应该会看到一些交易被打印出来：
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The data is in JSON format, with a schema that conforms to what was defined
    in Bitstamp’s WebSocket documentation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以 JSON 格式存储，其模式符合 Bitstamp 的 WebSocket 文档中定义的模式。
- en: 'With these few lines, we can write the first building block of our application.
    Create a new object, `StreamingProducerApp`, in the package `coinyser` in `src/main/scala` with
    the following content:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这几行代码，我们可以编写我们应用程序的第一个构建块。在 `src/main/scala` 中的 `coinyser` 包下创建一个新的对象，名为 `StreamingProducerApp`，内容如下：
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our function `subscribe` takes a `Pusher` instance (of type `Client`) and a
    callback function, `onTradeReceived`, and returns `IO[Unit]`. When `IO` is run,
    it will call `onTradeReceived` each time a new trade is received. The implementation
    is similar to the few lines that we typed into the console. It basically wraps
    every side-effecting function in an `IO`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `subscribe` 函数接受一个 `Pusher` 实例（类型为 `Client`）和一个回调函数 `onTradeReceived`，并返回
    `IO[Unit]`。当 `IO` 运行时，它将在每次收到新的交易时调用 `onTradeReceived`。实现与我们在控制台中输入的几行代码类似。它基本上将每个副作用函数包装在
    `IO` 中。
- en: For the sake of conciseness and readability, we have not exposed the details
    of this function's unit test. You can check it out in the GitHub repository.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁性和可读性，我们没有公开此函数的单元测试细节。您可以在 GitHub 仓库中查看它。
- en: For writing the test, we had to create a `FakePusher` class that implements
    a few methods of the `Client` interface.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了编写测试，我们不得不创建一个实现 `Client` 接口几个方法的 `FakePusher` 类。
- en: Deserializing live transactions
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反序列化实时交易
- en: The JSON payload that we receive when we subscribe to live transactions is slightly
    different from the one we had in the REST endpoint for fetching batch transactions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们订阅实时交易时收到的 JSON 负载数据与我们在 REST 端点获取批量交易时拥有的数据略有不同。
- en: 'We are going to need to deserialize it to a case class before we can transform
    it to the same `Transaction` case class that we used in the previous chapter.
    For this, first create a new case class, `coinyser.WebsocketTransaction` in `src/main/scala`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够将其转换为与上一章中使用的相同的 `Transaction` case class 之前，我们需要将其反序列化为 case class。为此，首先在
    `src/main/scala` 中创建一个新的 case class `coinyser.WebsocketTransaction`：
- en: '[PRE13]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The names and types of the attributes correspond to the JSON attributes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 属性的名称和类型与 JSON 属性相对应。
- en: 'After that, we can write a unit test for a new function, `deserializeWebsocketTransaction`.
    Create a new class, `coinyser.StreamingProducerSpec`, in `src/test/scala`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以为一个新的函数 `deserializeWebsocketTransaction` 编写单元测试。在 `src/test/scala` 中创建一个新的类
    `coinyser.StreamingProducerSpec`：
- en: '[PRE14]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The test is straightforward—we define a sample JSON string, call the function
    under `test`, and make sure the deserialized object `SampleWebsocketTransaction` contains
    the same values.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 测试很简单——我们定义一个样本 JSON 字符串，调用 `test` 下的函数，并确保反序列化的对象 `SampleWebsocketTransaction`
    包含相同的值。
- en: 'Now we need to implement the function. Add a new `val mapper: ObjectMapper` and
    a new function `deserializeWebsocketTransaction` to the `StreamingProducer` object:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们需要实现这个函数。向 `StreamingProducer` 对象添加一个新的 `val mapper: ObjectMapper` 和一个新的函数
    `deserializeWebsocketTransaction`：'
- en: '[PRE15]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For this part of the project, we use the **Jackson** Java library to deserialize/serialize
    JSON objects. It is the library that is used under the hood by Spark when it reads/writes
    dataframe from/to JSON. Hence, it is available without adding any more dependencies.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目部分，我们使用 **Jackson** Java 库来反序列化/序列化 JSON 对象。这是 Spark 在从/向 JSON 读取/写入 dataframe
    时在底层使用的库。因此，它无需添加任何更多依赖项即可使用。
- en: 'We define a constant, `mapper: ObjectMapper`, which is the entry point of Jackson
    for serializing/deserializing classes. We configure it to write timestamps in
    a format that is compatible with what Spark can parse. This will be necessary
    later on when we read the Kafka topic using Spark. Then the function’s implementation
    calls `readValue` to deserialize the JSON into `WebsocketTransaction`.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '我们定义一个常量 `mapper: ObjectMapper`，它是 Jackson 序列化/反序列化类的入口点。我们将其配置为以与 Spark 可以解析的格式写入时间戳。这将在我们使用
    Spark 读取 Kafka 主题时变得必要。然后函数的实现调用 `readValue` 将 JSON 反序列化为 `WebsocketTransaction`。'
- en: Converting to transaction and serializing
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换为交易并序列化
- en: 'We are able to listen to live transactions and deserialize them to objects
    of type `WebsocketTransaction`. The next steps are:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够监听实时交易并将它们反序列化为`WebsocketTransaction`类型的对象。下一步是：
- en: Convert these `WebsocketTransaction` objects into the same case class, `Transaction`,
    that we defined in the previous chapter.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些`WebsocketTransaction`对象转换为我们在上一章中定义的相同案例类`Transaction`。
- en: Send these `Transaction` objects to a Kafka topic. But for this, they need to
    be serialized first. The simplest way is to serialize to JSON.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些`Transaction`对象发送到Kafka主题。但为此，它们需要先进行序列化。最简单的方法是将它们序列化为JSON。
- en: 'As usual, we start by writing tests. Add the following tests to `StreamingProducerSpec`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们首先编写测试。将以下测试添加到`StreamingProducerSpec`：
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The test for `convertWsTransaction` checks that, once converted, `SampleWebsocketTransaction` is
    the same as `SampleTransaction`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`convertWsTransaction`的测试检查一旦转换，`SampleWebsocketTransaction`与`SampleTransaction`相同。'
- en: The test for `serializeTransaction` checks that, once serialized, `SampleTransaction` is
    the same as `SampleJsonTransaction`.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`serializeTransaction`的测试检查一旦序列化，`SampleTransaction`与`SampleJsonTransaction`相同。'
- en: 'The implementation of these two functions is straightforward. Add the following
    definitions in `StreamingProducer`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数的实现很简单。在`StreamingProducer`中添加以下定义：
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In `convertWsTransaction`, we have to multiply the timestamp by 1,000 to get
    the time in milliseconds. The other attributes are just copied.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在`convertWsTransaction`中，我们必须将时间戳乘以1,000以获得毫秒时间。其他属性只是复制。
- en: In `serializeTransaction`, we reuse the `mapper` object to serialize a `Transaction` object
    to JSON.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在`serializeTransaction`中，我们重用`mapper`对象将`Transaction`对象序列化为JSON。
- en: Putting it all together
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: 'We now have all the building blocks to create our application. Create a new
    object, `coinyser.StreamingProducerApp`, and type the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了创建应用程序的所有构建块。创建一个新的对象`coinyser.StreamingProducerApp`，并输入以下代码：
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Our object extends `cats.IOApp`, and as such we have to implement a `run` function
    that returns `IO[ExitCode]`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的对象扩展了`cats.IOApp`，因此我们必须实现一个返回`IO[ExitCode]`的`run`函数。
- en: In the `run` function, we first create `KafkaProducer[Int, String]`. This Java
    class from the Kafka client library will allow us to send messages to a topic.
    The first type parameter is the type of the message's key. The key of our message
    will be the `tid` attribute in the `Transaction` case class, which is of type `Int`.
    The second type parameter is the type of the message itself. In our case, we use `String`,
    because we are going to serialize our messages to JSON. If storage space was a
    concern, we could have used `Array[Byte]` and a binary serialization format such
    as Avro.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在`run`函数中，我们首先创建`KafkaProducer[Int, String]`。这个来自Kafka客户端库的Java类将允许我们向一个主题发送消息。第一个类型参数是消息键的类型。我们的消息键将是`Transaction`案例类中的`tid`属性，其类型为`Int`。第二个类型参数是消息本身的类型。在我们的例子中，我们使用`String`，因为我们打算将消息序列化为JSON。如果存储空间是一个关注点，我们可以使用`Array[Byte]`和如Avro的二进制序列化格式。
- en: The `props Map` passed to construct `KafkaProducer` contains various configuration
    options for interacting with the Kafka cluster. In our program, we pass the minimum
    set of properties and leave the others with default values, but there are many
    more fine-tuning options. You can find out more here: [http://kafka.apache.org/documentation.html#producerconfigs](http://kafka.apache.org/documentation.html#producerconfigs).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给构造`KafkaProducer`的`props Map`包含与Kafka集群交互的各种配置选项。在我们的程序中，我们传递最小集的属性，并将其他属性保留为默认值，但还有许多更多的微调选项。您可以在以下位置了解更多信息：[http://kafka.apache.org/documentation.html#producerconfigs](http://kafka.apache.org/documentation.html#producerconfigs)。
- en: 'Then we call the `StreamingProducer.subscribe` function that we implemented
    earlier, and pass a callback function that will be called each time we receive
    a new transaction. This anonymous function will:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们调用我们之前实现的`StreamingProducer.subscribe`函数，并传递一个回调函数，每次我们收到新的交易时都会调用该函数。这个匿名函数将：
- en: Deserialize the JSON into `WebsocketTransaction`.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将JSON反序列化为`WebsocketTransaction`。
- en: Convert `WebsocketTransaction` into `Transaction`.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`WebsocketTransaction`转换为`Transaction`。
- en: Serialize `Transaction` to JSON.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Transaction`序列化为JSON。
- en: Send the JSON transaction to the Kafka topic using `kafkaProducer.send`. For
    this, we have to create `ProducerRecord`, which contains the `topic` name, the
    key, and the content of the message.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kafkaProducer.send`将JSON交易发送到Kafka主题。为此，我们必须创建`ProducerRecord`，它包含主题名称、键和消息内容。
- en: The `subscribe` function returns `IO[Unit]`. This will start a background thread
    and complete immediately when we run it. But we do not want to stop the main thread
    immediately; we need to keep our program running forever. This is why we `flatMap` it
    and return `IO.never`, which will keep the main thread running until we kill the
    process.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`subscribe` 函数返回 `IO[Unit]`。这将启动一个后台线程，并在我们运行它时立即完成。但我们不想立即停止主线程；我们需要让我们的程序永远运行。这就是为什么我们使用
    `flatMap` 并返回 `IO.never`，这将使主线程运行直到我们杀死进程。'
- en: Running StreamingProducerApp
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 StreamingProducerApp
- en: 'Before running our application, we need to start a Kafka cluster. For the sake
    of simplicity, we are just going to start a single broker on your workstation.
    If you wish to set up a multinode cluster, please refer to the Kafka documentation:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行我们的应用程序之前，我们需要启动一个 Kafka 集群。为了简化，我们将在你的工作站上启动单个代理。如果你希望设置一个多节点集群，请参阅 Kafka
    文档：
- en: Download Kafka 0.10.2.2 from this URL: [https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz](https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz).
    We use the version 1.1.1 because we have tested it with the`spark-sql-kafka-0.10` library
    at the time of writing. You could use a later version of Kafka, but it is not
    guaranteed that an old Kafka client can always communicate with a more recent
    broker.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从此 URL 下载 Kafka 0.10.2.2：[https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz](https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.1/kafka_2.11-1.1.1.tgz)。我们使用
    1.1.1 版本，因为我们写作时已经与 `spark-sql-kafka-0.10` 库进行了测试。你可以使用 Kafka 的较新版本，但无法保证旧的 Kafka
    客户端总能与较新的代理通信。
- en: 'Open a console and then decompress the package in your favorite directory:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个控制台，然后在你的喜欢的目录中解压缩包：
- en: '`tar xfvz kafka*.tgz`.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tar xfvz kafka*.tgz`.'
- en: 'Go to the installation directory and start Zookeeper:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往安装目录并启动 Zookeeper：
- en: '[PRE19]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You should see a lot of log output. The last line should contain `INFO binding
    to port 0.0.0.0/0.0.0.0:2181`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到大量的日志输出。最后一行应该包含 `INFO binding to port 0.0.0.0/0.0.0.0:2181`。
- en: 'Open a new console, and start the Kafka server:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的控制台，并启动 Kafka 服务器：
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You should see some logs present in the last line.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在最后一行看到一些日志。
- en: 'Once Kafka has started, open a new console and run the following commands:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 Kafka 启动，打开一个新的控制台并运行以下命令：
- en: '[PRE21]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This command starts a console consumer that listens to the `transactions` topic.
    Any message sent to this topic will be printed on the console. This will allow
    us to test that our program works as expected.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令启动一个控制台消费者，监听 `transactions` 主题。任何发送到此主题的消息都将打印在控制台上。这将允许我们测试我们的程序是否按预期工作。
- en: 'Now run `StreamingProducerApp` in IntelliJ. After a few seconds, you should
    get an output similar to this in IntelliJ:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在 IntelliJ 中运行 `StreamingProducerApp`。几秒钟后，你应该在 IntelliJ 中得到类似以下的输出：
- en: '[PRE22]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our application received some messages from the WebSocket API and sent them
    to the Kafka topic transactions.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用程序从 WebSocket API 接收了一些消息并将它们发送到 Kafka 主题 transactions。
- en: 'If you then go back to the console on which you started the console consumer,
    you should see new `Transaction` serialized objects being printed in real time:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你然后回到启动控制台消费者的控制台，你应该看到新的 `Transaction` 序列化对象实时打印出来：
- en: '[PRE23]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This indicates that our streaming application works as expected. It listens
    to the Pusher channel to receive BTC/USD transactions and sends everything it
    receives to the Kafka topic `transactions`. Now that we have some data going to
    a Kafka topic, we can use Spark Streaming to run some analytics queries.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们的流式应用程序按预期工作。它监听 Pusher 通道以接收 BTC/USD 交易，并将接收到的所有内容发送到 Kafka 主题 `transactions`。现在我们已经有一些数据发送到
    Kafka 主题，我们可以使用 Spark Streaming 来运行一些分析查询。
- en: Introducing Spark Streaming
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Spark Streaming
- en: In [Chapter](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml) [10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml),* Fetching
    and Persisting Bitcoin Market Data*, we used Spark to save transactions in a batch
    mode. The batch mode is fine when you have to perform an analysis on a bunch of
    data all at once.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 10 章](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml) [10](0bd50ccf-a65b-4c51-99fa-e694b0be502a.xhtml)，*获取和持久化比特币市场数据*，我们使用
    Spark 以批处理模式保存交易。当你必须一次性对大量数据进行分析时，批处理模式是可行的。
- en: But in some cases, you might need to process data as it is entering into the
    system. For example, in a trading system, you might want to analyze all the transactions
    done by the broker to detect fraudulent transactions. You could perform this analysis
    in batch mode after the market is closed; but in this case, you can only act after
    the fact.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 但在某些情况下，你可能需要处理数据进入系统时的数据。例如，在一个交易系统中，你可能想分析经纪人完成的所有交易以检测欺诈交易。你可以在市场关闭后以批量模式执行此分析；但在这种情况下，你只能在事后采取行动。
- en: 'Spark Streaming allows you to consume a streaming source (file, socket, and
    Kafka topic) by dividing the input data into many micro-batches. Each micro-batch
    is an RDD that can then be processed by the **Spark Engine**. Spark divides the
    input data using a time window. So if you define a time window of 10 seconds,
    then Spark Streaming will create and process a new RDD every 10 seconds:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming允许你通过将输入数据划分为许多微批次来消费流式源（文件、套接字和Kafka主题）。每个微批次都是一个可以由**Spark引擎**处理的RDD。Spark使用时间窗口来划分输入数据。因此，如果你定义了一个10秒的时间窗口，那么Spark
    Streaming将每10秒创建并处理一个新的RDD：
- en: '![](img/f316f1f7-bd8f-4a80-852f-85472c21d2ce.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f316f1f7-bd8f-4a80-852f-85472c21d2ce.png)'
- en: Going back to our fraud detection system, by using Spark Streaming we could
    detect a pattern of a fraudulent transaction as it arises and immediately act
    on the broker to limit the damage.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的欺诈检测系统，通过使用Spark Streaming，我们可以在欺诈交易出现时立即检测到其模式，并在代理上采取行动以限制损害。
- en: 'In the previous chapter, you learned that Spark offers two APIs for processing
    batch data—RDD and `Dataset`. RDD is the original and core API, and `Dataset` is
    the more recent one that allows it to perform SQL queries and optimize the execution
    plan automatically. Similarly, Spark Streaming offers two APIs to process streams
    of data:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你了解到Spark提供了两个用于处理批量数据的API——RDD和`Dataset`。RDD是原始和核心API，而`Dataset`是较新的一个，它允许执行SQL查询并自动优化执行计划。同样，Spark
    Streaming提供了两个API来处理数据流：
- en: '**Discretized Stream** (**DStream**) is basically a continuous series of RDDs.
    Each RDD in a DStream contains data from a certain time interval. You can get
    more information about it here: [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散流**（**DStream**）基本上是一系列连续的RDD。DStream中的每个RDD都包含一定时间间隔内的数据。你可以在这里了解更多信息：[https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)。'
- en: '**Structured streaming** is more recent. It allows you to use the same methods
    as the Dataset API. The difference is that in `Dataset` you manipulate an unbound
    table that grows as new input data arrives. You can get more information about
    it here: [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化流**是较新的。它允许你使用与Dataset API相同的方法。区别在于在`Dataset`中，你操作的是一个随着新输入数据到达而增长的未绑定表。你可以在这里了解更多信息：[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)。'
- en: In the next section, we are going to use Zeppelin to run a Spark-structured
    streaming query on the BTC/USD transaction data that we previously produced in
    a Kafka topic.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用Zeppelin在之前在Kafka主题中产生的BTC/USD交易数据上运行一个Spark结构化流查询。
- en: Analyzing streaming transactions with Zeppelin
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Zeppelin分析流式交易
- en: 'At this point, if you have not done so yet, you should start the following
    processes on your machine. Please refer to the previous sections if you are not
    sure how to start them:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，如果你还没有这样做，你应该在你的机器上启动以下进程。如果你不确定如何启动它们，请参考前面的章节：
- en: Zookeeper
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zookeeper
- en: Kafka broker
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka代理
- en: The `StreamingProducerApp` consuming live BTC/USD transactions and pushing them
    to a Kafka topic named `transactions`.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StreamingProducerApp`正在消费实时BTC/USD交易并将其推送到名为`transactions`的Kafka主题。'
- en: Apache Zeppelin
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Zeppelin
- en: Reading transactions from Kafka
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Kafka读取交易
- en: 'Using your browser, create a new Zeppelin Notebook named `Streaming`, and 
    then type the following code in the first cell:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你的浏览器，创建一个名为`Streaming`的新Zeppelin笔记本，然后在第一个单元中输入以下代码：
- en: '[PRE24]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Execute the cell to define the `Transaction` class and the variable `schema`.
    We have to redefine the `Transaction` case class in Zeppelin because Zeppelin
    does not have access to the classes of our IntelliJ project, `bitcoin-analyser`.
    We could have instead packaged a `.jar` file and added it into Zeppelin's dependency
    settings, but as this is the only class we need, we found it easier to redefine
    it. The `schema` variable is of type `DataType`. We will use it in the following
    paragraph to deserialize the JSON transactions that we will consume from the Kafka
    topic.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单元格以定义`Transaction`类和变量`schema`。由于Zeppelin无法访问我们的IntelliJ项目`bitcoin-analyser`中的类，我们必须在Zeppelin中重新定义`Transaction`case类。我们本可以打包一个`.jar`文件并将其添加到Zeppelin的依赖设置中，但由于我们只需要这个类，我们发现重新定义它更容易。`schema`变量是`DataType`类型。我们将在下一段中使用它来反序列化从Kafka主题中消费的JSON事务。
- en: 'Then create a new cell underneath with the following code:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在下面创建一个新的单元格，并使用以下代码：
- en: '[PRE25]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This creates a new variable `dfStream` of type `DataFrame`. For creating this,
    we called:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个类型为`DataFrame`的新变量`dfStream`。为了创建这个，我们调用了：
- en: 'The method `readStream` on the `spark: SparkSession` object. It returns an
    object of the `DataStreamReader` type that we can configure further.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '在`spark: SparkSession`对象上的`readStream`方法。它返回一个`DataStreamReader`类型的对象，我们可以进一步配置它。'
- en: The methods `format("kafka")` and `option("kafka.bootstrap.servers", "localhost:9092")` specify
    that we want to read data from Kafka and point to the broker localhost on port `9092`.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法`format("kafka")`和`option("kafka.bootstrap.servers", "localhost:9092")`指定我们想要从Kafka读取数据，并指向端口`9092`上的本地主机代理。
- en: '`option("startingoffsets", "latest")` indicates that we only want to consume
    the data from the latest offset. Other options are `"earliest"`, or a JSON string
    specifying a starting offset for each `TopicPartition`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option("startingoffsets", "latest")`表示我们只想从最新偏移量处消费数据。其他选项是`"earliest"`，或一个JSON字符串，指定每个`TopicPartition`的起始偏移量。'
- en: '`option("subscribe", "transaction")` specifies that we want to listen to the
    topic `transactions`. This is the topic name that we used in our `StreamingProducerApp`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`option("subscribe", "transaction")`指定我们想要监听主题`transactions`。这是我们`StreamingProducerApp`中使用的主题名称。'
- en: The call to `load()` returns `DataFrame`. But at this stage, it only contains
    one `value` column with the raw JSON.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load()`调用返回`DataFrame`。但在这一阶段，它只包含一个包含原始JSON的`value`列。'
- en: We then deserialize the JSON using the `from_json` function and the `schema` object
    that we created in the previous paragraph. This returns a single column of type `struct`.
    In order to have all the columns at the root level, we rename the column using `alias("v")`, and
    select all columns inside it using `select("v.*")`.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们随后使用`from_json`函数和上一段中创建的`schema`对象反序列化JSON。这返回一个类型为`struct`的单列。为了使所有列都在根级别，我们使用`alias("v")`重命名列，并使用`select("v.*")`选择其内部的所有列。
- en: 'When you run the paragraph, you will get the following output:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行段落时，你会得到以下输出：
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'At this point, we have `DataFrame` with all the columns that we need to run
    further analysis on right? Let''s try to display it. Create a new paragraph with
    this code and run it:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有一个包含所有我们需要进一步分析的列的`DataFrame`，对吗？让我们尝试显示它。创建一个新的段落并运行以下代码：
- en: '[PRE27]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You should see the following error:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下错误：
- en: '[PRE28]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The problem here is that there is a missing writing step. Even though the type `DataFrame` is
    exactly the same as the one we obtained in the `Batch` notebook, we cannot use
    it in exactly the same way. We have created a streaming `DataFrame` that can consume
    and transform messages from Kafka, but what is it supposed to do with these messages?
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题是缺少一个写入步骤。尽管`DataFrame`类型与我们从`Batch`笔记本中获得的确切相同，但我们不能以完全相同的方式使用它。我们创建了一个可以消费和转换来自Kafka的消息的流式`DataFrame`，但它应该对这些消息做什么呢？
- en: In the Spark-structured streaming world, the action methods such as `show`, `collect`,
    or `take` cannot be used. You have to tell Spark where to write the data it consumes.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark结构化流世界中，不能使用`show`、`collect`或`take`等操作方法。你必须告诉Spark它消费的数据应该写入哪里。
- en: Writing to an in-memory sink
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 写入内存中的接收器
- en: 'A Spark structured streaming process has three types of components:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Spark结构化流处理过程有三个类型的组件：
- en: 'The **input source** is specified with the `format(source: String)` method
    on `DataStreamReader`. This source can be a file, a Kafka topic, a network socket,
    or a constant rate. Once configured with `option`, the call to `load()` returns
    a `DataFrame`.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入源**通过在`DataStreamReader`上调用`format(source: String)`方法进行指定。这个源可以是文件、Kafka主题、网络套接字或恒定速率。一旦通过`option`配置，调用`load()`将返回一个`DataFrame`。'
- en: '**Operations** are the classic `DataFrame`/`Dataset` transformations, such
    as `map`, `filter`, `flatMap`, and `reduce`. They take `Dataset` as input and
    return another transformed `Dataset` with the recorded transformation.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**是经典的`DataFrame`/`Dataset`转换，例如`map`、`filter`、`flatMap`和`reduce`。它们以`Dataset`作为输入，并返回另一个带有记录转换的转换后的`Dataset`。'
- en: 'The **output sink** writes the transformed data. For specifying the sink, we
    must first obtain `DataStreamWriter` by calling the `writeStream` method on `Dataset`, and
    then configure it. For this, we have to call the `format(source: String)` method
    on `DataStreamWriter`. The output sink can be a file, a Kafka topic, or `foreach` that
    takes a callback. For debugging purposes, there is also a console sink and a memory
    sink.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出目标**会将转换后的数据写入。为了指定目标，我们首先需要通过在`Dataset`上调用`writeStream`方法来获取`DataStreamWriter`，然后进行配置。为此，我们必须在`DataStreamWriter`上调用`format(source:
    String)`方法。输出目标可以是文件、Kafka主题或接受回调的`foreach`。为了调试目的，还有一个控制台目标和一个内存目标。'
- en: 'Going back to our streaming transactions, the error that we obtained after
    calling `z.show(dfStream)` indicated that we were missing a sink for our `DataFrame`.
    To remediate this, add a new paragraph with the following code:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的流交易，我们在调用`z.show(dfStream)`后获得的错误表明我们缺少`DataFrame`的目标。为了解决这个问题，添加一个包含以下代码的新段落：
- en: '[PRE29]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This code configures a memory sink for our `DataFrame`. This sink creates an
    in-memory Spark table whose name is given by `queryName("transactionsStream")`.
    A table named `transactionsStream` will be updated every time a new micro-batch
    of transactions is processed.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码为我们的`DataFrame`配置了一个内存目标。这个目标创建了一个名为`queryName("transactionsStream")`的内存Spark表。每当处理一个新的交易微批处理时，名为`transactionsStream`的表将被更新。
- en: 'There are several strategies for writing to a streaming sink, specified by `outputMode(outputMode:
    String)`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '有几种写入流目标的方法，由`outputMode(outputMode: String)`指定：'
- en: '`"append"` means that only the new rows will be written to the sink.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"append"`表示只有新行会被写入目标。'
- en: '`"complete"` will write all the rows every time there is an update.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"complete"`会在每次更新时写入所有行。'
- en: '`"update"` will write all the rows that were updated. This is useful when you
    perform some aggregation; for instance, counting the number of messages. You would
    want the new count to be updated every time there is a new row coming in.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"update"`将写入所有被更新的行。这在执行一些聚合操作时很有用；例如，计算消息数量。你希望每次有新行进入时，新的计数都会更新。'
- en: Once our `DataStreamWriter` is configured, we call the `start()` method. This
    will start the whole workflow in the background. It is only from this point that
    the data starts to be consumed from the Kafka topic and gets written to the in-memory
    table. All the previous operations were lazy and were just configuring the workflow.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的`DataStreamWriter`配置完成，我们调用`start()`方法。这将启动整个工作流程在后台运行。只有从这个点开始，数据才开始从Kafka主题中被消费并写入内存表。所有之前的操作都是懒加载的，只是配置工作流程。
- en: 'Run the paragraph now, and you will see an output which looks like this:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行这个段落，你将看到一个类似以下的输出：
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This `StreamingQuery` object is a handle to the streaming query running in the
    background. You can use it to monitor the progress of the streaming workflow,
    get some information about its execution plan, or `stop` it altogether. Feel free
    to explore the API of `StreamingQuery` and try to call its methods.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`StreamingQuery`对象是后台运行中的流查询的句柄。你可以用它来监控流工作流程的进度，获取一些关于其执行计划的信息，或者完全`stop`它。请随意探索`StreamingQuery`的API并尝试调用其方法。
- en: Drawing a scatter chart
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 绘制散点图
- en: 'Since we have a running `StreamingQuery` that writes to an in-memory table
    called `transactionsStream`, we can display the data contained in this table with
    the following paragraph:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个写入名为`transactionsStream`的内存表的运行中的`StreamingQuery`，我们可以使用以下段落显示这个表中的数据：
- en: '[PRE31]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Run this paragraph, and if you have some transactions coming into your Kafka
    topic, you should see a table which looks like this:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个段落，如果你有交易进入你的Kafka主题，你应该会看到一个类似以下的表格：
- en: '![](img/f8da5a58-b286-4807-baad-ebf627065be1.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8da5a58-b286-4807-baad-ebf627065be1.png)'
- en: 'You can then click on the scatter chart button. If you want to draw the evolution
    of the price, you can drag and drop the `timestamp` in the *x* axis and the `price` in
    the *y* axis. You should see something like this:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以点击散点图按钮。如果你想绘制价格的变化，你可以将`timestamp`拖放到*x*轴上，将`price`拖放到*y*轴上。你应该看到类似这样的：
- en: '![](img/10b668f8-f57c-4304-9815-eaf7bb93a653.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10b668f8-f57c-4304-9815-eaf7bb93a653.png)'
- en: If you want to refresh the chart with the latest transactions, you can just
    re-rerun the paragraph.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要用最新的交易刷新图表，你只需重新运行这个段落。
- en: This is pretty good, but if you let the query run for some time, you might end
    up with quite a lot of transactions. Ultimately, you will reach the limit of what
    Zeppelin can handle and you will get the same error, `OUTPUT IS TRUNCATED`, that
    we had in the section *Drawing our first chart*.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当不错，但如果让查询运行一段时间，你可能会得到相当多的交易。最终，你将达到Zeppelin可以处理的极限，并且你会得到与*绘制第一个图表*部分相同的错误，`OUTPUT
    IS TRUNCATED`。
- en: To remediate this, we have to group the transactions using a time window, in
    the same way that we did for the batch transactions that we read from `parquet`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们必须使用时间窗口对交易进行分组，就像我们对从`parquet`读取的批交易所做的那样。
- en: Aggregating streaming transactions
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合流式交易
- en: 'Add a new paragraph with the following code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码添加一个新的段落：
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This code is very similar to the one we wrote in the section *Drawing our first
    chart*. The only difference is the call to `withWatermark`, but the rest of the
    code is the same. This is one of the main benefits of using Spark-structured streaming—we
    can reuse the same code for transforming batch datasets and streaming datasets.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与我们*绘制第一个图表*部分所写的是非常相似的。唯一的区别是调用了`withWatermark`，但其余的代码是相同的。这是使用Spark结构化流的一个主要好处——我们可以重用相同的代码来转换批数据集和流数据集。
- en: Watermarking is mandatory when we aggregate streaming datasets. In a few words,
    we need to tell Spark how long it will wait to receive late data before discarding
    it, and what timestamp column it should use to measure the time between two rows.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们聚合流数据集时，水印是强制性的。简单来说，我们需要告诉Spark在丢弃之前它将等待多长时间以接收迟到数据，以及它应该使用哪个时间戳列来衡量两行之间的时间。
- en: As Spark is distributed, it can potentially consume the Kafka topic using several
    consumers, each of them consuming a different partition of the topic. This means
    that Spark Streaming could potentially process transactions out of order. In our
    case, we have just one Kafka broker and do not expect a high volume of transactions;
    hence, we use a low watermark of one second. Watermarking is a quite complex subject.
    You can find out more on the Spark website: [https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time](https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark是分布式的，它可以使用多个消费者潜在地消费Kafka主题，每个消费者消费主题的不同分区。这意味着Spark Streaming可以潜在地以乱序处理交易。在我们的案例中，我们只有一个Kafka代理，并且不期望有大量的交易；因此，我们使用一秒的低水印。水印是一个相当复杂的话题。你可以在Spark网站上找到更多信息：[https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time](https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html#window-operations-on-event-time)。
- en: 'Once you have run this new paragraph, you will have a new `Dataframe`, `aggDfStream`,
    which will aggregate transactions in 10-second windows. But before we can draw
    a chart of this aggregated data, we need to create a query to connect an in-memory
    sink. Create a new paragraph with this code:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行了这个新段落，你将得到一个新的 `Dataframe`，`aggDfStream`，它将在10秒的窗口内聚合交易。但在我们能够绘制这个聚合数据的图表之前，我们需要创建一个查询来连接一个内存中的接收器。使用以下代码创建一个新的段落：
- en: '[PRE33]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: It is nearly the same as the one we wrote in the section *Drawing a scatter
    chart*. We just used `aggDfStream` instead of `df`, and the output table name
    is now called `aggregateStream`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎与我们在*绘制散点图*部分所写的是一样的。我们只是用`aggDfStream`代替了`df`，并且输出表名现在称为`aggregateStream`。
- en: 'Finally, add a new paragraph to display the data contained in the `aggregateStream` table:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加一个新的段落来显示`aggregateStream`表中的数据：
- en: '[PRE34]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You will need to wait at least 30 seconds after having run `aggQuery` to get
    some aggregated transaction data. Wait a bit, then run the paragraph. After that,
    click on the line chart button, then drag and drop the `start` column in the keys section
    and `avgPrice` in the values section. You should see a chart that looks like this:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在运行`aggQuery`后至少等待30秒才能获取一些聚合的交易数据。稍等片刻，然后运行段落。之后，点击折线图按钮，然后将`start`列拖放到keys部分，将`avgPrice`拖放到values部分。你应该会看到一个看起来像这样的图表：
- en: '![](img/99fb5115-e696-40e8-b68f-b12278399905.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/99fb5115-e696-40e8-b68f-b12278399905.png)'
- en: If you re-run the paragraph after 10 seconds or more, you should see it being
    updated with new transactions.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你10秒或更长时间后重新运行段落，你应该会看到它更新了新的交易数据。
- en: It turns out that this `aggregateStream` `DataFrame` has exactly the same columns
    as the `aggregate` `DataFrame` that we created in the section *Drawing our first
    chart*. The difference between them is that `aggregate` is built using the historical
    batch data coming from Parquet files, and `aggregateStream` is built using the
    live data coming from Kafka. They are actually complementary—`aggregate` has got
    all the transactions from the last hours or days, while `aggregateStream` has
    got all the transactions from the point in time in which we started the `aggQuery`.
    If you want to draw a chart containing all the data up to the latest live transaction,
    you can simply use `union` of both dataframes, adequately filtered so that time
    windows are not duplicated.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个`aggregateStream``DataFrame`与我们之前在*绘制第一个图表*部分中创建的`aggregate``DataFrame`具有完全相同的列。它们之间的区别在于，`aggregate`是使用来自Parquet文件的历史批量数据构建的，而`aggregateStream`是使用来自Kafka的实时数据构建的。它们实际上是互补的——`aggregate`包含了过去几小时或几天的所有交易，而`aggregateStream`包含了从我们开始`aggQuery`的那个时间点以来的所有交易。如果你想绘制包含最新实时交易的所有数据的图表，你可以简单地使用两个dataframes的`union`，并适当过滤，以确保时间窗口不重复。
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have learned how to use Zeppelin to query parquet files
    and display some charts. Then, you developed a small program to stream transaction
    data from a WebSocket to a Kafka topic. Finally, you used Spark Streaming inside
    Zeppelin to query the data arriving in the Kafka topic in real time.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用Zeppelin查询parquet文件并显示一些图表。然后，你开发了一个小程序，从WebSocket流式传输事务数据到Kafka主题。最后，你使用Zeppelin中的Spark
    Streaming实时查询Kafka主题中的数据。
- en: With all these building blocks in place, you have all the tools to analyze the
    Bitcoin transaction data in more detail. You could let the `BatchProducerApp` run
    for several days or weeks to get some historical data. With the help of Zeppelin
    and Spark, you could then try to detect patterns and come up with a trading strategy.
    Finally, you could then use a Spark Streaming flow to detect in real time when
    some trading signal arises and perform a transaction automatically.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些构建块就绪的情况下，你拥有了分析比特币交易数据的所有工具。你可以让`BatchProducerApp`运行几天或几周以获取一些历史数据。借助Zeppelin和Spark，你可以尝试检测模式并提出交易策略。最后，你可以使用Spark
    Streaming流程实时检测何时出现某些交易信号并自动执行交易。
- en: We have produced streaming data on only one topic, but it would be quite straightforward
    to add other topics covering other currency pairs, such as BTC/EUR or BTC/ETH.
    You could also create another program that fetches data from another cryptocurrency
    exchange. This would enable you to create Spark Streaming queries that detect
    arbitrage opportunities (when the price of a product is cheaper in one market
    than in another).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只针对一个主题产生了流数据，但要添加其他主题，涵盖其他货币对，例如BTC/EUR或BTC/ETH，将会非常简单。你也可以创建另一个程序，从另一个加密货币交易所获取数据。这将使你能够创建Spark
    Streaming查询，以检测套利机会（当产品在一个市场上的价格比另一个市场便宜时）。
- en: The building blocks we implemented in this chapter can be also used in a Lambda
    architecture. A Lambda architecture is a data processing architecture designed
    to handle large volumes of data by using both batch processing and streaming methods.
    Many Lambda architectures involve having different code bases for the batch and
    streaming layers, but with Spark, this negative point can be greatly reduced.
    You can find out more about Lambda architectures on this website: [http://lambda-architecture.net/](http://lambda-architecture.net/).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们实现的基本模块也可以用于 Lambda 架构。Lambda 架构是一种数据处理架构，通过结合批处理和流式处理方法来处理大量数据。许多 Lambda
    架构涉及批处理层和流式处理层拥有不同的代码库，但使用 Spark，这一负面因素可以大大减少。您可以在本网站上了解更多关于 Lambda 架构的信息：[http://lambda-architecture.net/](http://lambda-architecture.net/).
- en: This completes the final chapter in our book. We hope you enjoyed reading it
    as much as we enjoyed writing it. You will be empowered by the ins and outs of
    various concepts of Scala. Scala is a language that is well designed and is not
    an end by himself, this is the basement to build more interesting concepts, like
    the type theory and for sure the category theory, I encourage your curiosity to
    look for more concepts to continually improve the readability and the quality
    of your code. You will also be able to apply it to solve a variety of real-world
    problems.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们书籍的最后一章。我们希望您阅读它时能像我们写作它时一样享受。您将通过 Scala 的各种概念的细节而获得力量。Scala 是一种设计良好的语言，它本身不是终点，这是构建更多有趣概念的基础，比如类型理论，当然还有范畴论，我鼓励您的好奇心去寻找更多概念，以不断改进代码的可读性和质量。您还将能够将其应用于解决各种现实世界的问题。
