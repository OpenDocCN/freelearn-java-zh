- en: Advanced Load Balancing and Circuit Breakers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级负载均衡和断路器
- en: In this chapter, we will continue the subject discussed in the previous chapter,
    inter-service communication. We will extend it to more advanced samples of load
    balancing, timeouts, and circuit breaking.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续讨论前一章中讨论的主题，即服务间通信。我们将扩展到更高级的负载均衡、超时和断路示例。
- en: Spring Cloud provides features that make implementation of communication between
    microservices nice and simple. However, we must not forget that the major difficulties
    we would face with such communication concern the processing time of the systems
    involved. If you have many microservices in your system, one of the first issues
    you need to deal with is the problem of latency. In this chapter, I would like
    to discuss a few Spring Cloud features that help us to avoid latency problems
    that are caused by many hops between services when processing a single input request,
    slow responses from several services, or a temporary unavailability of services.
    There are several strategies for dealing with partial failures. These include
    setting network timeouts, limiting the number of waiting requests, implementing
    different load balancing methods, or setting up a circuit breaker pattern and
    fallback implementation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud提供了使微服务间通信简单而优雅的功能。然而，我们绝不能忘记，这样的通信所面临的的主要困难涉及所涉及系统的处理时间。如果您系统中有很多微服务，您需要处理的第一个问题之一是延迟问题。在本章中，我想讨论一些Spring
    Cloud功能，帮助我们避免由于服务间处理单个输入请求时的许多跃点、多个服务的缓慢响应或服务的暂时不可用而引起的延迟问题。处理部分失败有几种策略，包括设置网络超时、限制等待请求的数量、实现不同的负载均衡方法，或设置断路器模式和回退实现。
- en: We will also talk about Ribbon and Feign clients once again, this time focusing
    on their more advanced configuration features. An entirely new library that will
    be introduced here is Netflix Hystrix. This library implements the circuit breaker
    pattern.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将再次讨论Ribbon和Feign客户端，这次重点关注它们更高级的配置功能。在这里将介绍一个全新的库，即Netflix Hystrix。这个库实现了断路器模式。
- en: 'The topics we will cover in this chapter include the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将覆盖以下主题：
- en: Different load balancing algorithms with Ribbon clients
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ribbon客户端的不同负载均衡算法
- en: Enabling a circuit breaker for the application
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用应用程序的断路器
- en: Customizing Hystrix with configuration properties
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用配置属性自定义Hystrix
- en: Monitoring interservice communication with the Hystrix dashboard
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hystrix仪表板监控服务间通信
- en: Using Hystrix together with Feign clients
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hystrix和Feign客户端一起
- en: Load balancing rules
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡规则
- en: 'Spring Cloud Netflix provides different load balancing algorithms in order
    to provide different benefits to the user. Your choice of supported method depends
    on your needs. In the Netflix OSS nomenclature, this algorithm is called a **rule**.
    The custom rule class should have implemented an `IRule` base interface. The following
    implementations are available by default inside Spring Cloud:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Netflix提供了不同的负载均衡算法，以向用户提供不同的好处。您支持的方法选择取决于您的需求。在Netflix OSS命名法中，此算法称为**规则**。自定义规则类应实现`IRule`基础接口。以下实现默认情况下在Spring
    Cloud中可用：
- en: '`RoundRobinRule`: This rule simply chooses servers using the well-known round
    robin algorithm, where incoming requests are distributed across all instances
    sequentially. It is often used as the default rule or fallbacks for more advanced
    rules, such as `ClientConfigEnabledRoundRobinRule` and `ZoneAvoidanceRule`. `ZoneAvoidanceRule`
    is the default rule for Ribbon clients.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RoundRobinRule`：此规则简单地使用众所周知的轮询算法选择服务器，其中传入请求按顺序分配到所有实例。它通常用作默认规则或更高级规则的回退，例如`ClientConfigEnabledRoundRobinRule`和`ZoneAvoidanceRule`。`ZoneAvoidanceRule`是Ribbon客户端的默认规则。'
- en: '`AvailabilityFilteringRule`: This rule will skip servers that are marked as
    circuit tripped or with a high number of concurrent connections. It also uses
    `RoundRobinRule` as a base class. By default, an instance is circuit tripped if
    an HTTP client fails to establish a connection with it three times in a row. This
    approach may be customized with the `niws.loadbalancer.<clientName>.connectionFailureCountThreshold` property.
    Once an instance is circuit tripped, it will remain in this state for the next
    30 seconds before the next retry. This property may also be overridden in the
    configuration settings.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AvailabilityFilteringRule`：This rule will skip servers that are marked as
    circuit tripped or with a high number of concurrent connections. It also uses
    `RoundRobinRule` as a base class. By default, an instance is circuit tripped if
    an HTTP client fails to establish a connection with it three times in a row. This
    approach may be customized with the `niws.loadbalancer.<clientName>.connectionFailureCountThreshold`
    property. Once an instance is circuit tripped, it will remain in this state for
    the next 30 seconds before the next retry. This property may also be overridden
    in the configuration settings.'
- en: '`WeightedResponseTimeRule`: With this implementation, a traffic volume forwarder
    to the instance is inversely proportional to the instance''s average response
    time. In other words, the longer the response time, the less weight it will get.
    In these circumstances, a load balancing client will record the traffic and response
    time of every instance of the service.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WeightedResponseTimeRule`： with this implementation, a traffic volume forwarder
    to the instance is inversely proportional to the instance''s average response
    time. In other words, the longer the response time, the less weight it will get.
    In these circumstances, a load balancing client will record the traffic and response
    time of every instance of the service.'
- en: '`BestAvailableRule`: According to the description from the class documentation,
    this rule skips servers with *tripped* circuit breakers and picks the server with
    the lowest concurrent requests.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BestAvailableRule`：According to the description from the class documentation,
    this rule skips servers with *tripped* circuit breakers and picks the server with
    the lowest concurrent requests.'
- en: Tripped circuit breaker is a term taken from electrical engineering, and means
    that there's no current flowing through a circuit. In IT terminology, it refers
    to the situation where too many consecutive requests that are sent to a service
    fail, and therefore any further attempts to invoke the remote service will be
    interrupted immediately by the software on the client side in order to relieve
    the server-side application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 跳闸断路器是一个来自电气工程的术语，指的是电路中没有电流流动。在IT术语中，它指的是发送给服务器的连续请求失败次数过多，因此客户端软件会立即中断对远程服务的进一步调用，以减轻服务器端应用程序的负担。
- en: The WeightedResponseTime rule
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重响应时间规则
- en: Until now, we have usually tested our services manually by calling them from
    a web browser or a REST client. The current changes do not allow such an approach
    because we need to set fake delays for the services, as well as generate many
    HTTP requests.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们通常还通过从网页浏览器或REST客户端调用服务来手动测试服务。目前的更改不允许采用这种方法，因为我们需要为服务设置模拟延迟，以及生成许多HTTP请求。
- en: Introducing Hoverfly for testing
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Hoverfly用于测试
- en: 'At this point, I would like to introduce an interesting framework that may
    be a perfect solution for these kinds of tests. I am talking about Hoverfly, a
    lightweight service virtualization tool that is used to stub or simulate HTTP
    services. It is originally written in Go, but also gives you an expressive API
    for managing Hoverfly in Java. Hoverfly Java, maintained by SpectoLabs, provides
    classes that abstract away the binary and API calls, a DSL for creating simulations,
    and an integration with the JUnit test framework. This framework has a feature
    that I really like. You may easily add a delay to every simulated service by calling
    one method in your DSL definition. To enable Hoverfly for your project, you have
    to include the following dependency in your Maven `pom.xml`:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，我想介绍一个可能完美解决这类测试的有趣框架。我指的是Hoverfly，一个轻量级的服务虚拟化工具，用于模拟或虚拟HTTP服务。它最初是用Go编写的，但还为您提供了用于管理Hoverfly的Java语言的丰富API。由SpectoLabs维护的Hoverfly
    Java提供了用于抽象二进制和API调用、创建模拟的DSL以及与JUnit测试框架集成的类。我喜欢这个框架的一个功能。您可以通过在DSL定义中调用一个方法，轻松地为每个模拟服务添加延迟。为了使Hoverfly适用于您的项目，您必须在Maven
    `pom.xml`中包含以下依赖项：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Testing the rule
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试规则
- en: The sample we are discussing here is available on GitHub. To access it, you
    have to switch to  `weighted_lb` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb](https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb)).
    Our JUnit test class, called `CustomerControllerTest`, is available under the `src/test/java`
    directory. To enable Hoverfly for the test, we should define the JUnit `@ClassRule`.
    The `HoverflyRule` class provides an API that allows us to simulate many services
    with different addresses, characteristics, and responses. In the following source
    code fragment, you may see that two instances of our sample microservice `account-service`
    have been declared inside `@ClassRule`.  As you probably remember, that service
    has been invoked by `customer-service` and `order-service`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的样本可以在GitHub上找到。要访问它，你必须切换到`weighted_lb`分支（[https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb](https://github.com/piomin/sample-spring-cloud-comm/tree/weighted_lb)）。我们的JUnit测试类，名为`CustomerControllerTest`，位于`src/test/Java`目录下。为了在测试中启用Hoverfly，我们应该定义JUnit
    `@ClassRule`。`HoverflyRule`类提供了一个API，允许我们模拟具有不同地址、特性和响应的许多服务。在下面的源代码片段中，你可以看到我们的示例微服务`account-service`的两个实例被声明在`@ClassRule`中。正如你可能记得的，那个服务已经被`customer-service`和`order-service`调用过。
- en: 'Let''s take a look at a test class from the `customer-service` module. It simulates
    the `GET /customer/*` method with a predefined response for two instances of `account-service` available
    on ports `8091` and `9091`. The first of them has been delayed by `200` milliseconds,
    while the second is delayed by `50` milliseconds:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下`customer-service`模块中的一个测试类。它模拟了`GET /customer/*`方法，并为`account-service`的两个实例（分别监听端口`8091`和`9091`）定义了一个预定义的响应。其中第一个实例延迟了`200`毫秒，而第二个实例延迟了`50`毫秒：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before running the test, we should also modify the `ribbon.listOfServers` configuration
    file by changing it to `listOfServers: account-service:8091, account-service:9091`.
    We should only make such a modification when working with Hoverfly.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '在运行测试之前，我们还应该修改`ribbon.listOfServers`配置文件，将其更改为`listOfServers: account-service:8091,
    account-service:9091`。我们只有在使用Hoverfly时才应该进行这样的修改。'
- en: 'Here''s a `test` method that invokes the `GET /withAccounts/ {id}` endpoint
    exposed by `customer-service` a thousand times. This, in turn, invokes the `GET
    customer/{customerId}` endpoint from `account-service`, with a list of accounts
    owned by the customer. Every request is load balanced between two instances of
    `account-service` using `WeightedResponseTimeRule`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个调用`customer-service`暴露的`GET /withAccounts/ {id}`端点的`test`方法，调用次数为一千次。反过来，它调用了`account-service`的`GET
    customer/{customerId}`端点，带有客户拥有的账户列表。每个请求都使用`WeightedResponseTimeRule`在`account-service`的两个实例之间进行负载均衡：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The method of working with a weighted response rule implementation is really
    interesting. Just after starting the test, the incoming requests are load balanced
    at a ratio of 50:50 between two instances of `account-service`. But, after some
    time, most of them are forwarded to the instance with the lesser delay.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用加权响应规则实现的工作方法真的很有趣。就在开始测试后，传入的请求在`account-service`的两个实例之间以50:50的比例进行了负载均衡。但是，过了一段时间后，大部分请求都被转发到了延迟较小的实例。
- en: Finally, 731 requests were processed by the instance available on port  `9091` and
    269 by the instance at port `8091` for a JUnit test launched on my local machine. However,
    at the end of the test, the proportion looked a bit different and was weighted
    in favor of the instance with the lesser delay, where incoming traffic is divided
    4:1 between the two instances.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我的本地机器上启动的JUnit测试中，端口`9091`上的实例处理了731个请求，端口`8091`上的实例处理了269个请求。然而，在测试结束时，比例看起来有点不同，并且倾向于延迟较小的实例，其中传入流量在两个实例之间以4:1的比例进行了加权。
- en: 'Now, we will change our test case a little by adding a third instance of `account-service`
    with a big delay of around 10 seconds. This modification aims to simulate a timeout
    in HTTP communication. Here''s the fragment from the JUnit `@ClassRule` definition
    with the newest service instance listening on port `10091`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将稍微改变一下我们的测试用例，通过添加一个延迟大约10秒的`account-service`的第三个实例。这个改动旨在模拟HTTP通信中的超时。以下是JUnit
    `@ClassRule`定义中的一个片段，最新的服务实例监听在端口`10091`上：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should accordingly perform a change in the Ribbon configuration to enable
    load balancing to the newest instance of `account-service`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该相应地在Ribbon配置中进行更改，以启用对`account-service`最新实例的负载均衡：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The last thing that has to be changed, but which is left as it is in the previous
    test case, is the `RestTemplate` bean declaration. In this instance, I have set
    both the read and the connect timeout to one second because the third instance
    of `account-service` launched during the test is delayed by 10 seconds. Every
    request sent there would be terminated by the timeout after one second:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个需要更改的东西，但在之前的测试用例中保持不变，就是`RestTemplate`bean的声明。在这个实例中，我将读取和连接超时都设置为1秒，因为测试中启动的`account-service`的第三个实例延迟了10秒。每发送一个请求都会在1秒后因超时而终止：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you run the same test as before, the result would not be satisfactory. The
    distribution between all declared instances will be 420, processed by the instance
    listening on port `8091` (with a delay of `200` milliseconds), 468, processed
    by the instance listening on port `9091` (with a delay of `50` milliseconds),
    and 112 sent to the third instance, terminated by the timeout. Why am I quoting
    all these statistics? We may change a default load balancing rule from `WeightedResponseTimeRule`
    to `AvailabilityFilteringRule` and rerun the test. If we do this, 496 requests
    will be sent to both the first and second instance, while only 8 will be sent
    to the third instance, with a one second timeout. Interestingly, if you set `BestAvailableRule`
    as the default rule, all requests would be sent to the first instance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像以前那样运行相同的测试，结果将不令人满意。所有声明的实例之间的分布将是420，由端口`8091`上的实例处理（延迟200毫秒），468，由端口`9091`上的实例处理（延迟50毫秒），而112发送到第三个实例，由超时终止。我为什么引用这些统计数据？我们可以将默认负载均衡规则从`WeightedResponseTimeRule`更改为`AvailabilityFilteringRule`，并重新运行测试。如果我们这样做，496个请求将发送给第一个和第二个实例，而只有8个请求将发送给第三个实例，有一个1秒的超时。有趣的是，如果您将`BestAvailableRule`设置为默认规则，所有请求都将发送到第一个实例。
- en: Now that you have read through this example, you can easily see the differences
    between all available load balancing rules for the Ribbon client.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您阅读了此示例，可以轻松地看到Ribbon客户端所有可用负载均衡规则之间的区别。
- en: Customizing the Ribbon client
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义Ribbon客户端
- en: 'Several configuration settings of the Ribbon client may be overridden with
    Spring bean declarations. As with Feign, it should be declared in the client annotation
    field named configuration, for example,`@RibbonClient(name = "account-service",
    configuration = RibbonConfiguration.class)`. The following features may be customized
    with this approach:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Ribbon客户端的几个配置设置可以通过Spring bean声明来覆盖。与Feign一样，它应该在名为configuration的客户端注解字段中声明，例如，`@RibbonClient(name
    = "account-service", configuration = RibbonConfiguration.class)`。使用这种方法可以自定义以下功能：
- en: '`IClientConfig`: The default implementation of this is `DefaultClientConfigImpl`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IClientConfig`：此接口的默认实现是`DefaultClientConfigImpl`。'
- en: '`IRule`: This component is used to determine which service instance should
    be selected from a list. The `ZoneAvoidanceRule` implementation class is auto-configured.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRule`：此组件用于从列表中确定应选择哪个服务实例。`ZoneAvoidanceRule`实现类是自动配置的。'
- en: '`IPing`: This is a component that runs in the background. It is responsible
    for ensuring that the instances of service are running.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IPing`：这是一个在后台运行的组件。它负责确保服务实例正在运行。'
- en: '`ServerList<Server>`: This can be static or dynamic. If it is dynamic (as used
    by `DynamicServerListLoadBalancer`), a background thread will refresh and filter
    the list at a predefined interval. By default, Ribbon uses a static list of servers
    taken from configuration file. It is implemented by  `ConfigurationBasedServerList`.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServerList<Server>`：这可以是静态的或动态的。如果是动态的（如`DynamicServerListLoadBalancer`所使用），后台线程将在预定义的间隔刷新和过滤列表。默认情况下，Ribbon使用从配置文件中获取的服务器静态列表。它由`ConfigurationBasedServerList`实现。'
- en: '`ServerListFilter<Server>`: `ServerListFilter` is a component used by `DynamicServerListLoadBalancer`
    to filter the servers returned from a `ServerList` implementation. There are two
    implementations of that interface—auto-configured `ZonePreferenceServerListFilter`
    and `ServerListSubsetFilter`.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServerListFilter<Server>`：`ServerListFilter`是`DynamicServerListLoadBalancer`用来过滤`ServerList`实现返回的服务器的组件。该接口有两个实现——自动配置的`ZonePreferenceServerListFilter`和`ServerListSubsetFilter`。'
- en: '`ILoadBalancer`: This is responsible for performing load balancing between
    available instances of a service on the client side. By default, Ribbon uses `ZoneAwareLoadBalancer`.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ILoadBalancer`：此组件负责在客户端侧对服务的可用实例进行负载均衡。默认情况下，Ribbon使用`ZoneAwareLoadBalancer`。'
- en: '`ServerListUpdater`: This is responsible for updating the list of available
    instances of a given application. By default, Ribbon uses `PollingServerListUpdater`.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ServerListUpdater`：它负责更新给定应用程序可用的实例列表。默认情况下，Ribbon 使用 `PollingServerListUpdater`。'
- en: 'Let''s look at an example configuration class that defines the default implementation
    of the `IRule` and `IPing` components. Such a configuration may be defined for
    a single Ribbon client, as well as for all Ribbon clients available in the application
    classpath, by providing the `@RibbonClients(defaultConfiguration = RibbonConfiguration.class)` annotation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个定义 `IRule` 和 `IPing` 组件默认实现的配置类示例。这样的配置可以定义为单个 Ribbon 客户端，也可以定义为应用程序类路径中可用的所有
    Ribbon 客户端，通过提供 `@RibbonClients(defaultConfiguration = RibbonConfiguration.class)`
    注解来实现：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Even if you don''t have any experience with Spring, you may probably have guessed
    (based on the previous samples) that the configuration can also be customized
    using the `properties` file. In that case, Spring Cloud Netflix is compatible
    with the properties described in the Ribbon documentation provided by Netflix.
    The following classes are the supported properties, and they should be prefixed
    by `<clientName>.ribbon`, or, if they apply to all clients, by `ribbon`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有 Spring 的经验，你可能也已经猜到（根据之前的示例），配置也可以通过使用 `properties` 文件进行自定义。在这种情况下，Spring
    Cloud Netflix 与 Netflix 提供的 Ribbon 文档中描述的属性兼容。以下类是支持的属性，它们应该以 `<clientName>.ribbon`
    开头，或者如果它们适用于所有客户端，以 `ribbon` 开头：
- en: '`NFLoadBalancerClassName`: `ILoadBalancer` default implementation class'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NFLoadBalancerClassName`：`ILoadBalancer` 默认实现类'
- en: '`NFLoadBalancerRuleClassName`: `IRule` default implementation class'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NFLoadBalancerRuleClassName`：`IRule` 默认实现类'
- en: '`NFLoadBalancerPingClassName`: `IPing` default implementation class'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NFLoadBalancerPingClassName`：`IPing` 默认实现类'
- en: '`NIWSServerListClassName`: `ServerList` default implementation class'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NIWSServerListClassName`：`ServerList` 默认实现类'
- en: '`NIWSServerListFilterClassName`: `ServerListFilter` default implementation
    class'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NIWSServerListFilterClassName`：`ServerListFilter` 默认实现类'
- en: 'Here''s a similar sample to the preceding `@Configuration` class that overrides
    the `IRule` and `IPing` default implementations used by the Spring Cloud application:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个与前面 `@Configuration` 类相似的示例，它覆盖了 Spring Cloud 应用程序使用的 `IRule` 和 `IPing`
    默认实现：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The circuit breaker pattern with Hystrix
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hystrix 电路断路器模式
- en: We have already discussed the different implementations of load balancer algorithms
    in Spring Cloud Netflix. Some of them are based on monitoring the instance response
    time or the number of failures. In these cases, a load balancer makes decisions
    about which instance should be invoked based on these statistics. The circuit
    breaker pattern should be treated as an extension of that solution. The main idea
    behind a circuit breaker is very simple. A protected function call is wrapped
    in a circuit breaker object, which is responsible for monitoring a number of failure
    calls. If the failures reach a threshold, the circuit is opened, and all further
    calls will be failed automatically. Usually, it is also desirable to have some
    kind of monitor alert if a circuit breaker trips. Some crucial benefits derived
    from the usage of the circuit breaker pattern in your applications are the ability
    to continue operating when a related service fails, the prevention of a cascaded
    failure, and giving a failing service time to recover.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 Spring Cloud Netflix 中负载均衡算法的不同实现。其中一些是基于监控实例响应时间或失败次数。在这些情况下，负载均衡器根据这些统计数据来决定调用哪个实例。电路断路器模式应被视为该解决方案的扩展。电路断路器背后的主要想法非常简单。一个受保护的函数调用被包装在一个电路断路器对象中，该对象负责监控失败调用次数。如果失败次数达到阈值，电路将打开，所有后续调用都将自动失败。通常，如果电路断路器触发，也希望有一种监控警报。应用程序中使用电路断路器模式的一些关键好处是，当相关服务失败时能够继续运行，防止级联失败，并给失败的服务时间来恢复。
- en: Building an application with Hystrix
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hystrix 构建应用程序
- en: 'Netflix provides an implementation of the circuit breaker pattern in their
    library called **Hystrix**. That library has also been included as a default implementation
    of the circuit breaker for Spring Cloud. Hystrix has some other interesting features,
    and should also be treated as a comprehensive tool for dealing with latency and
    fault tolerance for distributed systems. What is important is that if the circuit
    breaker is opened, Hystrix redirects all calls to the specified fallback method.
    The fallback method is designed to provide a generic response without any dependency
    on a network, usually read from an in-memory cache or just implemented as static
    logic. If it becomes necessary to perform a network call, it is recommended that
    you implement it using another `HystrixCommand` or `HystrixObservableCommand`.
    To include Hystrix in your project, you should use the `spring-cloud-starter-netflix-hystrix`
    or `spring-cloud-starter-hystrix` starter for Spring Cloud Netflix versions older
    than 1.4.0:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Netflix 在他们的库中提供了一个名为 **Hystrix** 的断路器模式的实现。这个库也被作为 Spring Cloud 的默认断路器实现。Hystrix
    还有一些其他有趣的特性，也应该被视为一个用于处理分布式系统延迟和容错的综合工具。重要的是，如果打开断路器，Hystrix 将所有调用重定向到指定的回退方法。回退方法被设计为提供一个不依赖于网络的通用响应，通常从内存缓存中读取或简单实现为静态逻辑。如果需要执行网络调用，建议您使用另一个
    `HystrixCommand` 或 `HystrixObservableCommand` 来实现。为了在您的项目中包含 Hystrix，您应该使用 `spring-cloud-starter-netflix-hystrix`
    或 `spring-cloud-starter-hystrix` 作为 Spring Cloud Netflix 1.4.0 版本之前的启动器：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Implementing Hystrix's commands
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Hystrix 的命令
- en: 'Spring Cloud Netflix Hystrix looks for a method that is annotated with the
    `@HystrixCommand` annotation, and then wraps it in a proxy object connected to
    a circuit breaker. Thanks to this, Hystrix is able to monitor all calls of such
    a method. This annotation currently works only for a class marked with `@Component`
    or `@Service`. That''s important information for us, because we have implemented
    the logic related to other services calling in all the previous samples inside
    the REST controller class, which is marked with the `@RestController` annotation.
    So, in the `customer-service` application, all that logic has been moved to the
    newly created `CustomerService` class, which is then injected into the controller
    bean. The method responsible for communication with `account-service` has been
    annotated with `@HystrixCommand`. I have also implemented a fallback method, the
    name of which passes into the `fallbackMethod` annotation''s field. This method only returns
    an empty list:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Netflix Hystrix 会寻找带有 `@HystrixCommand` 注解的方法，然后将其包装在连接到断路器的代理对象中。正因为如此，Hystrix
    能够监控这类方法的所有的调用。这个注解目前只对标记有 `@Component` 或 `@Service` 的类有效。这对我们来说是很重要的信息，因为我们已经在带有
    `@RestController` 注解的 REST 控制器类中实现了与其它服务调用相关的所有逻辑。所以，在 `customer-service` 应用程序中，所有那部分逻辑都被移动到了新创建的
    `CustomerService` 类中，然后将其注入到控制器bean中。负责与 `account-service` 通信的方法已经被标记为 `@HystrixCommand`。我还实现了一个回退方法，其名称传递到
    `fallbackMethod` 注解的字段中。这个方法只返回一个空列表：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Don''t forget to mark your main class with `@EnableHystrix`, which is needed
    to tell Spring Cloud that it should use circuit breakers for the application.
    We may also optionally annotate a class with `@EnableCircuitBreaker`, which does
    the same. For test purposes, the `account-service.ribbon.listOfServers` property should
    have included the network addresses of two instances of the `localhost:8091, localhost:9091` service.
    Although we have declared two instances of `account-service` for the Ribbon client,
    we will start the only one that is available on the `8091` port. If you call the `customer-service`
    method `GET http://localhost:8092/withAccounts/{id}`, Ribbon will try to load
    balance every incoming request between those two declared instances, that is,
    once you receive the response containing a list of accounts and the second time
    you receive an empty account list, or vice versa. This is illustrated by the following
    fragment of the application logs. This is illustrated by the following fragment
    of application''s logs. To access the sample application''s source code, you should
    switch to the `hystrix_basic` branch ([https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_basic](https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_basic))
    in the same GitHub repository as the samples from the previous chapter:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记用`@EnableHystrix`标记你的主类，这是告诉Spring Cloud应该为应用程序使用断路器所必需的。我们也可以选择性地用`@EnableCircuitBreaker`注解一个类，它也能起到同样的作用。为了测试目的，`account-service.ribbon.listOfServers`属性应该包含`localhost:8091,
    localhost:9091`服务两个实例的网络地址。虽然我们为Ribbon客户端声明了两个`account-service`实例，但我们将在`8091`端口上启动唯一可用的一个。如果你调用`customer-service`方法的`GET
    http://localhost:8092/withAccounts/{id}`，Ribbon将尝试将在两个声明的实例之间平衡每个传入请求，即，一旦你收到包含账户列表的响应，第二次收到空账户列表，或相反。以下应用日志的片段说明了这一点。以下是对应用日志的一个片段。要访问示例应用程序的源代码，你应该切换到与前章示例相同的GitHub仓库中的`hystrix_basic`分支：（https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_basic）
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Implementing fallback with cached data
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带有缓存数据的回退
- en: 'The fallback implementation presented in the previous example is very simple.
    Returning an empty list does not make much sense for an application running in
    production. It makes more sense to use the fallback method in your application
    when you read data from a cache in case of a request failure, for example. Such
    a cache may be implemented inside the client application or with the use of third-party
    tools, such as Redis, Hazelcast, or EhCache. The simplest implementation is available
    within the Spring Framework, and can be used after including the `spring-boot-starter-cache` artifact with your
    dependencies. To enable caching for the Spring Boot application, you should annotate
    the main or configuration class with `@EnableCaching` and provide the `CacheManager`
    bean in the following context:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 前面示例中呈现的回退实现非常简单。对于在生产环境中运行的应用程序来说，返回一个空列表并没有多大意义。在请求失败时，例如从缓存中读取数据时，在应用程序中使用回退方法更有意义。这样的缓存可以在客户端应用程序内部实现，也可以使用第三方工具实现，如Redis、Hazelcast或EhCache。最简单的实现是在Spring框架内部提供的，在将`spring-boot-starter-cache`
    artifact包含在依赖项之后可以使用。要为Spring Boot应用程序启用缓存，你应该用`@EnableCaching`注解标注主类或配置类，并提供以下上下文中的`CacheManager`
    bean：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then you can mark the method wrapped with the circuit breaker using the `@CachePut`
    annotation. This will add the result returning from the calling method to the
    cache map. In that case, our map is named `accounts`. Finally, you may read the
    data inside your fallback method implementation by invoking the `CacheManager`
    bean directly. If you retry the same request a couple of times, you will see that
    the empty list of accounts is no longer returned as a response. Instead, the service
    always returns data that is cached during the first successful call:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用`@CachePut`注解标记被电路 breaker 包裹的方法。这会将调用方法的返回结果添加到缓存映射中。在这种情况下，我们的映射名为`accounts`。最后，您可以在回退方法实现内部直接调用`CacheManager`
    bean来读取数据。如果你多次重试同一个请求，你会看到空账户列表不再作为响应返回。相反，服务总是返回在第一次成功调用期间缓存的数据：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The tripping circuit breaker
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 触发断路器
- en: Let me suggest an exercise for you to do. Until now, you have learned how to
    enable and implement circuit breakers in your application using Hystrix, in conjunction
    with Spring Cloud, and how to use a fallback method to take data from the cache.
    But you still have not used a tripped circuit breaker to prevent the failure instance
    from being invoked by a load balancer. Now, I would like to configure Hystrix
    to open the circuit after three failed call attempts if the failure percentage
    is greater than `30` percent and prevent the API method from being called for
    the next 5 seconds. The measurement time window is around `10` seconds. To meet
    these requirements, we have to override several default Hystrix configuration
    settings. It may be performed using the `@HystrixProperty` annotation inside `@HystrixCommand`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我给你提个练习题。到目前为止，你已经学会了如何使用Hystrix，结合Spring Cloud，在应用程序中启用和实现断路器，以及如何使用回退方法从缓存中获取数据。但你还没有使用过触发断路器来防止负载均衡器调用失败实例。现在，我想配置Hystrix，在失败率超过`30`%的情况下，在三次失败的调用尝试后打开电路，并在接下来的5秒钟内防止API方法被调用。测量时间窗口大约是`10`秒。为了满足这些要求，我们必须重写几个默认的Hystrix配置设置。这可以在`@HystrixCommand`内的`@HystrixProperty`注解中执行。
- en: 'Here''s the current implementation of the method responsible for getting the
    account list from `customer-service`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`customer-service`中获取账户列表方法的当前实现：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The full list of Hystrix''s configuration properties is available on Netflix''s
    GitHub site at[ https://github.com/Netflix/Hystrix/wiki/Configuration](https://github.com/Netflix/Hystrix/wiki/Configuration).
    I won''t discuss all of them, only the most important properties for communication
    between microservices. Here''s the list of the properties used in our sample,
    along with their descriptions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Hystrix配置属性的完整列表，可以在Netflix的GitHub网站上找到，网址为[https://github.com/Netflix/Hystrix/wiki/Configuration](https://github.com/Netflix/Hystrix/wiki/Configuration)。我不会讨论所有属性，只讨论微服务间通信最重要的属性。以下是我们在示例中使用的属性列表及其描述：
- en: '`execution.isolation.thread.timeoutInMilliseconds`: This property sets the
    time in milliseconds, after which a read or connect timeout will occur and the
    client will walk away from the command execution. Hystrix marks such a method
    call as a failure, and performs fallback logic. That timeout may be completely
    turned off by setting the `command.timeout.enabled` property to `false`. The default
    is 1,000 milliseconds.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`execution.isolation.thread.timeoutInMilliseconds`：此属性设置在发生读取或连接超时的时间（以毫秒为单位），之后客户端将离开命令执行。Hystrix将此类方法调用标记为失败，并执行回退逻辑。可以通过将`command.timeout.enabled`属性设置为`false`来完全关闭超时。默认值为1,000毫秒。'
- en: '`circuitBreaker.requestVolumeThreshold`: This property sets the minimum number
    of requests in a rolling window that will trip the circuit. The default value
    is 20\. In our sample, this property is set to `10`, which means that the first
    nine will not trip the circuit, even if all of them fail. I set that value because
    we have assumed that the circuit should be opened if `30` percent of incoming
    requests fail, but the minimum number of incoming requests is three.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`circuitBreaker.requestVolumeThreshold`：此属性设置在滚动窗口中触发电路的最小请求数量。默认值是20。在我们的示例中，此属性设置为`10`，这意味着前九个请求不会触发电路，即使它们都失败了。我设置这个值是因为我们假设如果`30`%的传入请求失败，电路应该被打开，但最少传入请求数量是三个。'
- en: '`circuitBreaker.errorThresholdPercentage`: This property sets the minimum error
    percentage. Exceeding this percentage results in opening the circuit, and the
    system starts short-circuiting requests to fallback logic. The default value is
    50\. I set it to `30` because, in our sample, I want `30` percent of failed requests
    should open the circuit.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`circuitBreaker.errorThresholdPercentage`：此属性设置最小的错误百分比。超过此百分比将导致打开电路，系统开始短路请求以执行回退逻辑。默认值是50。我将其设置为`30`，因为在我们示例中，我希望`30`%的失败请求应该打开电路。'
- en: '`circuitBreaker.sleepWindowInMilliseconds`: This property sets a period of
    time between tripping the circuit and allowing attempts taken in order to determine
    whether the circuit should be closed again. During this time, all incoming requests
    are rejected. The default value is `5,000`. Because we would like to wait `10`
    seconds before the first call is retired after the circuit has been opened, I
    set it to `10,000`.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`circuitBreaker.sleepWindowInMilliseconds`：此属性设置在触发电路和允许尝试以确定是否应再次关闭电路之间的时间间隔。在这段时间内，所有传入请求都被拒绝。默认值是`5,000`。因为我们希望电路打开后在`10`秒内等待第一次调用被退休，所以我将其设置为`10,000`。'
- en: '`metrics.rollingStats.timeInMilliseconds`: This property sets the duration
    of the statistical rolling window in milliseconds. This is how long Hystrix keeps
    metrics for the circuit breaker to use, and for publishing.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics.rollingStats.timeInMilliseconds`：这个属性设置了统计滚动窗口的持续时间，单位为毫秒。Hystrix就是用这个时间来保持电路断路器使用的指标和发布用的。'
- en: 'With these settings, we may run the same JUnit test as for the previous example.
    We launch two stubs of `account-service` using `HoverflyRule`. The first of them
    would be delayed by 200 milliseconds, while a second one that is delayed by 2,000
    milliseconds is greater than the timeout set for `@HystrixCommand` with the `execution.isolation.thread.timeoutInMilliseconds` property.
    After running JUnit `CustomerControllerTest`, take a look at the printed logs.
    I have inserted the logs taken from the test launched on my machine. The first
    request from `customer-service` is load balanced to the first instance, delayed
    by 200 ms `(1)`. Every request sent to the instance available on `9091` finishes
    with a timeout after one second. After sending 10 requests, the first failure
    causes a trip of the circuit `(2)`. Then, for the next 10 seconds, every single
    request is handled by a fallback method, which returns cached data `(3)`, `(4)`.
    After 10 seconds, the client tries to call an instance of `account-service` again
    and succeeds `(5)` because it hits on the instance delayed by 200 ms. That success
    results in the closure of the circuit. Unfortunately, the second instance of `account-service`
    still responds slowly, so the scenario happens all over again until the JUnit
    test finishes `(6)` and `(7)`. This detailed description shows you exactly how
    a circuit breaker with Hystrix works for Spring Cloud:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些设置，我们可以运行与之前例子相同的JUnit测试。我们使用`HoverflyRule`启动两个`account-service`的存根。其中的第一个会被延迟200毫秒，而第二个延迟2000毫秒的会超过`@HystrixCommand`中`execution.isolation.thread.timeoutInMilliseconds`属性的设置。运行JUnit`CustomerControllerTest`后，查看打印的日志。我插入了我机器上运行的测试的日志。`customer-service`的第一个请求会被负载均衡到第一个延迟200毫秒的实例`(1)`。发送到`9091`端口可用的实例的每个请求，在一秒后都会超时完成。在发送10个请求后，第一个失败触发了电路的断开`(2)`。然后，在接下来的10秒内，每个请求都由回退方法处理，返回缓存数据`(3)`、`(4)`。10秒后，客户端再次尝试调用`account-service`的实例并成功`(5)`，因为它击中了延迟200毫秒的实例。这次成功导致电路关闭。不幸的是，`account-service`的第二个实例仍然响应缓慢，所以整个场景再次重演，直到JUnit测试结束`(6)`和`(7)`。这个详细的描述准确地展示了Spring
    Cloud中的Hystrix电路断路器是如何工作的：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Monitoring latency and fault tolerance
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控延迟和容错
- en: As I have already mentioned, Hystrix is not only a simple tool implementing
    a circuit breaker pattern. It is a solution that deals with latency and fault
    tolerance in distributed systems. One interesting feature provided by Hystrix
    is the ability to expose the most important metrics related to interservice communication
    and display them using a UI dashboard. This function is available for clients
    wrapped with the Hystrix command.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如我前面所提到的，Hystrix不仅仅是一个实现断路器模式的简单工具。它是一个解决方案，用于处理分布式系统中的延迟和容错。Hystrix提供的一个有趣功能是可以暴露与服务间通信相关的最重要的指标，并通过UI仪表板显示出来。这个功能适用于用Hystrix命令包装的客户端。
- en: In some previous samples, we have analyzed only a part of our system to simulate
    a delay in communication between `customer-service` and `account-service`. That's
    a really good approach when testing advanced load balancing algorithms or different
    circuit breaker configuration settings, but now we will go back to analyzing the
    whole of our sample system setup as a set of standalone Spring Boot applications.
    This allows us to observe how Spring Cloud, in conjunction with Netflix OSS tools,
    helps us to monitor and react to latency issues and failures in communication
    between our microservices. The sample system simulates a failure in a simple way.
    It has a static configuration with the network addresses of two instances, `account-service`,
    and `product-service`, but only one of them for each service is running.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的某些示例中，我们分析了我们系统的一部分，以模拟`customer-service`和`account-service`之间的通信延迟。当测试高级负载均衡算法或不同的断路器配置设置时，这是一种非常好的方法，但现在我们将回到分析我们示例系统的整体设置，作为一个独立的Spring
    Boot应用程序集合。这使我们能够观察到Spring Cloud与Netflix OSS工具结合在一起，如何帮助我们监控和响应微服务之间的通信延迟问题和故障。示例系统以一种简单的方式模拟了一个故障。它有一个静态配置，包含了两个实例`account-service`和`product-service`的网络地址，但每个服务只运行一个实例。
- en: 'In order to refresh your memory, the architecture of our sample system, taking
    into consideration assumptions about failure, is shown in the following diagram:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使您记忆犹新，以下是我们样本系统的架构，考虑到关于失败的假设：
- en: '![](img/ce848d7e-8834-48f2-885f-273126e8aa5c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce848d7e-8834-48f2-885f-273126e8aa5c.png)'
- en: 'This time, we''ll begin a bit differently, with a test. Here''s the fragment
    of the test method, which is being invoked in a loop. First, it calls the `POST
    http://localhost:8090/` endpoint from `order-service`, sending an `Order` object,
    and it receives a response with the `id`, `status`, and `price` set. Within that
    request, which has been labeled in the preceding diagram as `(1)`, `order-service`
    communicates with `product-service `and `customer-service` and, in addition, `customer-service`
    calls the endpoint from `account-service`. If the order has been accepted, the
    test client calls the `PUT http://localhost:8090/{id}` method with the order''s `id`
    to accept it and withdraw funds from the account. On the server side, there is
    only one interservice communication in that case, which is labeled `(2)` in the
    preceding diagram. Before running this test, you have to launch all microservices
    that are a part of our system:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将以一种稍微不同方式开始，进行一个测试。以下是正在循环调用测试方法的片段。首先，它调用来自`order-service`的`POST http://localhost:8090/`端点，发送一个`Order`对象，并收到具有`id`、`status`和`price`设置的响应。在该请求中，如前一个图中所标记的`(1)`，`order-service`与`product-service`和`customer-service`通信，并且，除此之外，`customer-service`调用来自`account-service`的端点。如果订单被接受，测试客户端调用`PUT
    http://localhost:8090/{id}`方法，带有订单的`id`来接受它并从账户中提取资金。在服务器端，在那情况下只有一次服务间通信，如前一个图中所标记的`(2)`。在运行这个测试之前，你必须启动我们系统中的所有微服务：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Exposing Hystrix's metrics stream
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 暴露 Hystrix 的指标流
- en: 'Each microservice that uses Hystrix in communication with other microservices
    may expose metrics of every integration wrapped with the Hystrix command. To enable
    such a metrics stream, you should include a dependency on `spring-boot-starter-actuator`.
    This will expose the `/hystrix.stream` object as a management endpoint. It is
    also necessary to include `spring-cloud-starter-hystrix`, which has already been
    added to our sample application:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个使用 Hystrix 在与其他微服务通信中可能暴露每个封装在 Hystrix 命令中的集成指标的微服务。要启用这样的指标流，你应该包括对`spring-boot-starter-actuator`的依赖。这将把`/hystrix.stream`对象作为管理端点暴露出来。还需要包括`spring-cloud-starter-hystrix`，这已经添加到我们的示例应用程序中：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'A generated stream is exposed as further JSON entries containing metrics characterizing
    a single call within a method. Here''s a single entry for a call within the `GET
    /withAccounts/{id}` method from `customer-service`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的流作为进一步的 JSON 条目暴露，包含描述单一调用内方法的指标。以下是来自`customer-service`的`GET /withAccounts/{id}`方法的一个调用条目：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Hystrix dashboard
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hystrix 仪表板
- en: 'Hystrix dashboard visualizes the following information:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Hystrix 仪表板可视化了以下信息：
- en: Health and traffic volume is displayed as a circle that is changing its color
    and size together with the changes in incoming statistics
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康和流量体积以一个随着传入统计数据变化而改变颜色和大小的圆形显示
- en: The error percentage over the last 10 seconds
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去10秒内的错误百分比
- en: The request rate over the last two minutes by number, displaying the results
    on a graph
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去两分钟内的请求速率，通过数字显示结果在图表上
- en: The circuit breaker status (open/closed)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断路器状态（开启/关闭）
- en: The number of service hosts
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务主机数量
- en: The latency percentiles over the last minute
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去一分钟内的延迟百分比
- en: The service's thread pools
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务的线程池
- en: Building an application with the dashboard
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建带有仪表板的应用程序
- en: 'The Hystrix dashboard is integrated with Spring Cloud. The best approach when
    implementing the dashboard inside a system is to separate out an independent Spring
    Boot application with the dashboard. To include the Hystrix dashboard in your
    project, use the `spring-cloud-starter-hystrix-netflix-dashboard` starter or `spring-cloud-starter-hystrix-dashboard`
    for Spring Cloud Netflix versions older than 1.4.0:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Hystrix 仪表板与 Spring Cloud 集成。在系统内实现仪表板的最佳方法是将仪表板分离为一个独立的 Spring Boot 应用程序。要将在项目中包含
    Hystrix 仪表板，请使用`spring-cloud-starter-hystrix-netflix-dashboard`启动器或对于旧于1.4.0的
    Spring Cloud Netflix 版本使用`spring-cloud-starter-hystrix-dashboard`：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The application''s main class should be annotated with `@EnableHystrixDashboard`.
    After launching it, the Hystrix dashboard is available under the `/hystrix` context
    path:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序的主类应使用`@EnableHystrixDashboard`注解。启动后，Hystrix 仪表板在`/hystrix`上下文路径下可用：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'I configured port `9000` as the default for the Hystrix application in our
    sample system, which is implemented in the `hystrix-dashboard` module. So, if
    you call the `http://localhost:9000/hystrix` address in a web browser after launching
    `hystrix-dashboard`, it will display the page as shown in the following screenshot.
    There, you should provide the Hystrix stream endpoint''s address, and, optionally,
    a title. If you would like to display metrics for all the endpoints that are called
    from `order-service`, type the address `http://localhost:8090/hystrix.stream`
    and then click the Monitor Stream button :'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我们示例系统中的Hystrix应用程序中配置了端口`9000`作为默认端口，该应用程序在`hystrix-dashboard`模块中实现。所以，在启动`hystrix-dashboard`后，用网络浏览器调用`http://localhost:9000/hystrix`地址，它会显示如下截图中的页面。在那里，您应提供Hystrix流端点的地址，可选提供一个标题。如果您想要为从`order-service`调用所有端点显示指标，请输入地址`http://localhost:8090/hystrix.stream`，然后点击监控流按钮：
- en: '![](img/0aa6b722-29ff-47e5-a8f4-34ea62bc2944.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0aa6b722-29ff-47e5-a8f4-34ea62bc2944.png)'
- en: Monitoring metrics on the dashboard
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在仪表板上监控指标
- en: 'In this section, we will look at calling the `GET /withAccounts/{id}` method from
    `customer-service`. It is wrapped with `@HystrixCommand`. It is displayed on the Hystrix
    dashboard under the title `customer-service.findWithAccounts`, taken from a `commandKey` attribute.
    In addition, the UI dashboard also shows information about the thread pools that
    are assigned to every Spring Bean that provides an implementation of methods wrapped
    with Hystrix''s command. In this case, it is `CustomerService`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看从`customer-service`调用`GET /withAccounts/{id}`方法。它被包裹在`@HystrixCommand`中。它显示在Hystrix仪表板上，标题为`customer-service.findWithAccounts`，来自一个`commandKey`属性。此外，UI仪表板还显示了分配给每个提供Hystrix命令封装方法实现的Spring
    Bean的线程池信息。在此案例中，它是`CustomerService`：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here''s the screen from the Hystrix dashboard just after the start of a JUnit
    test. We monitor the state of all three methods wrapped with `@HystrixCommand`.
    The circuit has been opened for the `findByIds` method from `product-service`,
    as expected. After a few seconds, the circuit has also been opened for the `withdraw`
    method from `account-service`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Hystrix仪表板在JUnit测试开始后的屏幕。我们监控了三个用`@HystrixCommand`包裹的方法的状态。`product-service`的`findByIds`方法的电路如预期般已被打开。几秒钟后，`account-service`的`withdraw`方法的电路也已打开：
- en: '![](img/fe9084d4-8482-4bd7-aad2-470cd1af41ad.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe9084d4-8482-4bd7-aad2-470cd1af41ad.png)'
- en: 'After a few moments, the situation will be stabilized. All the circuits remain
    closed because only a small percentage of traffic is sent to the inactive instances
    of applications. This shows the power of Spring Cloud with Hystrix and Ribbon.
    The system was able to automatically reconfigure itself in order to redirect most
    of the incoming requests to the working instances based on the metrics generated
    by the load balancers and circuit breakers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 片刻之后，情况将稳定下来。所有电路都保持关闭状态，因为只有少量的流量被发送到应用程序的不活动实例。这展示了Spring Cloud结合Hystrix和Ribbon的力量。系统能够自动重新配置自己，以便基于负载均衡器和断路器生成的指标，将大部分传入请求重定向到工作实例：
- en: '![](img/9a1a4f33-d388-46a9-a060-05d87303eb6c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a1a4f33-d388-46a9-a060-05d87303eb6c.png)'
- en: Aggregating Hystrix's streams with Turbine
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Turbine聚合Hystrix的流
- en: You have probably noticed that we were only able to look at an individual instance
    of the service in the Hystrix dashboard. There were no metrics from communication
    between `customer-service` and `account-service` when we were displaying the state
    of commands for `order-service`, and vice versa. We might also imagine that there
    is more than one instance of `order-service` running, which makes it necessary
    to switch regularly between different instances or services in the Hystrix dashboard.
    Fortunately, there is an application called **Turbine** that aggregates all of
    the relevant `/hystrix.stream` endpoints into a combined `/turbine.stream` and
    makes it possible for us to monitor the overall health of the whole system.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我们在Hystrix仪表板上只能查看服务的一个实例。当我们显示`order-service`命令的状态时，没有从`customer-service`和`account-service`之间的通信指标，反之亦然。我们可能还会想象`order-service`有不止一个实例在运行，这使得在Hystrix仪表板上定期切换不同的实例或服务变得必要。幸运的是，有一个名为**Turbine**的应用程序可以将所有相关的`/hystrix.stream`端点聚合到一个组合的`/turbine.stream`中，使我们能够监控整个系统的整体健康状况。
- en: Enabling Turbine
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用Turbine
- en: 'Before making any changes to enable Turbine for our application, we should
    start by enabling service discovery, which is required here. Switch to the `hystrix_with_turbine` branch to
    access the version of our sample system that supports service discovery with Eureka
    and aggregates Hystrix''s streams using Turbine. To enable Turbine for the project
    exposing the UI dashboard, just include `spring-cloud-starter-turbine` in the
    dependencies and annotate the main application class with `@EnableTurbine`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在为我们的应用程序启用Turbine之前，我们首先应该启用服务发现，这是在这里必需的。切换到`hystrix_with_turbine`分支，以访问支持通过Eureka进行服务发现并使用Turbine聚合Hystrix流的一个版本我们的示例系统。要为项目启用UI仪表板，只需在依赖项中包含`spring-cloud-starter-turbine`，并用`@EnableTurbine`注解主应用类：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `turbine.appConfig` configuration property is a list of Eureka service
    names that Turbine will use to look up instances. The Turbine stream is then available
    in the Hystrix dashboard under the URL `http://localhost:9000/turbine.stream`.
    The address is also determined by a value of the `turbine.aggregator.clusterConfig`
    property, `http://localhost:9000/turbine.stream?cluster=<clusterName>`. The cluster
    parameter can be omitted if the name is `default`. Here''s the Turbine configuration
    that combines all of Hystrix''s visualization metrics in a single UI dashboard:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`turbine.appConfig`配置属性是Turbine将要查找实例的Eureka服务名称列表。然后，在`http://localhost:9000/turbine.stream`
    URL下，Hystrix仪表板中的Turbine流即可使用。地址也由`turbine.aggregator.clusterConfig`属性的值决定，`http://localhost:9000/turbine.stream?cluster=<clusterName>`。如果集群名称为`default`，则可以省略集群参数。以下Turbine配置将所有Hystrix的可视化指标整合到单个UI仪表板上：'
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, all of Hystrix''s metrics for the whole sample system are displayed in
    a single dashboard site. All we need to display them is to monitor the statistics
    stream, available under `http://localhost:9000/turbine.stream`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，整个示例系统的所有Hystrix指标都可以在一个仪表板网站上显示出来。要显示它们，我们只需要监控位于`http://localhost:9000/turbine.stream`下的统计流：
- en: '![](img/1426d797-14f8-4198-8bc0-0090718a565c.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1426d797-14f8-4198-8bc0-0090718a565c.png)'
- en: 'Alternatively, we can configure a cluster per service by providing a list of
    services with the `turbine.aggregator.clusterConfig` property. In that case, you
    may switch between clusters by providing the service name `cluster` with the `http://localhost:9000/turbine.stream?cluster=ORDER-SERVICE` parameter.
    The cluster name must be provided in uppercase because values returned by the
    Eureka server are in uppercase:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以为每个服务配置一个集群，通过提供`turbine.aggregator.clusterConfig`属性的服务列表来实现。在这种情况下，您可以通过提供服务名称`cluster`以及`http://localhost:9000/turbine.stream?cluster=ORDER-SERVICE`参数，在集群之间进行切换。因为Eureka服务器返回的值是大写的，所以集群名称必须是大写的：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'By default, Turbine is looking for the `/hystrix.stream` endpoint on a registered
    instance under its `homePageUrl` address in Eureka. Then it appends `/hystrix.stream`
    to that URL. Our sample application `order-service` is launched under port `8090`,
    so we should also override the default management port to `8090`. The current
    configuration of `order-service` is shown in the following code fragment. Alternatively,
    you may also change that port with the `eureka.instance.metadata-map.management.port`
    property:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Turbine在其Eureka注册实例的`homePageUrl`地址下寻找`/hystrix.stream`端点。然后，它在该URL后附加`/hystrix.stream`。我们的示例应用`order-service`在端口`8090`上启动，因此我们应该也覆盖默认的管理端口为`8090`。下面是`order-service`的当前配置代码片段。另外，您还可以通过`eureka.instance.metadata-map.management.port`属性来更改端口：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Enabling Turbine with streaming
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启用Turbine流式处理
- en: 'The classic Turbine model of pulling metrics from all the distributed Hystrix
    commands is not always a good choice. An operation such as collecting metrics
    from HTTP endpoints may also be realized asynchronously with a message broker.
    To enable Turbine with streaming, we should include the following dependencies
    with the project and then annotate the main application with `@EnableTurbineStream`.
    The following sample uses RabbitMQ as a default message broker, but you may use
    Apache Kafka by including `spring-cloud-starter-stream-kafka`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 经典Turbine模型从所有分布式Hystrix命令中提取指标，并不总是一个好的选择。例如，收集HTTP端点的指标也可以通过消息代理异步实现。要使Turbine支持流式处理，我们应该在项目中包含以下依赖项，然后用`@EnableTurbineStream`注解主应用。下面的示例使用RabbitMQ作为默认消息代理，但您可以通过包含`spring-cloud-starter-stream-kafka`来使用Apache
    Kafka：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The dependencies visible in the preceding code should be included on the server
    side. For client applications, these are `order-service` and `customer-service`,
    and we need to add the `spring-cloud-netflix-hystrix-stream` library. If you have
    run your message broker locally, it should have worked successfully on auto-configured
    settings. You may also run RabbitMQ using a Docker container, as we did in the
    example of the Spring Cloud Config with AMQP bus described in [Chapter 5](37142825-02d0-48a0-99df-1a1a88a1bbd4.xhtml),
    *Distributed Configuration with Spring Cloud Config*. Then you should override
    the following properties in `application.yml` for both the client-side and server-side
    applications:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中可见的依赖项应该包含在服务器端。对于客户端应用程序，这些是`order-service`和`customer-service`，我们还需要添加`spring-cloud-netflix-hystrix-stream`库。如果你在本地运行了消息代理，它应该在自动配置的设置上成功工作。你也可以使用Docker容器运行RabbitMQ，正如我们在第5章中描述的Spring
    Cloud Config与AMQP总线一样，*分布式配置与Spring Cloud Config*。然后，你应该在客户端和服务器端应用程序的`application.yml`文件中覆盖以下属性：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If you log in to the RabbitMQ management console, available under `http://192.168.99.100:15672`, you
    will see that the new exchange with the name `springCloudHystrixStream` has been
    created after our sample application's startup. Now, the only thing left to do
    is to run the same JUnit test as we did for the sample that illustrated the classic
    Turbine approach, described in the previous section. All metrics are sent through
    the message broker and may be observed under the `http://localhost:9000` endpoint.
    If you would like to try it by yourself, switch to the `hystrix_with_turbine_stream` branch (see [https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream](https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream) for
    more information).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你登录到RabbitMQ管理控制台，该控制台可通过`http://192.168.99.100:15672`访问，你会看到在我们的示例应用程序启动后创建了一个名为`springCloudHystrixStream`的新交换机。现在，剩下要做的就是运行与之前部分中描述的经典Turbine方法的示例相同的JUnit测试。所有指标都通过消息代理发送，并可以在`http://localhost:9000`端点下观察。如果你想要亲自尝试，请切换到`hystrix_with_turbine_stream`分支（更多信息请参见[https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream](https://github.com/piomin/sample-spring-cloud-comm/tree/hystrix_with_turbine_stream)）。
- en: Failures and the circuit breaker pattern with Feign
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Feign的失败和断路器模式
- en: The Feign client is, by default, integrated with Ribbon and Hystrix. This means
    that, if you wish, you can apply different approaches to deal with latency and
    timeouts in your system when using that library. The first of these approaches
    is a connection retry mechanism provided by the Ribbon client. The second is a
    circuit breaker pattern and a fallback implementation available under the Hystrix
    project, which has already been discussed in the previous sections of this chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Feign客户端与Ribbon和Hystrix集成。这意味着，如果你愿意，你可以在使用该库时应用不同的方法来处理系统的延迟和超时。这些方法中的第一种是由Ribbon客户端提供的连接重试机制。第二种是在Hystrix项目中提供的断路器模式和回退实现，这在本书的上一节中已经讨论过了。
- en: Retrying the connection with Ribbon
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ribbon重试连接
- en: 'Hystrix is enabled by default for the application when using a Feign library.
    This means that you should disable it in the configuration settings if you do
    not want to use it. For the purpose of testing a retry mechanism with Ribbon,
    I suggest that you disable Hystrix. In order to enable connection retrying for
    Feign, you only have to set two configuration properties—`MaxAutoRetries` and `MaxAutoRetriesNextServer`.
    The important settings, in this case, are also `ReadTimeout` and `ConnectTimeout`.
    All of them may be overridden in the `application.yml` file. Here''s the list
    of the most important Ribbon settings:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Feign库时，应用程序默认启用Hystrix。这意味着如果你不想使用它，你应该在配置设置中禁用它。为了测试带有Ribbon的重试机制，我建议你禁用Hystrix。为了使Feign具有连接重试功能，你只需要设置两个配置属性—`MaxAutoRetries`和`MaxAutoRetriesNextServer`。在此情况下，重要的设置还包括`ReadTimeout`和`ConnectTimeout`。它们都可以在`application.yml`文件中覆盖。以下是Ribbon设置中最重要的一些：
- en: '`MaxAutoRetries`: This is the maximum number of retries on the same server
    or service instances. The first try is excluded from this count.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxAutoRetries`：这是在同一服务器或服务实例上进行重试的最大次数。第一次尝试不包括在内。'
- en: '`MaxAutoRetriesNextServer`: This is the maximum number of next servers or service
    instances to retry, excluding the first server.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxAutoRetriesNextServer`：这是要重试的最大下一个服务器或服务实例次数，不包括第一个服务器。'
- en: '`OkToRetryOnAllOperations`: This states that all operations can be retried
    for this client.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OkToRetryOnAllOperations`：这表示此客户端的所有操作都可以重试。'
- en: '`ConnectTimeout`: This is the maximum time waiting to establish a connection
    to a server or service instance.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConnectTimeout`：这是等待与服务器或服务实例建立连接的最大时间。'
- en: '`ReadTimeout`: This is the maximum time waiting for a response from the server
    after establishing a connection.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReadTimeout`：这是在建立连接后等待服务器响应的最大时间。'
- en: 'Let''s assume that we have two instances of a target service. The connection
    to the first has been established, but it responds too slowly and a timeout occurs.
    The client performs one retry to that instance in accordance with the `MaxAutoRetries=1` property.
    If it has still not been successful, it tries to connect with a second available
    instance of that service. This action is repeated twice in the case of a failure,
    according to what has been set in the `MaxAutoRetriesNextServer=2` property. If
    the described mechanism is ultimately *not successful*, the timeout is returned
    to the external client. In that case, it may happen even after more than four
    seconds. Take a look at the following configuration:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个目标服务的两个实例。第一个实例的连接已经建立，但它响应太慢并且发生了超时。根据`MaxAutoRetries=1`属性，客户端对该实例进行一次重试。如果仍然不成功，它尝试连接该服务的第二个可用实例。在失败的情况下，这一动作根据`MaxAutoRetriesNextServer=2`属性重复两次。如果描述的机制最终*不成功*，超时将被返回到外部客户端。在这种情况下，即使在四秒以上之后也可能会发生。请查看以下配置：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This solution is a standard retry mechanism implemented for a microservices-based
    environment. We may also look at some other scenarios related to the different
    configuration settings of Ribbon's timeouts and retries. There is no reason why
    we shouldn't use that mechanism together with Hystrix's circuit breaker. However,
    we have to remember that `ribbon.ReadTimeout` should be lower than the value of
    Hystrix's `execution.isolation.thread.timeoutInMilliseconds` property.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案是为微服务环境实现的标准重试机制。我们还可以看看与Ribbon的超时和重试不同配置设置相关的其他场景。我们没有理由不使用这个机制与Hystrix的断路器一起。然而，我们必须记住`ribbon.ReadTimeout`应该小于Hystrix的`execution.isolation.thread.timeoutInMilliseconds`属性的值。
- en: I suggest that you test the configuration settings that we just described as
    an exercise. You may use a previously introduced Hoverfly JUnit rule for simulating
    the delays and stubs of a service's instances.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您测试我们刚才描述的配置设置作为一个练习。您可以使用之前介绍的Hoverfly JUnit规则来模拟服务实例的延迟和存根。
- en: Hystrix's support for Feign
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hystrix对Feign的支持
- en: To begin with, I would like to reiterate that Hystrix is enabled by default
    for the application when using a Feign library, but only for the older versions
    of Spring Cloud. According to the documentation for the newest version of Spring
    Cloud, we should set the `feign.hystrix.enabled` property to `true`, which forces
    Feign to wrap all methods with a circuit breaker.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我想重申一下，当使用Feign库时，Hystrix默认对应用程序是启用的，但只适用于Spring Cloud的旧版本。根据最新版本Spring Cloud的文档，我们应该将`feign.hystrix.enabled`属性设置为`true`，这强制Feign包装所有方法为一个断路器。
- en: Prior to the Spring Cloud Dalston release, if Hystrix was on the classpath,
    Feign would have wrapped all methods in a circuit breaker by default. This default
    behavior was changed in Spring Cloud Dalston in favor of an opt-in approach.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spring Cloud Dalston版本之前，如果Hystrix在类路径上，Feign会默认包装所有方法为一个断路器。这一默认行为在Spring
    Cloud Dalston版本中为了采用可选参与方式而改变。
- en: 'When using Hystrix together with a Feign client, the simplest way to provide
    configuration properties previously set with `@HystrixProperty` inside `@HystrixCommand`
    is through the `application.yml` file. Here''s the equivalent configuration of
    the samples presented before:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Hystrix和Feign客户端一起时，提供之前用`@HystrixProperty`在`@HystrixCommand`内部设置的配置属性的最简单方法是通过`application.yml`文件。以下是之前示例的等效配置：
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Feign supports the notation of a fallback. To enable fallbacks for a given
    `@FeignClient`, we should set the `fallback` attribute with the class name that
    provides a fallback implementation. The implementation class should be defined
    as a Spring Bean:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Feign支持回退的表示。要为给定的`@FeignClient`启用回退，我们应该将`fallback`属性设置为提供回退实现的类名。实现类应该被定义为一个Spring
    Bean：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fallback implementation is based on a cache, and implements the interface annotated
    with `@FeignClient`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 回退实现基于缓存，并实现了带有`@FeignClient`注解的接口：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Optionally, we may implement a `FallbackFactory` class. That approach has one
    big advantage, it gives you access to the cause that made the fallback trigger. To
    declare a `FallbackFactory` class for Feign, just use the `fallbackFactory` attribute
    inside `@FeignClient`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性地，我们可以实现一个`FallbackFactory`类。这种方法有一个很大的优点，它让你能够访问触发回退的原因。要为Feign声明一个`FallbackFactory`类，只需在`@FeignClient`内部使用`fallbackFactory`属性：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The custom `FallbackFactory` class needs to implement a `FallbackFactory` interface, which
    declares the one `T create(Throwable cause)` method that has to be overridden:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义的`FallbackFactory`类需要实现一个`FallbackFactory`接口，该接口声明了一个必须重写的`T create(Throwable
    cause)`方法：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You may not be aware of the configuration settings or tools described in this
    chapter if you have already been using auto-configured clients for inter-service
    communication. However, I think that it is worth having some knowledge about a
    few of the advanced mechanisms, even if they can run in the background and/or
    out of the box. In this chapter, I have tried to give you a closer view on topics,
    such as load balancers, retries, fallbacks, or circuit breakers by demonstrating
    how they work using simple examples. After reading this chapter, you should be
    able to customize Ribbon, Hystrix, or Feign clients to suit your needs related
    to communication between microservices, both on a small and large scale. You should
    also understand the when and why of using them in your system. With this chapter,
    we are closing the discussion about the core elements inside microservices-based
    architecture. Now, we have got one more important component to look at that is
    outside the system by quite a bit, the gateway. This hides the system complexity
    from an external client.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经使用自动配置的客户端进行服务间通信，你可能不知道本章中描述的配置设置或工具。然而，我认为即使它们可以在后台运行，甚至可以开箱即用，了解一些高级机制也是值得的。在本章中，我试图通过演示它们如何使用简单示例来让你更接近主题，如负载均衡器、重试、回退或断路器。阅读本章后，你应该能够根据需要在微服务之间的通信中自定义Ribbon、Hystrix或Feign客户端。你也应该理解在系统中使用它们的何时何地。通过本章，我们结束了关于微服务架构内部核心元素的讨论。现在，我们需要关注的是系统外部的一个重要组件，即网关。它将系统复杂性隐藏在外部客户端之外。
