- en: Fetching and Persisting Bitcoin Market Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取并持久化比特币市场数据
- en: In this chapter, we will develop a data pipeline to fetch, store, and, later
    on, analyze bitcoin transaction data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开发一个数据管道来获取、存储和稍后分析比特币交易数据。
- en: After an introduction to Apache Spark, we will see how to call a REST API to
    fetch transactions from a cryptocurrency exchange. A cryptocurrency exchange allows
    customers to trade digital currencies, such as bitcoin, for fiat currencies, such
    as the US dollar. The transaction data will allow us to track the price and quantity
    exchanged at a certain point in time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍Apache Spark之后，我们将看到如何调用REST API从加密货币交易所获取交易。加密货币交易所允许客户用数字货币（如比特币）兑换法定货币（如美元）。交易数据将使我们能够追踪在特定时间点的价格和数量。
- en: We will then introduce the Parquet format. This is a columnar data format that
    is widely used for big data analytics. After that, we will build a standalone
    application that will produce a history of bitcoin/USD transactions and save it
    in Parquet. In the following chapter, we will use Apache Zeppelin to query and
    analyze the data interactively.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将介绍Parquet格式。这是一种广泛用于大数据分析的列式数据格式。之后，我们将构建一个独立的应用程序，该应用程序将生成比特币/美元交易的历史记录，并将其保存为Parquet格式。在下一章中，我们将使用Apache
    Zeppelin进行交互式查询和分析数据。
- en: The volume of data that we will deal with is not very large, but the tools and
    techniques used will be the same if the data were to grow or if we were to store
    the data for more currencies or from different exchanges. The benefit of using
    Apache Spark is that it can scale horizontally and that you can just add more
    machines to your cluster to speed up your processing, without having to change
    your code.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的数据量不是很大，但如果数据量增加或我们需要存储更多货币或来自不同交易所的数据，所使用的工具和技术将是相同的。使用Apache Spark的好处是它可以水平扩展，你可以只是向你的集群添加更多机器来加速你的处理，而不需要更改你的代码。
- en: Another advantage of using Spark is that it makes it easy to manipulate table-like
    data structures and to load and save them from/to different formats. This advantage
    remains even when the volume of data is small.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark的另一个优点是它使得操作类似表格的数据结构以及从/到不同格式的加载和保存变得容易。即使数据量很小，这个优点仍然存在。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Apache Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Calling the REST API of a cryptocurrency exchange
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用加密货币交易所的REST API
- en: Parquet format and partitioning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet格式和分区
- en: 'After we are done with this chapter, we have learned a few things such as the
    following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章学习后，我们将了解以下内容：
- en: How to store large volumes of data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何存储大量数据
- en: How to use the Spark Dataset API
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Spark Dataset API
- en: How to use the IO `Monad` to control side effects
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用IO `Monad`控制副作用
- en: Setting up the project
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置项目
- en: Create a new SBT project. In IntelliJ, go to **File** | **New** | **Project**
    | **Scala** | **sbt**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的SBT项目。在IntelliJ中，转到**文件** | **新建** | **项目** | **Scala** | **sbt**。
- en: 'Then edit `build.sbt` and paste the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后编辑`build.sbt`并粘贴以下内容：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We use Scala 2.11 because, at the time of writing, Spark does not provide its
    libraries for Scala 2.12\. We are going to use the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Scala 2.11，因为在写作的时候，Spark没有为Scala 2.12提供其库。我们将使用以下内容：
- en: '`spark-core` and `spark-sql` for reading the transactions and saving them to
    Parquet. The `Provided` configuration will make SBT exclude these libraries when
    we package the application in an assembly JAR file.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark-core`和`spark-sql`用于读取交易并将它们保存到Parquet。`Provided`配置将使SBT在将应用程序打包到assembly
    JAR文件时排除这些库。'
- en: ScalaTest for testing our code.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ScalaTest用于测试我们的代码。
- en: '`scala-logging`, a convenient and fast logging library that wraps SLF4J.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scala-logging`，这是一个方便且快速的日志库，它封装了SLF4J。'
- en: '`cats-core` and `cats-effects` for managing our side effects with the IO `Monad`.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`cats-core`和`cats-effects`来管理我们的IO `Monad`副作用。
- en: '`spark-streaming`, `spark-sql-kafka`, and `pusher` for the next chapter.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为下一章准备`spark-streaming`、`spark-sql-kafka`和`pusher`。
- en: The `-Ypartial-unification` compiler option is required by `cats`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`cats`需要`-Ypartial-unification`编译器选项。'
- en: The last line makes SBT write the classes to the `/tmp` folder, in order to
    avoid a *file name too long* bug with the Linux encrypted home folders. You might
    not need it on your platform.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行让SBT将类写入`/tmp`文件夹，以避免Linux加密主目录中的*文件名过长*错误。你可能不需要在你的平台上使用它。
- en: If you do not wish to retype the code examples, you can check out the complete
    project code from GitHub at [https://github.com/PacktPublishing/Scala-Programming-Projects](https://github.com/PacktPublishing/Scala-Programming-Projects).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想重新输入代码示例，你可以从 GitHub（[https://github.com/PacktPublishing/Scala-Programming-Projects](https://github.com/PacktPublishing/Scala-Programming-Projects)）检查完整的项目代码。
- en: Understanding Apache Spark
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Apache Spark
- en: Spark is an open source framework built to perform analytics on large datasets.
    Unlike other tools such as R, Python, and MathLab that are using in-memory processing,
    Spark gives you the possibility to scale out. And thanks to its expressiveness
    and interactivity, it also improves developer productivity.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个开源框架，用于对大数据集进行数据分析。与其他使用内存处理的工具（如 R、Python 和 MathLab）不同，Spark 给你提供了扩展的可能性。而且，得益于其表达性和交互性，它还提高了开发者的生产力。
- en: There are entire books dedicated to Spark. It has a vast number of components
    and lots of areas to explore. In this book, we aim to get you started with the
    fundamentals. You should then be more comfortable exploring the documentation
    if you want to.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有整本书是专门介绍 Spark 的。它有大量的组件和许多需要探索的领域。在这本书中，我们旨在帮助你从基础知识开始。如果你想要探索文档，你应该会感到更加自在。
- en: The purpose of Spark is to perform analytics on a collection. This collection
    could be in-memory and you could run your analytics using multiple threads, but
    if your collection is becoming too large, you are going to reach the memory limit
    of your system.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的目的是对集合进行数据分析。这个集合可以是内存中的，你可以使用多线程来运行你的分析，但如果你的集合变得太大，你将接近系统的内存限制。
- en: Spark solved this issue by creating an object to hold all of this data. Instead
    of keeping everything in the local computer's memory, Spark chunks the data into
    multiple collections and distributes it on multiple computers. This object is
    called an **RDD** (short for **Resilient Distributed Dataset**). RDD keeps references
    to all of the distributed chunks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 通过创建一个对象来解决这个问题，该对象用于存储所有这些数据。Spark 不是将所有内容都保存在本地计算机的内存中，而是将数据分成多个集合，并在多台计算机上分布。这个对象被称为
    **RDD**（即 **Resilient Distributed Dataset**）。RDD 保留了对所有分布式块的所有权。
- en: RDD, DataFrame, and Dataset
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD、DataFrame 和 Dataset
- en: 'The core concept of Spark is the RDD. From the user''s point of view, for a
    given type `A`, `RDD[A]` looks similar to a standard Scala collection, such as
    `Vector[A]`: they are both **immutable** and share many well-known methods, such
    as `map`, `reduce`, `filter`, and `flatMap`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的核心概念是 RDD。从用户的角度来看，对于给定的类型 `A`，`RDD[A]` 看起来与标准的 Scala 集合，如 `Vector[A]`
    类似：它们都是 **不可变** 的，并且共享许多知名的方法，如 `map`、`reduce`、`filter` 和 `flatMap`。
- en: 'However, the RDD has some unique characteristics. They are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，RDD 有一些独特的特性。它们如下：
- en: '**Lazy**: When you call a **transformation** function, such as `map` or `filter`,
    nothing happens immediately. The function call is just added to a computation
    graph that is stored in the RDD class. This computation graph is executed when
    you subsequently call an **action** function, such as `collect` or `take`.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟加载**：当你调用一个 **转换** 函数，如 `map` 或 `filter` 时，并不会立即发生任何事情。函数调用只是被添加到一个存储在
    RDD 类中的计算图中。这个计算图是在你随后调用一个 **行动** 函数，如 `collect` 或 `take` 时执行的。'
- en: '**Distributed**: The data in the RDD is split in several partitions that are
    scattered across different **executors** in a cluster. A **task** represents a
    chunk of data and the transformation that must be applied to it.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式**：RDD 中的数据被分割成几个分区，这些分区散布在集群中不同的 **executors** 上。一个 **task** 代表了一块数据以及必须对其应用的操作。'
- en: '**Resilient**: If one of the executors dies when you execute a job, Spark automatically
    resends the tasks that were lost to another executor.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性**：如果你在执行作业时，一个 executor 死亡，Spark 会自动将丢失的任务重新发送到另一个 executor。'
- en: 'Given two types, `A` and `B`, `RDD[A]` and `RDD[B]` can be joined together
    to obtain `RDD[(A, B)]`. For instance, consider `case class Household(id: Int,
    address: String)` and  `case class ElectrictyConsumption(houseHoldId: Int, kwh:
    Double)`. If you want to count the number of households consuming more than 2
    kWh, you could do either of the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '给定两种类型，`A` 和 `B`，`RDD[A]` 和 `RDD[B]` 可以连接在一起以获得 `RDD[(A, B)]`。例如，考虑 `case class
    Household(id: Int, address: String)` 和 `case class ElectricityConsumption(houseHoldId:
    Int, kwh: Double)`。如果你想计算消耗超过 2 kWh 的家庭数量，你可以执行以下任何一个操作：'
- en: Join `RDD[HouseHold]` with `RDD[ElectricityConsumption]` and then apply `filter`
    on the result
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `RDD[HouseHold]` 与 `RDD[ElectricityConsumption]` 连接，然后对结果应用 `filter`。
- en: Apply `filter` to `RDD[ElectricityConsumption]` first, and then join it with
    `RDD[HouseHold]`
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先对`RDD[ElectricityConsumption]`应用`filter`，然后将其与`RDD[HouseHold]`连接
- en: The result will be the same but the performances will be different; the second
    algorithm will be faster. Wouldn't it be nice if Spark could perform this kind
    of optimization for us?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将相同，但性能将不同；第二个算法将更快。如果Spark能为我们执行这种优化就好了？
- en: Spark SQL
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL
- en: The answer is yes and the module is called Spark SQL. Spark SQL sits on top
    of Spark Core and allows the manipulation of structured data. Unlike with the
    basic RDD API, the `DataFrame` API provides more information to the Spark engine.
    Using this information, it can change the execution plan and optimize it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的，该模块称为Spark SQL。Spark SQL位于Spark Core之上，允许操作结构化数据。与基本的RDD API不同，`DataFrame`
    API为Spark引擎提供了更多信息。使用这些信息，它可以更改执行计划并优化它。
- en: You can also use the module to execute SQL queries as you would with a relational
    database. It makes it easy for people comfortable with SQL to run queries on heterogeneous
    sources of data. You can, for instance, join a data table coming from a CSV file
    with another one stored in Parquet in a Hadoop filesystem and with yet another
    one coming from a relational database.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用该模块执行SQL查询，就像使用关系数据库一样。这使得熟悉SQL的人能够轻松地在异构数据源上运行查询。例如，你可以将来自CSV文件的数据表与存储在Hadoop文件系统中的另一个Parquet文件中的数据表以及来自关系数据库的另一个数据表进行连接。
- en: Dataframe
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dataframe
- en: 'Spark SQL is composed of three main APIs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL由三个主要API组成：
- en: The SQL literal syntax
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL字面量语法
- en: The `DataFrame` API
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: The `DataFrame` API
- en: '`DataSet`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataSet`'
- en: '`DataFrame` is conceptually the same as a table in the relational database.
    The data is distributed in the same way as in an RDD. `DataFrame` has a schema
    but is untyped. You can create `DataFrame` from an RDD or manually build it. Once
    created, `DataFrame` will contain a schema that maintains the name and type for
    each column (field).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrame`在概念上与关系数据库中的表相同。数据分布的方式与RDD相同。`DataFrame`有一个模式，但未指定类型。你可以从RDD创建`DataFrame`或手动构建它。一旦创建，`DataFrame`将包含一个模式，该模式维护每个列（字段）的名称和类型。'
- en: 'If you then want to use `DataFrame` in an SQL query, all you need to do is
    create a named view (equivalent to the table name in the relational database)
    using the `Dataframe.createTempView(viewName: String)` method. In the SQL query,
    the fields that are available in the `SELECT` statement will come from the schema
    of `DataFrame` and the name of the table used in the `FROM` statement will come
    from `viewName`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你然后想在SQL查询中使用`DataFrame`，你所需要做的就是使用`Dataframe.createTempView(viewName: String)`方法创建一个命名视图（相当于关系数据库中的表名）。在SQL查询中，`SELECT`语句中可用的字段将来自`DataFrame`的模式，而`FROM`语句中使用的表名将来自`viewName`。'
- en: Dataset
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dataset
- en: As Scala developers, we are used to working with types and with a friendly compiler
    that infers types and tells us our mistakes. The problem with `DataFrame` API
    and Spark SQL is that you can write a query such as `Select lastname From people`,
    but in your `DataFrame`, you might not have a `lastname` column, but `surname`
    one. In this case, you are only going to discover that mistake at runtime with
    a nasty exception!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Scala开发者，我们习惯于与类型和友好的编译器一起工作，编译器可以推断类型并告诉我们错误。`DataFrame` API和Spark SQL的问题在于，你可以编写一个查询，如`Select
    lastname From people`，但在你的`DataFrame`中，你可能没有`lastname`列，而是有`surname`列。在这种情况下，你只能在运行时通过一个讨厌的异常来发现这个错误！
- en: Wouldn't it be nice to have a compilation error instead?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 难道不好有一个编译错误吗？
- en: This is why Spark introduced `Dataset` in version 1.6\. `Dataset` attempts to
    unify the RDD and the `DataFrame` APIs. `Dataset` has a type parameter, and you
    can use anonymous functions to manipulate the data as you would with an RDD or
    a vector.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么Spark在1.6版本中引入了`Dataset`。`Dataset`试图统一RDD和`DataFrame` API。`Dataset`有一个类型参数，你可以使用匿名函数来操作数据，就像使用RDD或向量一样。
- en: Actually, `DataFrame` is, in fact, a type alias for `DataSet[Row]`. This means
    you can seamlessly mix the two APIs and use in the same query a filter using a
    Lambda expression followed by another filter using a `DataFrame` operator.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`DataFrame`实际上是一个`DataSet[Row]`的类型别名。这意味着你可以无缝地混合这两个API，并在同一个查询中使用Lambda表达式后的过滤器，然后使用`DataFrame`操作符的另一个过滤器。
- en: In the next sections, we are only going to use `Dataset`, which is a good compromise
    between code quality and performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们只将使用`Dataset`，这是代码质量和性能之间一个好的折衷方案。
- en: Exploring the Spark API with the Scala console
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala控制台探索Spark API
- en: 'If you are not already familiar with Spark, it can be a bit intimidating to
    write Spark jobs straight away. To make it easier, we are first going to explore
    the API using a Scala console. Start a new Scala console (*Ctrl* + *Shift* + *D*
    in IntelliJ), and type the following code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不熟悉Spark，直接编写Spark作业可能会有些令人畏惧。为了使其更容易，我们首先将使用Scala控制台来探索API。在IntelliJ中启动一个新的Scala控制台（*Ctrl*
    + *Shift* + *D*），并输入以下代码：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will initialize a new Spark session and bring some handy implicit in scope.
    The master `"local[*]"` URL means that we will use all of the cores available
    on the localhost when running jobs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将初始化一个新的Spark会话，并引入一些有用的隐式函数。主`"local[*]"`URL意味着在运行作业时，我们将使用本地主机上可用的所有核心。
- en: 'The Spark session is available to accept new jobs. Let''s use it to create
    `Dataset` containing a single string:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Spark会话可用，可以接受新的作业。让我们用它来创建包含单个字符串的`Dataset`：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The implicit that we imported earlier let us use the `.toDS()` function on `Seq`
    to produce `Dataset`. We can observe that the `.toString` method that was called
    by the Scala console was output by the schema of `Dataset`—it has a single column
    `value` of the `string` type.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前导入的隐式函数允许我们在`Seq`上使用`.toDS()`函数来生成`Dataset`。我们可以观察到，由Scala控制台调用的`.toString`方法是由`Dataset`的模式输出的——它有一个单列`value`，其类型为`string`。
- en: 'However, we could not see the content of `Dataset`. This is because `Dataset`
    is a lazy data structure; it just stores a computation graph, which is not evaluated
    until we call one of the action methods. Still, for debugging, it is very handy
    to be able to evaluate `Dataset` and print its content. For that we need to call
    `show`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们看不到`Dataset`的内容。这是因为`Dataset`是一个懒数据结构；它只存储一个计算图，直到我们调用其中一个操作方法才会进行评估。尽管如此，对于调试来说，能够评估`Dataset`并打印其内容是非常方便的。为此，我们需要调用`show`：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should see the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`show()` is an **action**; it will submit a job to the Spark cluster, collect
    the results in the driver, and print them. By default, `show` limits the number
    of rows to 20 and truncates the columns. You can call it with extra parameters
    if you want more information.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`show()`是一个**操作**；它将提交一个作业到Spark集群，在驱动程序中收集结果并打印它们。默认情况下，`show`限制行数为20，并截断列。如果你想获取更多信息，可以带额外的参数调用它。'
- en: Now we would like to convert each string to `Int`, in order to obtain `Dataset[Int]`.
    We have two ways of doing that.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想将每个字符串转换为`Int`，以便获得`Dataset[Int]`。我们有两种方法可以实现这一点。
- en: Transforming rows using map
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用map转换行
- en: 'Type the following in the Scala console:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala控制台中输入以下内容：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should see something similar to this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似以下的内容：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `explain()` method shows the execution plan that will be run if we call
    an action method, such as `show()` or `collect()`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`explain()`方法显示了如果调用操作方法（如`show()`或`collect()`）将运行的执行计划。'
- en: From this plan, we can deduce that calling `map` is not very efficient. Indeed,
    Spark stores the rows of `Dataset` off-heap in binary format. Whenever you call
    `map`, it has to deserialize this format, apply your function, and serialize the
    result in binary format.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个计划中，我们可以推断出调用`map`不是非常高效。实际上，Spark将`Dataset`的行以二进制格式存储在off-heap中。每次调用`map`时，它都必须反序列化此格式，应用你的函数，并将结果以二进制格式序列化。
- en: Transforming rows using select
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用选择转换行
- en: 'A more efficient way to transform rows is to use `select(cols: Column*)`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '转换行的更高效方法是使用`select(cols: Column*)`：'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The implicits that we imported earlier let us use the `$"columnName"` notation
    to produce a `Column` object from `String`. The string after the `$` sign must
    refer to a column that exists in the `DataFrame` source; otherwise, you would
    get an exception.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前导入的隐式函数允许我们使用`$"columnName"`记法从`String`生成一个`Column`对象。`$`符号后面的字符串必须引用`DataFrame`源中存在的列；否则，你会得到一个异常。
- en: We then call the `.cast` method to transform each `String` into `Int`. But,
    at this stage, the resulting `df` object is not of the `Dataset[Int];` type; it
    is `DataFrame`. `DataFrame` is actually a type alias for `Dataset[Row]`, and `Row`
    is akin to a list of key-value pairs. `DataFrame` is a representation of the distributed
    data in an untyped way. The compiler does not know the type or names of each column;
    they are only known at runtime.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用`.cast`方法将每个`String`转换为`Int`。但是，在这个阶段，生成的`df`对象不是`Dataset[Int];`类型；它是`DataFrame`。`DataFrame`实际上是`Dataset[Row]`的类型别名，`Row`类似于键值对列表。`DataFrame`以无类型方式表示分布式数据。编译器不知道每列的类型或名称；它们只在运行时才知道。
- en: In order to obtain `Dataset[Int]`, we need to cast the type of the elements
    using `.as[Int]`. This would fail at runtime if the elements of `DataFrame` cannot
    be cast to the target type.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得`Dataset[Int]`，我们需要使用`.as[Int]`将元素的类型进行转换。如果`DataFrame`的元素无法转换为目标类型，这将导致运行时失败。
- en: Force a specific type for the elements of your `Dataset`; this will make your
    programs safer. You should only expose `DataFrame` in a function if it genuinely
    does not know what the types of the columns will be at runtime; for instance,
    if you are reading or writing arbitrary files.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 强制为你的`Dataset`元素指定特定类型；这将使你的程序更安全。你应该只在函数真正不知道运行时列的类型时暴露`DataFrame`；例如，如果你正在读取或写入任意文件。
- en: 'Let''s see what our `explain` plan looks like now:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看现在我们的`explain`计划是什么样的：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should see this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到这个：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This time we can see that there is no extra serialization/deserialization step.
    The evaluation of this `Dataset` will be faster than when we used `map`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们可以看到没有额外的序列化/反序列化步骤。这个`Dataset`的评估将比我们使用`map`时更快。
- en: 'Exercise: Filter the elements of `Dataset[Int]` to keep only the elements that
    are greater than 2\. First use `filter(func: Int => Boolean)`, and then use `filter(condition:
    Column)`. Compare the execution plans for both implementations.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '练习：过滤`Dataset[Int]`的元素，只保留大于2的元素。首先使用`filter(func: Int => Boolean)`，然后使用`filter(condition:
    Column)`。比较两种实现的执行计划。'
- en: The conclusion of this is that you should prefer functions that use `Column`
    arguments whenever possible. They can fail at runtime, as opposed to the type-safe
    alternatives because they can refer to column names that do not exist in your
    `Dataset`. However, they are more efficient.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结论是，你应该尽可能优先选择使用`Column`参数的函数。它们可能在运行时失败，与类型安全的替代方案相比，因为它们可能引用你`Dataset`中不存在的列名。然而，它们更高效。
- en: Fortunately, there is an open source library called **Frameless** that can let
    you use these efficient methods in a type-safe way. If you are writing large programs
    that use `Dataset`, I recommend that you check it out here: [https://github.com/typelevel/frameless.](https://github.com/typelevel/frameless)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个名为**Frameless**的开源库，它可以让你以类型安全的方式使用这些高效的方法。如果你正在编写使用`Dataset`的大型程序，我建议你在这里查看：[https://github.com/typelevel/frameless.](https://github.com/typelevel/frameless)
- en: Execution model
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行模型
- en: 'The methods available in `Dataset` and the RDD are of two kinds:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`和RDD中可用的方法有两种：'
- en: '**Transformations**: They return a new `Dataset` API that will apply the transformation
    later when an action method is called. For instance, `map`, `filter`, `join`,
    and `flatMap`.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：它们返回一个新的`Dataset` API，该API将在调用动作方法时应用转换。例如，`map`、`filter`、`join`和`flatMap`。'
- en: '**Actions**: They trigger the execution of a Spark job, for instance, `collect`,
    `take`, and `count`.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：它们触发Spark作业的执行，例如`collect`、`take`和`count`。'
- en: When a **job** is triggered by an action method, it is divided into several
    **stages**. A stage is a part of a job that can be run without having to **shuffle**
    the data across different nodes of the cluster. It can encompass several transformations,
    such as `map` and `filter`. But as soon as one transformation, such as `join`,
    requires the data be moved (shuffling), another stage must be introduced.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个**作业**由动作方法触发时，它被分成几个**阶段**。阶段是作业的一部分，可以在不跨集群的不同节点**洗牌**数据的情况下运行。它可以包括几个转换，如`map`和`filter`。但是，一旦一个转换，如`join`，需要移动数据（洗牌），就必须引入另一个阶段。
- en: The data contained in `Dataset` is split into several **partitions**. The combination
    of a stage (code to execute) with a partition (data used by the stage) is a **task**.
    The ideal parallelism would be achieved when you have *nb of tasks* = *nb of cores*
    in the cluster. When you need to optimize a job, it can be beneficial to repartition
    your data to better match the number of cores at your disposal.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataset`中的数据被分成几个**分区**。一个阶段（要执行的代码）与一个分区（阶段使用的数据）的组合是一个**任务**。当你在集群中拥有*nb
    of tasks* = *nb of cores*时，可以实现理想的并行性。当你需要优化一个作业时，重新分区你的数据以更好地匹配你拥有的核心数量可能会有所帮助。'
- en: 'When Spark starts executing a job, the **Driver** program distributes the tasks
    to all the executors of the cluster:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当Spark开始执行一个作业时，**Driver**程序将任务分配给集群中的所有执行器：
- en: '![](img/a6d611c2-4d81-4318-938e-ada40bc6e02f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6d611c2-4d81-4318-938e-ada40bc6e02f.png)'
- en: 'The preceding diagram is described as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示描述如下：
- en: The driver is on the same JVM as the code that called the action method
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序与调用动作方法的代码位于同一个JVM上
- en: An executor runs on its own JVM on a remote node of the cluster. It can use
    many cores to execute several tasks in parallel
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个executor在集群的远程节点上的自己的JVM上运行。它可以使用多个核心并行执行多个任务。
- en: The Spark **master** coordinates several **worker** nodes in a cluster. When
    you start a Spark application, you have to specify the URL of the master. It will
    then ask its worker to spawn executor processes that will be dedicated to the
    jobs of your application. At a given point in time, a worker can manage several
    executors running completely different applications.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Spark **master**在集群中协调多个**worker**节点。当你启动一个Spark应用程序时，你必须指定master的URL。然后它会要求其worker生成专门用于应用程序任务的executor进程。在某个特定时间点，一个worker可以管理多个运行完全不同应用程序的executor。
- en: When you want to quickly run or test an application without using a cluster,
    you can use a **local** master. In this special mode, only one JVM is used for
    the master, driver, and executor.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要在不使用集群的情况下快速运行或测试应用程序时，您可以使用**本地**master。在这种特殊模式下，master、driver和executor只使用一个JVM。
- en: Implementing the transaction batch producer
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现交易批量生产者
- en: In this section, we will first discuss how to call a REST API to fetch BTC/USD
    transactions. Then we will see how to use Spark to deserialize the JSON payload
    into a well-typed distributed `Dataset`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论如何调用REST API以获取BTC/USD交易。然后我们将看到如何使用Spark将JSON有效负载反序列化为类型良好的分布式`Dataset`。
- en: After that, we will introduce the parquet format and see how Spark makes it
    easy to save our transactions in this format.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将介绍parquet格式，并看看Spark如何使将我们的交易保存为此格式变得容易。
- en: With all of these building blocks, we will then implement our program in a purely
    functional way using the **Test-Driven-Development** (**TDD**) technique.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有这些构建块，我们将以纯函数式方式使用**测试驱动开发**（**TDD**）技术来实现我们的程序。
- en: Calling the Bitstamp REST API
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调用Bitstamp REST API
- en: Bitstamp is a cryptocurrency exchange that people use to trade a cryptocurrency,
    such as bitcoin, for a conventional currency, such as US dollar or euro. One of
    the good things about Bitstamp is that it provides a REST API, which can be used
    to get information about the latest trades, and can also be used to send orders
    if you have an account.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Bitstamp是一个加密货币交易所，人们用它来用加密货币，如比特币，交换传统货币，如美元或欧元。Bitstamp的一个好处是它提供了一个REST API，可以用来获取有关最新交易的信息，如果您有账户，还可以用来发送订单。
- en: You can find out more here: [https://www.bitstamp.net/api/.](https://www.bitstamp.net/api/)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处了解更多信息：[https://www.bitstamp.net/api/](https://www.bitstamp.net/api/)
- en: For this project, the only endpoint we are interested in is the one that gets
    the latest transactions that happened on the exchange. It will give us an indication
    of the price and of the quantity of currency exchanged in a given period of time.
    This endpoint can be called with the following URL: [https://www.bitstamp.net/api/v2/transactions/btcusd/?time=hour.](https://www.bitstamp.net/api/v2/transactions/btcusd/?time=hour)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们唯一感兴趣的端点是获取交易所上最近发生的交易的最新交易。这将给我们一个关于在给定时间段内货币交换的价格和数量的指示。此端点可以通过以下URL调用：[https://www.bitstamp.net/api/v2/transactions/btcusd/?time=hour](https://www.bitstamp.net/api/v2/transactions/btcusd/?time=hour)
- en: 'If you paste this URL in your favorite browser, you should see a JSON array
    containing all of the BTC (Bitcoin)/USD (US dollar) transactions that happened
    during the last hour. It should look similar to this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将此URL粘贴到您喜欢的浏览器中，您应该会看到一个包含在过去一小时发生的所有BTC（比特币）/USD（美元）交易的JSON数组。它看起来应该类似于以下内容：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the previous result, if we inspect the first transaction, we can see that
    0.05 bitcoins were sold (`"type": "1"` means sell) at a price of 6488.27 USD for
    1 BTC.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的结果中，如果我们检查第一笔交易，我们可以看到以6488.27美元的价格出售了0.05个比特币（`"type": "1"`表示出售）。'
- en: 'There are many Java and Scala libraries to call a REST endpoint, but to keep
    things simple, we are just going to use the Scala and Java SDK to call the endpoint.
    Start a new Scala console, and run this code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多Java和Scala库可以调用REST端点，但为了保持简单，我们只是将使用Scala和Java SDK来调用端点。启动一个新的Scala控制台，并运行以下代码：
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With the help of the `scala.io.Source` class, we can get the HTTP response in
    a string. This is the first building block of our program. The next thing we need
    to do is parse the JSON objects into a collection of Scala objects to make them
    easier to manipulate.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scala.io.Source`类的帮助下，我们可以以字符串形式获取HTTP响应。这是我们程序的第一个构建块。接下来我们需要做的是将JSON对象解析为Scala对象的集合，以便更容易地操作。
- en: As we read the whole HTTP response in a string in one go, we need enough memory
    on the driver process to keep that string in the heap. You might think that it
    would be better to read it with `InputStream`, but unfortunately, it is not possible
    with Spark Core to split a stream of data. You would have to use Spark Streaming
    to do this.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们一次性将整个 HTTP 响应读入一个字符串，所以在驱动程序进程中需要有足够的内存来在堆中保持这个字符串。你可能认为使用 `InputStream`
    来读取会更好，但不幸的是，使用 Spark Core 无法分割数据流。你必须使用 Spark Streaming 来完成这个操作。
- en: Parsing the JSON response
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析 JSON 响应
- en: We have observed that when we call the Bitstamp endpoint, we get a string containing
    a JSON array, and each element of the array is a JSON object that represents a
    transaction. But it would be nicer to have the information in a Spark `Dataset`.
    This way, we will be able to use all of the powerful Spark functions to store,
    filter, or aggregate the data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，当我们调用 Bitstamp 端点时，我们得到一个包含 JSON 数组的字符串，数组的每个元素都是一个表示交易的 JSON 对象。但将信息放在
    Spark `Dataset` 中会更好。这样，我们将能够使用所有强大的 Spark 函数来存储、过滤或聚合数据。
- en: Unit testing jsonToHttpTransaction
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试 jsonToHttpTransaction
- en: 'First, we can start by defining a case class that represents the same data
    as in our JSON payload. Create a new package, `coinyser`, and then a class, `coinyser.HttpTransaction`,
    in `src/main/scala`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以从定义一个与我们的 JSON 有效负载中相同数据的 case class 开始。创建一个新的包，`coinyser`，然后在 `src/main/scala`
    中创建一个类，`coinyser.HttpTransaction`：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In Scala, if you want to use a variable name already defined as a Scala keyword,
    you can enclose the variable with backticks such as the `type` variable name in
    this example: `` `type`: String ``.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Scala 中，如果你想使用已经定义为 Scala 关键字的变量名，你可以用反引号包围变量，例如这个例子中的 `type` 变量名：`` `type`:
    String ``。'
- en: 'This class has the same attribute names with the same types (all string) as
    the JSON objects. The first step is to implement a function that transforms a
    JSON string into `Dataset[HttpTransaction]`. For this purpose, let''s create a
    new test class, `coinyser.BatchProducerSpec`, in `src/test/scala`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类具有与 JSON 对象相同的属性名称和相同的类型（所有都是字符串）。第一步是实现一个函数，将 JSON 字符串转换为 `Dataset[HttpTransaction]`。为此，让我们在
    `src/test/scala` 中创建一个新的测试类，`coinyser.BatchProducerSpec`：
- en: '[PRE13]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our test extends `SharedSparkSession`. This trait provides an implicit `SparkSession`
    that can be shared across several tests.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试扩展了 `SharedSparkSession`。这个特质提供了一个隐式的 `SparkSession`，可以在多个测试之间共享。
- en: First, we defined a string containing a JSON array with two transactions that
    we extracted from Bitstamp's endpoint. We defined two instances of `HttpTransaction`
    that we expect to have in our `Dataset` outside of the test because we will reuse
    them in another test later on.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了一个包含两个交易的 JSON 数组字符串，这两个交易是从 Bitstamp 的端点提取出来的。我们定义了两个 `HttpTransaction`
    实例，我们期望在测试之外的数据集中有这些实例，因为我们稍后将在另一个测试中重用它们。
- en: After the call to `jsonToHttpTransaction` that we are going to implement, we
    obtain `Dataset[HttpTransaction]`. However, Spark's `Dataset` is lazy—at this
    stage, nothing has been processed yet. In order to *materialize* `Dataset`, we
    need to force its evaluation by calling `collect()`. The return type of `collect()`
    here is `Array[HttpTransaction]`, and we can therefore use ScalaTest's assertion, `contain
    theSameElementsAs`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们即将实现的 `jsonToHttpTransaction` 调用之后，我们获得了 `Dataset[HttpTransaction]`。然而，Spark
    的 `Dataset` 是惰性的——在这个阶段，还没有进行任何处理。为了 *物化* `Dataset`，我们需要通过调用 `collect()` 来强制其评估。这里
    `collect()` 的返回类型是 `Array[HttpTransaction]`，因此我们可以使用 ScalaTest 的断言，`contain theSameElementsAs`。
- en: Implementing jsonToHttpTransaction
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 jsonToHttpTransaction
- en: 'Create the `coinyser.BatchProducer` class and type the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `coinyser.BatchProducer` 类并输入以下代码：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Some of the imports will be used later. Do not worry if they show up as being
    unused in IntelliJ. Let''s explain, step by step, what is happening here. I would
    encourage you to run each step in a Scala console and call `.show()` after each
    `Dataset` transformation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一些导入将在以后使用。如果它们在 IntelliJ 中显示为未使用，请不要担心。让我们一步一步地解释这里发生的事情。我鼓励你在 Scala 控制台中运行每个步骤，并在每个
    `Dataset` 转换后调用 `.show()`：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As specified in the unit test, the signature of our function takes `String`
    containing a JSON array of transactions and returns `Dataset[HttpTransaction].`
    As we need to produce `Dataset`, we also need to pass a `SparkSession` object.
    It is a good practice to pass it as an implicit parameter, as there is only one
    instance of this class in a typical application:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如单元测试中指定，我们函数的签名接受包含交易JSON数组的`String`，并返回`Dataset[HttpTransaction]`。由于我们需要生成`Dataset`，我们还需要传递一个`SparkSession`对象。在典型应用中，这个类只有一个实例，因此将其作为隐式参数传递是一个好习惯：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first step is to produce `Dataset[String]` from our JSON string. This `Dataset`
    will have a single row containing the whole JSON array of transactions. For this,
    we use the `.toDS()` method that was made available when we called `import spark.implicits:`
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从我们的JSON字符串生成`Dataset[String]`。这个`Dataset`将包含一个包含整个交易JSON数组的单行。为此，我们使用了在调用`import
    spark.implicits:`时提供的`.toDS()`方法。
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We have seen in the previous section that it is more efficient to use Spark
    functions that take `Column` as an argument. In order to parse the JSON, we use
    the `from_jso` function, which is in the `org.apache.spark.sql.functions` package.
    We use this particular signature: `def from_json(e: Column, schema: StructType):
    Column`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '在前一个章节中，我们已经看到使用接受`Column`作为参数的Spark函数更有效率。为了解析JSON，我们使用了`from_jso`函数，该函数位于`org.apache.spark.sql.functions`包中。我们使用这个特定的签名：`def
    from_json(e: Column, schema: StructType): Column`：'
- en: The first parameter is the column we want to parse. We pass the `"value"` column,
    which is the default column name for our single-column `Dataset`.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数是我们想要解析的列。我们传递了`"value"`列，这是我们的单列`Dataset`的默认列名。
- en: 'The second argument is the target schema. `StructType` represents the structure
    of `Dataset`—the names, types, and order of its columns. The schema we pass to
    the function must match the names and types of the JSON string. You can create
    a schema by hand but, to make things easier, we first create `txSchema` using
    an empty `Dataset[HttpTransaction]`. `txSchema` is the schema for a single transaction,
    but as our JSON string contains an array of transactions, we must wrap `txSchema`
    in `ArrayType`:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数是目标模式。`StructType`表示`Dataset`的结构——其列的名称、类型和顺序。传递给函数的模式必须与JSON字符串的名称和类型匹配。你可以手动创建一个模式，但为了简化操作，我们首先使用一个空的`Dataset[HttpTransaction]`创建`txSchema`。`txSchema`是单个交易的模式，但由于我们的JSON字符串包含一个交易数组，我们必须将`txSchema`封装在`ArrayType`中：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If we just select `arrayColumn`, we would obtain `Dataset[Seq[HttpTransaction]]`—one
    row containing a collection. But what we want is `Dataset[HttpTransaction]`—one
    row per element of the array.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只选择`arrayColumn`，我们会得到`Dataset[Seq[HttpTransaction]]`——一个包含集合的单行。但我们的目标是`Dataset[HttpTransaction]`——数组中的每个元素一行。
- en: For this purpose, we use the `explode` function, which is similar to flatten
    for vectors. After `explode`, we obtain several rows, but at this stage, each
    row has a single `StructType` column that contains the desired columns—`date`,
    `tid`, and `price`. Our transaction data is actually wrapped in an object. In
    order to unwrap it, we first rename this `StructType` column `"v"`, and then call
    `select("v.*")`. We obtain `DataFrame` with the `date`, `tid`, and `price` columns,
    so that we can safely cast to `HttpTransaction`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们使用了`explode`函数，它类似于向量的`flatten`。在`explode`之后，我们获得了多行，但在这一阶段，每一行只有一个包含所需列（`date`、`tid`和`price`）的`StructType`列。我们的交易数据实际上被封装在一个对象中。为了解包它，我们首先将这个`StructType`列重命名为`"v"`，然后调用`select("v.*")`。我们得到了包含`date`、`tid`和`price`列的`DataFrame`，这样我们就可以安全地将它们转换为`HttpTransaction`。
- en: You can run the unit test; it should pass now.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以运行单元测试；现在它应该可以通过。
- en: Unit testing httpToDomainTransactions
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单元测试httpToDomainTransactions
- en: 'We now have all of the pieces to fetch transactions and put them in `Dataset[HttpTransaction]`.
    But it would not be wise to store these objects as they are and then run some
    analytics with them, because of the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有必要的组件来获取交易并将它们放入`Dataset[HttpTransaction]`中。但将这些对象直接存储并对其进行分析并不是一个明智的选择，原因如下：
- en: The API could change in the future, but we would want to keep the same storage
    format regardless of these changes
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API在未来可能会发生变化，但我们希望无论这些变化如何，都能保持相同的存储格式。
- en: As we will see in the next chapter, the Bitstamp WebSocket API for receiving
    live transactions uses a different format
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如我们在下一章中将要看到的，Bitstamp WebSocket API用于接收实时交易使用的是不同的格式。
- en: All of the attributes of `HttpTransaction` are of the `String` type. It would
    be easier to run analytics if the attributes were properly typed
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HttpTransaction` 的所有属性都是 `String` 类型。如果属性有适当的类型，运行分析将更容易'
- en: 'For these reasons, it would better to have a different class that represents
    a transaction. Let''s create a new class called `coinyser.Transaction`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，最好有一个代表交易的不同的类。让我们创建一个新的类，称为 `coinyser.Transaction`：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It has the same attributes as `HttpTransaction`, but with better types. We have
    to use `java.sql.Timestamp` and `java.sql.Date`, because they are the types exposed
    externally by Spark for timestamps and dates. We also added a `date` attribute
    that will contain the date of the transaction. The information is already contained
    in `timestamp`, but this denormalization will be useful later on when we want
    to filter transactions for a specific date range.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有与 `HttpTransaction` 相同的属性，但类型更好。我们必须使用 `java.sql.Timestamp` 和 `java.sql.Date`，因为它们是
    Spark 向外部公开的时间戳和日期的类型。我们还添加了一个 `date` 属性，它将包含交易的日期。信息已经包含在 `timestamp` 中，但这种反规范化将在我们想要过滤特定日期范围的交易时非常有用。
- en: 'In order to avoid having to pass the date, we can create a new `apply` method
    in the companion object:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免传递日期，我们可以在伴生对象中创建一个新的 `apply` 方法：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we can write a unit test for a new function, `httpToDomainTransactions`,
    that you need to create inside the existing `BatchProducerSpec`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以为需要创建在现有 `BatchProducerSpec` 内部的 `httpToDomainTransactions` 新函数编写单元测试：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The test is quite straightforward. We build `Dataset[HttpTransaction]`, call
    the `httpToDomainTransactions` function, and make sure that the result contains
    the expected `Transaction` objects.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 测试相当直接。我们构建 `Dataset[HttpTransaction]`，调用 `httpToDomainTransactions` 函数，并确保结果包含预期的
    `Transaction` 对象。
- en: Implementing httpToDomainTransactions
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 `httpToDomainTransactions`
- en: 'This implementation uses `select` to avoid an extra serialization/deserialization.
    Add the following function in `BatchProducer`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此实现使用 `select` 来避免额外的序列化/反序列化。在 `BatchProducer` 中添加以下函数：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We use `cast` to convert the string columns into the appropriate types. For
    converting in to `TimeStampType`, we have to first convert in to `LongType`, and
    for converting in to `DateType`, we have to first convert in to `TimestampType`.
    Since all the types match the target `Transaction` object, we can call `.as[Transaction]`
    at the end to obtain `Dataset[Transaction]`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `cast` 将字符串列转换为适当的类型。对于转换为 `TimeStampType`，我们必须首先将其转换为 `LongType`，而对于转换为
    `DateType`，我们必须首先将其转换为 `TimestampType`。由于所有类型都与目标 `Transaction` 对象匹配，我们可以在最后调用
    `.as[Transaction]` 以获得 `Dataset[Transaction]`。
- en: You can now run `BatchProducerSpec` and make sure the two tests pass.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以运行 `BatchProducerSpec` 并确保两个测试通过。
- en: Saving transactions
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存交易
- en: We now have all of the functions required to fetch the last 24 hours of transactions
    from the Bitstamp API, and produce well-typed transaction objects inside `Dataset`.
    What we need after that is to persist this data on disk. This way, once we have
    run our program for many days, we will be able to retrieve transactions that happened
    in the past.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了从 Bitstamp API 获取过去 24 小时交易并生成 `Dataset` 内部有良好类型的交易对象的所需所有函数。接下来我们需要做的是将这些数据持久化到磁盘上。这样，一旦我们运行了我们的程序很多天，我们就能检索过去发生的交易。
- en: Introducing the Parquet format
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Parquet 格式
- en: 'Spark supports many different formats for persisting `Datasets`: CSV, Parquet,
    ORC, JSON, and many others, such as Avro, with the appropriate library.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 支持多种不同的 `Datasets` 存储格式：CSV、Parquet、ORC、JSON 以及许多其他格式，例如 Avro，使用适当的库。
- en: With a row format, such as CSV, JSON, or Avro, the data is saved row by row.
    With a columnar format, such as Parquet or ORC, the data in the file is stored
    by columns.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用行格式，如 CSV、JSON 或 Avro，数据是按行保存的。使用列格式，如 Parquet 或 ORC，文件中的数据按列存储。
- en: 'For instance, we might have the following dataset of transactions:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能有以下交易数据集：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we write using a row format, the file would look like this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用行格式，如 CSV、JSON 或 Avro，数据将按行保存。使用列格式，如 Parquet 或 ORC，文件中的数据按列存储。
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In contrast, if we write using a columnar format, the file would look like
    this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们使用列式格式编写，文件将看起来像这样：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using a columnar data format offers several performance benefits when reading
    the data, including the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列式数据格式在读取数据时提供几个性能优势，包括以下内容：
- en: 'Projection push-down: When you need to select a few columns, you do not need
    to read the whole row. In the preceding example, if I am only interested in the
    evolution of the price of transactions, I can select only the timestamp and price,
    and the rest of the data will not be read from the disk.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投影下推：当你需要选择几个列时，你不需要读取整个行。在前面的例子中，如果我只对交易的定价演变感兴趣，我可以只选择时间戳和价格，其余的数据将不会从磁盘读取。
- en: 'Predicate push-down: When you want to retrieve only the rows where a column
    has a specific value, you can quickly find these rows by scanning the column data.
    In the preceding example, if I want to retrieve the transactions that happened
    between 07:22:00 and 07:22:30, the columnar storage will allow me to find these
    rows by only reading the timestamp column on the disk.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谓词下推：当你只想检索具有特定值的列的行时，你可以通过扫描列数据快速找到这些行。在前面的例子中，如果我想检索在 07:22:00 到 07:22:30
    之间发生的交易，列式存储将允许我通过只读取磁盘上的时间戳列来找到这些行。
- en: 'Better compression: Row formats can be compressed before being stored on the
    disk, but the columnar format has a better compression ratio. The data is indeed
    more homogeneous, as consecutive column values differ less than consecutive rows.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的压缩：行格式可以在存储到磁盘之前进行压缩，但列式格式有更好的压缩率。数据确实更加均匀，因为连续的列值之间的差异小于连续的行值。
- en: 'Splittable: When a Spark cluster runs a job that reads or writes to Parquet,
    the job''s tasks are distributed across many executors. Each executor will read/write
    chunks of rows from/to its own set of files in parallel.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可分割的：当 Spark 集群运行一个读取或写入 Parquet 的作业时，作业的任务会分布到许多执行器上。每个执行器将并行地从/向其自己的文件集读取/写入行块。
- en: All these benefits make a columnar format particularly well suited for running
    analytics queries. This is why, for our project, we are going to use Parquet to
    store transactions. The choice is a bit arbitrary; ORC would work equally well.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些优点使得列式格式特别适合运行分析查询。这就是为什么，在我们的项目中，我们将使用 Parquet 来存储交易数据。这个选择有点随意；ORC 也能同样好地工作。
- en: In a typical Spark cluster production setting, you have to store files in a
    **distributed filesystem**.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 Spark 集群生产环境中，你必须将文件存储在 **分布式文件系统** 中。
- en: Every node of the cluster must indeed have access to any chunk of the data.
    If one of your Spark nodes were to die, you would still want to have access to
    the data that was saved by it. You would not be able to do so if the files were
    stored on the local filesystem. Generally, people use the **Hadoop** filesystem
    or **Amazon S3** to store their parquet files. They both offer a distributed,
    reliable way of storing files, and they have good parallelism characteristics.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的每个节点确实必须能够访问数据中的任何一块。如果你的一个 Spark 节点崩溃了，你仍然希望能够访问它保存的数据。如果文件存储在本地文件系统中，你就无法做到这一点。通常，人们使用
    **Hadoop** 文件系统或 **Amazon S3** 来存储他们的 parquet 文件。它们都提供了分布式、可靠的文件存储方式，并且具有良好的并行性特征。
- en: In a production project, it can be beneficial to benchmark the performance of
    different formats. Depending on the shape of your data and the type of queries,
    one format might be better suited than the others.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产项目中，对不同格式的性能进行基准测试可能是有益的。根据你的数据形状和查询类型，一种格式可能比其他格式更适合。
- en: Writing transactions in Parquet
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Parquet 中写入交易
- en: 'Add the following `import` and function declaration in `BatchProducer`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `BatchProducer` 中添加以下 `import` 和函数声明：
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s have a look in more detail:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看：
- en: Writing to a file is a side effect; this is why we prefixed our function with
    `unsafe`. As functional programmers, we strive to control side effects, and it
    is a good practice to name any side effecting function explicitly. We will see
    in the next section how to use the IO `Monad` to push this side effect to the
    boundaries of our application.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写入文件是一个副作用；这就是为什么我们在函数前加了 `unsafe` 前缀。作为函数式程序员，我们努力控制副作用，并且明确命名任何有副作用的函数是一种好习惯。我们将在下一节中看到如何使用
    IO `Monad` 将这个副作用推送到我们应用程序的边界。
- en: We use `java.net.URI` to pass the path to the directory where our files will
    be written. This makes sure that the path we pass to the function is really a
    path. As usual, we try to avoid using strings for our parameters to make our code
    more robust.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `java.net.URI` 来传递文件将被写入的目录路径。这确保了我们传递给函数的路径确实是一个路径。像往常一样，我们尽量避免使用字符串作为参数，以使我们的代码更加健壮。
- en: The corresponding test will actually write to the filesystem; hence, it is rather
    more an integration test than a unit test. We are therefore going to create a
    new test with the `IT` suffix for the integration test.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的测试实际上会写入文件系统；因此，它更像是集成测试而不是单元测试。因此，我们将创建一个新的测试，带有 `IT` 后缀，用于集成测试。
- en: 'Create a new test called `coinyser.BatchProducerIT` in `src/test/scala`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `src/test/scala` 中创建一个新的测试，名为 `coinyser.BatchProducerIT`：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We use the handy `withTempDir` function from `SharedSparkSession`. It creates
    a temporary directory and deletes it after the test is finished. Then, we create
    a sample `Dataset[Transaction]`, and call the function we want to test.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 `SharedSparkSession` 的方便的 `withTempDir` 函数。它创建一个临时目录，并在测试完成后将其删除。然后，我们创建一个示例
    `Dataset[Transaction]`，并调用我们想要测试的函数。
- en: After having written the dataset, we assert that the target path contains a
    directory named `date=2018-07-23`. We indeed want to organize our storage with
    a `date` partition to make it faster to retrieve a specific date range.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在写入数据集之后，我们断言目标路径包含一个名为 `date=2018-07-23` 的目录。我们确实希望使用 `date` 分区来组织我们的存储，以便更快地检索特定日期范围。
- en: Finally, when we read back the file, we should get the same elements as in the
    original `Dataset`. Run the test and make sure it fails as expected.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们读取文件时，我们应该得到与原始 `Dataset` 中相同的元素。运行测试并确保它按预期失败。
- en: 'Now that we have a failing test, we can implement `BatchProducer.unsafeSave`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个失败的测试，我们可以实现 `BatchProducer.unsafeSave`：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'First, `transactions.write` creates `DataFrameWriter`. This is an interface
    that lets us configure some options before calling a final action method, such
    as `parquet(path: String): Unit`.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，`transactions.write` 创建 `DataFrameWriter`。这是一个接口，它允许我们在调用最终操作方法（如 `parquet(path:
    String): Unit`）之前配置一些选项。'
- en: 'We configure the `DataFrameWriter` with the following options:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下选项配置 `DataFrameWriter`：
- en: '`mode(SaveMode.Append)`: With this option, if there is already some data saved
    in the path, the content of `Dataset` will be appended to it. This will be useful
    when we call `unsafeSave` at regular intervals to get new transactions.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode(SaveMode.Append)`：使用此选项，如果路径中已经保存了一些数据，则 `Dataset` 的内容将被追加到其中。当我们在常规间隔调用
    `unsafeSave` 以获取新事务时，这将很有用。'
- en: '`partitionBy("date")`: In the context of storage, a partition is an intermediate
    directory that will be created under the path. It will have a name, such as `date=2018-08-16`.
    Partitioning is a good technique for optimizing the storage layout. This will
    allow us to speed up all the queries that only need the data for a specific date
    range.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`partitionBy("date")`：在存储的上下文中，分区是一个将在路径下创建的中间目录。它将有一个名称，例如 `date=2018-08-16`。分区是一种优化存储布局的好技术。这将使我们能够加快所有只需要特定日期范围数据的查询。'
- en: Do not confuse a storage partition (an intermediate folder in the filesystem)
    with a Spark partition (a chunk of the data, stored on a node in the cluster).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将存储分区（文件系统中的一个中间文件夹）与 Spark 分区（存储在集群节点上的数据块）混淆。
- en: You can now run the integration test; it should pass.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以运行集成测试；它应该通过。
- en: An interesting property of storage partitioning is that it further reduces file
    sizes. You might be worried that by storing both the timestamp and date, we would
    waste some storage space to store the date. It turns out that when the date is
    a storage partition, it is not stored in the Parquet files at all.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 存储分区的一个有趣特性是它进一步减少了文件大小。您可能会担心，通过存储时间戳和日期，我们会浪费一些存储空间来存储日期。但事实是，当日期是存储分区时，它根本不会存储在
    Parquet 文件中。
- en: 'To convince yourself, add the following line in the unit test, just after the
    call to `unsafeSave`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说服自己，在单元测试中，在调用 `unsafeSave` 之后添加以下行：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then run the unit test again. You should see this in the console:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后再次运行单元测试。您应该在控制台看到以下内容：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The date column is missing! This means that the `date` column is not stored
    at all in the Parquet files. In the unit test, when we were reading from the URI,
    Spark detected that there was a partition, `date=2018-07-23`, under that directory
    and added a column, `date`, containing the value `2018-07-23` for all values.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 日期列缺失！这意味着 `date` 列根本未存储在 Parquet 文件中。在单元测试中，当我们从 URI 读取时，Spark 检测到该目录下有一个分区
    `date=2018-07-23`，并为所有值添加了一个包含 `2018-07-23` 的 `date` 列。
- en: If you want to add a new column that has the same value for all rows, the easiest
    way is to create an intermediate directory, `myColum=value`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想添加一个所有行都具有相同值的列，最简单的方法是创建一个中间目录，`myColum=value`。
- en: Using the IO Monad
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 IO 模态
- en: We mentioned earlier that our function, `unsafeSave`, has a side effect, which
    is to write to a file. But as functional programmers, we try to only write pure
    functions that have no side effects. However, at the end of the program, you still
    want this side effect to happen; otherwise, there would be no point in running
    it!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，我们的函数`unsafeSave`有一个副作用，即写入文件。但是作为函数式程序员，我们尽量只编写没有副作用的纯函数。然而，在程序结束时，你仍然希望这个副作用发生；否则，就没有运行它的意义了！
- en: A common way of solving this dilemma is to use a parametrized type that encapsulates
    the side effect to run it asynchronously. A good candidate for that is the `cats.effect.IO`
    class in the `cats.effect` library (see [https://typelevel.org/cats-effect/datatypes/io.html](https://typelevel.org/cats-effect/datatypes/io.html)).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种困境的一种常见方法是使用参数化类型来封装副作用，以便异步运行它。`cats.effect`库中的`cats.effect.IO`类是一个很好的候选者（见[https://typelevel.org/cats-effect/datatypes/io.html](https://typelevel.org/cats-effect/datatypes/io.html)）。
- en: 'Here is an example that you can try in a Scala Console:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个你可以在Scala控制台中尝试的示例：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We can observe that nothing happened when we declared the `io` variable. At
    this point, the block passed to the `IO` constructor is only registered and will
    be executed later. The actual execution only happens when we call `unsafeRunSync()`.
    Our `io` variable is a pure, immutable value, and hence preserves referential
    transparency.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，当我们声明`io`变量时，没有发生任何事情。此时，传递给`IO`构造函数的块仅被注册，将在以后执行。实际的执行只有在调用`unsafeRunSync()`时才会发生。我们的`io`变量是一个纯的、不可变的价值，因此保持了引用透明性。
- en: '`IO` is `Monad`, and as such we can use `map`, `flatMap` and `for` comprehensions
    to compose side effects:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`IO`是`Monad`，因此我们可以使用`map`、`flatMap`和`for`表达式来组合副作用：'
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can reuse the `io` variable many times; the side effect that it encapsulates
    will be run as many times as necessary *at the end of the world* when we call
    `unsafeRunSync()`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以多次重用`io`变量；它封装的副作用将在我们调用`unsafeRunSync()`时，在“世界末日”时按需运行多次 *。
- en: If we had used `scala.concurrent.Future` instead of `cats.effect.IO`, the side
    effect would have been only run once. This is because `Future` memorizes the result.
    The behavior of `Future` may be desirable in some cases, but in some other cases,
    you really want your effects to be performed as many times as you define them
    in your code. The approach of `IO` also avoids shared state and memory leaks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用了`scala.concurrent.Future`而不是`cats.effect.IO`，副作用将只运行一次。这是因为`Future`会记住结果。在某些情况下，`Future`的行为可能是可取的，但在某些其他情况下，你真的希望你的效果按照你在代码中定义的次数执行。`IO`的方法也避免了共享状态和内存泄漏。
- en: '`IO` values can also be run in parallel. They can effectively replace `scala.concurrent.Future`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`IO`值也可以并行运行。它们可以有效地替换`scala.concurrent.Future`：'
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The `IO` block returns the current thread's name as a string. We create a program
    of the `IO[String]` type using `parMapN` to indicate that we want to execute the
    `IO` values in the tuple in parallel. The output of `unsafeRunSync` shows that
    the program was executed in three different threads.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`IO`块返回当前线程的名称作为字符串。我们使用`parMapN`创建一个`IO[String]`类型的程序，以表示我们想要并行执行元组中的`IO`值。`unsafeRunSync`的输出显示程序在三个不同的线程中执行。'
- en: 'Going back to our transaction saving, all we have to do to make our `unsafeSave`
    function safe is to wrap it in `IO`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的交易保存，我们使`unsafeSave`函数安全所需要做的全部事情就是将其包裹在`IO`中：
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Alternatively, you can inline `unsafeSave` and change the integration test
    to call `save` instead:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以内联`unsafeSave`并将集成测试改为调用`save`：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can now save transactions while controlling side effects and keeping our
    functions pure.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在控制副作用的同时保存事务，并保持我们的函数纯。
- en: Putting it all together
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: At this point, we can read transactions from the REST API, transform the JSON
    payload in `Dataset[Transaction]`, and save it to parquet. It is time to put all
    these pieces together.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以从REST API读取事务，将`Dataset[Transaction]`中的JSON有效负载进行转换，并将其保存到parquet。现在是时候将这些部分组合在一起了。
- en: The Bitstamp API allows us to get the transactions that happened in the last
    24 hours, in the last hour, or in the last minute. At the end of the day, we would
    like to build an application that regularly fetches and saves new transactions
    for long-term analysis. This application is our *batch* layer, and it is not meant
    to get real-time transactions. Therefore, it will be enough to get the transactions
    for the last hour. In the next chapter, we will build a *speed* layer to process
    the live transactions.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: Bitstamp API 允许我们获取过去 24 小时、过去一小时或过去一分钟发生的交易。在一天结束时，我们希望构建一个定期获取并保存新交易以进行长期分析的应用程序。这个应用程序是我们的
    *批处理* 层，它不是为了获取实时交易。因此，获取过去一小时的交易就足够了。在下一章中，我们将构建一个 *速度* 层来处理实时交易。
- en: 'Our `BatchProducer` application will work as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `BatchProducer` 应用程序将按以下方式工作：
- en: On startup, fetch the last 24 hours of transactions. Set `start` = current day
    at midnight UTC and `end` = last transaction's timestamp.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在启动时，获取最后 24 小时的交易。将 `start` 设置为当前午夜 UTC 日期，将 `end` 设置为最后交易的戳记。
- en: Filter the transactions to only keep those between `start` and `end` and save
    them to Parquet.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤交易，仅保留 `start` 和 `end` 之间的交易，并将它们保存到 Parquet。
- en: Wait 59 minutes.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待 59 分钟。
- en: Fetch the last one hour of transactions. We have a one minute overlap to make
    sure that we do not miss any transaction. Set the `start` = `end` and `end` =
    `last` transaction timestamps.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取最后一个小时的交易。我们有一个一分钟的重叠，以确保我们不会错过任何交易。将 `start` 设置为 `end`，将 `end` 设置为最后交易的戳记。
- en: Go to step 2.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到步骤 2。
- en: To implement this algorithm, we are going to write a `processOneBatch` function
    that encompasses steps 2 to 4, and after that, we will implement step 1 and the
    infinite loop.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这个算法，我们将编写一个 `processOneBatch` 函数，它包含步骤 2 到 4，然后我们将实现步骤 1 和无限循环。
- en: Testing processOneBatch
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试 processOneBatch
- en: 'Our function will need a few configuration parameters and implicit values.
    To keep our signature tidy, we are going to put them in a class. Create a new
    class, `coinyser.AppContext`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能需要一些配置参数和隐式值。为了保持我们的签名整洁，我们将它们放在一个类中。创建一个新的类，`coinyser.AppContext`：
- en: '[PRE36]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`AppContext` contains the target location for the Parquet files, the `SparkSession`
    object, and a `Timer[IO]` object that is required by `cats.effect` when we need
    to call `IO.sleep`.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`AppContext` 包含 Parquet 文件的目标位置、`SparkSession` 对象以及当需要调用 `IO.sleep` 时 `cats.effect`
    所需的 `Timer[IO]` 对象。'
- en: 'Then declare the `processOneBach` function in `BatchProducer`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在 `BatchProducer` 中声明 `processOneBach` 函数：
- en: '[PRE37]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The function accepts these parameters:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 函数接受以下参数：
- en: '`fetchNextTransactions` is an `IO` operation that will return the transactions
    of the past hour when run. We pass it as a parameter so that we can simulate the
    call to the Bitstamp API in a unit test.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fetchNextTransactions` 是一个 `IO` 操作，当运行时将返回过去一小时的交易。我们将其作为参数传递，以便在单元测试中模拟对
    Bitstamp API 的调用。'
- en: '`transactions` is `Dataset` containing the last transactions that were read
    (steps 1 or 4 in our algorithm).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transactions` 是包含已读取的最后交易的 `Dataset`（我们的算法中的步骤 1 或 4）。'
- en: '`saveStart` and `saveEnd` is the time interval used to filter `transactions`
    before saving them.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`saveStart` 和 `saveEnd` 是在保存 `transactions` 之前用于过滤的时间间隔。'
- en: '`appCtx` is as described previously.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`appCtx` 如前所述。'
- en: 'Our function will have to perform side effects; hence, it returns `IO`. This
    `IO` will contain a tuple with the following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将必须执行副作用；因此，它返回 `IO`。这个 `IO` 将包含一个元组，其中包含以下内容：
- en: '`Dataset[Transaction]` that will be obtained by running `fetchNextTransactions`'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过运行 `fetchNextTransactions` 获得的 `Dataset[Transaction]`。
- en: The next `saveStart` and the next `saveEnd`
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个 `saveStart` 和下一个 `saveEnd`
- en: 'Now that we have a good declaration of our function, we can write an integration
    test for it. The test is quite long; hence, we are going to describe it bit by
    bit. Create a new integration test in `BatchProducerIT`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地声明了我们的函数，我们可以为它编写一个集成测试。测试相当长；因此，我们将一点一点地描述它。在 `BatchProducerIT` 中创建一个新的集成测试：
- en: '[PRE38]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We first define `FakeTimer` that implements the `Timer[IO]` interface. This
    timer lets us simulate a clock that starts at `2018-08-02T01:00:00Z`. This way,
    we will not have to wait 59 minutes to run our test. The implementation uses `var
    clockRealTimeInMillis` that keeps the current time of our fake clock and updates
    it when `sleep` is called.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义 `FakeTimer`，它实现了 `Timer[IO]` 接口。这个计时器让我们模拟一个从 `2018-08-02T01:00:00Z`
    开始的时钟。这样，我们就不必等待 59 分钟来运行我们的测试。实现使用 `var clockRealTimeInMillis`，它保持我们的假时钟的当前时间，并在调用
    `sleep` 时更新它。
- en: 'Then, we create `AppContext` using the temporary directory, and the implicits
    that are in scope: `FakeTimer` and `SparkSession`.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用临时目录和作用域内的隐式转换：`FakeTimer` 和 `SparkSession` 创建 `AppContext`。
- en: 'The next portion of the test defines some transactions:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 测试的下一部分定义了一些交易：
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `implicit` conversion `toTimestamp` lets us declare our transaction objects
    with `String` instead of `Timestamp`. This makes the test easier to read. We use
    it to declare five `Transaction` objects with timestamps ranging around the initial
    clock of `FakeTimer`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`implicit` 转换 `toTimestamp` 允许我们使用 `String` 而不是 `Timestamp` 来声明我们的交易对象。这使得测试更容易阅读。我们用它来声明五个
    `Transaction` 对象，其时间戳围绕 `FakeTimer` 的初始时钟。'
- en: 'Then, we declare batches of transactions that simulate what would have been
    read from the Bitstamp API. We cannot indeed call the real Bitstamp API from our
    integration test; the data would be random and our integration test could fail
    if the API is not available:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们声明了一系列模拟从 Bitstamp API 读取的内容的交易批次。实际上，我们无法从我们的集成测试中调用真实的 Bitstamp API；数据将是随机的，如果
    API 不可用，我们的集成测试可能会失败：
- en: '`txs0` is `Seq[Transaction]`, which simulates an initial batch of transactions
    that we read at 01:00\. If you remember the `BatchProducer` algorithm, this initial
    batch would contain the last 24 hours of transactions. In our example, this batch
    only contains `tx1`, even though `tx2`''s timestamp is 01:00\. This is because,
    with the real API, we would not get a transaction that happened exactly at the
    same time. There is always a bit of lag.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`txs0` 是 `Seq[Transaction]`，它模拟我们在 01:00 读取的初始交易批次。如果你还记得 `BatchProducer` 算法，这个初始批次将包含最后
    24 小时的交易。在我们的例子中，这个批次只包含 `tx1`，即使 `tx2` 的时间戳是 01:00。这是因为，在真实的 API 中，我们不会得到在完全相同时间发生的交易。总会有一些延迟。'
- en: '`txs1` is the batch of transactions that we read 59 minutes after, at 01:59\.
    In this batch, we consider that the API lag makes us miss `tx4`, which happens
    at 01:58:59.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`txs1` 是我们在 01:59 读取的交易批次。在这个批次中，我们考虑 API 延迟使我们错过了在 01:58:59 发生的 `tx4`。'
- en: '`txs2` is the batch that we read 59 minutes after `txs1`, at 02:58.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`txs2` 是在 `txs1` 59 分钟后读取的批次，在 02:58。'
- en: '`txs3` is the batch that we read 59 minutes after `txs2`, at 03:57.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`txs3` 是在 `txs2` 59 分钟后读取的批次，在 03:57。'
- en: 'The following portion actually calls the function under test, `processOneBatch`,
    three times:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的部分实际上调用了被测试的函数 `processOneBatch` 三次：
- en: '[PRE40]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'For the first call, we pass the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次调用，我们传递以下内容：
- en: '`txs0.toDS()` represents the initial batch of transactions. This would cover
    the last 24 hours of transactions.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`txs0.toDS()` 表示初始的交易批次。这将涵盖最后 24 小时的交易。'
- en: '`start0` = 00:00\. In our algorithm, we choose to cut the first batch to start
    at midnight. This way, we won''t save partial data for the previous day.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start0` = 00:00。在我们的算法中，我们选择将第一个批次从午夜开始。这样，我们就不会保存前一天的任何部分数据。'
- en: '`end0` = 00:59:55\. Our clock starts at 01:00, but the API has always some
    lag for making a transaction visible. We estimate that lag to not exceed five
    seconds.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end0` = 00:59:55。我们的时钟从 01:00 开始，但 API 总是会有一些延迟来使交易可见。我们估计这个延迟不会超过五秒。'
- en: '`IO(txs1.toDS())` represents the next batch of transactions to be fetched.
    It will be fetched 59 minutes after the initial one.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IO(txs1.toDS())` 表示下一个要获取的交易批次。它将在初始批次 59 分钟后获取。'
- en: The subsequent calls pass the results of the previous calls, as well as the
    `IO` value to fetch the following batch.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 后续调用传递了前一个调用的结果，以及获取下一个批次的 `IO` 值。
- en: 'We then run the three calls with `unsafeRunSync()`, and obtain the results
    of the two first calls in `ds1`, `start1`, `end1`, `ds2`, `start2`, and `end2`.
    This allows us to verify the results with the following assertions:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后使用 `unsafeRunSync()` 运行这三个调用，并在 `ds1`、`start1`、`end1`、`ds2`、`start2` 和 `end2`
    中获得前两个调用的结果。这使我们能够通过以下断言来验证结果：
- en: '[PRE41]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Lets have a look in detail at the preceding code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看前面的代码：
- en: '`ds1` is the batch that was obtained by running the `IO(txs1.toDS())`. It must,
    therefore, be the same as `txs1`'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ds1` 是通过运行 `IO(txs1.toDS())` 获得的批次。因此，它必须与 `txs1` 相同'
- en: '`start1` must be equal to `end0`—we need to shift the time period without any
    gap'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start1` 必须等于 `end0`——我们需要无缝地移动时间周期'
- en: '`end1` must be equal to the initial clock *(01:00) + 59 mn (wait time) - 5
    seconds* (API lag)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end1` 必须等于初始时钟 *(01:00) + 59 分钟 (等待时间) - 5 秒* (API 延迟)'
- en: '`ds2`, `start2`, and `end2` follow the same logic'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ds2`、`start2` 和 `end2` 遵循相同的逻辑'
- en: '`lastClock` must be equal to the initial clock *+ 3 * 59* mn'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lastClock` 必须等于初始时钟 *+ 3 * 59* 分钟'
- en: 'Finally, we can assert that the right transactions were saved to disk:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以断言正确的交易已保存到磁盘：
- en: '[PRE42]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The assertion excludes `tx1`, as it happened in the previous day. It also verifies
    that even though our batches, `txs1` and `txs2`, had some overlap, there is no
    duplicate transaction in our Parquet file.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 该断言排除了`tx1`，因为前一天已经发生。它还验证了尽管我们的批次`txs1`和`txs2`有一些重叠，但我们的Parquet文件中没有重复的交易。
- en: You can compile and run the integration test. It should fail with `NotImplementedError`
    as expected.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以编译并运行集成测试。它应该像预期的那样失败，抛出`NotImplementedError`异常。
- en: Implementing processOneBatch
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现processOneBatch
- en: 'Here is the implementation of  `BatchProducer.processOneBatch`. As is often
    the case, the implementation is much shorter than the test:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`BatchProducer.processOneBatch`的实现。正如通常情况一样，实现比测试要短得多：
- en: '[PRE43]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We first filter the transactions using a `filterTxs` function that we will
    define shortly. Then, using a `for` comprehension, we chain several `IO` values:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用我们很快将要定义的`filterTxs`函数过滤交易。然后，使用`for` comprehension，我们链接几个`IO`值：
- en: Save the filtered transactions, using the `save` function that we implemented
    earlier
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前实现的`save`函数保存过滤后的交易
- en: Wait 59 minutes, using the implicit `Timer` that was brought in scope with `import
    appCtx._`
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`import appCtx._`引入的作用域中的隐式`Timer`等待59分钟
- en: Get the current time, using a `currentInstant` function that we will define
    shortly
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们很快将要定义的`currentInstant`函数获取当前时间
- en: Fetch the next transactions using the first argument
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第一个参数获取下一批交易
- en: 'Here is the implementation of the helper function, `filterTxs`:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是辅助函数`filterTxs`的实现：
- en: '[PRE44]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We did not need to pass an implicit `SparkSession`, as it is already available
    in the transaction `Dataset`. We only keep transactions for the interval `(fromInstant,
    untilInstant)`. The end instant is excluded so that we do not have any overlap
    when we loop over `processOneBatch`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要传递隐式的`SparkSession`，因为它已经在交易`Dataset`中可用。我们只为`(fromInstant, untilInstant)`区间保留交易。排除结束时间，这样我们在`processOneBatch`循环时就不会有任何重叠。
- en: 'Here is the definition of `currentInstant`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`currentInstant`的定义：
- en: '[PRE45]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We use the `Timer` class to get the current time. As we saw while writing the
    integration test, this allowed us to use a fake timer to simulate a clock.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Timer`类来获取当前时间。正如我们在编写集成测试时看到的，这允许我们使用一个假的计时器来模拟时钟。
- en: Implementing processRepeatedly
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现processRepeatedly
- en: We are now ready to implement the algorithm of our `BatchProducer` application,
    which will loop repeatedly over `processOneBatch`. We are not going to write an
    integration test for it, as it merely assembles other parts that have been tested.
    Ideally, in a production system, you should write an end-to-end test that would
    start the application and connect to a fake REST server.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始实现我们的`BatchProducer`应用程序的算法，该算法将反复执行`processOneBatch`。我们不会为它编写集成测试，因为它只是组装了已经测试过的其他部分。理想情况下，在一个生产系统中，你应该编写一个端到端测试，该测试将启动应用程序并连接到一个假的REST服务器。
- en: 'Here is the implementation of `processRepeatedly`:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`processRepeatedly`的实现：
- en: '[PRE46]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In the function''s signature, we have the following:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的签名中，我们有以下内容：
- en: A parameter `initialJsonTxs`, which is `IO`  that will fetch the last 24 hours
    of transactions in `Dataset`
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数`initialJsonTxs`，它是一个`IO`，将获取`Dataset`中最后24小时的交易
- en: A second parameter, `jsonTxs`, which fetches the last hour of transactions
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数`jsonTxs`，用于获取最后小时的交易
- en: A return type, `IO[Unit]`, which will run infinitely when we call `unsafeRunSync`
    in the main application
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回类型`IO[Unit]`，当我们在主应用程序中调用`unsafeRunSync`时将无限运行
- en: 'The functions'' body is a `for` comprehension that chains the `IO` values as
    follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的主体是一个`for` comprehension，按照以下方式链接`IO`值：
- en: We first calculate `firstEnd`= current time - 5 seconds. By using an `ApiLag`
    of 5 seconds, when we then fetch transactions using `initialJsonTxs`, we are certain
    that we will get all the transactions until `firstEnd`.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算`firstEnd` = 当前时间 - 5秒。通过使用5秒的`ApiLag`，当我们使用`initialJsonTxs`获取交易时，我们确定我们将获取到`firstEnd`的所有交易。
- en: '`firstStart` is set to midnight on the current day. For the initial batch,
    we want to filter out transactions from the previous day.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`firstStart`设置为当前天的午夜。对于初始批次，我们希望过滤掉前一天的交易。'
- en: We fetch the last 24 hours of transactions in `firstTxs`, of the `Dataset[Transaction]` type.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`firstTxs`中获取了`Dataset[Transaction]`类型的最后24小时的交易。
- en: We call `tailRecM` from `Monad`. It calls the anonymous function in the block
    until it returns `Monad[Right[Unit]]`. But since our function always returns `Left`,
    it will loop infinitely.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 `Monad` 中调用 `tailRecM`。它调用块中的匿名函数，直到它返回 `Monad[Right[Unit]]`。但由于我们的函数总是返回
    `Left`，它将无限循环。
- en: Implementing BatchProducerApp
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 BatchProducerApp
- en: Finally, all we need to do is create an application that will call `processRepeatedly`
    with the right parameters.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们所需做的就是创建一个应用程序，该应用程序将使用正确的参数调用 `processRepeatedly`。
- en: 'Create a new class, `coinyser.BatchProducerApp`, in `src/main/scala` and type
    the following:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `src/main/scala` 中创建一个新的类 `coinyser.BatchProducerApp` 并输入以下内容：
- en: '[PRE47]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The class extends `cats.effect.IOApp`. It is a helper trait that will call
    `unsafeRunSync` on `IO` returned by the `run` method. It also extends `StrictLogging`.
    This trait brings an attribute `logger` in scope that we will use to log messages.
    The body of our object defines the following members:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 该类扩展了 `cats.effect.IOApp`。这是一个辅助特质，它将在 `run` 方法返回的 `IO` 上调用 `unsafeRunSync`。它还扩展了
    `StrictLogging`。这个特质在作用域中引入了一个 `logger` 属性，我们将使用它来记录消息。我们的对象体定义了以下成员：
- en: '`spark` is the `SparkSession`, required for manipulating datasets. The master
    is set to `local[*]`, which means that Spark will use all of the cores available
    on the localhost to execute our jobs. But, as we will see in the next section,
    this can be overridden when using a Spark cluster.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark` 是 `SparkSession`，用于操作数据集。主节点设置为 `local[*]`，这意味着 Spark 将使用本地主机上所有可用的核心来执行我们的作业。但是，正如我们将在下一节中看到的，当使用
    Spark 集群时，这可以被覆盖。'
- en: '`appContext` requires the path for saving our transaction. Here, we use a relative
    directory on the local filesystem. In a production environment, you would typically
    use an S3 or HDFS location. `AppContext` also requires two implicits: `SparkSession`
    and `Timer[IO]`. We already defined the former, and the latter is provided by
    `IOApp`.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`appContext` 需要保存我们的交易的路径。在这里，我们使用本地文件系统上的相对目录。在生产环境中，你通常会使用 S3 或 HDFS 位置。`AppContext`
    还需要两个隐式参数：`SparkSession` 和 `Timer[IO]`。我们已定义了前者，后者由 `IOApp` 提供。'
- en: '`bitstampUrl` is a function that returns the URL that is used to retrieve the
    transactions that happened in the last day or in the last hour'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bitstampUrl` 是一个函数，它返回用于检索过去一天或过去一小时发生的交易的 URL。'
- en: '`transactionsIO` fetches the transactions by calling the Bitstamp URL. As seen
    at the beginning of this chapter, we use `scala.io.Source` to create a string
    from the HTTP response. We then transform it into `Dataset[Transaction]` using
    the two functions, `jsonToHttpTransactions` and  `httpToDomainTransactions`, that
    we implemented earlier.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transactionsIO` 通过调用 Bitstamp URL 获取交易。如本章开头所见，我们使用 `scala.io.Source` 从 HTTP
    响应中创建一个字符串。然后，我们使用我们之前实现的两个函数 `jsonToHttpTransactions` 和 `httpToDomainTransactions`
    将其转换为 `Dataset[Transaction]`。'
- en: '`initialJsonTxs` and `nextJsonTxs` are the IO values that, respectively, retrieve
    the last 24 hours of transactions and the last hour of transactions.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`initialJsonTxs` 和 `nextJsonTxs` 是 IO 值，分别检索过去 24 小时的交易和过去一小时的交易。'
- en: '`run` implements the only abstract method of `IOApp`. It produces `IO[ExitCode]`
    to be run as an application. Here we just call `processRepeatedly` with `vals`
    that were defined previously. Then, we have to map to change the unit result to
    `ExitCode.Success` in order to type check. Actually, this exit code will never
    be returned, because `processRepeatedly` loops infinitely.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`run` 实现了 `IOApp` 的唯一抽象方法。它生成 `IO[ExitCode]` 以作为应用程序运行。在这里，我们只是使用之前定义的 `vals`
    调用 `processRepeatedly`。然后，我们必须映射以将单元结果转换为 `ExitCode.Success` 以进行类型检查。实际上，这个退出代码永远不会返回，因为
    `processRepeatedly` 是无限循环的。'
- en: If you now try to run `BatchProducerAppSpark`, you will get `ClassNotFoundException`
    about a Spark class. This is because in `build.sbt`**,** we declared some libraries
    as `% Provided`. As we shall see, this configuration is useful for packaging the
    application, but right now it prevents us from testing our program easily.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在尝试运行 `BatchProducerAppSpark`，你将得到关于 Spark 类的 `ClassNotFoundException`。这是因为我们在
    `build.sbt` 中**，将一些库声明为 `% Provided`。正如我们将看到的，这种配置对于打包应用程序很有用，但到目前为止，它阻止我们轻松地测试我们的程序。
- en: 'The trick to avoid that is to create another object, `coinyser.BatchProducerAppIntelliJ`,
    in the `src/test/scala` directory, that also extends the class `BatchProducerApp`.
    IntelliJ indeed brings all of the provided dependencies to the test runtime classpath:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这种情况的技巧是在 `src/test/scala` 目录中创建另一个对象 `coinyser.BatchProducerAppIntelliJ`，它也扩展了
    `BatchProducerApp` 类。IntelliJ 确实将所有提供的依赖项带到测试运行时类路径中：
- en: '[PRE48]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is why we defined a class and an object in `BatchProducerApp.scala`. We
    can have one implementation that will be used with `spark-submit`, and one that
    we can run from IntelliJ.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们在 `BatchProducerApp.scala` 中定义了一个类和一个对象。我们可以有一个实现，它将用于 `spark-submit`，还有一个可以从
    IntelliJ 中运行的实现。
- en: 'Now run the `BatchProducerAppIntelliJ` application. After a couple of seconds,
    you should see something similar to this in the console:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行 `BatchProducerAppIntelliJ` 应用程序。几秒钟后，你应该在控制台看到类似以下内容：
- en: '[PRE49]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: After this point, you should have a Parquet file in the `data/transactions/date=<current
    date>` directory, containing all of the transactions that happened from midnight
    on the current day. If you wait one more hour, you will get another Parquet file
    containing the transactions of the last hour.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，你应该在 `data/transactions/date=<当前日期>` 目录中有一个 Parquet 文件，其中包含从午夜开始当天发生的所有交易。如果你再等一个小时，你将得到另一个包含最后一个小时交易的
    Parquet 文件。
- en: 'If you do not want to wait one hour to see it happening, you can fetch transactions
    every minute instead:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想等一个小时来看到它发生，你可以每分钟获取一次交易：
- en: Change `BatchProducerApp.nextJsonTxs` to `jsonIO("?time=minute")`
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `BatchProducerApp.nextJsonTxs` 改为 `jsonIO("?time=minute")`。
- en: Change `BatchProducer.WaitTime` to `45.seconds` to have a 15 seconds overlap
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `BatchProducer.WaitTime` 改为 `45.seconds` 以实现 15 秒的重叠。
- en: The `WARN` message in the console tells us that `"Stage 0 contains a task of
    very large size"`. This is because `String` that contains the HTTP response is
    sent as a whole to one Spark task. If we had a much larger payload (several hundreds
    of MB), it would be less memory intensive to split it and write it to a file.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台中的 `WARN` 消息告诉我们 `"Stage 0 包含一个非常大的任务"`。这是因为包含 HTTP 响应的 `String` 被作为一个整体发送到单个
    Spark 任务。如果我们有一个更大的有效负载（几百 MB），将其分割并写入文件将更节省内存。
- en: 'In the next chapter, we will see how to use Zeppelin to query these Parquet
    files and plot some charts. But we can already check them using the Scala Console.
    Start a new Scala Console and type the following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用 Zeppelin 查询这些 Parquet 文件并绘制一些图表。但我们可以使用 Scala 控制台来检查它们。启动一个新的
    Scala 控制台并输入以下内容：
- en: '[PRE50]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Feel free to play around with the Dataset API. You can try to count the transactions,
    filter them for a specific period, and find the maximum price or quantity.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 随意使用 Dataset API 进行实验。你可以尝试计算交易数量，对特定时间段进行过滤，并找到最高价格或数量。
- en: Running the application with spark-submit
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 spark-submit 运行应用程序。
- en: We have run our application in a standalone way, but when you want to process
    large datasets you would need to use a Spark cluster. It is out of the scope of
    this book to explain how to set up a Spark cluster. If you want to set up one,
    you can refer to the Spark documentation or use an off-the-shelf cluster from
    a cloud computing vendor.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经以独立方式运行了我们的应用程序，但当你想要处理大型数据集时，你需要使用 Spark 集群。本书的范围不包括解释如何设置 Spark 集群。如果你想设置一个，你可以参考
    Spark 文档或使用云计算供应商提供的现成集群。
- en: Nonetheless, the submission process is the same whether we run Spark in local
    mode or in cluster mode.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 不论我们是在本地模式下运行 Spark 还是集群模式下运行，提交过程都是相同的。
- en: Installing Apache Spark
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Apache Spark。
- en: We are going to install Spark to run it in local mode. This mode only uses the
    CPU cores of the localhost to run jobs. For this, download Spark 2.3.1 from this
    page: [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装 Spark 以在本地模式下运行它。此模式仅使用本地主机的 CPU 核心来运行作业。为此，从以下页面下载 Spark 2.3.1： [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)。
- en: 'Then extract it to some folder for instance, `~/` for your `home` folder on
    Linux or macOS:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其提取到某个文件夹中，例如，Linux 或 macOS 上的 `~/`，即你的主目录：
- en: '[PRE51]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can try running `spark shell` to verify that the installation is correct:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试运行 `spark shell` 来验证安装是否正确：
- en: '[PRE52]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After a couple of seconds, you should see a welcome message followed by the
    same `scala>` prompt that we had in the Scala console:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，你应该看到一条欢迎消息，然后是 Scala 控制台中相同的 `scala>` 提示符：
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'A Spark shell is actually a Scala Console connected to a Spark cluster. In
    our case, it is a Spark local cluster, as you can see with `master = local[*]`.
    Spark-shell provides a variable, `spark: SparkSession`, that you can use to manipulate
    datasets. It can be a handy tool, but I generally prefer using IntelliJ''s console
    and create `SparkSession` by hand. IntelliJ''s console has the benefit of having
    syntax highlighting and better code completion.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 'Spark shell 实际上是一个连接到 Spark 集群的 Scala 控制台。在我们的例子中，它是一个 Spark 本地集群，正如您通过 `master
    = local[*]` 看到的。Spark-shell 提供了一个变量，`spark: SparkSession`，您可以使用它来操作数据集。它可能是一个方便的工具，但我通常更喜欢使用
    IntelliJ 的控制台，并手动创建 `SparkSession`。IntelliJ 的控制台有语法高亮和更好的代码补全的优点。'
- en: Packaging the assembly JAR
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打包装配 JAR 文件
- en: 'In order to run our application with a Spark distribution, we need to package
    our compiled classes and their dependencies in a JAR file. This is what we call
    an assembly JAR or fat JAR: its size can be quite large if you have many dependencies.
    To do that, we have to modify our SBT build files.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 Spark 分发运行我们的应用程序，我们需要将编译好的类及其依赖项打包到一个 JAR 文件中。这就是我们所说的装配 JAR 或胖 JAR：如果有很多依赖项，其大小可能相当大。为此，我们必须修改我们的
    SBT 构建文件。
- en: 'First, we need to enable the assembly plugin. Add a new file, `assembly.sbt`**,**
    in the `bitcoin-analyser/project` folder and type the following:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要启用装配插件。在 `bitcoin-analyser/project` 文件夹中添加一个新文件，`assembly.sbt`**，并输入以下内容：
- en: '[PRE54]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We also need to exclude all the Spark dependencies from our assembly JAR. There
    is no point in having them as they are already present in the Spark distribution.
    Excluding them will save space and build time. For this, we had already scoped
    these dependencies to `% Provided` in `build.sbt`. This will make sure that the
    dependencies are present for compiling the project and running the tests, but
    are excluded when building the assembly JAR.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要排除所有 Spark 依赖项从我们的装配 JAR 中。没有必要保留它们，因为它们已经在 Spark 分发中存在。排除它们将节省空间和构建时间。为此，我们已经在
    `build.sbt` 中将这些依赖项范围设置为 `% Provided`。这将确保依赖项在编译项目和运行测试时存在，但在构建装配 JAR 时被排除。
- en: 'Then we have to add a few configuration options at the end of `build.sbt`:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须在 `build.sbt` 的末尾添加一些配置选项：
- en: '[PRE55]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here is a short explanation talking about what line performs what action:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简短的解释，说明每一行执行了什么操作：
- en: The first line excludes all Scala runtime JARs.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行排除了所有 Scala 运行时 JAR 文件。
- en: The second line tells SBT to skip the tests when running the assembly task
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行告诉 SBT 在运行装配任务时跳过测试
- en: The last line declares what is our main class. This declaration will end up
    in the `MANIFEST.MF` file, and will be used by Spark to bootstrap our program.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一行声明了我们的主类。这个声明最终会出现在 `MANIFEST.MF` 文件中，并被 Spark 用于启动我们的程序。
- en: 'Our build files are ready; we can run the assembly task. Open the SBT shell
    in IntelliJ (*Ctrl* + *Shift* + *S*), and type `assembly` after the `sbt>` prompt.
    This should compile the project and package the assembly JAR. The output of your
    SBT shell should look like this:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的构建文件已准备好；我们可以运行装配任务。在 IntelliJ 中打开 SBT 控制台（*Ctrl* + *Shift* + *S*），在 `sbt>`
    提示符后输入 `assembly`。这应该会编译项目并打包装配 JAR。您 SBT 控制台的输出应该如下所示：
- en: '[PRE56]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The assembly JAR is ready; we can submit it to Spark.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 装配 JAR 已准备好；我们可以将其提交给 Spark。
- en: Running spark-submit
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行 spark-submit
- en: 'Using a console, go to the Spark distribution''s `bin` folder, and `run spark-submit`
    with the path of the assembly JAR:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 使用控制台，转到 Spark 分发的 `bin` 文件夹，并使用装配 JAR 的路径运行 `spark-submit`：
- en: '[PRE57]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`spark-submit` has lots of options that let you change the Spark master, the
    number of executors, their memory requirements, and so on. You can find out more
    by running `spark-submit -h`. After having submitted our JAR, you should see something
    like this in your console (we only show the most important parts):'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark-submit` 有很多选项，允许您更改 Spark 主节点、执行器的数量、它们的内存需求等。您可以通过运行 `spark-submit
    -h` 来了解更多信息。提交我们的 JAR 后，您应该在控制台看到类似以下内容（我们只显示最重要的部分）：'
- en: '[PRE58]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'You would see a similar output if you were using a remote cluster:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是远程集群，您将看到类似的输出：
- en: The line `Submitted application`: Tells us what `main` class we submitted. This
    corresponds to the `mainClass` setting that we put in our SBT file. This can be
    overridden with the `--class` option in `spark-submit`.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行 `Submitted application`：告诉我们我们提交了哪个 `main` 类。这对应于我们在 SBT 文件中设置的 `mainClass`
    设置。这可以通过 `spark-submit` 中的 `--class` 选项来覆盖。
- en: A few lines after, we can see that Spark started a **SparkUI** web server on
    port `4040`. With your web browser, go to the URL `http://localhost:4040` to explore
    this UI. It allows you to see the progress of running jobs, their execution plan,
    how many executors they use, the logs of the executors, and so on. SparkUI is
    a precious tool when you need to optimize your jobs.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几行之后，我们可以看到Spark在端口`4040`上启动了一个**SparkUI**网络服务器。使用您的网页浏览器，访问URL `http://localhost:4040`
    来探索这个UI。它允许您查看运行作业的进度、它们的执行计划、使用的执行器数量、执行器的日志等信息。当您需要优化作业时，SparkUI是一个宝贵的工具。
- en: '`Added JAR file`: Before Spark can run our application, it must distribute
    the assembly JAR to all the cluster nodes. For doing this, we can see that it
    starts a server on port `37370`. The executors would then connect to that server
    to download the JAR file.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`添加 JAR 文件`: 在Spark可以运行我们的应用程序之前，它必须将assembly JAR分发到所有集群节点。为此，我们可以看到它启动了一个端口为`37370`的服务器。执行器随后会连接到该服务器以下载JAR文件。'
- en: '`Starting Executor ID driver`: The driver process coordinates the execution
    of jobs with the executors. The following line shows that it listens on port `37370`
    to receive updates from the executors.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`启动 Executor ID driver`: 驱动进程协调作业与执行器的执行。以下行显示它监听端口`37370`以接收执行器的更新。'
- en: '`Calling https//`: This corresponds to what we logged in our code, `logger.info(s"calling
    $url")`.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`调用 https//`: 这对应于我们在代码中记录的内容，`logger.info(s"调用 $url")`。'
- en: '`Starting job`: Our application started a Spark job. Line 115 in `BatchProducer`
    corresponds to the `.parquet(path.toString)` instruction in `BatchProducer.save`.
    This `parquet` method is indeed an action and as such triggers the evaluation
    of `Dataset`.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`开始作业`: 我们的应用程序启动了一个Spark作业。`BatchProducer`中的第115行对应于`BatchProducer.save`中的`.parquet(path.toString)`指令。这个`parquet`方法确实是一个动作，因此会触发`Dataset`的评估。'
- en: '`Job 0 finished`: The job finishes after a few seconds.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`作业 0 完成`: 作业在几秒钟后完成。'
- en: After this point, you should have a `parquet` file saved with the last transactions.
    If you let the application continue for 1 hour, you will see that it starts another
    job to get the last hour of transactions.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，你应该已经保存了一个包含最后交易的`parquet`文件。如果您让应用程序继续运行1小时，您将看到它开始另一个作业来获取最后1小时的交易。
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: By now, you should be more comfortable using Spark's `Dataset` API. Our little
    program is focused on fetching BCT/USD transactions, but it could be interesting
    to enhance it. For instance, you could fetch and save other currency pairs, such
    as ETH/EUR or XRP/USD. Use a different cryptocurrency exchange. This would allow
    you to compare prices in different exchanges, and possibly work out an arbitrage
    strategy. Arbitrage is a simultaneous purchase and sale of an asset in different
    marketplaces to profit from an imbalance in the price. You could get data for
    traditional currency pairs, such as EUR/USD, or use Frameless to refactor the
    `Dataset` manipulations and make them more type-safe. See the website for further
    clarification [https://github.com/typelevel/frameless.](https://github.com/typelevel/frameless)
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该更熟悉使用Spark的`Dataset` API。我们的小程序专注于获取BCT/USD交易，但它可以增强。例如，您可以获取并保存其他货币对，如ETH/EUR或XRP/USD。使用不同的加密货币交易所。这将允许您比较不同交易所的价格，并可能制定套利策略。套利是在不同市场同时购买和出售资产以从价格不平衡中获利。您可以为传统货币对，如EUR/USD，获取数据，或使用Frameless重构`Dataset`操作，使其更类型安全。请访问网站以获取进一步说明
    [https://github.com/typelevel/frameless.](https://github.com/typelevel/frameless)
- en: In the next chapter, we are going to exploit saved transaction data to perform
    some analytics queries.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将利用保存的交易数据来执行一些分析查询。
