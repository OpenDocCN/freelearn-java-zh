- en: Asynchronous Processing with the Messaging Pattern
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用消息模式进行异步处理
- en: In the last chapter, we discussed the Fan-out Pattern, which we can implement
    using different strategies. At the end of that section, we reviewed an implementation
    of the Fan-out Pattern, which used AWS's Simple Queuing Service (SQS) as a destination
    for an event trigger. Queuing systems such as SQS provide a level of safety and
    security because they're intended to be a mostly durable persistent store where
    data lives until some process has the chance to pull it out, perform some work,
    and delete the item. If a downstream worker processes a crash entirely and processing
    stops for some time, queues merely back up, drastically reducing the risk of data
    loss. If a worker process runs into some unrecoverable problem in the middle of
    processing, queue items will typically be left on the queue to be retried by another
    processor in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了Fan-out模式，我们可以使用不同的策略来实现它。在该部分的末尾，我们回顾了一个Fan-out模式的实现，它使用了AWS的简单队列服务（SQS）作为事件触发的目标。像SQS这样的队列系统提供了一定程度的安全性和安全性，因为它们旨在成为一个主要持久存储的地方，数据会一直保留，直到某个进程有机会将其取出，执行一些工作，并删除该条目。如果一个下游工作进程完全崩溃，并且处理停止了一段时间，队列只会备份，大大降低了数据丢失的风险。如果一个工作进程在处理过程中遇到一些无法恢复的问题，队列中的条目通常会留在队列上，以便将来由另一个处理器重试。
- en: In this chapter, we will cover using queues as messaging systems to glue together
    multiple serverless components. Readers can already be familiar with queuing systems
    such as RabbitMQ, ActiveMQ, or SQS. We will learn how to pass messages between
    serverless systems using queues to provide durable and fault-tolerant distributed
    systems for data-heavy applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用队列作为消息系统来连接多个无服务器组件。读者可能已经熟悉诸如RabbitMQ、ActiveMQ或SQS之类的队列系统。我们将学习如何使用队列在无服务器系统之间传递消息，以提供持久性和容错性强的分布式系统，适用于数据密集型应用程序。
- en: 'At the end of this chapter, you can expect to understand the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你可以期待理解以下主题：
- en: What queuing systems are available and make sense in a serverless architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在无服务器架构中可用的队列系统及其合理性
- en: Options for processing messages using serverless functions (polling and fan-out)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用无服务器函数处理消息的选项（轮询和扇出）
- en: Differences between queues and streaming systems and when to use one over the
    other
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 队列和流系统之间的区别以及何时使用其中一个而不是另一个
- en: Dead letter queues to ensure messages are never dropped
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死信队列以确保消息永远不会丢失
- en: Using queues as a way of rate limiting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用队列作为速率限制的方法
- en: Basics of queuing systems
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 队列系统的基本原理
- en: Queuing systems are by no means new in the world of software. Generally speaking,
    queues are one of the fundamental data structures most introductory computer science courses
    cover. Before going any further, let's briefly review the queue as a fundamental
    data structure in computer science.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件领域，队列系统绝非新鲜事物。一般来说，队列是大多数计算机科学入门课程中涵盖的最基本的数据结构之一。在继续之前，让我们简要回顾一下队列作为计算机科学中基本数据结构的概念。
- en: Simply put, a queue is a collection of items where new items are pushed onto
    the back and pulled off the front. Consider that we're all waiting in line for
    a movie. Provided people follow the rules and don't line up out of order, you've
    waited in a queue (which is, of course, the reason British English uses *queue,* which
    is more accurate than the U.S. term *line*). Formally, we can define a queue as
    a collection of items that have the property of first-in-first-out (FIFO). The
    primary operators of a queue data type are `enqueue` and `dequeue`. These operators
    add new items to the back of the queue and pop items off the front, respectively.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，队列是一系列项目的集合，新项目被推到后面，而项目从前面被拉出。考虑一下我们都在排队等待看电影。只要人们遵守规则，不按顺序排队，你就已经排队了（这当然也是为什么英国英语使用*queue*，这个词比美国术语*line*更准确）。正式来说，我们可以将队列定义为具有先入先出（FIFO）属性的项目集合。队列数据类型的主要操作是`enqueue`和`dequeue`。这些操作分别将新项目添加到队列的后面，并从前面弹出项目。
- en: 'In software, queueing systems such as RabbitMQ and the like are commonly used
    to deal with asynchronous processing. A triggered event can mean that your system
    needs to perform some recalculations or data processing that doesn''t need to
    occur in real time. Rather than having a user sit and wait until they click a
    button, an application will place metadata into a queue that contains enough information
    for a downstream worker process to do its job. These worker processes'' sole responsibility
    is to sit and wait until a new item arrives and then carry out some computation.
    As messages show up in the queue, workers pluck off those messages, do their work,
    and return for more. This architecture has multiple benefits:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件中，如RabbitMQ之类的队列系统通常用于处理异步处理。触发的事件可能意味着您的系统需要执行一些不需要实时发生的重新计算或数据处理。而不是让用户坐着等待直到他们点击按钮，应用程序会将元数据放入一个包含足够信息的队列中，以便下游的工作进程执行其任务。这些工作进程的唯一责任是坐着等待直到新的项目到来，然后执行一些计算。随着消息出现在队列中，工作进程会摘取这些消息，完成工作，然后返回等待更多消息。这种架构具有多个优点：
- en: '**Durability**: Provided clients write data to the queue successfully and the
    queuing system is healthy, messages will persist in the queue until there is enough
    computing power available to pull them off, process, and finally remove them.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久性**：只要客户端成功将数据写入队列，并且队列系统运行正常，消息就会在队列中持续存在，直到有足够的计算能力将它们取出、处理并最终删除。'
- en: '**Scalability**: Most queuing architectures are parallelizable, meaning multiple
    workers can pull messages from a queue and process individual items in parallel.
    To operate with more throughput, we can add more workers into the system, which
    results in faster processing through greater parallelism.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：大多数队列架构都是可并行的，这意味着多个工作进程可以从队列中提取消息并并行处理单个项目。为了提高吞吐量，我们可以向系统中添加更多的工作进程，这将通过更大的并行性实现更快的处理速度。'
- en: '**Predictable load:** Often, worker processes will need to read and/or write
    data from/to a database. When the load is exceptionally high, the queue can serve
    as a buffer between the processing tasks and database. To limit pressure on a
    database we can scale the number of worker processes such that the parallelism
    is as high as possible, but not so much we overwhelm the database with an inordinate
    amount of reads or writes.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可预测的负载**：通常，工作进程需要从数据库中读取和/或写入数据。当负载异常高时，队列可以作为处理任务和数据库之间的缓冲。为了减轻对数据库的压力，我们可以扩展工作进程的数量，以便尽可能提高并行性，但不要过多，以免因过多的读取或写入而使数据库不堪重负。'
- en: Readers should note that they can implement a Messaging Pattern with either
    a queue or streaming system. In this chapter, we focus on queues but later discuss
    the merits of stream systems and the differences between the two types of message
    broker. In subsequent chapters, we will work through the details of streaming
    systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应注意，他们可以使用队列或流系统来实现消息模式。在本章中，我们专注于队列，但稍后讨论流系统的优点以及两种类型消息代理之间的差异。在后续章节中，我们将探讨流系统的细节。
- en: One of the most dangerous spots to be in is when all the incredible benefits
    of a queue are in place, only to have the actual queue server (RabbitMQ and so
    on) running as a single node. I have worked at multiple companies that have relied
    quite heavily on RabbitMQ as the queueing backbone, running very high business-critical
    workloads from it. However, we ran these RabbitMQ deployments as a single EC2
    instance with lots of computing capacity. Inevitably, when an individual instance
    runs into problems or for some reason dies, the entire system falls apart, resulting
    in lost messages, failing clients who attempt to write the queue and error out,
    and an all-around bad day.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最危险的情况之一是，当队列的所有令人难以置信的优点都得到实现时，实际的队列服务器（如RabbitMQ等）却作为一个单节点运行。我在多家公司工作过，这些公司非常依赖RabbitMQ作为队列骨干，从它那里运行着非常高的业务关键型工作负载。然而，我们将这些RabbitMQ部署作为一个具有大量计算能力的单个EC2实例运行。不可避免的是，当单个实例遇到问题或由于某种原因死亡时，整个系统就会崩溃，导致消息丢失、尝试写入队列并失败的客户端，以及一个全面糟糕的一天。
- en: Choosing a queue service
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择队列服务
- en: 'The good news today is that multiple cloud providers now offer queuing systems
    as a service. In the following examples, we''ll use SQS from AWS. While I haven''t
    worked with them directly, Google Compute Cloud has Task Queue, and Azure has
    Queue Storage. Undoubtedly, other cloud providers offer similar services. When
    evaluating hosted queuing services, there are several factors to consider:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，现在多个云提供商现在提供队列系统作为服务。在以下示例中，我们将使用AWS的SQS。虽然我并没有直接与他们合作，但Google Compute
    Cloud有任务队列，Azure有队列存储。毫无疑问，其他云提供商也提供类似的服务。在评估托管队列服务时，有几个因素需要考虑：
- en: What type of data fetching model is supported, pull or push?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持哪种类型的数据获取模型，拉取还是推送？
- en: What is the maximum lifetime of messages or the maximum queue depth?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息的最大生命周期或队列的最大深度是多少？
- en: What happens with messages that consistently fail? Is there a dead-letter queue
    option?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于持续失败的消息会发生什么？是否有死信队列选项？
- en: Is there a guarantee of exactly-once delivery, or can messages be delivered
    multiple times?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有保证一次且仅一次的交付，或者消息可以被多次交付？
- en: Is ordering guaranteed, or can messages arrive out of order relative to the
    order from which they were sent?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否保证顺序，或者消息的到达顺序可以相对于发送顺序出现乱序？
- en: 'Answers to these questions will vary by provider and by service offering for
    a given cloud provider. SQS, for example, comes in two flavors: Standard Queues
    and FIFO Queues. Which one you''ll pick when building on top of AWS will come
    down to your particular use case. If building with a different cloud provider,
    you''ll need to dig into their documentation to fully understand the behavior
    and semantics of whatever queueing service you''re using.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的答案将因提供商而异，也因特定云提供商的服务提供而异。例如，SQS有两大类：标准队列和FIFO队列。当你基于AWS构建时，你选择哪一种将取决于你的特定用例。如果你使用不同的云提供商，你需要深入研究他们的文档，以完全理解你所使用的队列服务的特性和语义。
- en: Queues versus streams
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 队列与流
- en: You can think of a queue as a broker of messages. Some data producer will place
    messages onto the queue, and some data consumer will read those messages. The
    queue simply brokers that exchange of message passing. Stream processing systems
    provide similar functionality, but with much different behavior, features, and
    applications. I'll present a brief discussion of the differences between queues
    and streams for the sake of clarity.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将队列视为消息的经纪人。某些数据生产者会将消息放置到队列中，而某些数据消费者会读取这些消息。队列只是简单地作为消息传递交换的经纪人。流处理系统提供了类似的功能，但行为、特性和应用却大不相同。为了清晰起见，我将简要讨论队列和流之间的差异。
- en: 'Apache Kafka is a very popular stream processing system in widespread use,
    of which you can have heard. Today, cloud providers have come out with hosted
    stream processing systems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka是一个非常流行的流处理系统，广泛使用，你可能已经听说过。今天，云提供商已经推出了托管流处理系统：
- en: '**Azure**: Event Hubs'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure**: Event Hubs'
- en: '**AWS**: Kinesis'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS**: Kinesis'
- en: '**Google Compute Cloud**: Cloud Dataflow'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Compute Cloud**: Cloud Dataflow'
- en: So, what exactly is a stream processing system as opposed to a queueing system?
    To my mind, the most significant and most easily understood difference is the
    way in which items are processed or delivered. In a queuing system, messages that
    arrive in the queue are typically processed once, by a single process. There are
    of course exceptions to this rule, but in all systems I've worked on that use
    a queue, the *happy path* was designed such that a single message would be read
    and processed once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，与队列系统相比，流处理系统究竟是什么呢？在我看来，最显著且最容易理解的区别在于项目处理或交付的方式。在队列系统中，进入队列的消息通常由单个进程一次性处理。当然，这个规则有例外，但在我所工作的所有使用队列的系统里，*正常路径*的设计是这样的：单个消息会被读取并处理一次。
- en: 'Streaming systems, on the other hand, can be thought of as a collection of
    records where old records eventually expire off the back (the oldest expiring
    first), and new records are added to the front. Rather than being processed and
    removed, messages sit there, without knowledge of who is reading them and without
    being deleted by consumers. Data consumers are responsible for keeping track of
    their position within the stream using an offset value. The streaming service
    itself is responsible for retaining the messages, generally with some configurable
    expiration period:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，流系统可以被视为记录的集合，其中旧记录最终从后面（最旧的先过期）过期，而新记录被添加到前面。消息不是被处理和删除，而是坐在那里，不知道谁在读取它们，也不会被消费者删除。数据消费者负责使用偏移量值跟踪其在流中的位置。流服务本身负责保留消息，通常具有一些可配置的过期期限：
- en: '![](img/fc1b1e00-3624-4611-8c52-b6dac6bd25bb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fc1b1e00-3624-4611-8c52-b6dac6bd25bb.png)'
- en: It is possible to have multiple consumers reading and processing the same message
    when using a streaming system. For example, one consumer can be reading items
    on the stream and calculating a running average of some metric, while another
    consumer can be reading the same messages and calculating the standard deviation.
    In a queuing system, this wouldn't be possible without duplicating messages across
    different queues or implementing some logic or heuristic to remove messages only
    after all consumers have done their job. Either way, a queue would not be a good
    fit for such a problem whereas streaming systems are purpose-built just for this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用流系统时，可能存在多个消费者读取和处理相同消息的情况。例如，一个消费者可以读取流上的项目并计算某些指标的运行平均值，而另一个消费者可以读取相同的消息并计算标准差。在队列系统中，如果没有在不同队列中复制消息或实现一些逻辑或启发式方法来仅在所有消费者完成其工作后删除消息，这是不可能的。无论如何，队列对于此类问题来说都不是一个好的选择，而流系统正是为此而专门设计的。
- en: Another exciting feature of streaming systems is that new consumers who come
    online can start at the back of the stream and work forward. For example, if a
    stream holds one week's worth of data, any new system that starts will be able
    to go back seven days and begin its processing from there. Because consumers keep
    track of their location or offset within the stream, they can pick up where they
    left off in the case of failure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 流系统的另一个令人兴奋的特性是，新上线的新消费者可以从流的末尾开始，并向前工作。例如，如果一个流包含一周的数据，任何新启动的系统都能回溯七天，并从那里开始处理。因为消费者跟踪其在流中的位置或偏移量，所以在失败的情况下，他们可以从中断的地方继续。
- en: Technically speaking, you can implement the Messaging Pattern with a queue or
    a stream, and the choice depends on the problem at hand. We'll look at AWS Kinesis
    and discuss streaming systems in later chapters. For now, we'll focus on using
    queues and specifically SQS for the example application. In my mind, a Message
    Pattern at its core entails separating the communication between different system
    via some message broker, such as a queue or streaming system.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，你可以使用队列或流来实现消息模式，具体选择取决于手头的问题。我们将在后续章节中探讨 AWS Kinesis 和流系统。现在，我们将专注于使用队列，特别是
    SQS 作为示例应用程序。在我看来，消息模式的核心在于通过某种消息代理（如队列或流系统）将不同系统之间的通信分离。
- en: Asynchronous processing of Twitter streams
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter 流的异步处理
- en: 'Twitter is an excellent source of random data. Given the volume and variety
    of data, we can readily come up with example (and real) problems to solve. In
    our case, we''re going to build a serverless processing system by sipping off
    the public twitter stream. Our example system will  have the following workflow:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 是一个随机数据的绝佳来源。鉴于数据的量和种类，我们可以轻松地想出（以及解决）示例（和真实）问题。在我们的案例中，我们将通过提取公共 Twitter
    流来构建一个无服务器处理系统。我们的示例系统将具有以下工作流程：
- en: Read a tweet with cat or dog images from the Twitter firehose
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Twitter 火炬中读取包含猫或狗图像的推文
- en: Place messages on an SQS queue.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将消息放置在 SQS 队列中。
- en: Worker processes will read those image URLs off the queue and perform image
    recognition.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作进程将从队列中读取这些图像 URL 并执行图像识别。
- en: While this example can be a bit contrived, the concepts demonstrated are true
    to life. We'll use the AWS Rekognition service to perform image recognition and
    labeling of any cat or dog images we find. Rekognition is quite fast at what it
    does, but it's easy to imagine processing images with a much slower service. In
    that case, adding items onto a queue and processing them at our leisure with one
    or more worker processes would allow us to scale out to achieve a higher processing
    rate.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子可能有点牵强，但演示的概念是真实的。我们将使用 AWS Rekognition 服务来对找到的任何猫或狗图像进行识别和标记。Rekognition
    在其工作上非常快，但很容易想象使用速度慢得多的服务处理图像。在这种情况下，将项目添加到队列中，并在我们的空闲时间通过一个或多个工作进程进行处理，这将使我们能够扩展以实现更高的处理速率。
- en: You can find all code in this chapter at [https://github.com/brianz/serverless-design-patterns/tree/master/ch6](https://github.com/brianz/serverless-design-patterns/tree/master/ch6).[](https://github.com/brianz/serverless-design-patterns/tree/master/ch6)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个章节中找到所有代码：[https://github.com/brianz/serverless-design-patterns/tree/master/ch6](https://github.com/brianz/serverless-design-patterns/tree/master/ch6)。
- en: System architecture
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统架构
- en: 'The system architecture for our image analysis example is quite simple. Ingestion
    of tweets will begin using a stream listener for the Twitter API via the Python
    `tweepy` library, [https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy).
    This listener will filter out only specific tweets on our behalf. From there,
    the listener will place messages onto an SQS queue. Once it delivers messages
    to the queue, the job of our stream listener is complete. With this type of design,
    we realize a real separation of concerns. If we enumerated the things our stream
    listener cares about, the list would be quite short:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们图像分析示例的系统架构非常简单。推文的摄取将通过 Twitter API 的流监听器开始，使用 Python 的 `tweepy` 库，[https://github.com/tweepy/tweepy](https://github.com/tweepy/tweepy)。这个监听器将只代表我们过滤出特定的推文。从那里，监听器将消息放入
    SQS 队列。一旦将消息交付到队列，我们的流监听器的任务就完成了。通过这种设计，我们实现了真正的关注点分离。如果我们列出我们的流监听器关心的事情，列表会相当短：
- en: Twitter access
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter 访问
- en: Some business logic as to what types of data to extract from tweets
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些业务逻辑，关于从推文中提取哪些类型的数据
- en: Which queue to place the extracted tweet information in
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提取的推文信息放在哪个队列中
- en: That is it. Once the stream listener performs the last step of adding items
    onto the queue, it neither cares about nor is affected by, any downstream processing
    or lack thereof.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。一旦流监听器完成了将项目添加到队列中的最后一步，它就不再关心也不受任何下游处理或缺乏处理的影响。
- en: From there, worker processes will pull images off the queue and perform their
    bit of work, ultimately calling AWS Rekognition and storing the results in DynamoDB
    for future review. Our example will use a classifier processor, which will run
    with a single level of parallelism. That is, at any given time there will only
    be a single classifier process running. However, there would be very few changes
    if we wished to scale this out and operate multiple classifiers at the same time,
    increasing our parallelism and hence the overall throughput of the system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，工作进程将从队列中提取图像并执行它们的工作，最终调用 AWS Rekognition 并将结果存储在 DynamoDB 中以供未来审查。我们的示例将使用一个分类器处理器，它将以单级并行性运行。也就是说，在任何给定时间，只有一个分类器进程正在运行。然而，如果我们希望扩展并同时运行多个分类器，这将增加我们的并行性和系统的整体吞吐量，这只会带来很少的变化。
- en: 'Again, with this design, the classifier''s job is much simpler than if we implemented
    all of this work as a single process. Classifiers also care about a small number
    of items to perform their work:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，与将所有这些工作作为一个单一进程实现相比，这个设计中的分类器的工作要简单得多。分类器也只关心少量项目以执行其工作：
- en: Which queue to get data from
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从哪个队列获取数据
- en: A small bit of business logic to perform the image classification
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一点业务逻辑来执行图像分类
- en: Where to put the results
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果放置的位置
- en: Our classifier neither knows nor cares how data arrived in the queue. All that
    matters from the classifier's perspective is that data comes with the correct
    format (which is quite simple) and that is has access to the resources it needs
    to perform its job.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类器既不知道也不关心数据是如何进入队列的。从分类器的角度来看，唯一重要的是数据以正确的格式（相当简单）到来，并且它有权访问执行其工作所需的资源。
- en: 'With a queue acting as the broker between data producer (stream listener) and
    data consumer (classifier), we have a reasonably good level of durability. If
    our listener dies (which it will by design, as you''ll see shortly) or if our
    classifiers die, SQS will hold onto our data, ensuring we can get to it and process
    it when our systems are back to full health. Also, we''d be able to scale this
    up as needed, adding more classifiers in the event that the stream listener produced
    more messages than a single classifier could keep up:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用队列作为数据生产者（流监听器）和数据消费者（分类器）之间的经纪人，我们有一个相当好的持久性水平。如果我们的监听器死亡（按照设计，你很快就会看到）或者我们的分类器死亡，SQS将保留我们的数据，确保我们可以在系统恢复健康时访问并处理它们。此外，我们还可以根据需要扩展它，在流监听器产生的消息多于单个分类器可以处理的情况下，添加更多的分类器：
- en: '![](img/e7a8be1d-12e9-4613-b019-4c371afb0eb2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e7a8be1d-12e9-4613-b019-4c371afb0eb2.png)'
- en: Messages are placed onto an SQS queue by a stream listener process, which is
    implemented as an AWS Lambda function and runs on a schedule. A single classifier
    Lambda function also runs on a schedule and is responsible for pulling messages
    from the queue, classifying them with AWS Rekognition and finally storing results
    in DynamoDB.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 消息由一个流监听器进程放入SQS队列中，该进程实现为一个AWS Lambda函数，并按计划运行。单个分类器Lambda函数也按计划运行，负责从队列中提取消息，使用AWS
    Rekognition进行分类，并将结果最终存储在DynamoDB中。
- en: Furthermore, this design allows for a level of flexibility that would be difficult
    otherwise. Our example is processing tweets tagged with `#cat` or `#dog` and a
    few other related hashtags. We could also modify our stream processor to grab
    a more extensive set of tweets, perhaps directed at `@realDonaldTrump`. Those
    tweets could be directed to an entirely different queue, which would be processed
    separated and completely different. Since the volume of `@realDonalTrump` tweets
    is much higher than `#cat` and `#dog` tweets, separating them out and handling
    them differently would be an excellent idea from a systems architecture perspective.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种设计提供了一种在其他情况下难以实现的可扩展性。我们的例子是处理带有`#cat`或`#dog`以及一些其他相关标签的推文。我们也可以修改我们的流处理器以获取更广泛的推文集，例如针对`@realDonaldTrump`的推文。这些推文可以发送到完全不同的队列中，该队列将单独处理并完全不同。由于`@realDonalTrump`的推文量远高于`#cat`和`#dog`推文，从系统架构的角度来看，将它们分开并分别处理是一个非常好的主意。
- en: Data producer
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据生产者
- en: 'Most of the complexities in this code revolve around the Twitter API, which
    I won''t go into in detail. I haven''t worked with the Twitter API much myself,
    but the tweepy GitHub page and website have plenty of resources and example code
    to get you started, which is just what I followed to get this working. The following
    code is the entry point to the entire process, which begins reading the public
    Twitter stream for tweets related to cats or dogs and placing a subset of each
    tweet onto the SQS queue:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中的大多数复杂性都围绕着Twitter API，我不会详细说明。我自己并没有太多使用Twitter API的经验，但tweepy的GitHub页面和网站上有很多资源和示例代码，可以帮助你开始，这正是我遵循的方法来使它工作。以下代码是整个过程的入口点，它开始读取与猫或狗相关的公共Twitter流，并将每条推文的子集放入SQS队列中：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let's begin by looking at the `start` function, which does what you'd expect.
    Once the listener class is instantiated and begins running, it will operate as
    a long-lived daemon process invoking the `on_status` function whenever it encounters
    a tweet. Since we are only interested in certain types of message, I'll pass a
    list of tags to the `filter` function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看`start`函数开始，它做你预期的事情。一旦监听器类被实例化并开始运行，它将作为一个长期运行的守护进程，每当遇到推文时就会调用`on_status`函数。由于我们只对某些类型的消息感兴趣，我将一个标签列表传递给`filter`函数。
- en: 'All of our application logic is wrapped up in the `on_status` method. Tweets
    are a reasonably elaborate data structure, and the actual image URLs we''re interested
    in can live in multiple locations. As a Twitter API novice, I''m not entirely
    sure of the exact logic to look for image URLs, but the little bit of logic in
    `on_status` seems to get enough images for our example. After grabbing as many
    image URLs as we can along with some extracted hashtags, we will publish that
    data structure to our SQS queue using our `publish_tweet` wrapper function. Details
    on `publish_tweet` can be found in the following queue-specific code block. It''s
    not complex at all, and the only really important bit is to understand what exactly
    ends up on the queue. In this case, we''re placing a Python dictionary onto SQS,
    which ultimately gets serialized as a JSON record. This record contains the original
    tweet text, a URL for the cat or dog image, and any hashtags embedded in the tweet:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的所有应用程序逻辑都封装在`on_status`方法中。推文是一个相当复杂的数据结构，我们感兴趣的图像URL可能存在于多个位置。作为一个Twitter
    API的新手，我并不完全确定查找图像URL的确切逻辑，但`on_status`中的少量逻辑似乎足以为我们示例获取足够的图像。在尽可能多地获取图像URL和一些提取的标签后，我们将使用我们的`publish_tweet`包装函数将这个数据结构发布到我们的SQS队列中。有关`publish_tweet`的详细信息可以在以下队列特定的代码块中找到。它并不复杂，真正重要的是要理解最终会出现在队列上的内容。在这种情况下，我们将一个Python字典放置在SQS上，它最终被序列化为JSON记录。这个记录包含原始推文文本、猫或狗图像的URL以及推文中嵌入的任何标签：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Mimicking daemon processes with serverless functions
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用无服务器函数模拟守护进程进程
- en: By definition, serverless functions are short-lived and have a maximum lifetime
    before your platform of choice kills them. At the time of writing, the current
    limitation for an AWS Lambda function is 300 seconds (5 minutes) and the default
    6 seconds. Our example relies on a long-lived process that is continually reading
    from the Twitter stream and publishing those results to the queue. How then can
    we accomplish this long-lived behavior with an inherently short-lived system?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，无服务器函数是短暂的，并且在你选择的平台将其终止之前有一个最大生命周期。在撰写本文时，AWS Lambda函数的当前限制是300秒（5分钟）和默认的6秒。我们的示例依赖于一个持续读取Twitter流并将结果发布到队列中的长期运行的过程。那么，我们如何使用本质上短暂的系统来实现这种长期运行的行为呢？
- en: To mimic a constantly-running process, we can take advantage of the scheduled
    invocation of Lambda functions. Other cloud providers should provide similar functionality.
    In short, we can use the maximum lifetime of Lambda functions to our advantage.
    The trick here is to set the `timeout` value of our Lambda function to 58 seconds,
    which is just below the scheduled invocation rate of 60 seconds.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟一个持续运行的过程，我们可以利用Lambda函数的定时调用。其他云提供商应该提供类似的功能。简而言之，我们可以利用Lambda函数的最大生命周期来达到我们的目的。这里的技巧是将我们的Lambda函数的`timeout`值设置为58秒，这正好低于60秒的预定调用速率。
- en: Since that code will run indefinitely, we can rely on AWS killing the Lambda
    function after 58 seconds. After a running `Firehose` Lambda function is killed,
    we know another one will start up within a second or two, which results in a continually
    running `Firehose` process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该代码将无限期运行，我们可以依赖AWS在58秒后终止Lambda函数。一个正在运行的`Firehose` Lambda函数被终止后，我们知道另一个将在一秒或两秒内启动，这导致`Firehose`进程持续运行。
- en: 'There is a chance that there will be one or two seconds when two instances
    of firehose processing run concurrently. In this case, that''s not a concern since
    the data consumer can handle duplicate data elegantly by merely ignoring duplicates.
    If you plan on using the same pattern, it''s essential to ensure your data consumer
    can deal with duplicates and is idempotent with its computation and processing.
    This pattern may not be applicable for all problems, but it works well for this
    and similar systems:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能出现一秒钟或两秒钟内同时运行两个firehose处理实例的情况。在这种情况下，这并不是一个问题，因为数据消费者可以通过简单地忽略重复数据来优雅地处理重复数据。如果你打算使用相同的模式，确保你的数据消费者能够处理重复数据，并且其计算和处理是幂等的，这是至关重要的。这种模式可能并不适用于所有问题，但对于这种和类似系统来说效果很好：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Data consumers
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据消费者
- en: If you've worked with RabbitMQ or other queuing systems, you can be used to
    registering workers on specific queues or topics where those listeners/workers
    get notified when messages of interest arrive. With SQS, the model is entirely
    different. SQS is a purely poll-based system; that is, any code that is interested
    in reading data from a queue needs to poll using the appropriate AWS APIs. Additionally,
    application code must explicitly delete messages from the queue once it has completed
    its processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过RabbitMQ或其他队列系统，你可能习惯于在特定的队列或主题上注册工作进程，当感兴趣的消息到达时，这些监听器/工作进程会收到通知。与SQS不同，SQS是一个纯轮询系统；也就是说，任何想要从队列中读取数据的代码都需要使用适当的AWS
    API进行轮询。此外，应用程序代码必须明确地从队列中删除消息，一旦它完成了处理。
- en: Some APIs for other queuing systems will automatically `ack` a message provided
    no exception occurs, resulting in the removal of those message from the queue.
    It's imperative to remember to delete messages from an SQS queue even if no processing
    should occur.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他队列系统的API在没有异常发生的情况下会自动`ack`一条消息，从而导致这些消息从队列中移除。务必记住，即使没有处理，也要从SQS队列中删除消息。
- en: 'Just as the `Firehose` function executes on a one-minute interval, so too will
    our `Classify` process. When this function runs, it starts by pulling batches
    of records from the SQS queue in quantities of 10\. You can see in the following
    that there is an infinite loop with the `while True:` statement. Again, once this
    loop starts, it will run until the Lambda itself terminates it according to our
    58-second timeout. If there aren''t any messages available for processing, everything
    just shuts down. This technique is more straightforward to implement and less
    expensive than dealing with the process sleeping. By merely quitting we can rely
    on the next run to pick up the next batch of work and don''t need to waste CPU
    cycles doing anything but wait for messages to arrive:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如`Firehose`函数在每分钟执行一次一样，我们的`Classify`过程也是如此。当这个函数运行时，它首先从SQS队列中拉取10条记录的批次。你可以在下面的代码中看到有一个使用`while
    True:`语句的无穷循环。同样，一旦这个循环开始，它将一直运行，直到Lambda根据我们的58秒超时自行终止。如果没有可处理的消息，一切都会关闭。这种方法比处理进程休眠更简单直接，成本也更低。仅仅退出，我们就可以依赖下一次运行来获取下一批工作，而无需浪费CPU周期去做除了等待消息到来之外的事情：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After the `classify_photos` function finds some messages, processing them isn't
    very complicated. With tweets, there is a good chance our classifier will encounter
    duplicate photos. This job will store results in DynamoDB, so the first step is
    to check whether that URL already exists. Our DynamoDB table will use the URL
    as the partition key, which is analogous to a simple primary key in a relational
    database. This DynamoDB schema means that the URL must be unique. If we've already
    stored a particular URL, we won't do any more processing. Still, we need to remember
    to delete the message from the queue. Without that step, worker processes would
    repeatedly process a queue item, resulting in a never empty queue.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`classify_photos`函数找到一些消息后，处理它们并不复杂。对于推文，我们的分类器很可能遇到重复的照片。这项工作将结果存储在DynamoDB中，所以第一步是检查该URL是否已经存在。我们的DynamoDB表将使用URL作为分区键，这类似于关系数据库中的简单主键。这个DynamoDB模式意味着URL必须是唯一的。如果我们已经存储了特定的URL，我们不会进行任何更多的处理。然而，我们还需要记住从队列中删除消息。如果没有这一步，工作进程会反复处理队列项，导致队列永远不会为空。
- en: 'For any new URLs, we''ll download the image and throw it over to the AWS Rekognition
    service to get a listing of labels and associated scores. If you''re unfamiliar
    with Rekognition, it''s quite a fantastic service. Rekognition provides several
    impressive features such as facial recognition. We''ll be using the image detection
    or *labeling* feature, which will detect objects in a given image with a corresponding
    score:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何新的URL，我们将下载图片并将其发送到AWS的Rekognition服务以获取标签和关联分数的列表。如果你不熟悉Rekognition，它是一个非常出色的服务。Rekognition提供了许多令人印象深刻的功能，例如面部识别。我们将使用图像检测或*标签*功能，该功能将检测给定图像中的对象及其相应的分数：
- en: '![](img/f7568edd-f0b9-4119-bd20-50e2415432f0.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7568edd-f0b9-4119-bd20-50e2415432f0.jpg)'
- en: 'As an example, this image of a cat results in the following `Labels` from Rekognition:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这张猫的图片会导致Rekognition产生以下`Labels`：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, our worker process will fetch pictures embedded in tweets and hand them
    off to Rekognition for labeling. Once Rekognition finishes its work, the worker
    process will store the scores and other data about the image and tweet in DynamoDB.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的工作进程将检索推文中嵌入的图片，并将它们交给Rekognition进行标记。一旦Rekognition完成工作，工作进程将把图像和推文的分数以及其他数据存储在DynamoDB中。
- en: Viewing results
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看结果
- en: 'In this example, we don''t have a custom user interface to view results, so
    the DynamoDB console will have to do. As we can see in the following screenshot,
    I''m storing image URLs along with any embedded hashtags from the original tweet
    as well as the detected labels and scores from the Rekognition query:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们没有自定义的用户界面来查看结果，所以必须使用DynamoDB控制台。正如我们可以在下面的屏幕截图中所见，我存储了图像URL以及原始推文中嵌入的任何标签，以及Rekognition查询中检测到的标签和分数：
- en: '![](img/b8aa2feb-fdf8-453a-a307-0b7ca37b3a4f.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8aa2feb-fdf8-453a-a307-0b7ca37b3a4f.png)'
- en: 'Using the DynamoDB API, let''s take a look at one of the records in detail
    using Python''s `boto3` library from Amazon:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DynamoDB API，让我们使用亚马逊的`boto3`库来详细查看一条记录，以下是一个例子：
- en: '[PRE5]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With that, we have a reasonably sophisticated system with very few lines of
    application code. Most importantly and keeping this in context, every single system
    we've leveraged is entirely managed. Amazon will do the hard work of maintaining
    and scaling Lambda, SQS, and DynamoDB on our behalf. There are some tricks and
    essential details about managing DynamoDB read and write capacity and I encourage
    you to read up on that on your own.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们有一个相当复杂的系统，但应用程序代码行数却很少。最重要的是，在这个背景下，我们利用的每个系统都是完全管理的。亚马逊将代表我们维护和扩展Lambda、SQS和DynamoDB。关于管理DynamoDB的读写容量有一些技巧和关键细节，我鼓励你自己去了解这些。
- en: Alternate Implementations
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代实现
- en: Our example application is quite robust and can handle quite a bit of load and
    traffic with few to no changes. As easy as this pattern is to understand, implement,
    and run, it's not a silver bullet. You will likely require different implementations
    of this Messaging Pattern in your scenarios. We'll review a few alternative applications
    of the same pattern, which uses a queue as a message broker between disparate
    systems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例应用程序非常健壮，可以在几乎不需要更改的情况下处理相当大的负载和流量。尽管这种模式易于理解、实施和运行，但它并非万能药。你可能会在自己的场景中需要不同实现的消息模式。我们将回顾一些相同模式的替代应用，该模式使用队列作为不同系统之间的消息代理。
- en: Using the Fan-out and Messaging Patterns together
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合使用Fan-out和消息模式
- en: 'Earlier, during the explanation of our system architecture, I briefly discussed
    the possibility of fanning out messages from the stream listener to multiple queues.
    A design such as this would be useful when there are different types of workload
    to be performed from a single data producer. The following example architecture
    shows a system made up of an individual Twitter stream data producer that fans
    out messages to multiple queues based on the payload:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前解释我们的系统架构时，我简要讨论了从流监听器向多个队列扇出消息的可能性。当有不同类型的工作负载需要从单个数据生产者执行时，这种设计非常有用。以下示例架构展示了一个由单个Twitter流数据生产者组成的系统，该生产者根据有效负载将消息扇出到多个队列：
- en: '![](img/ab38d077-cdc4-4d47-8a72-3807825b645e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ab38d077-cdc4-4d47-8a72-3807825b645e.png)'
- en: For this example, assume we're interested in processing a more extensive range
    of tweets. This system will still classify dog and cat images as before; however,
    this time we can split the processing apart more granularly by using separate
    queues for cat images and dog images. We can not be able to warrant this split
    in processing at the beginning, but it will allow us to treat and scale those
    systems separately.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，假设我们感兴趣的是处理更广泛的推文范围。这个系统仍然会像以前一样对狗和猫的图像进行分类；然而，这次我们可以通过使用单独的队列来更细致地分离处理，分别为猫图像和狗图像使用不同的队列。我们无法一开始就保证这种处理分离，但它将允许我们分别处理和扩展这些系统。
- en: A better example is the splitting of `@realDonaldTrump` tweets into an entirely
    different processing pipeline using a dedicated queue. The volume on this queue
    would be much higher than cat and dog images. Likely, we'd want to be running
    multiple concurrent workers to process this higher amount. Additionally, we could
    do something completely different from the labeling of images, such as running
    sentiment analysis on those tweets. Even in cases where the sentiment analysis
    was underprovisioned and got behind, we could feel confident knowing that any
    message on the queue could eventually be processed either by adding more worker
    processes or by an eventual slowdown of new messages from the data producer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的例子是将`@realDonaldTrump`推文分割到完全不同的处理管道中，使用专门的队列。这个队列上的流量将远高于猫和狗的图片。很可能我们希望运行多个并发工作者来处理这个更高的流量。此外，我们还可以做一些与图像标记完全不同的事情，比如对这些推文进行情感分析。即使在情感分析资源不足且落后于进度的情况下，我们也可以有信心知道队列上的任何消息最终都可以通过添加更多工作进程或数据生产者最终放缓新消息的发送来得到处理。
- en: Using a queue as a rate-limiter
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用队列作为速率限制器
- en: 'Many public APIs have rate limits. If you are attempting to pull down any substantial
    amount of data using a particular API that requires many API calls, you''ll undoubtedly
    need to work around those rate limits and find a way to get to your data as fast
    as possible without exceeding your request quota. In cases such as this, a queue
    architecture can help out:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公共API都有速率限制。如果你试图使用需要许多API调用的特定API来拉取大量数据，你无疑需要绕过这些速率限制，并找到一种方法以尽可能快的速度获取数据，同时不超过你的请求配额。在这种情况下，队列架构可以提供帮助：
- en: '![](img/26af2a8a-757f-4401-be7f-352c137c42c9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/26af2a8a-757f-4401-be7f-352c137c42c9.png)'
- en: I have personally implemented this exact pattern with great success. Here, a
    third-party API provides an endpoint to `/items`. The actual data being retrieved
    is of little importance to explain the details. Here, the challenge is that we
    can only fetch the required details of these items by making another API call
    to an `/items/${id}` endpoint. When there are hundreds or thousands of things
    to download, which each require a separate API call (or more), we need to be careful
    to stay below the rate limit threshold. Typically, we would prefer that the system
    also runs as quickly as possible, so the overall process of retrieving item details
    doesn't take days or weeks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人已经成功实现了这个模式。在这里，第三方API提供了一个到`/items`的端点。实际检索的数据对解释细节来说并不重要。在这里的挑战是，我们只能通过向`/items/${id}`端点进行另一个API调用来获取这些项目的所需详细信息。当有数百或数千个项目需要下载，每个项目都需要单独的API调用（或更多）时，我们需要小心保持在速率限制阈值以下。通常，我们更希望系统尽可能快地运行，这样检索项目细节的整体过程就不会花费数天或数周。
- en: 'With such constraints, we can use a queue and inspection of our consumption
    rate limit to download items as fast as possible while also staying within the
    bounds of our allotted rate limit. The trick here is to break up the work of producing
    objects for download from the act of downloading those objects. For the sake of
    this example, assume the `/items` endpoint retrieved up to 500 items at a time,
    where each element has a structure that includes a unique numeric `id` along with
    some metadata. Our goal of retrieving the entire view of each item requires another
    API call to `/item/${id}`. The data producer would make a single call to the `/items`
    endpoint and place messages onto the queue for each item that needs to be downloaded
    and stored. Each message on the queue would be somewhat generic, comprising a
    simple data structure such as `{''url'': ''https://some-domain.io/item/1234''}`.
    This process could go as rapidly as it needed, since fetching an entire list of
    objects could realistically be done quite quickly and probably under whatever
    rate limit is imposed.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在这样的限制下，我们可以使用队列和检查我们的消费速率限制，尽可能快地下载项目，同时保持在分配的速率限制范围内。这里的技巧是将生成下载对象的工作与下载这些对象的行为分开。为了这个示例，假设`/items`端点一次检索最多500个项目，其中每个元素都包含一个唯一的数字`id`和一些元数据。我们检索每个项目完整视图的目标需要另一个对`/item/${id}`的API调用。数据生产者会对`/items`端点进行单个调用，并为每个需要下载和存储的项目将消息放入队列。队列上的每个消息都会相对通用，包含如`{''url'':
    ''https://some-domain.io/item/1234''}`这样的简单数据结构。这个过程可以快速进行，因为获取整个对象列表实际上可以非常快地完成，并且可能低于任何施加的速率限制。'
- en: 'I put any intelligence about downloading item details and dealing with rate
    limiting into the downloader process. Just as with our cat and dog classifier,
    the downloader job is scheduled to wake up every minute and download as many messages
    as possible. Upon fetching the first item from the queue, the downloader will
    check the consumed rate limit which is provided by our third-party API via HTTP
    headers. There is no standard way of providing usage statistics to clients, but
    I''ve seen this type of data returned in the following header format: `X-Ratelimit-Usage:
    2142,3000`. In this example, the API enforces a limit of 3,000 requests per unit
    time while the client has currently consumed 2,142 requests. If you do the math,
    2,142 consumed units compared with a threshold of 3,000 equate to 71.4% usage.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我将有关下载项目详情和处理速率限制的任何智能功能都放入下载器进程中。就像我们的猫和狗分类器一样，下载器作业被安排每分钟唤醒一次，尽可能多地下载消息。在从队列中获取第一个项目后，下载器将检查由第三方API通过HTTP头提供的消耗速率限制。没有标准的方式来向客户端提供使用统计信息，但我看到这种类型的数据以下列头格式返回：`X-Ratelimit-Usage:
    2142,3000`。在这个例子中，API强制执行每单位时间3,000个请求的限制，而客户端目前消耗了2,142个请求。如果你做数学计算，2,142个消耗单位与3,000个阈值的比较等于71.4%的使用率。'
- en: After each API call, the downloader job checks its consumed API usage by doing
    this simple calculation. Once it nears some upper limit, the downloader can merely
    shut itself down and cease making API requests (perhaps when it gets above 90%
    usage). Of course, there must be a single API call made to inspect this usage.
    If the worker/downloader processes start up every two minutes, the worst-case
    scenario is that the system makes a single API call every two minutes. Only after
    some time has elapsed and the rate limits are reset (perhaps every 15 minutes)
    can the downloader start pulling items in bulk again. By using the same trick
    as our classifier example, it's trivial to have one or more downloader processing
    continually running by playing with the timeout value along with the scheduled
    invocation time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每次API调用后，下载器作业通过以下简单计算来检查其消耗的API使用情况。一旦接近某个上限，下载器可以简单地关闭自己并停止发出API请求（可能是在使用率达到90%以上时）。当然，必须有一个API调用来检查这种使用情况。如果工作器/下载器进程每两分钟启动一次，最坏的情况是系统每两分钟发出一个API调用。只有在经过一段时间并且速率限制被重置（可能每15分钟一次）之后，下载器才能再次开始批量拉取项目。通过使用与我们的分类器示例相同的技巧，通过调整超时值以及计划调用时间，可以轻松地让一个或多个下载器持续不断地运行。
- en: Using a dead-letter queue
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用死信队列
- en: Under certain circumstances, worker processes can never successfully process
    a message sitting in a queue. Take for example our image classifier problem. The
    worker processes aren't responsible for much other than downloading an image,
    sending it to Rekognition, and storing those results in DynamoDB. However, what
    happens if, in between a tweet arriving in the queue and our processing of that
    tweet, a Twitter user deletes the original image or tweet. In this case, our classifier
    process would fail hard. Look for yourself, and you'll see there are no guards
    against an HTTP 404 response code from the image fetch.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，工作进程可能永远无法成功处理队列中等待的消息。以我们的图像分类器问题为例。工作进程除了下载图像、将其发送到Rekognition以及将这些结果存储在DynamoDB中之外，不负责其他任何事情。然而，如果在一条推文到达队列并处理该推文之间，Twitter用户删除了原始图像或推文，会发生什么情况。在这种情况下，我们的分类器进程会严重失败。你自己看看，你会发现没有任何防护措施来防止从图像获取中返回HTTP
    404响应代码。
- en: A hard failure like this will result in the application code skipping the `sqs.delete_message`
    function altogether. After a configurable amount of time, that same message will
    be available on the queue to another worker who will encounter the same problem
    and fail in the same way. Without some protections in place, this cycle will repeat
    itself indefinitely.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种严重的失败会导致应用程序代码完全跳过`sqs.delete_message`函数。在可配置的时间后，相同的信息将再次出现在队列中，另一个工作器将遇到相同的问题并以相同的方式失败。如果没有一些保护措施，这种循环将无限期地重复。
- en: It would be quite trivial to work around this case in the application code,
    since dealing with any non-200 HTTP response codes is quite easy and missing a
    few images isn't a significant problem. In more complicated applications where
    the failure scenarios cannot be as easy to foresee, setting up some fallback mechanism
    can be very helpful for debugging and for making the entire system more reliable.
    Specific queuing systems, including SQS, offer what is called a dead-letter queue.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序代码中处理这种情况相当简单，因为处理任何非 200 HTTP 响应代码都很容易，缺少几张图片并不是一个重大问题。在更复杂的应用程序中，其中失败场景不容易预见，设置一些回退机制对于调试和使整个系统更可靠非常有帮助。特定的队列系统，包括
    SQS，提供了所谓的死信队列。
- en: A dead-letter queue is a separate queue where messages that cannot be successfully
    processed wind up. We can set up a dead-letter queue and configure our primary
    queue to place messages there if workers cannot successfully process messages
    after ten attempts. In that case, we guarantee the messages will eventually be
    removed from the primary queue either due to successful processing or by forceful
    removal due to 10 failures. A useful benefit of this is that we'll catch any problematic
    messages and can eventually inspect them and make changes to application code
    as needed. Since the dead-letter queue is a queue itself, we're still responsible
    for maintaining it and ensuring its health and size are kept in check.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 死信队列是一个独立的队列，无法成功处理的消息最终会进入这个队列。我们可以设置一个死信队列，并配置我们的主队列，如果工人在尝试了十次之后仍然无法成功处理消息，就将消息放置在那里。在这种情况下，我们保证消息最终会从主队列中移除，无论是由于成功处理还是由于强制移除（由于
    10 次失败）。这个好处是，我们可以捕获任何有问题的消息，并最终检查它们，根据需要修改应用程序代码。由于死信队列本身也是一个队列，我们仍然负责维护它，并确保其健康和大小得到控制。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the details of the Messaging Pattern and walked
    through a complete example using AWS SQS as a message broker. The example application
    comprised a Lambda function as the data producer, SQS as the message broker, and
    a Lambda function as the data consumer, which ultimately stored results in DynamoDB.
    We also discussed the difference between queues and streaming systems and reviewed
    their merits and use cases when one may be preferable over another. I also explained
    alternative architectures and implementations of the Messaging Pattern with specific
    problems and examples given for context.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了消息模式的细节，并通过使用 AWS SQS 作为消息代理的完整示例进行了演示。该示例应用程序包括一个 Lambda 函数作为数据生产者，SQS
    作为消息代理，以及一个 Lambda 函数作为数据消费者，最终将结果存储在 DynamoDB 中。我们还讨论了队列和流系统之间的区别，并回顾了当一种系统可能比另一种系统更可取时的优点和用例。我还解释了针对特定问题的消息模式的替代架构和实现，并给出了具体的例子以供参考。
- en: At this point, readers should have a good understanding of how to break apart
    data-heavy serverless applications using queuing systems to provide scalability,
    fault tolerance, and reliability. I presented alternative architectures, which
    should give readers some insight into how they can structure their applications
    for improved decoupling and performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，读者应该已经很好地理解了如何使用队列系统将数据密集型无服务器应用程序拆分，以提供可伸缩性、容错性和可靠性。我提出了替代架构，这应该让读者对如何为改进解耦和性能来结构化他们的应用程序有一些了解。
- en: In the following chapter, we'll review another data-processing type of pattern
    which is useful in big data systems, the Lambda Pattern.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾另一种在大型数据系统中有用的数据处理模式，即 Lambda 模式。
